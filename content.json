{"meta":{"title":"代码推演录","subtitle":"分享运维开发相关技术，另外好想下班","description":"运维开发相关技术","author":"Aquarius","url":"https://AquaPluto.github.io","root":"/"},"pages":[{"title":"categories","date":"2025-08-15T08:20:20.000Z","updated":"2025-08-28T12:33:05.419Z","comments":true,"path":"categories/index.html","permalink":"https://aquapluto.github.io/categories/","excerpt":"","text":""},{"title":"home","date":"2025-08-15T08:19:44.000Z","updated":"2025-08-28T12:33:05.420Z","comments":true,"path":"home/index.html","permalink":"https://aquapluto.github.io/home/","excerpt":"","text":""},{"title":"","date":"2025-08-28T12:33:05.420Z","updated":"2025-08-28T12:33:05.420Z","comments":true,"path":"css/wave.css","permalink":"https://aquapluto.github.io/css/wave.css","excerpt":"","text":"/* 波浪css */ .main-hero-waves-area { width: 100%; position: absolute; left: 0; bottom: -11px; z-index: 5; } .waves-area .waves-svg { width: 100%; height: 5rem; } /* Animation */ .parallax > use { animation: move-forever 25s cubic-bezier(0.55, 0.5, 0.45, 0.5) infinite; } .parallax > use:nth-child(1) { animation-delay: -2s; animation-duration: 7s; fill: #f7f9febd; } .parallax > use:nth-child(2) { animation-delay: -3s; animation-duration: 10s; fill: #f7f9fe82; } .parallax > use:nth-child(3) { animation-delay: -4s; animation-duration: 13s; fill: #f7f9fe36; } .parallax > use:nth-child(4) { animation-delay: -5s; animation-duration: 20s; fill: #f7f9fe; } /* 黑色模式背景 */ [data-theme=\"dark\"] .parallax > use:nth-child(1) { animation-delay: -2s; animation-duration: 7s; fill: #18171dc8; } [data-theme=\"dark\"] .parallax > use:nth-child(2) { animation-delay: -3s; animation-duration: 10s; fill: #18171d80; } [data-theme=\"dark\"] .parallax > use:nth-child(3) { animation-delay: -4s; animation-duration: 13s; fill: #18171d3e; } [data-theme=\"dark\"] .parallax > use:nth-child(4) { animation-delay: -5s; animation-duration: 20s; fill: #18171d; } @keyframes move-forever { 0% { transform: translate3d(-90px, 0, 0); } 100% { transform: translate3d(85px, 0, 0); } } /*Shrinking for mobile*/ @media (max-width: 768px) { .waves-area .waves-svg { height: 40px; min-height: 40px; } }"},{"title":"tags","date":"2025-08-18T06:48:32.000Z","updated":"2025-08-28T12:33:05.455Z","comments":true,"path":"tags/index.html","permalink":"https://aquapluto.github.io/tags/","excerpt":"","text":""}],"posts":[{"title":"Role角色","slug":"CICD/ansible/role","date":"2025-09-13T10:07:53.000Z","updated":"2025-09-13T13:30:24.560Z","comments":true,"path":"CICD/ansible/role/","permalink":"https://aquapluto.github.io/CICD/ansible/role/","excerpt":"","text":"Ansible Roles目录编排角色是ansible自1.2版本引入的新特性，用于层次性、结构化地组织playbook。roles能够根据层次型结构自动装载变量文件、tasks以及handlers等。要使用roles只需要在playbook中使用 include 指令即可。简单来讲，roles就是通过分别将变量、文件、任务、模板及处理器放置于单独的目录中，分门别类的管理起来，使其看起来更像一个项目，并可以便捷地 include 它们的一种机制，主要用来解决多文件之间的相互包含，引用，组合等问题 角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程等场景中，在复杂的场景中，建议使用 roles，代码复用度高 思想：比如在构建LNMP架构中，有许多文件，nginx.yml，mysql.yml，php.yml，nginx.yml.j2，mysql.yml.j2，php.xml.j2等等，而且hosts和task合在一起，以后要修改文件就很麻烦，本来只是修改一下hosts就可以了，可能会有人工误操作把task改错了，所以我们需要将hosts和task分离，实现程序代码和业务数据分离，即解耦 官方文档 默认roles存放路径 123/root/.ansible/roles/usr/share/ansible/roles/etc/ansible/roles roles：多个角色的集合目录， 可以将多个的role，分别放至roles目录下的独立子目录中，如下示例 12345roles/ mysql/ nginx/ tomcat/ redis/ roles目录结构如下所示，每个角色，以特定的层级目录结构进行组织 roles目录结构： 12345678910111213141516171819playbook1.ymlplaybook2.ymlroles/ project1/ tasks/ files/ vars/ templates/ handlers/ default/ meta/ project2/ tasks/ files/ vars/ templates/ handlers/ default/ meta/ roles&#x2F;project&#x2F;：项目名称，有以下子目录 files&#x2F;：该 role 使用过程中要用到的文件，比如存放由copy或script模块等调用的文件，安装包 templates&#x2F;：模版目录，template模块查找所需要模板文件的目录，如果没有特别指定，则在该role中其它文件要引用模板文件都默认存放在此目录 tasks&#x2F;：任务目录，定义task,role的基本元素，至少应该包含一个名为main.yml的文件；该文件通过 include 来引用目录下的其它文件 handlers&#x2F;：触发器目录，至少应该包含一个名为main.yml的文件；此目录下的其它的文件需要在此文件中通过include进行引用 vars&#x2F;：变量目录，定义变量，至少应该包含一个名为main.yml的文件；此目录下的其它的变量文件需要在此文件中通过include进行引用，也可以通过项目目录中的 group_vars&#x2F;all 定义变量，从而实现角色通用代码和项目数据的分离 meta&#x2F;：额外信息需要用到的一些数据，定义当前角色的特殊设定及其依赖关系，至少应该包含一个名为main.yml的文件，其它文件需在此文件中通过include进行引用 default&#x2F;：在该 role 中会用到的默认变量，设定默认变量时使用此目录中的main.yml文件，比vars的优先级低 创建Role创建role的步骤 创建role的目录结构，在以roles命名的目录下分别创建以各角色名称命名的目录，如mysql等，在每个角色命名的目录中分别创建相关的目录和文件，比如tasks、files、handlers、templates和vars等目录；用不到的目录可以创建为空目录，也可以不创建 编写和准备指定role的功能文件，包括tasks、templates、vars等相关文件 编写playbook文件调用上面定义的role，应用到指定的主机 Ansible 查找角色时，会先从 roles_path 配置项指定的路径里查找（ansible.cfg设置），再去当前 Playbook 文件所在目录的 roles 子目录中查找 针对大型项目使用Roles进行编排 范例: 利用 ansible-galaxy 创建角色目录的结构 1234567891011121314151617181920212223#创建初始化目录结构[root@ansible roles]#ansible-galaxy role init test_role- Role test_role was created successfully[root@ansible roles]#tree test_role/test_role/├── defaults│ └── main.yml├── files├── handlers│ └── main.yml├── meta│ └── main.yml├── README.md├── tasks│ └── main.yml├── templates├── tests│ ├── inventory│ └── test.yml└── vars └── main.yml8 directories, 8 files Playbook调用Role一、直接调用 123456- hosts: webservers remote_user: root roles: - mysql - memcached - nginx 二、传参 键role用于指定角色名称，后续的k&#x2F;v用于传递变量给角色 12345- hosts: all remote_user: root roles: - role: mysql username: mysql 三、结合条件测试调用 123456- hosts: all remote_user: root roles: - role: nginx username: nginx when: ansible_distribution_major_version == &#x27;7&#x27; 四、结合tags调用 12345678- hosts: appsrvs remote_user: root tags: app roles: - role: nginx tags: - nginx - web 五、依赖其它角色，在meta目录下编写与该role相关的依赖角色 1234dependencies: - role: nginx - role: php-fpm - role: mysql 实战案例实现 Httpd 角色12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#创建角色相关的目录[root@ansible ~]#mkdir -pv /data/ansible/roles/httpd/&#123;tasks,handlers,files&#125; #创建角色相关的文件[root@ansible ~]#cd /data/ansible/roles/httpd/#main.yml是task的入口文件[root@ansible ~]#vim tasks/main.yml- include: group.yml- include: user.yml- include: install.yml- include: config.yml- include: index.yml- include: service.yml[root@ansible ~]#vim tasks/group.yml- name: create apache group group: name=apache system=yes gid=80 [root@ansible ~]#vim tasks/user.yml- name: create apache user user: name=apache system=yes shell=/sbin/nologin home=/var/www/ uid=80 group=apache [root@ansible ~]#vim tasks/install.yml- name: install httpd package yum: name=httpd #注意: 文件是放在files目录下,但src的路径无需写files目录名[root@ansible ~]#vim tasks/config.yml- name: config file copy: src=httpd.conf dest=/etc/httpd/conf/ backup=yes notify: restart [root@ansible ~]# tasks/index.yml- name: index.html copy: src=index.html dest=/var/www/html/ [root@ansible ~]#vim tasks/service.yml- name: start service service: name=httpd state=started enabled=yes [root@ansible ~]#vim handlers/main.yml- name: restart service: name=httpd state=restarted #在files目录下准备两个文件[root@ansible ~]#ls files/httpd.conf index.html[root@ansible ~]#tree /data/ansible/roles/httpd//data/ansible/roles/httpd/├── files│ ├── httpd.conf│ └── index.html├── handlers│ └── main.yml└── tasks ├── config.yml ├── group.yml ├── index.yml ├── install.yml ├── main.yml ├── service.yml └── user.yml3 directories, 10 files#在playbook中调用角色[root@ansible ~]#vim /data/ansible/role_httpd.yml- hosts: webservers remote_user: root roles: - httpd #运行playbook[root@ansible ~]#ansible-playbook /data/ansible/role_httpd.yml 实现 Nginx 角色1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[root@ansible ~]#mkdir -pv /data/ansible/roles/nginx/&#123;tasks,handlers,templates,vars&#125;#创建task文件[root@ansible ~]#cd /data/ansible/roles/nginx/[root@ansible nginx]#vim tasks/main.yml- include: install.yml- include: config.yml- include: index.yml- include: service.yml[root@ansible nginx]#vim tasks/install.yml- name: install yum: name=nginx [root@ansible nginx]#vim tasks/config.yml- name: config file for centos7 template: src=nginx7.conf.j2 dest=/etc/nginx/nginx.conf when: ansible_distribution_major_version==&quot;7&quot; notify: restart - name: config file for centos8 template: src=nginx8.conf.j2 dest=/etc/nginx/nginx.conf when: ansible_distribution_major_version==&quot;8&quot; notify: restart #跨角色调用文件[root@ansible nginx]#vim tasks/index.yml- name: index.html copy: src=roles/httpd/files/index.html dest=/usr/share/nginx/html/ [root@ansible nginx]#vim tasks/service.yml- name: start service service: name=nginx state=started enabled=yes #创建handler文件[root@ansible nginx]#cat handlers/main.yml- name: restart service: name=nginx state=restarted #创建两个template文件[root@ansible nginx]#cat templates/nginx7.conf.j2...省略...user &#123;&#123;user&#125;&#125;;worker_processes &#123;&#123;ansible_processor_vcpus+3&#125;&#125;; #修改此行error_log /var/log/nginx/error.log;pid /run/nginx.pid;...省略...[root@ansible nginx]#cat templates/nginx8.conf.j2...省略...user &#123;&#123;user&#125;&#125;;worker_processes &#123;&#123;ansible_processor_vcpus**3&#125;&#125;; #修改此行error_log /var/log/nginx/error.log;pid /run/nginx.pid;...省略...#创建变量文件[root@ansible nginx]#vim vars/main.ymluser: daemon#目录结构如下[root@ansible ~]#tree /data/ansible/roles/nginx//data/ansible/roles/nginx/├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── file.yml│ ├── install.yml│ ├── main.yml│ └── service.yml├── templates│ ├── nginx7.conf.j2│ └── nginx8.conf.j2└── vars └── main.yml4 directories, 9 files#在playbook中调用角色[root@ansible ~]#vim /data/ansible/role_nginx.yml- hosts: webservers roles: - role: nginx #运行playbook[root@ansible ~]#ansible-playbook /data/ansible/role_nginx.yml 实现 Memcached 角色1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@ansible ~]#mkdir -pv /data/ansible/roles/memcached/&#123;tasks,templates&#125;[root@ansible ~]#cd /data/ansible/roles/memcached[root@ansible memcached]#vim tasks/main.yml- include: install.yml- include: config.yml- include: service.yml[root@ansible memcached]#vim tasks/install.yml- name: install yum: name=memcached [root@ansible memcached]#vim tasks/config.yml- name: config file template: src=memcached.j2 dest=/etc/sysconfig/memcached [root@ansible memcached]#vim tasks/service.yml- name: service service: name=memcached state=started enabled=yes [root@ansible memcached]#vim templates/memcached.j2PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;&#123;&#123;ansible_memtotal_mb//4&#125;&#125;&quot;OPTIONS=&quot;&quot;[root@ansible memcached]#tree /data/ansible/roles/memcached//data/ansible/roles/memcached/├── tasks│ ├── config.yml│ ├── install.yml│ ├── main.yml│ └── service.yml└── templates └── memcached.j22 directories, 5 files[root@ansible ~]#vim /data/ansible/role_memcached.yml- hosts: appsrvs roles: - role: memcached [root@ansible ~]#ansible-play /data/ansible/role_memcached.yml 实现 MySQL 5.7 或 8.0 的角色123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112[root@ansible mysql]#pwd/data/ansible/roles/mysql[root@ansible mysql]#tree /data/ansible/roles/mysql/.├── files│ ├── my.cnf│ └── mysql-8.0.23-linux-glibc2.12-x86_64.tar.xz├── tasks│ ├── config.yml│ ├── data.yml│ ├── group.yml│ ├── install.yml│ ├── linkfile.yml│ ├── main.yml│ ├── path.yml│ ├── script.yml│ ├── secure.yml│ ├── service.yml│ ├── unarchive.yml│ └── user.yml└── vars └── main.yml3 directories, 15 files[root@ansible mysql]#cat /data/ansible/roles/mysql/files/my.cnf[mysqld]server-id=1log-bindatadir=/data/mysqlsocket=/data/mysql/mysql.sock log-error=/data/mysql/mysql.logpid-file=/data/mysql/mysql.pid[client]socket=/data/mysql/mysql.sock[root@ansible mysql]#cat /data/ansible/roles/mysql/vars/main.ymlmysql_version: 8.0.23mysql_file: mysql-&#123;&#123;mysql_version&#125;&#125;-linux-glibc2.12-x86_64.tar.xzmysql_root_password: 123456[root@ansible mysql]#cat /data/ansible/roles/mysql/tasks/main.yml- include: install.yml- include: group.yml- include: user.yml- include: unarchive.yml- include: linkfile.yml- include: data.yml- include: config.yml- include: script.yml- include: path.yml- include: service.yml- include: secure.yml[root@ansible mysql]#cat /data/ansible/roles/mysql/tasks/install.yml- name: install packages yum: name: - libaio - numactl-libs [root@ansible mysql]#cat tasks/group.yml- name: create mysql group group: name=mysql gid=306 [root@ansible mysql]#cat tasks/user.yml- name: create mysql user user: name=mysql uid=306 group=mysql shell=/sbin/nologin system=yes create_home=no home=/data/mysql [root@ansible mysql]#cat tasks/unarchive.yml- name: copy tar to remote host and file mode unarchive: src=&#123;&#123;mysql_file&#125;&#125; dest=/usr/local/ owner=root group=root [root@ansible mysql]#cat tasks/linkfile.yml- name: create linkfile /usr/local/mysql file: src=/usr/local/mysql-&#123;&#123; mysql_version &#125;&#125;-linux-glibc2.12-x86_64 dest=/usr/local/mysql state=link [root@ansible mysql]#cat tasks/data.yml- name: data dir shell: /usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql -- datadir=/data/mysql tags: data [root@ansible mysql]#cat tasks/config.yml- name: config my.cnf copy: src=/data/ansible/files/my.cnf dest=/etc/my.cnf [root@ansible mysql]#cat tasks/script.yml- name: service script shell: /bin/cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld [root@ansible mysql]#cat tasks/path.yml- name: PATH variable copy: content=&#x27;PATH=/usr/local/mysql/bin:$PATH&#x27; dest=/etc/profile.d/mysql.sh [root@ansible mysql]#cat tasks/service.yml- name: enable service shell: chkconfig --add mysqld;/etc/init.d/mysqld start tags: service [root@ansible mysql]#cat tasks/secure.yml- name: change password shell: /usr/local/mysql/bin/mysqladmin -uroot password &#123;&#123;mysql_root_password&#125;&#125; [root@ansible ansible]#cat /data/ansible/role_mysql.yml- hosts: dbsrvs remote_user: root gather_facts: no roles: - mysql [root@ansible ansible]#ansible-playbook role_mysql.yml 实现 NFS 服务1234567891011121314151617181920212223242526272829303132333435363738394041[root@ansible ~]#cat /data/ansible/roles/nfs-server/tasks/main.yml- name: Configre NFS Server template: src: exports.j2 dest: /etc/exports owner: root group: root mode: &#x27;0644&#x27; notify: Restart NFS Server - name: Create NFS dir file: path: &quot;&#123;&#123; item &#125;&#125;&quot; state: directory owner: &quot;&#123;&#123; all_user &#125;&#125;&quot; group: &quot;&#123;&#123; all_group &#125;&#125;&quot; mode: &#x27;0755&#x27; recurse: yes loop: - &quot;&#123;&#123; nfs_share_blog &#125;&#125;&quot; - &quot;&#123;&#123; nfs_share_zrlog &#125;&#125;&quot; - name: Start NFS Server systemd: name: nfs state: started [root@ansible ~]#cat /data/ansible/roles/nfs-server/handlers/main.yml- name: Restart NFS Server systemd: name: nfs state: restarted [root@ansible ~]#cat /data/ansible/roles/nfs-server/templates/exports.j2&#123;&#123; nfs_share_blog &#125;&#125; &#123;&#123; nfs_allow_ip &#125;&#125;(rw,all_squash,anonuid=&#123;&#123; all_uid &#125;&#125;,anongid=&#123;&#123; all_gid &#125;&#125;)&#123;&#123; nfs_share_zrlog &#125;&#125; &#123;&#123; nfs_allow_ip &#125;&#125;(rw,all_squash,anonuid=&#123;&#123; all_uid &#125;&#125;,anongid=&#123;&#123; all_gid &#125;&#125;)[root@ansible ~]#cat /data/ansible/roles/group_vars/allnfs_share_blog: /data/blognfs_share_zrlog: /data/zrlognfs_allow_ip: 10.0.0.0/24 实现 Redis 服务123456789101112131415161718192021222324252627[root@ansible ~]#cat /data/ansible/roles/redis/tasks/main.yml- name: Installed Redis Server yum: name: redis state: present- name: Configure Redis Server template: src: redis.conf.j2 dest: /etc/redis.conf owner: redis group: root mode: &#x27;0640&#x27; notify: Restart Redis Server- name: Start Redis Server systemd: name: redis state: started enabled: yes [root@ansible ~]#cat /data/ansible/roles/redis/templates/redis.conf.j2bind 127.0.0.1 &#123;&#123; ansible_eth0.ipv4.address &#125;&#125;[root@ansible ~]#cat /data/ansible/roles/redis/handlers/main.yml- name: Restart Redis Server systemd: name: redis state: restarted 实现 LNMP主机清单 节点 IP 服务 备注 ansible 10.0.0.208 node-1 10.0.0.206 nginx，php，wordpress blog.magedu.com node-2 10.0.0.210 mysql 准备工作12#创建目录[root@ubuntu ~]# mkdir -pv roles/&#123;mysql,nginx,php,service,wordpress&#125;/&#123;tasks,files,templates&#125; nginx role 实现1234567891011121314151617181920212223242526[root@ubuntu ~]# tree roles/nginx/roles/nginx/├── files├── tasks│ ├── group.yaml│ ├── install.yaml│ ├── main.yaml│ └── user.yaml└── templates[root@ubuntu ~]# cat roles/nginx/tasks/group.yaml- name: add-nginx-group group: name=nginx gid=800 system=yes[root@ubuntu ~]# cat roles/nginx/tasks/user.yaml- name: add-nginx-user user: name=nginx group=800 system=yes uid=800 create_home=no[root@ubuntu ~]# cat roles/nginx/tasks/install.yaml- name: install-nginx apt: name=nginx state=present[root@ubuntu ~]# cat roles/nginx/tasks/main.yaml- include_tasks: group.yaml- include_tasks: user.yaml- include_tasks: install.yaml php role 实现123456789101112131415161718192021222324252627[root@ubuntu ~]# tree roles/php/roles/php/├── files├── tasks│ ├── group.yaml│ ├── install.yaml│ ├── main.yaml│ └── user.yaml└── templates[root@ubuntu ~]# cat roles/php/tasks/group.yaml- name: add-php-group group: name=www-data gid=33 system=yes [root@ubuntu ~]# cat roles/php/tasks/user.yaml- name: add-php-user user: name=www-data group=33 system=yes uid=33 create_home=yes home=/var/www shell=/usr/sbin/nologin[root@ubuntu ~]# cat roles/php/tasks/install.yaml- name: install-php apt: name=php-fpm,php-mysqlnd,php-json,php-gd,php-xml,php-mbstring,php-zip state=present [root@ubuntu ~]# cat roles/php/tasks/main.yaml- include_tasks: group.yaml- include_tasks: user.yaml- include_tasks: install.yaml wordpress role 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@ubuntu ~]# tree roles/wordpress/roles/wordpress/├── files├── tasks│ ├── main.yaml│ ├── wp_get_code.yaml│ ├── wp_set_domain.yaml│ └── wp_unarchive.yaml└── templates └── domain.conf.j23 directories, 5 files[root@ubuntu ~]# cat roles/wordpress/tasks/wp_get_code.yaml- name: wget-wordpress get_url: url=https://cn.wordpress.org/latest-zh_CN.zip dest=/var/www/html/wordpress.zip [root@ubuntu ~]# cat roles/wordpress/tasks/wp_unarchive.yaml- name: wp-unarchive unarchive: src=/var/www/html/wordpress.zip dest=/var/www/html/ owner=www-data group=www-data remote_src=yes [root@ubuntu ~]# cat roles/wordpress/tasks/wp_set_domain.yaml- name: set-wp-domain template: src=domain.conf.j2 dest=/etc/nginx/sites-enabled/&#123;&#123; WP_DOMAIN &#125;&#125;.conf [root@ubuntu ~]# cat roles/wordpress/tasks/main.yaml- include_tasks: wp_get_code.yaml- include_tasks: wp_unarchive.yaml- include_tasks: wp_set_domain.yaml[root@ubuntu ~]# cat roles/wordpress/templates/domain.conf.j2server&#123; listen &#123;&#123; WP_PORT &#125;&#125;; server_name &#123;&#123; WP_DOMAIN &#125;&#125;; include /etc/nginx/default.d/*.conf; root &#123;&#123; WP_PATH &#125;&#125;; index index.php index.html; location / &#123; &#125; location ~ \\.php$ &#123; include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php8.1-fpm.sock; &#125;&#125; service role 实现12345678910111213141516[root@ubuntu ~]# tree roles/service/roles/service/├── files├── tasks│ ├── main.yaml│ └── service.yaml└── templates3 directories, 2 files[root@ubuntu ~]# cat roles/service/tasks/service.yaml- name: service service: name=&#123;&#123; item.name &#125;&#125; state=&#123;&#123; item.state &#125;&#125; enabled=&#123;&#123; item.enabled &#125;&#125; loop: &quot;&#123;&#123; SERVICE_LIST &#125;&#125;&quot; [root@ubuntu ~]# cat roles/service/tasks/main.yaml- include_tasks: service.yaml mysql role 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@ubuntu lnmp_wp]# tree roles/mysql/roles/mysql/├── files│ └── grant.sql├── tasks│ ├── copy_file.yaml│ ├── grant.yaml│ ├── group.yaml│ ├── install.yaml│ ├── main.yaml│ ├── restart.yaml│ └── user.yaml└── templates3 directories, 8 files[root@ubuntu ~]# cat roles/mysql/tasks/group.yaml- name: add-mysql-group group: name=mysql gid=306 system=yes [root@ubuntu ~]# cat roles/mysql/tasks/user.yaml- name: add-mysql-user user: name=mysql group=306 system=yes uid=306 create_home=no [root@ubuntu ~]# cat roles/mysql/tasks/install.yaml- name: apt-install-mysql-server apt: name=mysql-server state=present update_cache=yes - name: set-mysqld-conf-task-1 lineinfile: path=/etc/mysql/mysql.conf.d/mysqld.cnf backrefs=yes regexp=&#x27;^(bind-address.*)$&#x27; line=&#x27;#\\1&#x27;- name: set-mysqld-conf-task-2 lineinfile: path=/etc/mysql/mysql.conf.d/mysqld.cnf line=&#x27;skip-name-resolve&#x27; - name: set-mysqld-conf-task-3 lineinfile: path=/etc/mysql/mysql.conf.d/mysqld.cnf line=&#x27;default-authentication-plugin=mysql_native_password&#x27; [root@ubuntu ~]# cat roles/mysql/tasks/restart.yaml- name: restart-mysql-service service: name=mysql enabled=yes state=restarted [root@ubuntu ~]# cat roles/mysql/tasks/copy_file.yaml- name: copy-mysql-file copy: src=files/grant.sql dest=/tmp/grant.sql [root@ubuntu ~]# cat roles/mysql/tasks/grant.yaml- name: mysql-client-init shell: mysql &lt;/tmp/grant.sql [root@ubuntu ~]# cat roles/mysql/tasks/main.yaml- include_tasks: group.yaml- include_tasks: user.yaml- include_tasks: install.yaml- include_tasks: restart.yaml- include_tasks: copy_file.yaml- include_tasks: grant.yaml[root@ubuntu ~]# cat roles/mysql/files/grant.sqlcreate database if not exists wordpress;create user &#x27;wp_user&#x27;@&#x27;10.0.0.%&#x27; identified by &#x27;123456&#x27;;grant all on wordpress.* to &#x27;wp_user&#x27;@&#x27;10.0.0.%&#x27;;flush privileges; playbook 中配置 role12345678910111213141516171819202122232425262728293031[root@ubuntu ~]# ls lnmp_wp/lnmp_wp.yaml mysql.yaml roles#安装 nginx,php,wordpress，并配置域名[root@ubuntu ~]# cat lnmp_wp/lnmp_wp.yaml--- #lnmp-wp- hosts: 10.0.0.206 gather_facts: no vars: WP_PORT: 80 WP_DOMAIN: blog.magedu.com WP_PATH: /var/www/html/wordpress SERVICE_LIST: [ &#123;name: nginx, state: restarted, enabled: yes&#125;,&#123;name: php8.1-fpm, state: started,enabled: yes&#125; ] roles: - nginx - php - wordpress - service #安装mysql，并创建空数据库，并完成授权[root@ubuntu ~]# cat lnmp_wp/mysql.yaml--- #mysql- hosts: 10.0.0.210 gather_facts: no roles: - mysql #测试[root@ubuntu ~]# cd lnmp_wp/[root@ubuntu lnmp_wp]# ansible-playbook mysql.yaml[root@ubuntu lnmp_wp]# ansible-playbook lnmp_wp.yaml","categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"}]},{"title":"PlayBook","slug":"CICD/ansible/playbook","date":"2025-09-13T10:07:48.000Z","updated":"2025-09-13T13:29:55.452Z","comments":true,"path":"CICD/ansible/playbook/","permalink":"https://aquapluto.github.io/CICD/ansible/playbook/","excerpt":"","text":"1 YAML语言介绍YAML官方网站 ansible yaml 官网 1.1 YAML语法简介在单一文件第一行，用连续三个连字号”-“ 开始，还有选择性的连续三个点号( … )用来表示文件的结尾 次行开始正常写Playbook的内容，一般建议写明该Playbook的功能 使用#号注释代码 缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结合换行来实现的 缩进不支持tab，必须使用空格进行缩进 缩进的空格数不重要，只要相同层级的元素左对齐即可 YAML文件内容是区别大小写的，key&#x2F;value的值均需大小写敏感 多个key&#x2F;value可同行写也可换行写，同行使用，分隔 key后面冒号要加一个空格，比如 key: value value可是个字符串，也可是另一个列表 YAML文件扩展名通常为yml或yaml 1.2 支持的数据类型YAML 支持以下常用几种数据类型： 标量：单个的、不可再分的值 对象：键值对的集合，又称为: 字典（dictionary）&#x2F; 哈希（hashes） &#x2F; 映射（mapping） 数组：一组按次序排列的值，又称为: 列表（list）&#x2F; 序列（sequence） 1.2.1 scalar 标量key对应value 12name: wangage: 18 使用缩进的方式 1234name: wangage: 18 标量是最基本的，不可再分的值，包括： 字符串 布尔值 整数 浮点数 Null 时间 日期 1.2.2 Dictionary 字典一个字典是由一个或多个key与value构成 key和value之间用冒号 ：分隔 冒号 : 后面有一个空格 所有 k&#x2F;v 可以放在一行，,每个 k&#x2F;v 之间用逗号分隔 所有每个 k&#x2F;v 也可以分别放在不同行,一对k&#x2F;v放在独立的一行 1account: &#123; name: wang, age: 30 &#125; 使用缩进方式 1234account: name: wang age: 18 gender: male 范例： 123456789#不同行# An employee recordname: Example Developerjob: Developerskill: Elite(社会精英)#同一行,也可以将key:value放置于&#123;&#125;中进行表示，用,分隔多个key:value# An employee record&#123;name: &quot;Example Developer&quot;, job: &quot;Developer&quot;, skill: &quot;Elite&quot;&#125; 1.2.3 List 列表列表由多个元素组成, 本质就是数组 每个元素放在不同行，每个元素一行,且元素前均使用中横线 - 开头，并且中横线 - 和元素之间有一个空格 也可以将所有元素用 [ ] 括起来放在同一行,每个元素之间用逗号分隔 1course: [ linux , golang , python ] 也可以写成以 - 开头的多行 1234course: - linux - golang - python 元素里也可以包含字典 123456course:- linux: manjaro price: 10000- golang: gin class: 49- python: django 范例： 123456789#不同行,行以-开头,后面有一个空格# A list of tasty fruits- Apple- Orange- Strawberry- Mango#同一行[Apple,Orange,Strawberry,Mango] 范例：YAML 表示一个家庭、 123456789101112131415161718192021:#同一行name: John Smithage: 41gender: Malespouse: &#123; name: Jane Smith, age: 37, gender: Female &#125; children: [ &#123;name: Jimmy Smith,age: 17, gender: Male&#125;, &#123;name: Jenny Smith, age:13, gender: Female&#125;, &#123;name: hao Smith, age: 20, gender: Male &#125; ] #不同行name: John Smithage: 41gender: Malespouse: name: Jane Smith age: 37 gender: Femalechildren: - name: Jimmy Smith age: 17 gender: Male - &#123;name: Jenny Smith, age: 13, gender: Female&#125; - &#123;name: hao Smith, age: 20, gender: Male &#125; 1.3 三种常见的数据格式XML：Extensible Markup Language，可扩展标记语言，可用于数据交换和配置 JSON：JavaScript Object Notation, JavaScript 对象表记法，主要用来数据交换或配置，不支持注释 YAML：YAML Ain’t Markup Language YAML 不是一种标记语言， 主要用来配置，大小写敏感，不支持tab 可以用工具互相转换，参考网站： https://www.json2yaml.com/ http://www.bejson.com/json/json2yaml/ https://www.json.cn/ 2 Playbook命令格式 1ansible-playbook &lt;filename.yml&gt; ... [options] 执行结果 1234567OK=3 #3个task 执行成功change=3 #3个task 都导致了系统状态发生了改变，如果是查询类task，可以在playbook文件中加上 changed_when:falseunreachable=0 #0个task不可达failed=0 #0个task执行失败skipped=0 #跳过0个taskrescued=0 #0个task被拒绝执行ignored=0 #忽略0个task 选项 12345678910111213141516--syntax,--syntax-check #语法检查,功能相当于bash -n-C --check #模拟执行dry run ,只检测可能会发生的改变，但不真正执行操作--list-hosts #列出运行任务的主机--list-tags #列出tag--list-tasks #列出plabook中所有 task--limit 主机列表 #只针对主机列表中的特定主机执行-i INVENTORY, --inventory INVENTORY #指定主机清单文件,通常一个项对应一个主机清单文件--start-at-task START_AT_TASK #从指定task开始执行,而非从头开始,START_AT_TASK为任务的name-v -vv -vvv #显示过程-k|--ask-pass #远程连接密码-T|--timeout #连接超时时长，默认10S-e|--extra-vars #定义变量--become-user #以指定用户执行--flush-cache #刷新facts-skip-tags #跳过指定的tags-t|--tags #只执行特定tag对应的task 范例 123456789101112131415161718192021222324252627282930[root@rocky86 ~]# cat hello.yaml--- # this is test playbook- hosts: group3 #指定主机分组 gather_facts: no #不收集主机信息 remote_user: root #远程执行用户 tasks: #task列表 - name: task-cmd #task名称 command: echo &quot;hello ansible&quot; #具体执行的模块和参数 - name: task-shell #task名称 shell: id #具体执行的模块和参数 remote_user: tom #指定模块的执行用户 #语法检查[root@ubuntu ~]# ansible-playbook --syntax-check hello.yaml#模拟执行[root@ubuntu ~]# ansible-playbook -C hello.yaml#只在指定主机上执行[root@ubuntu ~]# ansible-playbook -C hello.yaml -l 10.0.0.150#列出所有主机[root@ubuntu ~]# ansible-playbook hello.yaml --list-hosts#列出所有tag[root@ubuntu ~]# ansible-playbook hello.yaml --list-tags#列出所有task[root@ubuntu ~]# ansible-playbook hello.yaml --list-tasks 3 Playbook概念在 ansible 中，较简单的任务，我们可以直接调用单个模块来完成，但是，如果遇到复杂的需求，需要调用大量模块才能完成一个需求，或多个任务间有依赖的时候，使用单条命令就特别不方便，这种情况下，我们就可以使用 playbook 来实现这种需求 在 ansible 中，我们写好 playbook，服务器作演员，由服务器根据我们编排的剧本，完成环境安装，服务部署，系统搭建，状态检测等各种各样的功能。 playbook 文件规范 扩展名为 yaml 或 yml 第一行用 “---” (英文输入法下的中划线) 表示 yaml 的开始，一般不写，如果有多个 yaml 文件合并，则可用此标识每个 yaml 文件内容，使用 # 作为注释 大小写敏感 缩进符要统一，用空格键缩进，不要用 tab 缩进空格的数量不重要，但是，相同层级的左边缩进，要保持一致，要对齐，就是说，同样的缩进，代表同样的级别 使用 “-”(英文输入法下的中划线)+一个空格 来表示单个列表项 使用 ”:”(冒号)+空格 来表示一个键值对 使用 “&#123;&#125;”(花括号) 来表示一个键值表 一个 name 只能有一个 task 4 Playbook组成 一个 playbook(剧本)文件是一个YAML语言编写的文本文件 通常一个playbook只包括一个play，但可以包括多个Play 一个play的主要包括两部分: 主机和tasks. 即实现在指定一组主机上执行一个tasks定义好的任务列表。 一个tasks中可以有一个或多个task任务 每一个Task本质上就是调用ansible的一个module 在复杂场景中，一个playbook中也可以包括多个play，实现对多组不同的主机执行不同的任务 5 Playbook组件playbook 是由一个或多个 “play” 组成的列表。playbook 的主要功能在于，将多个 play 组织在一个playbook 文件中，让多台预定义的主机按照 playbook 中编排的内容来完成一系列复杂的功能。一个 playbook 至少要包含 name 和 tasks 两部份。 5.1 hosts组件Hosts：playbook中的每一个play的目的都是为了让特定主机以某个指定的用户身份执行任务。hosts用于指定要执行指定任务的主机列表，须事先定义在主机清单中，其值可以是通配符，主机或组，可以用 -i 选项指定自定义的主机清单文件 1234567- hosts: 10.0.0.206 #IP写法- hosts: 192.168.1.* #通配符- hosts: node.linux-magedu.com #主机名写法- hosts: group1 #分组写法- hosts: group1:group2 #分组写法，或者关系，两个分组并集- hosts: group1:&amp;group2 #分组写法，与关系，两个分组交集- hosts: group1:!group2 #分组写法，非关系，两个分组差集 5.2 tasks列表和task组件tasks：定义要在远程主机上执行的任务列表，play的主体部分，各任务按顺序在 hosts 中指定的主机上执行，即所有主机做完当前任务，才会开始下一个任务。 task：执行指定模块，后面跟上预定的参数，参数中可以使用变量。每个task是一个字典，一个完整的代码块功能需最少元素需包括 name 和 task，一个name只能包括一个task 模块的执行是幂等的，这意味着多次执行是安全的，因为其结果均一致，如果在执行过程中发生错误，则会全部回滚（包括前面己执行成功的） 每个 task 都应该定义name，用于playbook的执行结果输出，建议其内容能清晰地描述任务执行步骤 如果没有指定 name ，则 action 的结果将用于输出 1234- hosts: group tasks: - name: test command: id 5.3 remote_user组件remote_user：指定任务在远程主机上执行时所使用的用户，可以是任意用户，前提是用户是存在的，默认 root 可用于 Host 和 task 中。也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务；此外，甚至可以在sudo时使用 sudo_user 指定sudo时切换的用户 可以全局指定，也可以在特定 task 中指定，切换用户执行需要先完成登录校验或者用 -k 选项手动输入ssh 密码 123456789101112131415161718192021- hosts: group1 #指定主机分组 gather_facts: no #不收集主机信息 remote_user: tom #全局设置远程执行用户 tasks: #task列表 - name: task1 #task名称,此task以tom身份认行 shell: id #具体执行的模块和参数 - name: task2 #task名称,此task以jerry身份连接主机执行 shell: id #具体执行的模块和参数 remote_user: jerry - name: task3 #task名称,此task以jerry身份运行sudo到root执行 shell: id #具体执行的模块和参数 remote_user: jerry sudo: yes #默认sudo为root - name: task4 #task名称,此task以jerry身份运行sudo到tom执行 shell: id #具体执行的模块和参数 remote_user: jerry sudo: yes #默认sudo为root sudo_user: tom #sudo为tom 5.4 ignore_errors组件在同一个 playbook中，如果一个task出错，默认将不会继续执行后续的其它 task 利用 ignore_errors: yes 可以忽略此task的错误,继续向下执行playbook其它 task，此项也可以配置为全局选项 123456- hosts: group ignore_errors: yes # 全局配置 tasks: - name: task-1 command: id1 ignore_errors: yes # 忽略此task的错误 5.5 handlers和notify组件notify 和 handlers：配合使用，某任务的状态在运行后为changed时，可通过”notify”通知给相应的 handlers 任务，可以达到一种触发调用的效果，(类似于函数调用或触发器)，由特定条件触发的操作，满足条件方才执行，否则不执行 handlers本质上也是 tasks，其中的 task 与前述的 task 并没有本质上的不同，只不过 handlers 中定义的 task，不会主动执行，只有notify关注的资源（task）发生变化时， notify去通知相应的handelrs，才会调用handler中定义的 task，没有改变则不会触发handlers handers中的 task，是在playbook的 tasks 中所有的 task 都执行完成之后才调用，就算多个task触发了相同的handlers， 此handlers也仅会在所有task结束后运行一次，这样是为了避免多次触发同一个hander handlers是在所有前面的 tasks 都成功执行才会执行，如果前面任何一个task失败，会导致handler跳过执行 范例：当文件发生变化时，重启服务 12345678- hosts: rocky tasks: - name: config file copy: src=/root/nginx_v2/linux.magedu.com.conf dest=/etc/nginx/conf.d/ notify: restart service #当配置文件发生了变化时,通知重启服务,与handlers的name相匹配 handlers: - name: restart service service: name=nginx state=restarted 在 task 中使用 notify 来调用 handlers 中的任务，如果该 task 执行成功，但在该 task 之后的其它 task 执行失败，则会导致 notify 通知的 handlers 中的任务不会被调用，如果不论后面的task成功与否，都希望handlers能执行, 可以使用 force_handlers: yes 强制执行handler force_handlers 是以整个 playbook 的角度来理解的，在 playbook 中，如果有 task 执行失败，那整个 playbook 也执行失败，就算后面有task，也不会继续执行，那么对应的handler也就不会执行了，所以force_handlers保证的是己成功执行的 task 对应的 handlers 一定会被执行。 123456789101112131415161718192021222324- hosts: localhost force_handlers: yes #force_handlers 可以保证handlers-1被执行，因为task-1任务执行成功 tasks: - name: task-1 shell: echo &quot;task-1&quot; notify: handlers-1 - name: task-2 shell: echoooooo &quot;task-2&quot; notify: handlers-2 #task-2执行失败，handlers-2不会执行 - name: task-3 shell: echo &quot;task-3&quot; notify: handlers-3 #因为task-2执行失败，playbook不会继续执行task-3，所以不论成功与否，handers-3都不会执行 handlers: - name: handlers-1 debug: msg=&quot;handlers-1&quot; - name: handlers-2 debug: msg=&quot;handlers-2&quot; - name: handlers-3 debug: msg=&quot;handlers-3&quot; 5.6 tags组件tags：还可以通过”tags”给 task 打标签，可在ansible-playbook命令上使用 -t 指定进行调用，定义哪些代码内容可以被忽略。ansible虽然具有幂等性，会自动跳过没有变化的部分，即便如此，有些代码为测试其确实没有发生变化的时间依然会非常地长。此时，如果确信其没有变化，就可以通过tags执行特定的 task 可以一个task对应多个tag，也可以多个task对应同一个tag 还有另外3个特殊关键字用于标签，tagged，untagged 和 all，tagged 表示所有被 tag 标记的任务，untagged 表示所有没有被标记的任务，all 表示所有任务。 12345678- hosts: all tasks: - name: configure file copy: src: files/httpd.conf dest: /etc/httpd/conf/ tags: - conf 6 Variables变量Playbook中同样也支持变量，通过 variables 定义 playbook 运行时需要的使用的变量，有多种定义方式，内置变量或自定义变量 变量名：仅能由字母、数字和下划线组成，区分大小写，且只能以字母开头 变量定义：variable=value 或 variable: value 变量调用方式：通过 &#123;&#123; variable_name &#125;&#125; 调用变量，且变量名前后建议加空格，有时用 &quot;&#123;&#123; variable_name &#125;&#125;&quot; 才生效 变量优先级（从高到低））：-e 选项定义变量 --&gt; playbook中vars_files --&gt; playbook中vars变量定义 --&gt; host_vars/主机名文件 --&gt; 主机清单中主机变量--&gt; group_vars/主机组名文件 --&gt; group_vars/all文件 --&gt; 主机清单组变量 6.1 使用 setup 模块中变量setup：默认会自动调用的模块，获取远程主机的一些信息都是放在 facts 变量中的，在playbook中，可以直接使用 gather_facts 选项来获得目标主机的信息，默认是yes facts中所有可用变量请参考官方文档：ansible_facts 12345678910111213- hosts: rocky gather_facts: yes tasks: - name: show-facts debug: msg=&#123;&#123; ansible_facts &#125;&#125; - name: show-facts-hostname debug: msg=&#123;&#123; ansible_hostname &#125;&#125; - name: show-facts-ipv4-A debug: msg=&#123;&#123; ansible_facts[&quot;eth0&quot;][&quot;ipv4&quot;][&quot;address&quot;] &#125;&#125;--&#123;&#123; ansible_facts.eth0.ipv4.address &#125;&#125; - name: show-facts-ipv4-B debug: msg=&#123;&#123; ansible_eth0[&quot;ipv4&quot;][&quot;address&quot;] &#125;&#125;--&#123;&#123; ansible_eth0.ipv4.address &#125;&#125; - name: show-facts-ipv4-C debug: msg=&#123;&#123; ansible_default_ipv4[&quot;address&quot;] &#125;&#125;--&#123;&#123; ansible_default_ipv4.address &#125;&#125; 每次执行playbook，默认会收集每个主机的所有facts变量，每次执行都需要消耗一定的资源和时间，在不需要facts变量时可以禁用该项 gather_facts: no 来加快 playbook 的执行效率。 如果又需要使用 facts 中的内容，还希望执行的速度快，这时候可以设置 facts 的缓存，可以在 ansible 的配置文件中设置在本地做缓存，也可以将 facts 变量信息存在 redis 服务器中，以便在频繁执行时减少资源消耗 12345[defaults]gathering=smart|implicit|explicit #facts策略，默认smart，新版默认implicitfact_caching_timeout=86400 #本地缓存时长，默认86400Sfact_caching=memeory|jsonfile|redis #缓存类型，默认memoryfact_caching_connection=/path|REDIS #缓存路径或redis信息 gathering 参数说明 smart：远程主机信息如果有本地缓存，则使用缓存，如果没有，就去抓取，再存到缓存中，下次就直接使用缓存信息 implicit：不使用缓存，每次都抓取新的，可以使用 gather_facts: no 来限制不去抓取新的 explicit：不收集主机信息，除非在playbook中用 gather_facts: yes 显式指定 fact_caching 参数说明 memeory：只在当前有效，下次还是要重新抓取 jsonfile：本地缓存，fact_caching_connection 需要指定本地缓存路径 redis：使用 redis 缓存，fact_caching_connection 的格式为 REDIS_IP:REDIS_PORT:REDIS_DB_NUMBER[:REDIS_PASSWORD] 6.2 register注册变量在playbook中可以使用register将捕获命令的输出保存在临时变量中，方便后续调用此变量，比如可以使用debug模块进行显示输出 123456789101112131415- hosts: dbsrvs tasks: - name: get variable shell: hostname register: name - name: &quot;print variable&quot; debug: msg: &quot;&#123;&#123; name &#125;&#125;&quot; #输出register注册的name变量的全部信息,注意变量要加&quot; &quot;引起来 #msg: &quot;&#123;&#123; name.cmd &#125;&#125;&quot; #显示命令 #msg: &quot;&#123;&#123; name.rc &#125;&#125;&quot; #显示命令成功与否，相当于$? #msg: &quot;&#123;&#123; name.stdout &#125;&#125;&quot; #显示命令的输出结果为字符串形式,所有结果都放在一行里显示,适合于结果是单行输出 #msg: &quot;&#123;&#123; name.stdout_lines &#125;&#125;&quot; #显示命令的输出结果为列表形式,逐行标准输出,适用于多行显示 #msg: &quot;&#123;&#123; name[&#x27;stdout_lines&#x27;] &#125;&#125;&quot; #显示命令的执行结果为列表形式,和效果上面相同 #msg: &quot;&#123;&#123; name.stdout_lines[0] &#125;&#125;&quot; #显示命令的输出结果的列表中的第一个元素 范例：修改主机名形式为 web_&lt;随机字符&gt; 12345678910111213- hosts: webservers tasks: - name: generate random shell: openssl rand -base64 12 | tr -dc &#x27;[:alnum:]&#x27; register: num - name: show random debug: msg: &quot;&#123;&#123; num &#125;&#125;&quot; - name: change hostname hostname: name: web-&#123;&#123; num.stdout &#125;&#125; 6.3 在playbook命令行中定义变量123456789101112131415#直接在命令行中指定 [root@ubuntu ~]# ansible-playbook -e uname=tom -e age=18 -e gender=M var1.yaml#多个变量一次定义[root@ubuntu ~]# ansible-playbook -e &quot;uname=tom age=18 gender=M&quot; var1.yaml#从文件中读取[root@ubuntu ~]# cat var.txtuname: jerryage: 20gender: F#文件前面要加 @[root@ubuntu ~]# ansible-playbook -e &quot;@/root/var.txt&quot; var1.yaml[root@ubuntu ~]# ansible-playbook -e &quot;@./var.txt&quot; var1.yaml 6.4 在playbook文件中定义变量此方式定义的是私有变量，即只能在当前playbook中使用，不能被其它Playbook共用 1234567891011- hosts: webservers remote_user: root vars: username: user1 groupname: group1 tasks: - name: create group &#123;&#123; groupname &#125;&#125; group: name=&#123;&#123; groupname &#125;&#125; state=present - name: create user &#123;&#123; username &#125;&#125; user: name=&#123;&#123; username &#125;&#125; group=&#123;&#123; groupname &#125;&#125; state=present 范例：变量的相互调用 1234567- hosts: webservers vars: suffix: &quot;txt&quot; file: &quot;&#123;&#123; ansible_nodename &#125;&#125;.&#123;&#123;suffix&#125;&#125;&quot; tasks: - name: test var file: path=&quot;/data/&#123;&#123;file&#125;&#125;&quot; state=touch 6.5 playbook公共的变量文件可以在一个独立的 yaml 文件中定义公共变量，在其它的playbook文件中可以引用变量文件中的变量 此方式比playbook中定义的变量优化级高 1234567891011121314[root@ansible ~]#cat vars.ymlvar1: httpdvar2: nginx [root@ansible ~]#cat var.yml- hosts: web vars_files: - vars.yml # 可以使用相对路径和绝对路径的写法 tasks: - name: create httpd log file: name=/app/&#123;&#123; var1 &#125;&#125;.log state=touch - name: create nginx log file: name=/app/&#123;&#123; var2 &#125;&#125;.log state=touch 6.6 在主机清单中定义主机和主机组的变量主机变量：在 inventory 主机清单文件中为指定的主机定义变量以便于在playbook中使用 123[webservers]www1.wang.org http_port=80 maxRequestsPerChild=808www2.wang.org http_port=8080 maxRequestsPerChild=909 主机组变量：在inventory 主机清单文件中赋予给指定组内所有主机上的在playbook中可用的变量，如果和主机变量是同名，优先级低于主机变量 1234567891011[webservers]10.0.0.8 10.0.0.18[webservers:vars] # webservers组所拥有ntp_server=ntp.wang.orgnfs_server=nfs.wang.org[all:vars] # 所有主机所拥有ntp_server=ntp.wang.orgnfs_server=nfs.wang.org 6.7 针对当前项目的主机和主机组的变量上面的方式是针对所有项目都有效，而官方更建议的方式是使用ansible特定项目的主机变量和组变量 生产建议在每个项目对应的目录中创建额外的两个变量目录，分别是host_vars和group_vars host_vars下面的文件名和主机清单主机名一致，针对单个主机进行变量定义 格式：host_vars&#x2F;hostname group_vars下面的文件名和主机清单中组名一致，针对单个组进行变量定义 格式：group_vars&#x2F;groupname group_vars&#x2F;all 文件内定义的变量对所有组都有效 123456789101112131415161718192021222324252627282930313233343536373839404142[root@ubuntu ~]#cat hostname.yaml- hosts: all tasks: - name: change hostname hostname: name=web-&#123;&#123; id &#125;&#125;.&#123;&#123; domain &#125;&#125; [root@ubuntu ~]#cat /etc/ansible/hosts[webserver]10.0.0.810.0.0.18[appserver]10.0.0.2810.0.0.38[root@ubuntu ~]#cat host_vars/10.0.0.8id: 8[root@ubuntu ~]#cat host_vars/10.0.0.18id: 18[root@ubuntu ~]#cat host_vars/10.0.0.28id: 28[root@ubuntu ~]#cat host_vars/10.0.0.38id: 38[root@ubuntu ~]#cat group_vars/webserverdomain: wang.org#不属于webserver组的使用[root@ubuntu ~]#cat group_vars/alldomain: wu.org[root@ubuntu ~]#ansible-playbook hostname.yaml#最终结果是[root@10.0.0.8 ~]#hostnameweb-8.wang.org[root@10.0.0.18 ~]#hostnameweb-18.wang.org[root@10.0.0.28 ~]#hostnameweb-28.wu.org[root@10.0.0.38 ~]#hostnameweb-38.wu.org 7 Templates模板templates：模板模块，配合变量使用的模板文件，可以实现批量执行时，在不同主机上用同一个模板生成不同配置文件的功能，可替换模板文件中的变量并实现一些简单逻辑的文件，用于根据每个主机的不同环境而为生成不同的文件。模板文件中支持嵌套jinja2语言的指令，来实现变量、条件判断、循环等功能。 template文件建议存放在和 playbook 文件同级目录的 templates 目录下，且以 .j2 结尾，这样在 playbook 中使用模板文件时，就不需要指定模板文件的路径了。 范例：利用template生成不同的nginx配置文件 12345678910111213141516171819[root@ansible ~]#mkdir templates[root@ansible ~]#vim templates/nginx.conf.j2......worker_processes &#123;&#123; ansible_processor_vcpus*2 &#125;&#125;; #根据不同主机的CPU核数生成不同的配置......[root@ansible ~]#vim temnginx.yml---- hosts: webservers remote_user: root tasks: - name: install nginx yum: name=nginx - name: template config to remote hosts template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf - name: start service service: name=nginx state=started enabled=yes template中也可以使用流程控制 for 循环和 if 条件判断，实现动态生成文件功能 for 循环格式 123&#123;% for i in EXPR %&#125; ...&#123;% endfor %&#125; if 条件判断格式 1234567891011121314151617181920#单分支&#123;% if EXPR %&#125;...&#123;% endif %&#125;#双分支&#123;% if EXPR %&#125;...&#123;% else %&#125;...&#123;% endif %&#125;#多分支&#123;% if EXPR %&#125;...&#123;% elif EXPR %&#125;...&#123;% else %&#125;...&#123;% endif %&#125; playbook 文件 templnginx5.yml 1234567891011121314151617- hosts: webservers remote_user: root vars: nginx_vhosts: - listen: 8080 root: &quot;/var/www/nginx/web1/&quot; - listen: 8080 server_name: &quot;web2.wang.org&quot; root: &quot;/var/www/nginx/web2/&quot; - listen: 8080 server_name: &quot;web3.wang.org&quot; root: &quot;/var/www/nginx/web3/&quot; tasks: - name: template config template: src: nginx.conf5.j2 dest: /data/nginx5.conf 模版文件 templates/nginx.conf5.j2 123456789&#123;% for vhost in nginx_vhosts %&#125;server &#123; listen &#123;&#123; vhost.listen &#125;&#125;; &#123;% if vhost.server_name is defined %&#125;server_name &#123;&#123; vhost.server_name &#125;&#125;; &#123;% endif %&#125;root &#123;&#123; vhost.root &#125;&#125;;&#125;&#123;% endfor %&#125; 生成结果 1234567891011121314server &#123; listen 8080; root /var/www/nginx/web1/;&#125;server &#123; listen 8080; server_name web2.wang.org; root /var/www/nginx/web2/;&#125;server &#123; listen 8080; server_name web3.wang.org; root /var/www/nginx/web3/;&#125; 8 流程控制8.1 循环迭代 loop ansible 的 task 中，如果要重复执行相同的模块，则可以使用循环的方式来实现 对于迭代项的引用，要使用固定内置变量 item 来引用，这是固定写法。 范例：安装多个安装包 123456789- hosts: webservers tasks: - name: Install Httpd and Php-fpm Package yum: name: &quot;&#123;&#123; item &#125;&#125;&quot; state: latest loop: - httpd - php-fpm 在迭代中，还可以嵌套子变量，关联多个变量在一起使用 范例：批量修改用户密码 12345678910- hosts: ssh-host tasks: - name: change user passwd user: name: &quot;&#123;&#123; item.name &#125;&#125;&quot; password: &quot;&#123;&#123; item.chpass | password_hash(&#x27;sha512&#x27;) &#125;&#125;&quot; update_password: always loop: - &#123; name: &#x27;root&#x27;, chpass: &#x27;123456&#x27; &#125; - &#123; name: &#x27;app&#x27;, chpass: &#x27;654321&#x27; &#125; 8.2 条件判断 when可以实现条件测试。例如可以根据变量、facts或此前任务的执行结果来做为某task执行与否的前提，通过在task后添加when子句即可使用 jinja2 的语法格式条件测试 范例: 判断服务状态决定是否重新启动 1234567891011121314151617- hosts: webservers tasks: - name: Check nginx Service command: systemctl is-active nginx ignore_errors: yes register: check_nginx - name: debug var debug: var: check_nginx - name: Httpd Restart service: name: nginx state: restarted when: check_nginx.rc == 0 #如果nginx服务没启动，就重新启动 #failed_when: check_nginx.rc != 0 #条件不成立才执行,和when相反 范例: 多条件判断写法 123456789101112- hosts: all tasks: - name: &quot;shut down CentOS 6 and Debian 7 systems&quot; shell: cmd: /sbin/shutdown -h now when: (ansible_facts[&#x27;distribution&#x27;] == &quot;CentOS&quot; and ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;6&quot;) or (ansible_facts[&#x27;distribution&#x27;] == &quot;Debian&quot; and ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;7&quot;) - name: &quot;shut down CentOS 7 systems&quot; shell: /sbin/shutdown -h now when: # when的列表形式表示 and 关系 - ansible_facts[&#x27;distribution&#x27;] == &quot;CentOS&quot; - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;7&quot; 其他判断条件 操作系统版本判断的第二种写法：ansible_distribution_file_variety == &quot;Debian&quot; 操作系统主版本号判断的第二种写法：ansible_distribution_major_version == &quot;7&quot; 判断变量是否被定义：name is undefined，name指定要判断的变量名 对主机名进行条件判断：ansible_fqdn is match (&quot;web*&quot;) 8.3 分组 block当想在满足同样条件下，执行多个任务时，就需要分组。而不再针对每个任务都用相同的when来做条件判断 使用 block 可以对 task 任务进行分组，将多个 task 任务放到一个 block 下，可以在写一个 when 判断的情况下调用多个 task 任务 12345678910111213141516171819202122- hosts: localhost tasks: - block: - debug: msg=&quot;first&quot; - debug: msg=&quot;second&quot; when: - ansible_facts[&#x27;distribution&#x27;] == &quot;CentOS&quot; - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;8&quot; #相当于下面写法- hosts: localhost tasks: - debug: msg=&quot;first&quot; when: - ansible_facts[&#x27;distribution&#x27;] == &quot;CentOS&quot; - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;8&quot; - debug: msg=&quot;second&quot; when: - ansible_facts[&#x27;distribution&#x27;] == &quot;CentOS&quot; - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;8&quot; 8.4 changed_when第一个作用：关闭 changed 状态 只有在 task 的执行结果返回状态为 changed 的时候，我们才认为该 task 是真实执行了，在远程主机上产生了数据变化，但是在 ansible 中，不是所有模块都具有幂等性，对于某些不会产生数据变化的 task，ansible 也会给出 changed 输出。所以当确定某个task不会对被控制端产生数据变化时，而不想显示的是黄色的changed状态，可以通过 changed_when: false 来避免这一情况 12345- hosts: localhost tasks: - name: task-1 shell: id changed_when: false 第二个作用：自定义条件判断 Ansible 在执行任务之后，会依据任务的执行状况判断该任务是否使系统状态发生了改变，并且在输出结果里标记为 changed 或者 ok。不过，有时候 Ansible 默认的判断规则不符合我们的需求，这时就可以使用 changed_when 来自定义判断逻辑。 123456- hosts: webservers tasks: - name: check config shell: /usr/sbin/nginx -t register: check_nginx_config changed_when: &quot;&#x27;successful&#x27; in check_nginx_config.stdout&quot; #自定义条件判断任务是否发生了变更 8.5 滚动执行默认情况下，Ansible将尝试并行管理playbook中所有的机器，如果一个 playbook 中有多个 task，在有多台远程主机的情况下，需要在所有远程主机上执行完当前的 task 之后才执行下一个 task，如果主机过多，或者需要执行的task 比较消耗时间，则会导致所有主机都处于一个执行中状态。如下图，先在 A,B,C 三台主机上执行 Task-1，再在三台主机上执行 Task-2 这样的话，在有些场景中是不利于的。比如升级场景，我们希望是先A主机执行完所有task，再到B主机依次执行，就不会导致所有主机都处于一个执行中状态，都无法对外提供服务。所以对于滚动更新用例，可以使用 serial 关键字定义Ansible一次应管理多少主机，还可以将 serial 关键字指定为百分比，表示每次并行执行的主机数占总数的比例 12345678910- hosts: all serial: 2 #每次只同时处理2个主机 #serial: &quot;20%&quot; #每次只同时处理20%的主机 gather_facts: false tasks: - name: task one command: hostname - name: task two command: hostname 8.6 委派至其它主机执行利用委托技术，可以在非当前被控主机的其它主机上执行指定操作，即可以在非指定的主机上执行 task，委派时必须要有key验证 范例：将任务委派给指定的主机执行 1234567#在10.0.0.8上执行hostname -I，而非当前主机localhost- hosts: localhost tasks: - name: show ip address command: hostname -I delegate_to: 10.0.0.8 #指定当前任务被委派给的目标主机 delegate_facts: true #收集被委派的目标主机的facts信息 范例: 将任务被委派给控制端ansible主机执行 123456789101112131415#在本地执行ifconfig,而非10.0.0.8，下面展示三种实现方式- hosts: 10.0.0.8 tasks: - name: show ip address local_action: #旧语法，委派给控制端ansible主机执行 module: command args: ifconfig - name: show hostname shell: hostname connection: local #委派给控制端ansible主机执行 - name: kernel version shell: uname -r delegate_to: localhost #委派给控制端ansible主机执行 8.7 只执行一次 run_once利用 run_once 指令可以只执行一次，而非在所有被控主机都执行，通常会在第一个匹配的主机上执行该任务 12345- hosts: webservers tasks: - name: Run hostname command once command: hostname run_once: true 8.8 yaml文件的相互调用利用 include 或 include_tasks 可以在某个 task 中调用其它的只有 task 内容的 yaml 文件，include 在 2.16 版本之后被弃用，建议使用 include_tasks 来实现包含。include_tasks 一次只能引用一个 yaml 文件 注意：一个task只有一个include_tasks生效，也可以配合when和变量一起使用 123456789101112#a.yml- hosts: webservers tasks: - name: run a job command: wall run a job - name: excute b.yml include_tasks: b.yml #调用b.yml执行task #b.yml- name: run b job command: wall run b job import_playbook 合并多个 playbook 文件，将多个包含完整内容的 yaml 文件交由一个 yaml 统一调用 123456789101112131415[root@ansible ansible]#cat main.yml- import_playbook: tasks1.yml- import_playbook: tasks2.yml[root@ansible ansible]#cat tasks1.yml- hosts: webservers tasks: - name: run task1 job command: wall run task1 job [root@ansible ansible]#cat tasks2.yml- hosts: dbsrvs tasks: - name: run task2 job command: wall run task2 job 9 实战案例9.1 创建 mysql 用户1234567891011121314151617- hosts: 192.168.100.46 remote_user: root gather_facts: no tasks: - name: &quot;create mysql_group&quot; group: name: mysql system: yes gid: 306 - name: &quot;create mysql_user&quot; user: name: mysql create_home: no shell: /bin/nologin system: yes uid: 306 group: mysql 9.2 安装 nginx1234567891011121314- hosts: webservers remote_user: root gather_facts: no tasks: - name: add group nginx group: name=nginx state=present - name: add user nginx user: name=nginx state=present group=nginx - name: Install Nginx yum: name=nginx state=present - name: web page copy: src=files/index.html dest=/usr/share/nginx/html/index.html - name: Start Nginx service: name=nginx state=started enabled=yes 9.3 安装 docker需要调用的文件 1234567891011121314[root@centos8 ansible]# vim /etc/ansible/hosts[ubuntu]10.0.0.10010.0.0.200[root@centos8 ansible]# vim vars.ymldocker_version: 5:19.03.15~3-0~ubuntu-ubuntu1804: bionicubuntu2004: focal[root@centos8 ansible]# vim files/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://si7y70hh.mirror.aliyuncs.com&quot;] &#125; install_docker.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071- hosts: ubuntu remote_user: root vars_files: - vars.yml tasks: - name: install packages apt: name: - apt-transport-https - ca-certificates - curl - software-properties-common state: present - name: import key apt_key: url: https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu/gpg state: present - name: import installation source on ubuntu1804 apt_repository: repo: &quot;deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu &#123;&#123; ubuntu1804 &#125;&#125; stable&quot; state: present when: - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;18&quot; - name: import installation source on ubuntu2004 apt_repository: repo: &quot;deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu &#123;&#123; ubuntu2004 &#125;&#125; stable&quot; state: present when: - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;20&quot; - name: install docker for ubuntu1804 apt: name: - docker-ce=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu1804 &#125;&#125; - docker-ce-cli=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu1804 &#125;&#125; state: present when: - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;18&quot; - name: install docker for ubuntu2004 apt: name: - docker-ce=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu2004 &#125;&#125; - docker-ce-cli=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu2004 &#125;&#125; state: present when: - ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;20&quot; - name: mkdir /etc/docker file: path: /etc/docker state: directory - name: aliyun Mirror acceleration copy: src: /data/ansible/files/daemon.json dest: /etc/docker/ - name: load daemon systemd: daemon_reload: yes - name: start docker service: name: docker state: started enabled: yes 验证安装是否成功 1[root@centos8 ansible]# ansible ubuntu -a &quot;docker version&quot; 9.4 安装 docker harbor需要调用的文件 12345678910111213141516171819202122232425262728293031323334353637[root@centos8 ansible]# vim vars.ymldocker_version: 5:19.03.15~3-0~ubuntu-ubuntu1804: bionicubuntu2004: focaldocker_compose_version: 1.27.4harbor_version: 1.7.6[root@centos8 ansible]# vim files/harbor.sh#!/bin/bashIPADDR=`hostname -I|awk &#x27;&#123;print $1&#125;&#x27;`HARBOR_ADMIN_PASSWORD=123456sed -i.bak -e &#x27;s/^hostname =.*/hostname = &#x27;$IPADDR&#x27;/&#x27; -e &#x27;s/^harbor_admin_password =.*/harbor_admin_password = &#x27;$HARBOR_ADMIN_PASSWORD&#x27;/&#x27; /apps/harbor/harbor.cfg[root@centos8 files]# vim files/harbor.service[Unit]Description=HarborAfter=docker.service systemd-networkd.service systemd-resolved.serviceRequires=docker.serviceDocumentation=http://github.com/vmware/harbor[Service]Type=simpleRestart=on-failureRestartSec=5ExecStart=/usr/bin/docker-compose -f /apps/harbor/docker-compose.yml upExecStop=/usr/bin/docker-compose -f /apps/harbor/docker-compose.yml down[Install]WantedBy=multi-user.target[root@centos8 ansible]# vim files/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://si7y70hh.mirror.aliyuncs.com&quot;] &#125; [root@centos8 ansible]# ls files/daemon.json docker-compose-Linux-x86_64-1.27.4 harbor-offline-installer-v1.7.6.tgz harbor.service harbor.sh install_docker_harbor.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109- hosts: ubuntu remote_user: root vars_files: - vars.yml gather_facts: true tasks: - name: install packages apt: name: - apt-transport-https - ca-certificates - curl - software-properties-common state: present - name: import key apt_key: url: https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu/gpg state: present - name: import installation source on ubuntu1804 apt_repository: repo: &quot;deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu &#123;&#123; ubuntu1804 &#125;&#125; stable&quot; state: present when: ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;18&quot; - name: import installation source on ubuntu2004 apt_repository: repo: &quot;deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu &#123;&#123; ubuntu2004 &#125;&#125; stable&quot; state: present when: ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;20&quot; - name: install docker for ubuntu1804 apt: name: - &quot;docker-ce=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu1804 &#125;&#125;&quot; - &quot;docker-ce-cli=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu1804 &#125;&#125;&quot; state: present when: ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;18&quot; - name: install docker for ubuntu2004 apt: name: - &quot;docker-ce=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu2004 &#125;&#125;&quot; - &quot;docker-ce-cli=&#123;&#123; docker_version &#125;&#125;&#123;&#123; ubuntu2004 &#125;&#125;&quot; state: present when: ansible_facts[&#x27;distribution_major_version&#x27;] == &quot;20&quot; - name: mkdir /etc/docker file: path: /etc/docker state: directory - name: aliyun Mirror acceleration copy: src: /data/ansible/files/daemon.json dest: /etc/docker/ - name: load daemon systemd: daemon_reload: yes - name: start docker service: name: docker state: started enabled: yes - name: install compose copy: src: &quot;/data/ansible/files/docker-compose-Linux-x86_64-&#123;&#123; docker_compose_version &#125;&#125;&quot; dest: /usr/bin/docker-compose - name: set excute permission file: path: /usr/bin/docker-compose mode: &#x27;0755&#x27; - name: mkdir /apps file: path: /apps state: directory - name: unarchive harbor package unarchive: src: &quot;/data/ansible/files/harbor-offline-installer-v&#123;&#123; harbor_version &#125;&#125;.tgz&quot; dest: /apps/ - name: set harbor.cfg script: /data/ansible/files/harbor.sh - name: install python apt: name: python state: present - name: install harbor shell: /apps/harbor/install.sh - name: copy harbor.service copy: src: /data/ansible/files/harbor.service dest: /lib/systemd/system/ - name: service enable systemd: name: harbor enabled: yes daemon_reload: yes 9.5 安装 Redisredis_server.yml 123456789101112131415161718192021222324252627- hosts: dbservers tasks: - name: Install Redis Server yum: name: redis state: present - name: Configure Redis Server copy: src: ./files/redis.conf.j2 dest: /etc/redis.conf owner: redis group: root mode: &#x27;0640&#x27; notify: Restart Redis Server - name: Start Redis Server with Systemd systemd: name: redis state: started enabled: yes handlers: - name: Restart Redis Server systemd: name: redis state: restarted 9.6 安装 NFS 服务端需要调用的文件 12[root@ansible playbook]# cat ./exports.j2/nfs_test 10.0.0.0/24(rw,all_squash,anonuid=666,anongid=666) install_nfs_server.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445- hosts: webservers tasks: - name: 1. Install NFS Server yum: name: nfs-utils state: present - name: 2. Configure NFS Server copy: src: ./exports.j2 dest: /etc/exports notify: Restart NFS Server - name: 3. Init Group group: name: www gid: 666 - name: 4. Init User user: name: www uid: 666 group: www shell: /sbin/nologin create_home: no - name: 5. Init Create Directory file: path: /nfs_test state: directory owner: www group: www mode: &#x27;0755&#x27; - name: 6. Start NFS Server systemd: name: nfs state: started enabled: yes handlers: - name: Restart NFS Server systemd: name: nfs state: restarted","categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"}]},{"title":"常用模块","slug":"CICD/ansible/module","date":"2025-09-13T10:07:42.000Z","updated":"2025-09-13T13:22:38.575Z","comments":true,"path":"CICD/ansible/module/","permalink":"https://aquapluto.github.io/CICD/ansible/module/","excerpt":"","text":"1 命令执行相关1.1 Command 模块功能：在远程主机执行命令，此为默认模块，可忽略 -m 选项，但有些操作不支持，比如管道，重定向，通配符等，如果要实现这些功能，可以用 shell 模块实现 注意：此命令不支持 $VARNAME &lt; &gt; | ; &amp; 等，可能用shell模块实现 注意：此模块不具有幂等性 常见选项 123chdir=dir #执行命令前,先切换至目录dircreates=file #当file不存在时才会执行removes=file #当file存在时才会执行 范例 1234567891011121314151617181920212223242526272829303132#远程主机[root@ubuntu ~]# ll /root/testtotal 0#当/root/test/abc不存在时执行[root@ubuntu ~]# ansible 10.0.0.206 -m command -a &quot;chdir=/root/test creates=/root/test/abc touch abc&quot; -kSSH password:10.0.0.206 | CHANGED | rc=0 &gt;&gt;#查看远程主机[root@ubuntu ~]# ll /root/test/total 0-rw-r--r-- 1 root root 0 Jun 17 12:12 abc#当/root/test/abc存在时不执行，提示skipped[root@ubuntu ~]# ansible 10.0.0.206 -m command -a &quot;chdir=/root/test creates=/root/test/abc touch bcd&quot; -kSSH password:10.0.0.206 | SUCCESS | rc=0 &gt;&gt;skipped, since /root/test/abc existsDid not run command since &#x27;/root/test/abc&#x27; exists#查看远程主机[root@ubuntu ~]# ll /root/test/total 0-rw-r--r-- 1 root root 0 Jun 17 12:12 abc#不支持多条命令[root@ubuntu ~]# ansible 10.0.0.206 -a &quot;id;id&quot; -kSSH password:10.0.0.206 | FAILED | rc=2 &gt;&gt;[Errno 2] No such file or directory: b&#x27;id;id&#x27;#创建文件最好使用file模块，因为command模块不具有幂等性 范例 123[root@ansible ~]#ansible webservers -m command -a &#x27;chdir=/etc cat centos-release&#x27;[root@ansible ~]#ansible webservers -m command -a &#x27;chdir=/etc creates=/data/f1.txt cat centos-release&#x27;[root@ansible ~]#ansible webservers -m command -a &#x27;chdir=/etc removes=/data/f1.txt cat centos-release&#x27; 1.2 Shell 模块功能：和command相似，用shell执行命令,支持各种符号,比如:*,$, &gt; , 相当于增强版的command模块 注意：此模块不具有幂等性,建议能不能就用此模块,最好使用专用模块 在调用shell模块执行命令时，复杂的命令也有可能会执行失败，类似于包含多重管道，正则表达式这些，这种情况下，我们需要写成 shell 脚本，用 copy 模块直接推送到远程执行。 常见选项 123chdir=dir #执行命令前,先切换至目录dircreates=file #当file不存在时才会执行removes=file #当file存在时才会执行 范例 1234567891011121314151617181920212223242526272829#支持管道[root@ubuntu ~]# ansible 10.0.0.206 -m shell -a &quot;echo 1+2|bc&quot;10.0.0.206 | CHANGED | rc=0 &gt;&gt;3#支持重定向[root@ubuntu ~]# ansible 10.0.0.206 -m shell -a &quot;echo 1+2|bc &gt; /root/test/abc&quot;10.0.0.206 | CHANGED | rc=0 &gt;&gt;[root@ubuntu ~]# cat /root/test/abc3#支持多条命令[root@ubuntu ~]# ansible 10.0.0.206 -m shell -a &quot;id;id&quot; -kSSH password:10.0.0.206 | CHANGED | rc=0 &gt;&gt;uid=0(root) gid=0(root) groups=0(root)uid=0(root) gid=0(root) groups=0(root)#不具有幂等性[root@ubuntu ~]# ansible 10.0.0.206 -m shell -a &quot;mkdir dira&quot;10.0.0.206 | CHANGED | rc=0 &gt;&gt;[root@ubuntu ~]# ansible 10.0.0.206 -m shell -a &quot;mkdir dira&quot;10.0.0.206 | FAILED | rc=1 &gt;&gt;mkdir: cannot create directory ‘dira’: File existsnon-zero return code#修改配置文件[root@ubuntu ~]#ansible all -m shell -a &quot;sed -i &#x27;s/^Listen 80/Listen 8080/&#x27; /etc/httpd/conf/httpd.conf&quot; 注意：调用bash执行命令 类似 cat &#x2F;tmp&#x2F;test.md | awk -F’|’ ‘{print 2}’ &amp;&gt; &#x2F;tmp&#x2F;example.txt 这些复杂命令，即使使用shell也可能会失败，解决办法：写到脚本时，copy到远程，执行，再把需要的结果拉回执行命令的机器 范例：将shell模块代替command，设为模块 123[root@ansible ~]#vim /etc/ansible/ansible.cfg#修改下面一行module_name = shell 1.3 Script 模块功能：在远程主机上运行ansible服务器上的脚本(无需执行权限) 注意：此模块不具有幂等性 这里的脚本并不仅仅只是 shell脚本，只要远程主机上能执行的，都可以，包括但不限于 php, sh, py 等 常见选项 1234chdir=dir #执行命令前,先切换至目录dircmd #指定ansible主机的命令creates=file #当file不存在时才会执行removes=file #当file存在时才会执行 范例 123456789101112131415161718192021222324252627282930313233343536373839404142#查看脚本[root@ubuntu ~]# cat test.sh#!/bin/bashhostname -I &gt;/tmp/ansible-script.loghostname -I#可以不需要x权限[root@ubuntu ~]# ll test.sh-rw-r--r-- 1 root root 61 Jun 17 12:41 test.sh#测试[root@ubuntu ~]# ansible &quot;10.0.0.150 10.0.0.206&quot; -m script -a &quot;./test.sh&quot; -k#在远程主机上查看log[root@ubuntu ~]# cat /tmp/ansible-script.log10.0.0.206[root@rocky ~]# cat /tmp/ansible-script.log10.0.0.150 192.168.10.150#执行python脚本[root@ubuntu ~]# cat test.py#!/bin/python3import timeprint(&quot;this is python script&quot;);time.sleep(15);[root@ubuntu ~]# ansible &quot;10.0.0.150&quot; -m script -a &quot;./test.py&quot; -k#在远程主机上查看脚本[root@rocky ~]# tree .ansible.ansible└── tmp └── ansible-tmp-1686979905.168881-3117-38009294106067 └── test.py2 directories, 1 file#查看脚本文件内容[root@rocky ~]# cat .ansible/tmp/ansible-tmp-1686979905.168881-3117-38009294106067/test.py#!/bin/python3import timeprint(&quot;this is python script&quot;);time.sleep(15); 2 文件管理相关2.1 Copy 模块功能：复制ansible服务器主控端或远程的本机的文件到远程主机，具有幂等性 注意: src&#x3D;file 如果是没指明路径,则为当前目录或当前目录下的files目录下的file文件 可以是控制端的文件传到被控制端，也可以是被控制端传到被控制端（要加remote_src选项） 常见选项 12345678910src #控制端的源文件路径dest #被控端的文件路径owner #属主group #属组mode #权限backup #是否备份validate #验证成功才会执行copyremote_src #no是默认值,表示src文件在ansible主机,yes表示src文件在远程主机backup=yes|no #是否备份，默认nocontent=str #使用str生成新文件 范例 1234567891011121314#如目标存在，默认覆盖，此处指定先备份ansible webservers -m copy -a &quot;src=/root/test1.sh dest=/tmp/test2.sh owner=wang mode=600 backup=yes&quot;#指定内容，直接生成目标文件 ansible webservers -m copy -a &quot;content=&#x27;wang 123456\\nxiao 654321\\n&#x27; dest=/etc/rsync.pas owner=root group=root mode=0600&quot;#复制/etc目录自身,注意/etc/后面没有/ansible webservers -m copy -a &quot;src=/etc dest=/backup&quot;#复制/etc/下的文件，不包括/etc/目录自身,注意/etc/后面有/ansible webservers -m copy -a &quot;src=/etc/ dest=/backup&quot;#复制/etc/suders,并校验语法ansible webservers -m copy -a &quot;src=/etc/suders dest=/etc/sudoers.edit remote_src=yes validate=/usr/sbin/visudo -csf %s&quot; 范例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[root@ubuntu ~]# echo &quot;this is test file&quot; &gt; test.txt[root@ubuntu ~]# ll test.txt-rw-r--r-- 1 root root 18 Jun 17 13:32 test.txt#将 ansible 主机上的文件 copy 到远程主机[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &quot;src=/root/test.txt dest=/tmp/test.txt.bak&quot;10.0.0.206 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;4a3adea6a7ca4a8ac6ccb8605317572c319786dc&quot;, &quot;dest&quot;: &quot;/tmp/test.txt.bak&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;a7b2aeb79ab928a4598fe9f80241e5f7&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 18, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1686980235.2434347-3212-180319983005615/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;#远程主机上查看[root@ubuntu ~]# ll /tmp/test.txt.bak-rw-r--r-- 1 root root 18 Jun 17 13:37 /tmp/test.txt.bak[root@ubuntu ~]# cat /tmp/test.txt.bakthis is test file#修改文件[root@ubuntu ~]# echo &quot;12345678&quot; &gt;&gt; test.txt[root@ubuntu ~]# cat test.txtthis is test file12345678#备份，指定属主属组，指定权限[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &quot;src=/root/test.txt dest=/tmp/test.txt.bak backup=yes owner=tom group=mage mode=000&quot;10.0.0.206 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;1e853908f0f72eefe7cd0d82332967ecb7f447f9&quot;, &quot;dest&quot;: &quot;/tmp/test.txt.bak&quot;, &quot;gid&quot;: 1000, &quot;group&quot;: &quot;mage&quot;, &quot;mode&quot;: &quot;0000&quot;, &quot;owner&quot;: &quot;tom&quot;, &quot;path&quot;: &quot;/tmp/test.txt.bak&quot;, &quot;size&quot;: 27, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 1001&#125;#在远程主机上查看[root@ubuntu ~]# ll /tmp/test.txt.bak*---------- 1 tom mage 27 Jun 17 13:39 /tmp/test.txt.bak-rw-r--r-- 1 root root 18 Jun 17 13:37 &#x27;/tmp/test.txt.bak.3957.2023-06-17@13:39:40~&#x27;#源和目标都在远程主机上#将远程主机的 abc.txt 再复制一份到 from-ansible.txt[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &quot;src=/root/abc.txt dest=/root/from-ansible.txt remote_src=yes&quot;10.0.0.206 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;03cfd743661f07975fa2f1220c5194cbaff48451&quot;, &quot;dest&quot;: &quot;/root/from-ansible.txt&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;0bee89b07a248e27c83fc3d5951213c1&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 4, &quot;src&quot;: &quot;/root/abc.txt&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;#在远程主机上查看[root@ubuntu ~]# diff abc.txt from-ansible.txt[root@ubuntu ~]# echo $?0 范例：拷贝目录 123456789101112131415161718192021222324252627282930313233343536#将 ansible 主机的 /etc/ansible 目复制到远程主机，保存到 /tmp/ 下[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &#x27;src=/etc/ansible dest=/tmp/&#x27;10.0.0.206 | CHANGED =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/tmp/&quot;, &quot;src&quot;: &quot;/etc/ansible&quot;&#125;#在远程主机上查看[root@ubuntu ~]# ll /tmp/ansible/ -ddrwxr-xr-x 3 root root 4096 Jun 17 13:46 /tmp/ansible//[root@ubuntu ~]# tree /tmp/ansible/tmp/ansible├── ansible.cfg├── hosts└── roles1 directory, 2 files#复制目录下所有文件，将 ansible 主机的 /etc/ansible/ 目录下所有文件复制到远程主机，保存到/tmp/ansible-2/中[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &#x27;src=/etc/ansible/ dest=/tmp/ansible-2&#x27;10.0.0.206 | CHANGED =&gt; &#123; &quot;changed&quot;: true, &quot;dest&quot;: &quot;/tmp/ansible-2/&quot;, &quot;src&quot;: &quot;/etc/ansible/&quot;&#125;[root@ubuntu ~]# ll -d /tmp/ansible-2/drwxr-xr-x 3 root root 4096 Jun 17 13:53 /tmp/ansible-2/[root@ubuntu ~]# tree /tmp/ansible-2//tmp/ansible-2/├── ansible.cfg├── hosts└── roles1 directory, 2 files 范例：幂等性验证 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#从指定内容中生成文件[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &#x27;content=#!/usr/bin/python3\\nprint(&quot;123&quot;) dest=/tmp/test.py&#x27;10.0.0.206 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;checksum&quot;: &quot;94998339860e5d0c935236649e6b2f75bfa6fab6&quot;, &quot;dest&quot;: &quot;/tmp/test.py&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;4351397767a6c8c2af9a27e564147557&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 31, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1686981543.715741-3495-158242536378013/source&quot;, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;[root@ubuntu ~]# ll /tmp/test.py-rw-r--r-- 1 root root 31 Jun 17 13:59 /tmp/test.py[root@ubuntu ~]# cat /tmp/test.py#!/usr/bin/python3print(&quot;123&quot;)#再次执行，绿色，提示成功[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &#x27;content=#!/usr/bin/python3\\nprint(&quot;123&quot;) dest=/tmp/test.py&#x27;10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;checksum&quot;: &quot;94998339860e5d0c935236649e6b2f75bfa6fab6&quot;, &quot;dest&quot;: &quot;/tmp/test.py&quot;, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;path&quot;: &quot;/tmp/test.py&quot;, &quot;size&quot;: 31, &quot;state&quot;: &quot;file&quot;, &quot;uid&quot;: 0&#125;#远程主机上的文件时间戳没有发生变化[root@ubuntu ~]# ll /tmp/test.py-rw-r--r-- 1 root root 31 Jun 17 13:59 /tmp/test.py#如果内容不一样，就会覆盖，因为幂等性是只有内容一样时才会有[root@ubuntu ~]# ansible 10.0.0.206 -m copy -a &#x27;content=#!/usr/bin/python3\\nprint(&quot;456&quot;) dest=/tmp/test.py&#x27;#远程主机[root@ubuntu ~]# ll /tmp/test.py-rw-r--r-- 1 root root 31 Jun 17 14:02 /tmp/test.py 2.2 Get_url 模块功能: 用于将文件从http、https或ftp下载到被管理机节点上 常用参数如下： 1234567891011121314url #下载文件的URL,支持HTTP，HTTPS或FTP协议dest #下载到目标路径（绝对路径），如果目标是一个目录，就用原文件名，如果目标设置了名称就用目标设置的名称owner #指定属主group #指定属组mode #指定权限force #如果yes，dest不是目录，将每次下载文件，如果内容改变替换文件。如果no，则只有在目标不存在时才会下载checksum #对目标文件在下载后计算摘要，以确保其完整性 示例: checksum=&quot;sha256:D98291AC[...]B6DC7B97&quot; checksum=&quot;sha256:http://example.com/path/sha256sum.txt&quot; url_username #用于HTTP基本认证的用户名。 对于允许空密码的站点，此参数可以不使用`url_password&#x27;url_password #用于HTTP基本认证的密码。 如果未指定`url_username&#x27;参数，则不会使用`url_password&#x27;参数validate_certs #如果“no”，SSL证书将不会被验证。 适用于自签名证书在私有网站上使用timeout #URL请求的超时时间,秒为单位 范例: 下载并MD5验证 1[root@ansible ~]#ansible webservers -m get_url -a &#x27;url=http://nginx.org/download/nginx-1.18.0.tar.gz dest=/usr/local/src/nginx.tar.gz checksum=&quot;md5:b2d33d24d89b8b1f87ff5d251aa27eb8&quot;&#x27; 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@ubuntu ~]# wget http://www.jose-404.com/aini.jpg[root@ubuntu ~]# md5sum aini.jpge8fee721ce6e1556e1bf93ae39c7324e aini.jpg[root@ubuntu ~]# ansible 10.0.0.206 -m get_url -a &#x27;url=http://www.jose-404.com/aini.jpg dest=/tmp/a.jpg checksum=&quot;md5:e8fee721ce6e1556e1bf93ae39c7324e&quot;&#x27;10.0.0.206 | CHANGED =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: true, &quot;checksum_dest&quot;: null, &quot;checksum_src&quot;: &quot;a025296e5a5d173e519cec5511c8f590f9e4fa33&quot;, &quot;dest&quot;: &quot;/tmp/a.jpg&quot;, &quot;elapsed&quot;: 0, &quot;gid&quot;: 0, &quot;group&quot;: &quot;root&quot;, &quot;md5sum&quot;: &quot;e8fee721ce6e1556e1bf93ae39c7324e&quot;, &quot;mode&quot;: &quot;0644&quot;, &quot;msg&quot;: &quot;OK (689635 bytes)&quot;, &quot;owner&quot;: &quot;root&quot;, &quot;size&quot;: 689635, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1686981788.8991709-3571-176490408299261/tmpy7aqnoid&quot;, &quot;state&quot;: &quot;file&quot;, &quot;status_code&quot;: 200, &quot;uid&quot;: 0, &quot;url&quot;: &quot;http://www.jose-404.com/aini.jpg&quot;&#125;#在目标主机上查看[root@ubuntu ~]# ll /tmp/a.jpg-rw-r--r-- 1 root root 689635 Jun 17 14:03 /tmp/a.jpg#指定错误的摘要，下载失败[root@ubuntu ~]# ansible 10.0.0.206 -m get_url -a &#x27;url=http://www.jose-404.com/aini.jpg dest=/tmp/a.jpg checksum=&quot;md5:e8fee721ce6e1556e1bf93ae39c7324e123&quot;&#x27;10.0.0.206 | FAILED! =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;checksum_dest&quot;: &quot;a025296e5a5d173e519cec5511c8f590f9e4fa33&quot;, &quot;checksum_src&quot;: &quot;a025296e5a5d173e519cec5511c8f590f9e4fa33&quot;, &quot;dest&quot;: &quot;/tmp/a.jpg&quot;, &quot;elapsed&quot;: 0, &quot;msg&quot;: &quot;The checksum for /tmp/a.jpg did not match e8fee721ce6e1556e1bf93ae39c7324e123; it was e8fee721ce6e1556e1bf93ae39c7324e.&quot;, &quot;src&quot;: &quot;/root/.ansible/tmp/ansible-tmp-1686981839.314159-3599-190398208112020/tmpqjyn1frk&quot;, &quot;url&quot;: &quot;http://www.jose-404.com/aini.jpg&quot;&#125; 2.3 Fetch 模块功能：从远程主机提取文件至ansible的主控端，copy相反，目前不支持目录 常见选项 123src #被控制端的源文件路径,只支持文件dest #ansible控制端的目录路径fail_on_missing=yes|no #默认yes,无法获取远程主机文件时，显示失败 范例 12345678910111213141516171819202122232425262728[root@ubuntu ~]# ansible &quot;10.0.0.206 10.0.0.150&quot; -m fetch -a &quot;src=/etc/issue dest=./ansible-fetch&quot;10.0.0.150 | CHANGED =&gt; &#123; &quot;changed&quot;: true, &quot;checksum&quot;: &quot;2cf4509acb4ba67280d4dff829ef4c550a45bb4b&quot;, &quot;dest&quot;: &quot;/root/ansible-fetch/10.0.0.150/etc/issue&quot;, &quot;md5sum&quot;: &quot;7886cd1627ee2af067882f3ba048cb17&quot;, &quot;remote_checksum&quot;: &quot;2cf4509acb4ba67280d4dff829ef4c550a45bb4b&quot;, &quot;remote_md5sum&quot;: null&#125;10.0.0.206 | CHANGED =&gt; &#123; &quot;changed&quot;: true, &quot;checksum&quot;: &quot;133e53602931fc3b50bc07e2d8fd5af745a4d531&quot;, &quot;dest&quot;: &quot;/root/ansible-fetch/10.0.0.206/etc/issue&quot;, &quot;md5sum&quot;: &quot;d5faf84af6529c6e6a364f164e953ff6&quot;, &quot;remote_checksum&quot;: &quot;133e53602931fc3b50bc07e2d8fd5af745a4d531&quot;, &quot;remote_md5sum&quot;: null&#125;#分开存放[root@ubuntu ~]# tree ansible-fetch/ansible-fetch/├── 10.0.0.150│ └── etc│ └── issue└── 10.0.0.206 └── etc └── issue4 directories, 2 files 2.4 File 模块功能：设置文件属性,创建文件,目录和软链接等 常见选项 123456789101112path #在被控端创建的路径owner #属主group #属组mode #权限state #状态 =touch #创建文件 =directory #创建目录 =link #软链接 =hard #硬链接 =absent #删除recurse #yes表示递归授权recurse=yes|no #仅在state=directory时生效 范例： 12345678910111213141516#创建空文件ansible all -m file -a &#x27;path=/data/test.txt state=touch&#x27;ansible all -m file -a &#x27;path=/data/test.txt state=absent&#x27;ansible all -m file -a &quot;path=/root/test.sh owner=wang mode=755&quot;#创建目录ansible all -m file -a &quot;path=/data/mysql state=directory owner=mysql group=mysql&quot;#创建软链接ansible all -m file -a &#x27;src=/data/testfile path|dest|name=/data/testfile-link state=link&#x27;#递归修改目录属性,但不递归至子目录ansible all -m file -a &quot;path=/data/mysql state=directory owner=mysql group=mysql&quot;#递归修改目录及子目录的属性ansible all -m file -a &quot;path=/data/mysql state=directory owner=mysql group=mysql recurse=yes&quot; 范例 12345678910111213141516171819#创建文件或目录[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;path=/ansible/test state=directory&quot;[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;path=/ansible/test/test.txt state=touch owner=tom group=root mode=777&quot;#递归设置属性[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;path=/ansible/dir2/dir3 state=directory owner=tom group=mage recurse=yes&quot;#获取文件信息[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;path=/ansible/test/test.txt state=file&quot;#创建链接#链接文件可以用dest|name 来指定[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;src=/ansible/test/test.txt dest=/ansible/test/test.link state=link owner=tom group=tom&quot;#删除文件[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;path=/ansible/test/test.link state=absent&quot;#递归删除目录[root@ubuntu ~]# ansible 10.0.0.206 -m file -a &quot;path=/ansible/dir2 state=absent&quot; 2.5 Stat 模块功能：检查文件或文件系统的状态 注意：对于Windows目标，请改用win_stat模块 常见选项 12path #文件/对象的完整路径（必须）follow=true|false #是否跟随链接，默认 false，不跟随 常用的返回值判断： 12exists #判断是否存在isuid #调用用户的ID与所有者ID是否匹配，即是否设置了用户ID位（SUID） 范例 1234567891011121314151617181920212223242526272829303132333435363738394041#文件存在[root@ubuntu ~]# ansible 10.0.0.206 -m stat -a &quot;path=/etc/issue&quot;10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;stat&quot;: &#123; &quot;atime&quot;: 1686968092.6200006, &quot;attr_flags&quot;: &quot;e&quot;, &quot;attributes&quot;: [ &quot;extents&quot; ],......#文件不存在[root@ubuntu ~]# ansible 10.0.0.206 -m stat -a &quot;path=/etc/abcd&quot;10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;stat&quot;: &#123; &quot;exists&quot;: false &#125;&#125;#目录[root@ubuntu ~]# ansible 10.0.0.206 -m stat -a &quot;path=/etc&quot;10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;stat&quot;: &#123; &quot;atime&quot;: 1686971594.3314898, &quot;attr_flags&quot;: &quot;e&quot;, &quot;attributes&quot;: [ &quot;extents&quot; ], ...... 案例：Playbook 12345678- name: install | Check if file is already configured. stat: path=&#123;&#123; nginx_file_path &#125;&#125; connection: local register: nginx_file_result- name: install | Download nginx file get_url: url=&#123;&#123; nginx_file_url &#125;&#125; dest=&#123;&#123; software_files_path &#125;&#125; validate_certs=no connection: local when:，not. nginx_file_result.stat.exists 2.6 Unarchive 模块功能：解包解压缩 实现有两种用法： 将ansible主机上的压缩包传到远程主机后解压缩至特定目录，设置remote_src&#x3D;no,此为默认值,可省略 将远程本主机上或非ansible的其它主机的某个压缩包解压缩到远程主机本机的指定路径下，需要设置remote_src&#x3D;yes 在使用此模块时，要保证远程主机能解压对应的压缩包 常见参数 12345678910remote_src #和copy功能一样且选项互斥，yes表示源文件在远程被控主机或其它非ansible的其它主机上，no表示文件在ansible主机上,默认值为no, 此选项代替copy选项copy #默认为yes，当copy=yes，拷贝的文件是从ansible主机复制到远程主机上，如果设置为copy=no，会在远程主机上寻找src源文件,此选项已废弃src #源路径，可以是ansible主机上的路径，也可以是远程主机(被管理端或者第三方主机)上的路径，如果是远程主机上的路径，则需要设置remote_src=yesdest #远程主机上的目标路径owner #默认递归group #默认递归mode #设置解压缩后的文件权限,默认递归creates=/path/file #当绝对路径/path/file不存在时才会执行owner=USER #设置目标属主group=GROUP #设置目标属组 范例 1234567#将ansible主机上的压缩包解压至远程主机,远程目录必须存在[root@ubuntu ~]# ansible 10.0.0.206 -m unarchive -a &quot;src=/root/dira.tar.gz dest=/tmp/test-dir owner=tom&quot;#若不存在，出现以下错误An exception occurred during task execution. To see the full traceback, use -vvv. The error was: NoneType: None#将第三方包解压缩到指定主机[root@ubuntu ~]# ansible 10.0.0.206 -m unarchive -a &quot;src=http://www.jose-404.com/web-dir.tar dest=/tmp remote_src=yes mode=777&quot; 范例 123456789ansible all -m unarchive -a &#x27;src=/data/foo.tgz dest=/var/lib/foo owner=wang group=bin&#x27;ansible all -m unarchive -a &#x27;src=/tmp/foo.zip dest=/data copy=no mode=0777&#x27;ansible all -m unarchive -a &#x27;src=https://example.com/example.zip dest=/data remote_src=yes&#x27;ansible all -m unarchive -a &#x27;src=https://nginx.org/download/nginx-1.20.2.tar.gz dest=/data remote_src=yes owner=wang group=wang&#x27;ansible webservers -m unarchive -a &#x27;src=https://releases.ansible.com/ansible/ansible-2.1.6.0-0.1.rc1.tar.gz dest=/data/ owner=root remote_src=yes&#x27; 2.7 Archive 模块功能：打包压缩保存在被管理节点 此模块的源和目标都在远程主机上 常见选项 1234567path #压缩的文件或目录dest #压缩后的文件format #压缩格式,支持gz,bz2,xz,tar,zipremove=false|true #是否删除源文件,默认 falsemode=777 #设置目标权限owner=USER #设置目标属主group=GROUP #设置目标属组 安裝 123456789#安装模块[root@ubuntu ~]# ansible-galaxy collection install community.generalStarting galaxy collection install processProcess install dependency mapERROR! Unexpected Exception, this is probably a bug:CollectionDependencyProvider.find_matches() got an unexpected keyword argument &#x27;identifier&#x27; to see the full traceback, use -vvv[root@ubuntu ~]# pip install -Ivvv &quot;resolvelib &gt;= 0.5.3, &lt; 0.6.0&quot;[root@ubuntu ~]# ansible-galaxy collection install community.general 范例 12345#有一台主机上没有要打包的目录[root@ubuntu ~]# ansible &quot;10.0.0.150 10.0.0.206&quot; -m archive -a &quot;path=/root/dira/ dest=/tmp/dira.bz2 format=bz2 mode=600&quot;#不保留源目录[root@ubuntu ~]# ansible 10.0.0.206 -m archive -a &quot;path=/root/dira/ dest=/tmp/dira.zip format=zip remove=true&quot; 范例 1ansible webservers -m archive -a &#x27;path=/var/log/ dest=/data/log.tar.bz2 format=bz2 owner=wang mode=0600&#x27; 2.8 Lineinfile 模块ansible在使用sed进行替换时，经常会遇到需要转义的问题，而且ansible在遇到特殊符号进行替换时，会存在问题，无法正常进行替换 ansible自身提供了两个模块：lineinfile模块和replace模块，可以方便的进行替换一般在ansible当中去修改某个文件的单行进行替换的时候需要使用lineinfile模块 功能：相当于sed，主要用于修改一行的文件内容 常见选项 123456789101112path #被控端文件的路径regexp #正则匹配语法格式,表示被替换的内容line #替换为的内容state #absent表示删除insertafter #插入到替换内容后面,如和regexp同时存在,只在没找到与regexp匹配时才使用insertafterinsertbefore #插入到替换内容前面,如和regexp同时存在,只在没找到与regexp匹配时才使用insertafterbackrefs #支持后面引用,yes和nobackup #修改前先备份create #如果文件不存在,则创建,默认不存在会出错mode #指定权限owner #指定用户group #指定组 注意: regexp参数 ：使用正则表达式匹配对应的行，当替换文本时，如果有多行文本都能被匹配，则只有最后面被匹配到的那行文本才会被替换，当删除文本时，如果有多行文本都能被匹配，这么这些行都会被删除。 如果想进行多行匹配进行替换需要使用replace模块 范例： 123456789101112131415161718192021222324252627282930#修改监听端口ansible webservers -m lineinfile -a &quot;path=/etc/httpd/conf/httpd.conf regexp=&#x27;^Listen&#x27; line=&#x27;Listen 8080&#x27;&quot;#修改SELinuxansible all -m lineinfile -a &quot;path=/etc/selinux/config regexp=&#x27;^SELINUX=&#x27; line=&#x27;SELINUX=disabled&#x27;&quot;#添加网关ansible webservers -m lineinfile -a &#x27;path=/etc/sysconfig/network-scripts/ifcfg-eth0 line=&quot;GATEWAY=10.0.0.254&quot;&#x27;#给主机增加一个网关，但需要增加到NAME=下面ansible webservers -m lineinfile -a &#x27;path=/etc/sysconfig/network-scripts/ifcfg-eth0 insertafter=&quot;^NAME=&quot; line=&quot;GATEWAY=10.0.0.254&quot;&#x27;#效果如下cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0NAME=eth0GATEWAY=10.0.0.254#给主机增加一个网关，但需要增加到NAME=上面ansible webservers -m lineinfile -a &#x27;path=/etc/sysconfig/network-scripts/ifcfg-eth0 insertbefore=&quot;^NAME=&quot; line=&quot;GATEWAY=10.0.0.254&quot;&#x27;#效果如下cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0GATEWAY=10.0.0.254NAME=eth0#删除网关ansible webservers -m lineinfile -a &#x27;path=/etc/sysconfig/network-scripts/ifcfg-eth0 regexp=&quot;^GATEWAY&quot; state=absent&#x27;#删除#开头的行ansible all -m lineinfile -a &#x27;dest=/etc/fstab state=absent regexp=&quot;^#&quot;&#x27; 范例：替换 12345678910111213141516171819202122#在远程主机上查看[root@ubuntu ~]# cat -n test.txt 1 aaaa 2 bbbb 3 tom-123 4 tom-456 5 tom-789 6 jerry-123 7 jerry-456 #替换[root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &#x27;path=/root/test.txt regexp=&quot;^tom&quot; line=&quot;TOM&quot;&#x27;#再次查看[root@ubuntu ~]# cat -n test.txt 1 aaaa 2 bbbb 3 tom-123 4 tom-456 5 TOM 6 jerry-123 7 jerry-456 范例：新增内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#追加一行[root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &#x27;path=/root/test.txt line=&quot;abcdefg&quot;&#x27;#再次查看[root@ubuntu ~]# cat -n test.txt 1 aaaa 2 bbbb 3 tom-123 4 tom-456 5 TOM 6 jerry-123 7 jerry-456 8 abcdefg [root@ubuntu ~]# cat -n test2.txt 1 123 2 123 3 abc 4 ABCDEFG 5 123456 #在最后一行123后面插入新行，锚定整行[root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &#x27;path=/root/test2.txt insertafter=&quot;^123$&quot; line=&quot;AAAAAA&quot;&#x27;#再次查看[root@ubuntu ~]# cat -n test2.txt 1 123 2 123 3 AAAAAA 4 abc 5 ABCDEFG 6 123456 #描定开头，在以123 开头的行的上面插入新行[root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &#x27;path=/root/test2.txt insertbefore=&quot;^123&quot; line=&quot;BBBBBB&quot;&#x27;[root@ubuntu ~]# cat -n test2.txt 1 123 2 123 3 AAAAAA 4 abc 5 ABCDEFG 6 BBBBBB 7 123456 #删除文件中所有以 123 开头的行，修改前先备份[root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &#x27;path=/root/test2.txt regexp=&quot;^123&quot; backup=yes state=absent&#x27;#在远程主机上查看[root@ubuntu ~]# ls test2.txt*test2.txt test2.txt.5588.2023-06-19@10:02:14~[root@ubuntu ~]# cat test2.txtAAAAAAabcABCDEFGBBBBBB[root@ubuntu ~]# cat test2.txt.5588.2023-06-19@10\\:02\\:14~123123AAAAAAabcABCDEFGBBBBBB123456 范例：文件不存在，创建新文件 1[root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &#x27;path=/root/test3.txt line=&quot;ABCD&quot; create=yes owner=tom mode=777&#x27; 范例：匹配引用 123456789101112131415161718[root@ubuntu ~]# cat -n test4.txt 1 AAAAAA 2 abc 3 ABCDEFG 4 BBBBBB 5 abcdefg 6 ABC123 [root@ubuntu ~]# ansible 10.0.0.206 -m lineinfile -a &quot;path=/root/test4.txt regexp=&#x27;^ABC(.*)$&#x27; backrefs=yes line=&#x27;XYZ\\1&#x27;&quot;#再次查看[root@ubuntu ~]# cat -n test4.txt 1 AAAAAA 2 abc 3 ABCDEFG 4 BBBBBB 5 abcdefg 6 XYZ123 2.9 Replace 模块该模块有点类似于sed命令，主要也是基于正则进行匹配和替换，建议使用 功能: 多行修改替换 常见选项 123456789path #被控端文件的路径regexp #正则匹配语法格式,表示被替换的内容replace #替换为的内容after #插入到替换内容前面before #插入到替换内容后面backup #修改前先备份mode #指定权限owner #指定用户group #指定组 范例 1234567891011121314151617181920212223[root@ubuntu ~]# cat -n test5.txt 1 123 2 123 3 456 4 abcdefg 5 ABCDEFG 6 123root #替换所有3个数字的行[root@ubuntu ~]# ansible 10.0.0.206 -m replace -a &quot;path=/root/test5.txt regexp=&#x27;^([0-9]&#123;3&#125;)$&#x27; replace=&#x27;\\1---\\1&#x27; backup=yes&quot;[root@ubuntu ~]# cat -n test5.txt 1 123---123 2 123---123 3 456---456 4 abcdefg 5 ABCDEFG 6 123root #备份文件[root@ubuntu ~]# ll test5.txt*-rw-r--r-- 1 root root 54 Jun 19 10:22 test5.txt-rw-r--r-- 1 root root 36 Jun 19 10:21 &#x27;test5.txt.5936.2023-06-19@10:22:08~&#x27; 范例 12345678910111213141516[root@ubuntu ~]# cat -n test6.txt 1 aaa 2 123 3 456 4 bbb 5 ccc #只处理 aaa行到bbb行中间的内容[root@ubuntu ~]# ansible 10.0.0.206 -m replace -a &quot;path=/root/test6.txt after=&#x27;aaa&#x27; before=&#x27;bbb&#x27; regexp=&#x27;^(.+)$&#x27; replace=&#x27;# \\1&#x27;&quot;[root@ubuntu ~]# cat -n test6.txt 1 aaa 2 # 123 3 # 456 4 bbb 5 ccc 范例 12ansible all -m replace -a &quot;path=/etc/fstab regexp=&#x27;^(UUID.*)&#x27; replace=&#x27;#\\1&#x27;&quot; ansible all -m replace -a &quot;path=/etc/fstab regexp=&#x27;^#(UUID.*)&#x27; replace=&#x27;\\1&#x27;&quot; 范例：有一个 Nginx 配置文件，你想在 server_name 设置之后，但在 listen 指令之前添加一个新的服务器块 1234567- name: Add new server block after server_name and before listen ansible.builtin.replace: path: /etc/nginx/sites-available/default after: &#x27;^server_name&#x27; before: &#x27;^listen&#x27; regexp: &#x27;(?m)^#?\\\\s*$&#x27; replace: &#x27; location /new_path &#123;\\n root /var/www/html;\\n index index.html index.htm;\\n &#125;\\n&#x27; after: &#39;^server_name&#39; 确保替换发生在 server_name 指令之后。 before: &#39;^listen&#39; 确保替换发生在 listen 指令之前。 regexp: &#39;(?m)^#?\\\\s*$&#39; 是一个正则表达式，用于匹配行首的零个或多个空白字符或注释符 #，这通常是空行或注释行。 replace 参数指定了要插入的新文本。 3 用户和组相关3.1 User 模块功能：管理用户 常见选项 1234567891011121314151617name=USERNAME #指定用户名comment=str #用户描述信息create_home=yes|no #是否创建家目录,默认yesgroup=GROUPNAME #指定私有组groups=group1,group2... #指定附加组append #追加附加组使用,yes表示增加新的附加组home=/path #指定家目录路径shell=SHELL #指定shell，默认/bin/bashpassword=str #设置密码，必须是加密后的字符串，否则不生效state=absent|present #absent 删除用户,present 创建用户，默认 presentsystem=yes|no #是否创建系统账号，默认 nouid=UID #手动指定uidumask=UMASK #指定umaskremove=yes|no #删除用户时将家目录一起删除,默认 nogenerate_ssh_key=yes|no #是否创建私钥，默认 nossh_key_bits=2048 #指定私钥位数ssh_key_file=/path/file #指定私钥文件位置,默认 .ssh/id_rsa 范例 12345678910111213141516171819202122232425262728293031#创建用户[root@ubuntu ~]# ansible 10.0.0.206 -m user -a &#x27;name=user1 comment=&quot;ansible user&quot; uid=2048 home=/user1/&#x27;#在远程主机上查看[root@ubuntu ~]# getent passwd user1user1:x:2048:2048:ansible user:/user1/:/bin/sh#生成密码[root@ubuntu ~]# openssl passwd -6 123456$6$yaZYTO7B1gnMCfoK$MmKbbYKOX2c7Cx8mXBLSp1JGYbr6ZNTq0qAymnbU4dbTvLcuRmyOL5GPKSLpuYxDl2Ir7rkNr4gnuSlC0KSi10#创建系统用户，不创建家目录，指定shell，指定组，指定密码,组要先存在[root@ubuntu ~]# ansible 10.0.0.206 -m user -a &#x27;name=user2 comment=&quot;ansible user&quot; system=yes create_home=no shell=/sbin/nologin group=gb password=&quot;$6$yaZYTO7B1gnMCfoK$MmKbbYKOX2c7Cx8mXBLSp1JGYbr6ZNTq0qAymnbU4dbTvLcuRmyOL5GPKSLpuYxDl2Ir7rkNr4gnuSlC0KSi10&quot;&#x27;#在远程主机上查看[root@ubuntu ~]# getent passwd user2user2:x:998:123:ansible user:/home/user2:/sbin/nologin/[root@ubuntu ~]# getent shadow user2user2:$6$yaZYTO7B1gnMCfoK$MmKbbYKOX2c7Cx8mXBLSp1JGYbr6ZNTq0qAymnbU4dbTvLcuRmyOL5GPKSLpuYxDl2Ir7rkNr4gnuSlC0KSi10:19527::::::#创建用户并生成密钥[root@ubuntu ~]# ansible 10.0.0.206 -m user -a &#x27;name=user3 generate_ssh_key=yes ssh_key_bits=2048&#x27;#在远程主机上查看[root@ubuntu ~]# ls /home/user3/.ssh/id_rsa id_rsa.pub#删除用户[root@ubuntu ~]# ansible 10.0.0.206 -m user -a &#x27;name=user3 remove=yes state=absent&#x27;[root@ubuntu ~]# ansible 10.0.0.206 -m user -a &#x27;name=user2 remove=yes state=absent&#x27;[root@ubuntu ~]# ansible 10.0.0.206 -m user -a &#x27;name=user1 remove=yes state=absent&#x27; 范例 12345678910111213141516#创建用户ansible all -m user -a &#x27;name=user1 comment=&quot;test user&quot; uid=2048 home=/app/user1 group=root&#x27;ansible all -m user -a &#x27;name=nginx comment=nginx uid=88 group=nginx groups=&quot;root,daemon&quot; shell=/sbin/nologin system=yes create_home=no home=/data/nginx non_unique=yes&#x27;#remove=yes表示删除用户及家目录等数据,默认remove=noansible all -m user -a &#x27;name=nginx state=absent remove=yes&#x27;#生成123456加密的密码ansible localhost -m debug -a &quot;msg=&#123;&#123; &#x27;123456&#x27;| password_hash(&#x27;sha512&#x27;,&#x27;salt&#x27;)&#125;&#125;&quot;#用上面创建的密码创建用户ansible webservers -m user -a &#x27;name=www group=www system=yes shell=/sbin/nlogin password=&quot;$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.&quot;&#x27;#创建用户test,并生成4096bit的私钥ansible webservers -m user -a &#x27;name=test generate_ssh_key=yes ssh_key_bits=4096 ssh_key_file=.ssh/id_rsa&#x27; 3.2 Group 模块功能：管理组 常见选项 123456name #指定组名称gid #指定gidstate =present #创建,默认 =absent #删除system=yes|no #是否是系统组，默认no 范例 12345678#创建普通组[root@ubuntu ~]# ansible 10.0.0.206 -m group -a &#x27;name=ga gid=1024&#x27;#创建系统组[root@ubuntu ~]# ansible 10.0.0.206 -m group -a &#x27;name=gb gid=888 system=yes&#x27;#删除组[root@ubuntu ~]# ansible 10.0.0.206 -m group -a &#x27;name=ga state=absent&#x27; 4 仓库管理相关4.1 yum 模块功能：管理软件包 yum 管理软件包，只支持RHEL，CentOS，fedora，不支持Ubuntu其它版本 yum常见选项 1234567891011121314151617181920212223state #状态 =present #安装,此为默认值 =absent #删除 =latest #最新版list #列出指定包，此选项与name选项互斥，写具体包名是相当于执行yum list --showduplicates packagename =packagename =installed =updates =available =repos name #软件包名称enablerepo #启用哪个仓库安装disablerepo #不使用哪些仓库的包exclude #排除指定的包validate #是否检验,默认为yesdownload_dir=/path #指定下载目录download_only=yes|no #只下载不安装，默认 noupdate_only=yes|no #yes 仅更新,默认nouse_backend=auto|yum|yum4|dnf #指定真实执行的命令,默认 autoautoremove=yes|no #卸载依赖，仅在卸载时生效，默认novalidate_certs=yes|no #是否对包进行校验，默认yesdisable_gpg_check=yes|no #是否不对包进行校验，默认noupdate_only=yes|no #只更新不安装,默认no 范例 123456789101112131415#列出指定软件包，相当于 yum list --showduplicates nginx[root@ubuntu ~]# ansible 10.0.0.150 -m yum -a &#x27;list=nginx&#x27;#列出所有 repo包[root@ubuntu ~]# ansible 10.0.0.150 -m yum -a &#x27;list=repos&#x27;#卸载[root@ubuntu ~]# ansible 10.0.0.150 -m yum -a &#x27;name=sos state=removed&#x27;#从指定源安装[root@ubuntu ~]# ansible 10.0.0.150 -m yum -a &#x27;name=sos enablerepo=baseos&#x27;#从 rpm 包中安装，不校验 ssl 证书，不校验包[root@ubuntu ~]# ansible 10.0.0.150 -m yum -a&#x27;name=https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/6.0/rhel/8/x86_64/zabbix-agent-6.0.0-1.el8.x86_64.rpm state=present validate_certs=no disable_gpg_check=yes&#x27; 范例 123456789101112131415#安装[root@ansible ~]#ansible webservers -m yum -a &#x27;name=httpd state=present&#x27; #安装zabbix agent rpm包[root@ansible ~]#ansible webservers -m yum -a &#x27;name=https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/5.0/rhel/8/x86_64/zabbix-agent2-5.0.24-1.el8.x86_64.rpm state=present validate_certs=no&#x27;#启用epel源进行安装[root@ansible ~]#ansible webservers -m yum -a &#x27;name=nginx state=present enablerepo=epel&#x27; #升级除kernel和foo开头以外的所有包[root@ansible ~]#ansible webservers -m yum -a &#x27;name=* state=lastest exclude=kernel*,foo*&#x27;#删除[root@ansible ~]#ansible webservers -m yum -a &#x27;name=httpd state=absent&#x27; [root@ansible ~]#ansible webservers -m yum -a &#x27;name=sl,cowsay&#x27; 4.2 yum_repository 模块功能: 此模块实现yum的仓库配置管理 选项 12345678name #仓库iddescription #仓库描述名称,对应配置文件中的name=baseurl #仓库的地址gpgcheck #验证开启gpgkey #仓库公钥路径enabled=yes|no #是否启用state=absent|present #absent删除， present安装，默认presenttimeout=30 #超时时长，默认30s 范例 12345#添加yum 源[root@ubuntu ~]# ansible 10.0.0.150 -m yum_repository -a &#x27;name=nginx description=nginx-desc baseurl=&quot;http://nginx.org/packages/centos/$releasever/$basearch/&quot; gpgcheck=1 enabled=1 gpgkey=https://nginx.org/keys/nginx_signing.key&#x27;#删除[root@ubuntu ~]# ansible 10.0.0.150 -m yum_repository -a &#x27;name=nginx state=absent&#x27; 范例：安装zabbix-agent 123root@ansible ]#ansible 10.0.0.8 -m yum_repository -a &#x27;name=zabbix description=&quot;zabbix repo&quot; baseurl=&quot;https://mirrors.aliyun.com/zabbix/zabbix/6.0/rhel/$releasever/$basearch/&quot; gpgcheck=no&#x27;[root@ansible ~]#ansible 10.0.0.8 -m yum -a &#x27;name=zabbix-agent2&#x27; 范例: 12345678ansible webservers -m yum_repository -a &#x27;name=ansible_nginx description=&quot;nginx repo&quot; baseurl=&quot;http://nginx.org/packages/centos/$releasever/$basearch/&quot; gpgcheck=yes gpgkey=&quot;https://nginx.org/keys/nginx_signing.key&quot;&#x27;[root@rocky8 ~]#cat /etc/yum.repos.d/ansible_nginx.repo[ansible_nginx]baseurl = http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck = 1gpgkey = https://nginx.org/keys/nginx_signing.keyname = nginx repo 范例: 12345678910111213141516171819202122- name: Add multiple repositories into the same file (1/2) yum_repository: name: epel description: EPEL YUM repo file: external_repos baseurl: https://download.fedoraproject.org/pub/epel/$releasever/$basearch/ gpgcheck: no - name: Add multiple repositories into the same file (2/2) yum_repository: name: rpmforge description: RPMforge YUM repo file: external_repos baseurl: http://apt.sw.be/redhat/el7/en/$basearch/rpmforge mirrorlist: http://mirrorlist.repoforge.org/el7/mirrors-rpmforge enabled: no - name: Remove repository from a specific repo file yum_repository: name: epel file: external_repos state: absent 范例: 创建和删除仓库 1234567891011121314151617181920212223242526272829[root@ansible ~]#cat yum_repo.yml- hosts: webservers tasks: - name: Add multiple repositories into the same file yum_repository: name: test description: EPEL YUM repo file: external_repos baseurl:https://download.fedoraproject.org/pub/epel/$releasever/$basearch/ gpgcheck: no [root@ansible ~]#ansible-playbook yum_repo.yml[root@web1 ~]#cat /etc/yum.repos.d/external_repos.repo[test]baseurl = https://download.fedoraproject.org/pub/epel/$releasever/$basearch/gpgcheck = 0name = EPEL YUM repo[root@ansible ~]#cat remove_yum_repo.yml- hosts: webservers tasks: - name: remove repo yum_repository: name: test file: external_repos state: absent [root@ansible ~]#ansible-playbook remove_yum_repo.yml 范例: 常见仓库 123456789101112131415161718192021222324252627282930313233343536- name: Add Base Yum Repository yum_repository: name: base description: Base Aliyun Repository baseurl: http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ gpgcheck: yes gpgkey: http://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-7 - name: Add Epel Yum Repository yum_repository: name: epel description: Epel Aliyun Repository baseurl: http://mirrors.aliyun.com/epel/7/$basearch gpgcheck: no - name: Add Nginx Yum Repository yum_repository: name: nginx description: Nginx Repository baseurl: http://nginx.org/packages/centos/7/$basearch/ gpgcheck: no - name: Add PHP Yum Repository yum_repository: name: php71w description: php Repository baseurl: http://us-east.repo.webtatic.com/yum/el7/x86_64/ gpgcheck: no - name: Add Haproxy Yum Repository yum_repository: name: haproxy description: haproxy repository baseurl: https://repo.ius.io/archive/7/$basearch/ gpgcheck: yes gpgkey: https://repo.ius.io/RPM-GPG-KEY-IUS-7 4.3 apt 模块apt 模块管理 Debian 相关版本的软件包 apt 选项 1234567name=packagename #指定包名，可用通配符autoclean=yes|no #清除本地安装包，只删除己卸载的软件的 deb包deb=/path/file.deb #指定deb包，可以是本地的，也可以是网络的autoremove=yes|no #卸载依赖包,默认noonly_upgrade=yes|no #仅更新，不安装update_cache=yes|no #更新索引state=absent|build-dep|latest|present|fixed #absent卸载，build-dep安装依赖包,latest安装或升级到最新版,present安装，fixed修复 范例 1234567891011121314#更新索引[root@ubuntu ~]# ansible 10.0.0.206 -m apt -a &quot;update_cache=yes&quot;#清空缓存[root@ubuntu ~]# ansible 10.0.0.206 -m apt -a &quot;autoclean=yes&quot;#安装指定版本[root@ubuntu ~]# ansible 10.0.0.206 -m apt -a &quot;name=nginx=1.18.0-6ubuntu14.3 state=present&quot;#卸载[root@ubuntu ~]# ansible 10.0.0.206 -m apt -a &quot;name=nginx* state=absent&quot;#将所有包升级到最新版[root@ubuntu ~]# ansible 10.0.0.206 -m apt -a &quot;name=* state=latest&quot; 范例: Ubuntu 安装软件 123[root@centos8 ~]#ansible 10.0.0.100 -m apt -a &#x27;name=bb,sl,cowsay,cmatrix,oneko,hollywood,boxes,libaa-bin,x11-apps update_cache=yes&#x27;[root@centos8 ~]#ansible webservers -m apt -a &#x27;name=rsync,psmisc state=absent&#x27; 4.4 apt_repository 模块功能: 此模块实现apt的仓库配置管理 常见选项 1234repo=str #仓库信息，具体源filename=/path/file #仓库文件名，默认加到 /etc/apt/sources.list 中state=absent|present #absent 删除,present 新增，默认 presentupdate_cache=yes|no #yes 更新源，相当于执行 apt-get update 范例 1234567891011[root@ubuntu ~]# ansible 10.0.0.206 -m apt_repository -a &#x27;repo=&quot;deb http://dl.google.com/linux/chrome/deb/ stable main&quot; filename=&quot;chrome&quot; update_cache=no&#x27;#远程主机上查看[root@ubuntu ~]# ls /etc/apt/sources.list.d/chrome.list[root@ubuntu ~]# cat /etc/apt/sources.list.d/chrome.listdeb http://dl.google.com/linux/chrome/deb/ stable main#删除源[root@ubuntu ~]# ansible 10.0.0.206 -m apt_repository -a &#x27;repo=&quot;deb http://dl.google.com/linux/chrome/deb/ stable main&quot; filename=&quot;chrome&quot; state=absent&#x27; 4.5 apt_key 模块功能: 添加和删除 apt key 常见选项 123url=URL #key文件路径validate_certs=yes|no #是否校验https 的 ssl 证书，默认yesstate=absent|present #absent 删除，present 新增，默认 present 范例 12345#添加key[root@ubuntu ~]# ansible 10.0.0.206 -m apt_key -a &#x27;url=https://download.ceph.com/keys/release.asc state=present&#x27;#远程主机上查看[root@ubuntu ~]# apt-key list 范例: 生成ceph仓库配置 123456789#先导入key,注意先后顺序ansible ubuntu-servers -m apt_key -a &#x27;url=https://download.ceph.com/keys/release.asc state=present&#x27;#再生成apt配置,如果不导入key此步会出错ansible ubuntu-servers -m apt_repository -a &#x27;repo=&quot;deb http://mirror.tuna.tsinghua.edu.cn/ceph/debian-pacific focal main&quot; filename=ansible_ceph&#x27;#验证结果[root@ubuntu2004 ~]#cat /etc/apt/sources.list.d/ansible_ceph.listdeb http://mirror.tuna.tsinghua.edu.cn/ceph/debian-pacific focal main 5 系统配置相关5.1 Hostname 模块功能：管理主机名 修改后永久生效 常见选项 1name #修改后的主机名称 范例：Ubuntu上对rocky使用hostname模块失败，原因是兼容性的问题，需要改源代码 123456789101112131415[root@ubuntu2004 ~]#ansible 10.0.0.180 -m hostname -a name=&#x27;web&#x27;10.0.0.180 | FAILED! =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/libexec/platform-python&quot; &#125;, &quot;changed&quot;: false, &quot;msg&quot;: &quot;hostname module cannot be used on platform Linux (Rocky)&quot;&#125;#加上这一段代码[root@ubuntu2004 ~]#vim /usr/lib/python3/dist-packages/ansible/modules/system/hostname.py +721class ALTLinuxHostname(Hostname): platform = &#x27;Linux&#x27; distribution = &#x27;Rocky&#x27; strategy_class = RedHatStrategy 5.2 Cron 模块功能：计划任务 支持时间：minute，hour，day，month，weekday 常见选项 123456789101112name=str #任务名称,描述脚本的作用job=/path/cmd args #具体任务命令disabled=yes|no #是否禁用，默认 falsestate=absent|present #absent 删除,默认presentenv=yes|no #yes设置环境变量，no 设置定时任务，默认 nouser=UNAME #指定任务用户,默认rootspecial_time=annually|daily|hourly|monthly|reboot|weekly|yearly #指定特殊时间minute= #指定分钟参数hour= #小时参数day= #自然天参数month= #自然月参数weekday= #星期参数 范例 1234567891011121314151617181920212223242526272829303132#创建[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;job=&quot;/usr/bin/wall test-crontab&quot; name=test-crontab&#x27;#指定时间[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;job=&quot;/usr/bin/wall test-crontab2&quot; minute=*/5 hour=12 day=10,20,30 month=3,4,5 weekday=1-5 name=test-crontab2&#x27;#在远程主机上查看[root@ubuntu ~]# crontab -l#Ansible: test-crontab* * * * * /usr/bin/wall test-crontab#Ansible: test-crontab2*/5 12 10,20,30 3,4,5 1-5 /usr/bin/wall test-crontab2#修改[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;job=&quot;/usr/bin/wall test-crontab-new-msg&quot; name=test-crontab&#x27;#再次查看[root@ubuntu ~]# crontab -l#Ansible: test-crontab* * * * * /usr/bin/wall test-crontab-new-msg #此处被修改#Ansible: test-crontab2*/5 12 10,20,30 3,4,5 1-5 /usr/bin/wall test-crontab2#禁用[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;job=&quot;/usr/bin/wall test-crontab-new-msg&quot; name=test-crontab disabled=yes&#x27;#再次查看[root@ubuntu ~]# crontab -l#Ansible: test-crontab#* * * * * /usr/bin/wall test-crontab-new-msg#Ansible: test-crontab2*/5 12 10,20,30 3,4,5 1-5 /usr/bin/wall test-crontab2 范例：设置环境变量 12345678[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;name=APP_HOME job=/dir/app env=yes&#x27;[root@ubuntu ~]# crontab -lAPP_HOME=&quot;/dir/app&quot;#Ansible: test-crontab#* * * * * /usr/bin/wall test-crontab-new-msg#Ansible: test-crontab2*/5 12 10,20,30 3,4,5 1-5 /usr/bin/wall test-crontab2 范例：删除 12345678910#删除变量[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;name=APP_HOME job=/dir/app env=yes state=absent&#x27;#删除定时任务[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;job=&quot;/usr/bin/wall test-crontab-new-msg&quot; name=test-crontab state=absent&#x27;#查看远程主机[root@ubuntu ~]# crontab -l#Ansible: test-crontab2*/5 12 10,20,30 3,4,5 1-5 /usr/bin/wall test-crontab2 为远程主机上的指定用户创建 123456[root@ubuntu ~]# ansible 10.0.0.206 -m cron -a &#x27;job=&quot;/usr/bin/wall test-crontab3&quot; user=tom name=test-crontab&#x27;#在远程主机上查看[root@ubuntu ~]# crontab -u tom -l#Ansible: test-crontab* * * * * /usr/bin/wall test-crontab3 范例： 123456789101112131415161718#备份数据库脚本[root@centos8 ~]#cat /root/mysql_backup.sh#!/bin/bashmysqldump -A -F --single-transaction --master-data=2 -q -uroot |gzip &gt; /data/mysql_`date +%F_%T`.sql.gz#创建任务ansible 10.0.0.8 -m cron -a &#x27;hour=2 minute=30 weekday=1-5 name=&quot;backup mysql&quot; job=/root/mysql_backup.sh&#x27;ansible webservers -m cron -a &quot;minute=*/5 job=&#x27;/usr/sbin/ntpdate ntp.aliyun.com &amp;&gt;/dev/null&#x27; name=Synctime&quot;#禁用计划任务ansible webservers -m cron -a &quot;minute=*/5 job=&#x27;/usr/sbin/ntpdate 172.20.0.1 &amp;&gt;/dev/null&#x27; name=Synctime disabled=yes&quot;#启用计划任务ansible webservers -m cron -a &quot;minute=*/5 job=&#x27;/usr/sbin/ntpdate 172.20.0.1 &amp;&gt;/dev/null&#x27; name=Synctime disabled=no&quot;#删除任务ansible webservers -m cron -a &quot;name=&#x27;backup mysql&#x27; state=absent&quot;ansible webservers -m cron -a &#x27;state=absent name=Synctime&#x27; 5.3 Service 模块此模块和sytemd功能相似,选项很多相同 功能：管理服务 常见选项 123456789name #服务名称state #服务状态 =started #启动 =stopped #停止 =restarted #重启 =reloaded #重载enabled #开启自启动daemon_reload #加载新的配置文件,适用于systemd模块args=val #参数 范例 1234567891011#启动服务[root@ubuntu ~]# ansible 10.0.0.206 -m service -a &quot;name=nginx state=started&quot;#重载[root@ubuntu ~]# ansible 10.0.0.206 -m service -a &#x27;name=nginx state=reloaded&#x27;#加开机启动[root@ubuntu ~]# ansible 10.0.0.206 -m service -a &#x27;name=nginx enabled=yes&#x27;#重启动指定网卡服务ansible all -m service -a &#x27;name=network state=absent args=eth0&#x27; 5.4 SELinux 模块功能: 该模块管理 SELinux 策略 常见选项 123policy #指定SELINUXTYPE=targetedstate #指定SELINUX=disabledconfigfile=/path/file #selinux 配置文件路径，默认/etc/selinux/config 范例 12345678910111213[root@ubuntu ~]# ansible 10.0.0.150 -m selinux -a &#x27;state=enforcing policy=targeted&#x27;#python3.9没有相关模块，需要修改远程主机的python路径[root@rocky86 ~]# ansible 10.0.0.150 -m selinux -a &#x27;state=disabled policy=targeted&#x27;An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ModuleNotFoundError: No module named &#x27;selinux&#x27;#用 -e 选项指定远程主机的python[root@rocky86 ~]# ansible 10.0.0.150 -m selinux -e &#x27;ansible_python_interpreter=/usr/bin/python3.6&#x27; -a &#x27;state=enforcing policy=targeted&#x27;#或者修改配置[root@ubuntu ~]# vim /etc/ansible/hosts##10.0.0.150 ansible_python_interpreter=/usr/bin/python3.9 ansible_ssh_password=12345610.0.0.150 ansible_python_interpreter=/usr/bin/python3.6 ansible_ssh_password=123456 范例 12345678[root@ansible ~]#ansible 10.0.0.8 -m selinux -a &#x27;state=disabled&#x27;[root@centos8 ~]#grep -v &#x27;#&#x27; /etc/selinux/configSELINUX=disabledSELINUXTYPE=targeted[root@centos8 ~]#getenforcePermissive 5.5 Reboot 模块功能: 重启 常见选项 12345msg=str #广播重启提示消息，默认为空test_command=str #重启后执行验证命令，即执行测试成功与否的命令，默认 whoamireboot_timeout=600 #超时时长，即重启后延迟时间再执行测试成功与否的命令，默认600Spre_reboot_delay=0 #重启前等待时长，即重启前延迟时间的秒数，如果小于60S，此字段会被置0，也就是无效post_reboot_delay=0 #重启后等待一个时长后再验证是否重启完成 范例 123456#远程主机在重启过程中此进程会一直等待，直到超时[root@ubuntu ~]# ansible &quot;10.0.0.150 10.0.0.206&quot; -m reboot -a &#x27;pre_reboot_delay=65 msg=&quot;this is test msg&quot;&#x27;#远程主机上收到广播消息this is test msgThe system is going down for reboot at Sun 2023-02-05 13:48:10 CST! 5.6 Mount 模块功能: 挂载和卸载文件系统 常见选项 1234567891011src #源设备路径，或网络地址path #挂载至本地哪个路径下fstype #设备类型； nfsopts #挂载的选项fstab=/path/file #指定挂载配置文件路径，默认 /etc/fstabstate #挂载还是卸载 =present #永久挂载，但没有立即生效 =absent #卸载临时挂载,并删除永久挂载 =mounted #永久和临时挂载，立即生效，挂载点不存在会自动创建 =unmounted #临时卸载 =remounted #重新挂载，但不会改变配置文件 范例 12345678#挂载光盘,永久挂载，并立即生效[root@ubuntu ~]# ansible 10.0.0.206 -m mount -a &#x27;src=/dev/sr0 path=/mnt/ state=mounted fstype=iso9660&#x27;#取消挂载，临时生效[root@ubuntu ~]# ansible 10.0.0.206 -m mount -a &#x27;src=/dev/sr0 path=/mnt/ state=unmounted&#x27;#取消挂载，永久生效[root@ubuntu ~]# ansible 10.0.0.206 -m mount -a &#x27;src=/dev/sr0 path=/mnt/ state=absent&#x27; 范例 1234567891011#修改fstab文件永久挂载,但不立即生效mount webservers -m mount -a &#x27;src=&quot;UUID=b3e48f45-f933-4c8e-a700-22a159ec9077&quot; path=/home fstype=xfs opts=noatime state=present&#x27;#临时取消挂载mount webservers -m mount -a &#x27;path=/home fstype=xfs opts=noatime state=unmounted&#x27;#永久挂载,并立即生效ansible webservers -m mount -a &#x27;src=10.0.0.8:/data/wordpress path=/var/www/html/wp-content/uploads opts=&quot;_netdev&quot; state=mounted&#x27;#永久卸载,并立即生效ansible webservers -m mount -a &#x27;src=10.0.0.8:/data/wordpress path=/var/www/html/wp-content/uploads fstype=nfs state=absent&#x27; 5.7 sysctl 模块功能: 修改内核参数 常见选项 123456name=str #参数名称value=str #参数值reload=yes|no #默认yes，调用 /sbin/sysctl -p 生效state=present|absent #是否保存在sysctl.conf文件中，默认presentsysctl_file=/path/file #指定保存文件路径，默认 /etc/sysctl.confsysctl_set=yes|no #是否使用systctl -w 校验，默认no 范例 12345678910111213141516#修改内核参数，并写文件[root@ubuntu ~]# ansible 10.0.0.206 -m sysctl -a &#x27;name=net.ipv4.ip_forward value=1&#x27;#在远程主机上查看[root@ubuntu ~]# sysctl -a | grep &quot;ip_forward =&quot;net.ipv4.ip_forward = 1#从文件中删除[root@ubuntu ~]# ansible 10.0.0.206 -m sysctl -a &#x27;name=net.ipv4.ip_forward state=absent&#x27;#远程主机的文件中没有了[root@ubuntu ~]# cat /etc/sysctl.conf | grep ip_forward#但值没有刷新，从文件中同步，文件没有此项了，所以此项不会改变[root@ubuntu ~]# sysctl -a | grep &quot;ip_forward =&quot;net.ipv4.ip_forward = 1 范例: 内核参数优化 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556- name: Change Port Range sysctl: name: net.ipv4.ip_local_port_range value: &#x27;1024 65000&#x27; sysctl_set: yes - name: Enabled Forward sysctl: name: net.ipv4.ip_forward value: &#x27;1&#x27; sysctl_set: yes - name: Enabled tcp_reuse sysctl: name: net.ipv4.tcp_tw_reuse value: &#x27;1&#x27; sysctl_set: yes - name: Chanage tcp tw_buckets sysctl: name: net.ipv4.tcp_max_tw_buckets value: &#x27;5000&#x27; sysctl_set: yes - name: Chanage tcp_syncookies sysctl: name: net.ipv4.tcp_syncookies value: &#x27;1&#x27; sysctl_set: yes - name: Chanage tcp max_syn_backlog sysctl: name: net.ipv4.tcp_max_syn_backlog value: &#x27;8192&#x27; sysctl_set: yes - name: Chanage tcp Established Maxconn sysctl: name: net.core.somaxconn value: &#x27;32768&#x27; sysctl_set: yes state: present - name: Chanage tcp_syn_retries sysctl: name: net.ipv4.tcp_syn_retries value: &#x27;2&#x27; sysctl_set: yes state: present - name: Chanage net.ipv4.tcp_synack_retries sysctl: name: net.ipv4.tcp_synack_retries value: &#x27;2&#x27; sysctl_set: yes state: present 5.8 pam_limits模块功能: 管理资源限制 常用选项 1234domain=username|@groupname|UID|GID #具体对象limit_item=core|data|fsize|memlock|nofile|rss|stack|cpu|nproc|as|maxlogins|maxsyslogins|priority|locks|sigpending|msgqueue|nice|rtprio|chroot #修改内容limit_type=hard|soft|- #限定类型value=str #具体值 范例 1234[root@ubuntu ~]# ansible 10.0.0.206 -m pam_limits -a &#x27;domain=tom limit_type=- limit_item=nproc value=200&#x27;[root@ubuntu ~]# cat /etc/security/limits.conf | grep tomtom - nproc 200 范例: 123456789- name: Change Limit /etc/security/limit.conf pam_limits: domain: &quot;*&quot; limit_type: &quot;&#123;&#123; item.limit_type &#125;&#125;&quot; limit_item: &quot;&#123;&#123; item.limit_item &#125;&#125;&quot; value: &quot;&#123;&#123; item.value &#125;&#125;&quot; loop: - &#123; limit_type: &#x27;soft&#x27;, limit_item: &#x27;nofile&#x27;,value: &#x27;100000&#x27; &#125; - &#123; limit_type: &#x27;hard&#x27;, limit_item: &#x27;nofile&#x27;,value: &#x27;10000&#x27; &#125; 6 Setup 模块功能： setup 模块来收集主机的系统信息，这些 facts 信息可以直接以变量的形式使用，但是如果主机较多，会影响执行速度 可以使用 gather_facts: no 来禁止 Ansible 收集 facts 信息 常见选项 12filter=filed1,filed2 #指定过滤条件，只显示指定字段，可以用通配符，可以写多个，可以用 !取反gather_timeout=10 #超时时长，默认10S 范例 1234567891011#显示本机所有信息[root@ubuntu ~]# ansible 127.0.0.1 -m setup#收集所有字段[root@ubuntu ~]# ansible 10.0.0.206 -m setup#只显示主机名[root@ubuntu ~]# ansible 10.0.0.206 -m setup -a &#x27;filter=ansible_hostname&#x27;#显示主机名和IPV4[root@ubuntu ~]# ansible 10.0.0.206 -m setup -a 范例 1234567891011121314151617ansible all -m setupansible all -m setup -a &quot;filter=ansible_nodename&quot;ansible all -m setup -a &quot;filter=ansible_hostname&quot;ansible all -m setup -a &quot;filter=ansible_domain&quot;ansible all -m setup -a &quot;filter=ansible_memtotal_mb&quot; # 系统的总内存大小ansible all -m setup -a &quot;filter=ansible_memory_mb&quot; # 包含了关于内存的多种信息，如总内存、可用内存等ansible all -m setup -a &quot;filter=ansible_memfree_mb&quot; # 系统当前的空闲内存大小ansible all -m setup -a &quot;filter=ansible_os_family&quot; # 操作系统家族ansible all -m setup -a &quot;filter=ansible_distribution&quot; # 具体的 Linux 发行版名称ansible all -m setup -a &quot;filter=ansible_distribution_major_version&quot; # 发行版的主要版本号ansible all -m setup -a &quot;filter=ansible_distribution_version&quot; # 完整的发行版版本号ansible all -m setup -a &quot;filter=ansible_processor_vcpus&quot; # 处理器的虚拟 CPU 数量ansible all -m setup -a &quot;filter=ansible_all_ipv4_addresses&quot; # 主机上的所有 IPv4 地址ansible all -m setup -a &quot;filter=ansible_architecture&quot; # 主机的架构类型ansible all -m setup -a &quot;filter=ansible_uptime_seconds&quot; # 自上次启动以来的系统运行时间 ansible all -m setup -a &quot;filter=ansible_processor*&quot; # 匹配所有与处理器相关的变量ansible all -m setup -a &#x27;filter=ansible_env&#x27; 范例：取IP地址 12345#取所有IPansible 10.0.0.101 -m setup -a &#x27;filter=ansible_all_ipv4_addresses&#x27;#取默认IPansible all -m setup -a &#x27;filter=&quot;ansible_default_ipv4&quot;&#x27; 7 debug 模块功能: 此模块可以用于输出信息,并且通过 msg 定制输出的信息内容,功能类似于echo命令 注意: msg后面的变量有时需要加 “ “ 引起来 常见选项 123msg=str #输出消息内容，默认 Hello world!var #指定变量名,和msg互斥verbosity=0|1|2|3 #指定调试级别，verbosity=2 则表示仅在 -vv|-vvv|-vvvv 的级别下显示 范例 123456789101112131415#默认消息[root@ubuntu ~]# ansible 10.0.0.206 -m debug10.0.0.206 | SUCCESS =&gt; &#123; &quot;msg&quot;: &quot;Hello world!&quot;&#125;#设定了运行级别，当前跳过[root@ubuntu ~]# ansible 10.0.0.206 -m debug -a &#x27;msg=&quot;this is test msg&quot; verbosity=2&#x27;10.0.0.206 | SKIPPED#可以正常显示[root@ubuntu ~]# ansible 10.0.0.206 -m debug -a &#x27;msg=&quot;this is test msg&quot; verbosity=2&#x27; -vv10.0.0.206 | SUCCESS =&gt; &#123; &quot;msg&quot;: &quot;this is test msg&quot;&#125; 范例: 利用debug 输出shell命令的执行结果 1234567891011[root@ansible ansible]#cat debug.yml- hosts: webservers remote_user: root tasks: - name: echo hello shell: cmd: echo hello register: result - name: Display all variables/facts known for a host debug: msg: &quot;&#123;&#123; result.stdout &#125;&#125;&quot; 范例: 利用debug 模块输出变量 123456789101112131415[root@centos8 ~]#cat debug.yaml---- hosts: webservers tasks: - name: output variables debug: msg: Host &quot;&#123;&#123; ansible_nodename &#125;&#125;&quot; Ip &quot;&#123;&#123; ansible_default_ipv4.address &#125;&#125;&quot;[root@centos8 ~]#ansible-playbook debug.yamlok: [10.0.0.7] =&gt; &#123; &quot;msg&quot;: &quot;Host \\&quot;centos7.wangxiaochun.com\\&quot; Ip \\&quot;10.0.0.7\\&quot;&quot;&#125;ok: [10.0.0.8] =&gt; &#123; &quot;msg&quot;: &quot;Host \\&quot;centos8.wangxiaochun.com\\&quot; Ip \\&quot;10.0.0.8\\&quot;&quot;&#125; 范例: 显示字符串特定字符 12345678910111213141516171819# cat debug.yml- hosts: all gather_facts: no vars: a: &quot;12345&quot; tasks: - debug: msg: - &quot;&#123;&#123;a[0]&#125;&#125;&quot; - &quot;&#123;&#123;a[1]&#125;&#125;&quot; - &quot;&#123;&#123;a[2]&#125;&#125;&quot; #定义了一个字符串变量a，如果想要获取a字符串的第3个字符，则可以使用”a[2]”获取，索引从0开始，执行上例playbook，debug的输出信息如下：TASK [debug] *************************ok: [test1] =&gt; &#123; &quot;msg&quot;: &quot;1&quot; &quot;msg&quot;: &quot;2&quot; &quot;msg&quot;: &quot;3&quot;&#125; 范例: 12345678910111213141516171819[root@ansible ansible]#cat debug.yml- hosts: webservers remote_user: root tasks: - name: Display all variables/facts known for a host debug: var: ansible_nodename [root@ansible ansible]#ansible-playbook debug.yml......TASK [Display all variables/facts known for a host]********************************************************************************ok: [10.0.0.8] =&gt; &#123; &quot;ansible_nodename&quot;: &quot;rocky8.magedu.org&quot;&#125;ok: [10.0.0.7] =&gt; &#123; &quot;ansible_nodename&quot;: &quot;centos7.magedu.org&quot;&#125; 8 其他模块除了上述模块之外，ansible 还提供了很多其它的模块，在我们需要使用时，可以再进行查询 12ansible-doc -l | grep &quot;模块关键字&quot;ansible-doc 模块名 ansible 还提供了很多针对各种应用的模块,比如 123456789nginx_status_infonginx_status_factsmysql_db #需要安装MySQL-python包mysql_user #需要安装MySQL-python包redismongodbpostgresqlhaproxygit","categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"}]},{"title":"相关命令","slug":"CICD/ansible/command","date":"2025-09-13T10:07:38.000Z","updated":"2025-09-13T13:14:41.610Z","comments":true,"path":"CICD/ansible/command/","permalink":"https://aquapluto.github.io/CICD/ansible/command/","excerpt":"","text":"1 Ansible相关工具 命令 功能 &#x2F;usr&#x2F;bin&#x2F;ansible 临时命令执行工具，用于在目标主机上快速执行单个模块命令 &#x2F;usr&#x2F;bin&#x2F;ansible-doc 查看 Ansible 模块的详细文档和用法，相当于 man 命令 &#x2F;usr&#x2F;bin&#x2F;ansible-playbook 执行 YAML 格式的 Playbook 文件，用于编排复杂的自动化任务 &#x2F;usr&#x2F;bin&#x2F;ansible-pull 从 Git 等源拉取 Playbook 到本地并执行，适用于分散环境 &#x2F;usr&#x2F;bin&#x2F;ansible-vault 对敏感文件（如密码、密钥）进行加密&#x2F;解密，保障安全 &#x2F;usr&#x2F;bin&#x2F;ansible-console 提供交互式命令行界面，支持批量操作和上下文执行 &#x2F;usr&#x2F;bin&#x2F;ansible-galaxy 管理 Roles 和 Collections：下载、创建、上传到 Ansible Galaxy 平台 &#x2F;usr&#x2F;bin&#x2F;ansible-connection 管理和调试连接插件（如 ssh, docker, winrm 等） &#x2F;usr&#x2F;bin&#x2F;ansible-config 查看和管理 Ansible 配置（ansible.cfg） &#x2F;usr&#x2F;bin&#x2F;ansible-inventory 以多种格式（json, yaml, dot 等）显示解析后的主机清单 Ansible相关工具使用前准备 Ansible相关工具大多数是通过ssh协议，实现对远程主机的配置管理、应用部署、任务执行等功能 建议：使用此工具前，先配置ansible主控端能基于密钥认证的方式联系各个被管理节点 12root@ubuntu2004:~# ssh-keygenroot@ubuntu2004:~# ssh-copy-id 10.0.0.202 2 ansible-doc123456789ansible-doc [options] [module...]-l, --list #列出可用模块-s, --snippet #显示指定模块的playbook片段-t|--type #指定模块类型 become |cache |callback |cliconf |connection |httpapi |inventory |lookup |netconf |shell |vars |module |strategy |role |keyword-j|--json #以json格式显示-v|--verbose #显示详细信息，最多可以-vvvvvv# 更多信息查看帮助ansible-doc --help 范例 1234567891011#列出所有模块ansible-doc -l #查看指定模块帮助用法ansible-doc ping #查看指定模块帮助用法ansible-doc -s ping#以 json 格式输出ansible-doc ping -j 范例: 查看指定的插件 12[root@ansible ~]#ansible-doc -t connection -l[root@ansible ~]#ansible-doc -t lookup -l 3 ansible3.1 ansible命令用法123456789101112131415161718ansible &lt;host-pattern&gt; [-m module_name] [-a args]--version #显示版本-m module #指定模块，默认为command-v #详细过程 -vv -vvv更详细--list-hosts #显示主机列表，可简写 --list-C, --check #检查，并不执行-T, --timeout=TIMEOUT #执行命令的超时时间，默认10s-k, --ask-pass #提示输入ssh连接密码，默认Key验证-K, --ask-become-pass #提示输入sudo时的口令-u, --user=REMOTE_USER #执行远程执行的用户,默认root-b, --become #代替旧版的sudo实现通过sudo机制实现提升权限--become-user=USERNAME #指定sudo的runas用户，默认为root-f FORKS, --forks FORKS #指定并发同时执行ansible任务的主机数-i INVENTORY, --inventory INVENTORY #指定主机清单文件-h|--help #查看帮助-a|--args #指定模块选项-e|--extra-vars #指定执行选项 范例 1234567891011#以wang用户执行ping存活检测ansible all -m ping -u wang -k#以wang sudo至root执行ping存活检测ansible all -m ping -u wang -k -b#以wang sudo至mage用户执行ping存活检测ansible all -m ping -u wang -k -b --become-user=mage#以wang sudo至root用户执行lsansible all -m command -u wang -a &#x27;ls /root&#x27; -b --become-user=root -k -K 范例: 并发执行控制 1234567891011121314151617181920#分别执行下面两条命令观察结果[root@ansible ~]#ansible all -a &#x27;sleep 5&#x27; -f1[root@ansible ~]#ansible all -a &#x27;sleep 5&#x27; -f10[root@ubuntu ~]# ansible group3 --listhosts (2): 10.0.0.206 10.0.0.150 #并发数为1 ，每次执行一台机，总共需要4S [root@ubuntu ~]# ansible group3 -a &quot;sleep 2&quot; -f1 -kSSH password:10.0.0.206 | CHANGED | rc=0 &gt;&gt;10.0.0.150 | CHANGED | rc=0 &gt;&gt;#总共需要2S[root@ubuntu ~]# ansible group3 -a &quot;sleep 2&quot; -f2 -kSSH password:10.0.0.206 | CHANGED | rc=0 &gt;&gt;10.0.0.150 | CHANGED | rc=0 &gt;&gt; 范例: 使用普通用户进行远程管理 12345678910111213141516171819202122232425262728293031323334353637383940#在所有控制端和被控制端创建用户和密码[root@rocky8 ~]#useradd wang[root@rocky8 ~]#echo wang:123456 | chpasswd#在所有被控制端对用户sudo授权[root@rocky8 ~]#visudowang ALL=(ALL) NOPASSWD: ALL[root@rocky8 ~]#visudo -c/etc/sudoers: parsed OK#实现从控制端到被控制端的基于key验证[root@ansible ~]#su - wangwang@ansible:~$ssh-keygen -f ~/.ssh/id_rsa -P &#x27;&#x27;wang@ansible:~$ssh-copy-id wang@&#x27;10.0.0.8&#x27;#使用普通用户测试连接,默认连接权限不足失败wang@ansible:~$ ansible 10.0.0.8 -m shell -a &#x27;ls /root&#x27;10.0.0.8 | FAILED | rc=2 &gt;&gt;ls: cannot open directory &#x27;/root&#x27;: Permission deniednon-zero return code#使用普通用户通过-b选项连接实现sudo提权后连接成功wang@ansible:~$ ansible 10.0.0.8 -m shell -a &#x27;ls /root&#x27; -b --become-user root10.0.0.8 | CHANGED | rc=0 &gt;&gt;anaconda-ks.cfg#修改配置文件指定sudo机制[root@ansible ~]#vim /etc/ansible/ansible.cfg#取消下面行前面的注释[privilege_escalation]become=Truebecome_method=sudobecome_user=rootbecome_ask_pass=False#再次测试[root@ansible ~]#su - wangwang@ansible:~$ ansible 10.0.0.8 -m shell -a &#x27;ls /root&#x27;10.0.0.8 | CHANGED | rc=0 &gt;&gt;anaconda-ks.cfg 范例: 使用普通用户连接远程主机执行代替另一个用户身份执行操作 123456789101112[root@centos8 ~]#useradd wang[root@centos8 ~]#echo wang:123456 | chpasswd#先在被控制端能过sudo对普通用户授权[root@centos8 ~]#grep wang /etc/sudoerswang ALL=(ALL) NOPASSWD: ALL#以wang的用户连接用户,并利用sudo代表mage执行whoami命令[root@ansible ~]#ansible 10.0.0.8 -m shell -a &#x27;whoami&#x27; -u wang -k -b --become-user=mageSSH password: #输入远程主机wang用户ssh连接密码10.0.0.8 | CHANGED | rc=0 &gt;&gt;mage 范例：查看主机 12345#查看所有主机列表[root@ubuntu ~]# ansible all --list-hosts#查看指定组主机列表 [root@ubuntu ~]# ansible group1 --list-hosts 范例：自动添加主机到信任列表 12345678910111213141516171819202122232425262728293031[root@ubuntu ~]# rm -rf .ssh/*[root@ubuntu ~]# ansible 10.0.0.206 -m ping -kSSH password:10.0.0.206 | FAILED! =&gt; &#123; &quot;msg&quot;: &quot;to use the &#x27;ssh&#x27; connection type with passwords or pkcs11_provider,you must install the sshpass program&quot;&#125;#安装软件[root@ubuntu ~]# apt install sshpass#再次测试[root@ubuntu ~]# ansible 10.0.0.206 -m ping -kSSH password:10.0.0.206 | FAILED! =&gt; &#123; &quot;msg&quot;: &quot;Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host&#x27;s fingerprint to your known_hosts file to manage this host.&quot;&#125;#修改配置文件，自动添加目标主机到信任主机列表[root@ubuntu ~]# vim /etc/ansible/ansible.cfghost_key_checking=False[root@ubuntu ~]# ansible 10.0.0.206 -m ping -kSSH password:10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 范例：连接socket 缓存 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@ubuntu ~]# rm -rf .ansible#成功[root@ubuntu ~]# ansible 10.0.0.206 -m ping -kSSH password:10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;#生成一个 socket 文件[root@ubuntu ~]# tree .ansible.ansible├── cp│ └── 01622bc8ee└── tmp2 directories, 1 file[root@ubuntu ~]# file .ansible/cp/01622bc8ee.ansible/cp/01622bc8ee: socket#默认走 sshkey 验证，由于有缓存的 socket 文件，所以可以操作成功[root@ubuntu ~]# ansible 10.0.0.206 -m ping10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;#但该文件有生命周期，默认1分钟[root@ubuntu ~]# stat .ansible/cp/01622bc8eestat: cannot statx &#x27;.ansible/cp/01622bc8ee&#x27;: No such file or directory#没有上次连接的缓存，没有 -k 选项，默认走 sshkey 验证，失败[root@ubuntu ~]# ansible 10.0.0.206 -m ping10.0.0.206 | UNREACHABLE! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to connect to the host via ssh: root@10.0.0.206: Permission denied (publickey,password).&quot;, &quot;unreachable&quot;: true&#125; 范例：指定主机和用户 1234567891011121314151617181920212223242526272829#默认是用key验证[root@ubuntu ~]# ansible 10.0.0.206 -m ping10.0.0.206 | UNREACHABLE! =&gt; &#123; &quot;changed&quot;: false, &quot;msg&quot;: &quot;Failed to connect to the host via ssh: root@10.0.0.206: Permission denied (publickey,password).&quot;, &quot;unreachable&quot;: true&#125;#-k 选项使用 ssh 密码[root@ubuntu ~]# ansible 10.0.0.206 -m ping -kSSH password:10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;#指定用户要使用该用户密码[root@ubuntu ~]# ansible 10.0.0.206 -m ping -u mage -kSSH password:10.0.0.206 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python3&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 范例：以 sudo 身份执行 12345678910111213141516171819202122232425262728293031[root@ubuntu ~]# rm -rf .ansible# -m command 可以省略不写# tom 用户没有权限执行 ls /root[root@ubuntu ~]# ansible 10.0.0.206 -m command -a &quot;ls /root&quot; -u tom -kSSH password:10.0.0.206 | FAILED | rc=2 &gt;&gt;ls: cannot open directory &#x27;/root&#x27;: Permission deniednon-zero return code#修改目标主机[root@ubuntu ~]# vim /etc/sudoerstom ALL=(root) NOPASSWD: ALL#再次测试[root@ubuntu ~]# ansible 10.0.0.206 -m command -a &quot;sudo ls /root&quot; -u tom -kSSH password:10.0.0.206 | CHANGED | rc=0 &gt;&gt;0508nginx-1.22.1.tar.gzsnapxx.sh#另一种写法，-b 指定 sudo 执行 -K指定 sudo 密码[root@ubuntu ~]# ansible 10.0.0.206 -m command -a &quot;ls /root&quot; -u tom -k -b -KSSH password:BECOME password[defaults to SSH password]:10.0.0.206 | CHANGED | rc=0 &gt;&gt;0508nginx-1.22.1.tar.gzsnapxx.sh 3.2 ansible的Host-pattern用于匹配被控制的主机的列表 All ：表示所有Inventory中的所有主机 1ansible all -m ping *：通配符 1234567891011121314#用通配符表示所有主机ansible &quot;*&quot; -m ping#指定开头ansible 192.168.1.* -m ping#指定结尾ansible &quot;*com&quot; --list#指定开头和结尾ansible &quot;node*com&quot; --listansible &quot;srvs&quot; -m pingansible &quot;10.0.0.6 10.0.0.7&quot; -m ping 或关系 12ansible &quot;webservers:appsrvs&quot; -m pingansible &quot;192.168.1.10:192.168.1.20&quot; -m ping 逻辑与 12#在webservers组并且在dbsrvs组中的主机ansible &quot;webservers:&amp;dbsrvs&quot; -m ping 逻辑非 123#在所有主机,但不在webservers组和dbsrvs组中的主机#注意：此处为单引号ansible &#x27;all:!dbsrvs:!webservers&#x27; -m ping 综合逻辑 1ansible &#x27;webservers:dbsrvs:&amp;appsrvs:!ftpsrvs&#x27; -m ping 正则表达式 12345678ansible &quot;webservers:dbsrvs&quot; -m pingansible &quot;~(web|db).*\\.magedu\\.com&quot; -m ping#node 开头ansible &quot;~node&quot; --list#以cn 结尾ansible &quot;~.*cn&quot; --list 范例: 1[root@kube-master1 ~]#ansible &#x27;kube*:etcd:!10.0.0.101&#x27; -a reboot &amp;&amp; reboot 范例:查看主机清单 123456789101112131415161718192021222324252627[root@centos8 ~]#ansible all --list-hosts hosts (3): 10.0.0.6 10.0.0.7 10.0.0.8[root@centos8 ~]#ansible webservers --list-hosts hosts (3): 10.0.0.6 10.0.0.7 10.0.0.8[root@centos8 ~]#ansible appsrvs --list-hosts hosts (2): 10.0.0.7 10.0.0.8[root@centos8 ~]#ansible &quot;appsrvs:dbsrvs&quot; --list-hosts hosts (3): 10.0.0.7 10.0.0.8 10.0.0.6 #引用!号时,不要用双引号,而使用单引号[root@centos8 ~]#ansible &quot;appsrvs:!dbsrvs&quot; --list-hosts -bash: !dbsrvs: event not found[root@centos8 ~]#ansible &#x27;appsrvs:!dbsrvs&#x27; --list-hosts hosts (1): 10.0.0.8 范例：在主机清单中定义选项 1234567891011121314151617181920212223242526272829#修改配置文件[root@ubuntu ~]# vim /etc/ansible/hosts[group3]10.0.0.15010.0.0.157#指定ssh 密码[group3:vars]ansible_ssh_password=123456#执行 id 命令[root@ubuntu ~]# rm -rf .ansible[root@ubuntu ~]# ansible group3 -m command -a &quot;id&quot;10.0.0.150 | CHANGED | rc=0 &gt;&gt;uid=0(root) gid=0(root) groups=0(root)10.0.0.206 | CHANGED | rc=0 &gt;&gt;uid=0(root) gid=0(root) groups=0(root)#在配置文件中指定 ssh 用户名[root@ubuntu ~]# vim /etc/ansible/hosts[group3:vars]ansible_ssh_password=123456ansible_ssh_user=jose[root@ubuntu ~]# ansible group3 -m command -a &quot;id&quot;10.0.0.150 | CHANGED | rc=0 &gt;&gt;uid=1012(tom) gid=1012(tom) groups=1012(tom)10.0.0.206 | CHANGED | rc=0 &gt;&gt;uid=1001(tom) gid=1001(tom) groups=1001(tom) 4 ansible-console提示符格式 1执行用户@当前操作的主机组 (当前组的主机数量)[f:并发数]$ 范例 1root@all (0)[f:5]$ 常用子命令 1234567forks #设置并发数，例如：forks 10cd #切换组，例如：cd weblist #列出当前组主机列表?或help #列出所有的内置命令exit #退出ping #执行ping模块copy 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@ansible ~]#ansible-consoleWelcome to the ansible console.Type help or ? to list commands.#列出所有主机root@all (0)[f:5]$ listroot@all (0)[f:5]$#执行模块root@all (3)[f:5]$ pingroot@appsrvs (2)[f:5]$ yum name=httpd state=presentroot@appsrvs (2)[f:5]$ service name=httpd state=started#修改主机清单文件，新增[root@ubuntu ~]# vim /etc/ansible/hosts[group1]10.0.0.15010.0.0.15710.0.0.16410.0.0.167[group2]10.0.0.16410.0.0.184node[1:3].linux-magedu.com#再次执行[root@ubuntu ~]# ansible-consoleWelcome to the ansible console. Type help or ? to list commands.#己显示有8台主机root@all (8)[f:5]$#列出所有主机root@all (8)[f:5]$ list10.0.0.15010.0.0.15710.0.0.16410.0.0.16710.0.0.184node1.linux-magedu.comnode2.linux-magedu.comnode3.linux-magedu.com#切换分组root@all (3)[f:5]$ cd webserversroot@webservers (2)[f:5]$ list10.0.0.710.0.0.8#查看主机root@group2 (5)[f:5]$ list10.0.0.16410.0.0.184node1.linux-magedu.comnode2.linux-magedu.comnode3.linux-magedu.com#修改并发数，此处是临时修改，永久生效要修改配置文件root@group2 (5)[f:5]$ forks 10root@group2 (5)[f:10]$ 5 ansible-playbook1ansible-playbook hello.yml 6 ansible-vault1ansible-vault [-h] [--version] [-v]&#123;create,decrypt,edit,view,encrypt,encrypt_string,rekey&#125; ... 文档加解密命令 1234567#子命令create #新建加密文件decrypt #去掉加密文件密码edit #编辑加密文件view #查看encrypt #加密文件rekey #修改口令 范例 1234567891011121314151617181920212223242526272829303132#创建新文件[root@ubuntu ~]# ansible-vault create test2.yamlNew Vault password:Confirm New Vault password:#无法直接查看[root@ubuntu ~]# cat test2.yaml$ANSIBLE_VAULT;1.1;AES256323766373430363533616162363332333935363661386332656165646461323737343234316661623531666333393065656234656563386133356237623030300a663663363263643130323335333065383836636434653631366536343834313365666634356566313433393736663838393532333238636636306237343863610a63643161653333336139336564653739633238323932613536333932366632346335626563336538353134303861313566336163643136343830356537386362346438643561353162343263646532323937636332343135653130623734376637333565393837376164363362336364636134393665623161386232333263373939663237303639316131373038313338643031306133646532343663613664#输入密码后查看[root@ubuntu ~]# ansible-vault view test2.yamlVault password:- name: vault-test tasks: - name: task-1 shell: echo &quot;hello&quot; #输入密码后编辑[root@ubuntu ~]# ansible-vault edit test2.yamlVault password:#去掉密码保护[root@ubuntu ~]# ansible-vault decrypt test2.yamlVault password:Decryption successful#直接查看[root@ubuntu ~]# cat test2.yaml- name: vault-test tasks: - name: task-1 shell: echo &quot;hello&quot; 范例 1234567891011121314#执行加密的playbook,交互式输入密码chmod 600 hello.ymlansible-playbook --ask-vault-pass hello.yml#从pass.txt文件中读取密码ansible-playbook --vault-password-file pass.txt hello.yml#从配置文件中取得密码#vi /etc/ansible/ansible.cfg[defaults]ault-password-file=pass.txt#可以直接执行加密文件ansible-playbook hello.yml 7 ansible-galaxyGalaxy 是一个免费网站，类似于github网站，网站上发布了很多共享的roles角色。Ansible提供了ansible-galaxy命令行工具连接 https://galaxy.ansible.com 网站下载相应的roles，进行init初始化、search查拘、install安装、 remove移除等操作。 默认仓库地址：https://galaxy.ansible.com/ 官方文档：https://docs.ansible.com/ansible/latest/cli/ansible-galaxy.html 123456789101112131415161718ansible-galaxy [-h] [--version] [-v] TYPE ...#常用选项--version #显示版本信息-h|--help #查看帮助-v|--verbose #显示详细信息#TYPE,不写时默认 type 为 roelcollection #合集role #角色#常用子命令init #初始化list #列出所有己安装的role或collection，此处的己安装，表示将相关文本下载到本地了，role还要再调用ansible-playbooksearch #在服务器上搜索info #显示roleinstall #安装，即下载到本机，后面要再使用 ansible-playbook 进行安装remove #移除，即删除本地相关文件 范例 123456789#列出所有本地 role[root@ubuntu ~]# ansible-galaxy list[root@ubuntu ~]# ansible-galaxy role list#搜索与 redis 相关的 role[root@ubuntu ~]# ansible-galaxy search redis#查看 galaxy role 相关信息[root@ubuntu ~]# ansible-galaxy info davidwittman.redis 范例：安装 redis，这里的安装是指将相关的 role 文件从服务器上下载到本地，执行还要另外调用 ansible-playbook 1234567891011121314151617[root@ubuntu ~]# ansible-galaxy install davidwittman.redis#再次查看[root@ubuntu ~]# ansible-galaxy list# /root/.ansible/roles- davidwittman.redis, 1.2.9# /etc/ansible/roles[WARNING]: - the configured path /usr/share/ansible/roles does not exist.[root@ubuntu ~]# ls .ansible/roles/davidwittman.redis#移除[root@ubuntu ~]# ansible-galaxy remove davidwittman.redis#创建新角色的目录结构。指定角色的名称作为命令的参数，该命令在当前工作目录中为新角色创建子目录[root@ubuntu ~]# ansible-galaxy init test 范例： 123456789101112#搜索项目[root@ansible ~]#ansible-galaxy search lamp#列出所有已安装的galaxyansible-galaxy list#安装galaxy,默认下载到~/.ansible/roles下ansible-galaxy install geerlingguy.mysqlansible-galaxy install geerlingguy.redis#删除galaxyansible-galaxy remove geerlingguy.redis","categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"}]},{"title":"部署与配置","slug":"CICD/ansible/deploy","date":"2025-09-13T10:07:29.000Z","updated":"2025-09-13T13:04:08.207Z","comments":true,"path":"CICD/ansible/deploy/","permalink":"https://aquapluto.github.io/CICD/ansible/deploy/","excerpt":"","text":"1 Ansible安装ansible的安装方法有多种 官方文档 tar包下载 pip下载 1.1 包安装方式范例：查看ansible版本 12[root@centos8 ~]#yum info ansibleroot@ubuntu2004:~# apt show ansible 范例: Ubuntu20.04 安装ansible 1234[root@ubuntu2004 ~]#apt update[root@ubuntu2004 ~]#apt -y install ansible[root@ubuntu2004 ~]#ansible --versionansible 2.9.6 范例： CentOS8 安装ansible 123[root@centos8 ~]#yum install ansible -y[root@centos8 ~]#ansible --versionansible 2.9.16 1.2 pip 安装pip 是安装Python包的管理器，类似 yum 范例: 在rocky8上通过pip3安装ansible 1234[root@rocky8 ~]#yum -y install python39 rust[root@rocky8 ~]#pip3 install ansible[root@rocky8 ~]#ansible --versionansible [core 2.12.6] 范例：在 ubuntu 中用 pip 安装 1234[root@ubuntu ~]# apt install python3-pip[root@ubuntu ~]# pip3.10 install -i https://pypi.tuna.tsinghua.edu.cn/simple ansible[root@ubuntu ~]# ansible --versionansible [core 2.15.0] 范例：在 Ubuntu 中安装 ansible-core 1234[root@ubuntu ~]# apt update;apt install ansible-core[root@ubuntu ~]# dpkg -V ansibledpkg: package &#x27;ansible&#x27; is not installed[root@ubuntu ~]# dpkg -V ansible-core 关于ansible 和 ansible-core ansible-core 包中仅包含核心功能和核心模块，ansible 包中除了核心功能之外还包含大量外围功能模块。 在当前学习环境中(ubuntu2204)，ansible 和 ansible-core 包不能同时共存，都不提供配置文件，且ansible-core 的版本更新 2 Ansible相关文件2.1 Ansible配置文件列表 文件 说明 &#x2F;etc&#x2F;ansible&#x2F;ansible.cfg 主配置文件，配置ansible工作特性，也可以在项目的目录中创建此文件，当前目录下如果也有ansible.cfg，则此文件优先生效，建议每个项目目录下，创建独有的 ansible.cfg 文件 &#x2F;etc&#x2F;ansible&#x2F;hosts 主机清单 &#x2F;etc&#x2F;ansible&#x2F;roles&#x2F; 存放角色的目录，自己创建 查看 ansible 配置文件 12345678910111213#当前版本不提供配置文件[root@ubuntu ~]# ansible --versionansible [core 2.12.0] #版本号config file = None #配置文件configured module search path = [&#x27;/root/.ansible/plugins/modules&#x27;,&#x27;/usr/share/ansible/plugins/modules&#x27;] #第三方插件目录ansible python module location = /usr/lib/python3/dist-packages/ansible #官方模块目录ansible collection location =/root/.ansible/collections:/usr/share/ansible/collectionsexecutable location = /usr/bin/ansible #可执行程序python version = 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0] #python环境jinja version = 3.0.3 #jinja模板引擎版本libyaml = True #是否己安装libyaml#从ansible 2.12 开始，可以用工具生成主配置文件 如果要在 ubuntu 中使用，可以在 ansible-core 包中生成配置文件或从其它系统中 copy 文件到当前系统，或者从github上获取 1https://github.com/ansible/ansible/blob/stable-2.10/examples/ansible.cfg 范例 1234567891011121314#创建目录[root@ubuntu ~]# mkdir -pv /etc/ansible/rolesmkdir: created directory &#x27;/etc/ansible&#x27;mkdir: created directory &#x27;/etc/ansible/roles&#x27;[root@ubuntu ~]# touch /etc/ansible/hosts#生成配置文件[root@ubuntu ~]# ansible-config init -t all --disabled &gt; /etc/ansible/ansible.cfg[root@ubuntu ~]# tree /etc/ansible//etc/ansible/├── ansible.cfg ├── hosts #在此文件中定义要管理的主机└── roles #目录，存放角色文件 2.2 Ansible主配置文件官方文档 Ansible 的配置文件可以放在多个不同地方，优先级从高到低顺序如下 1234ANSIBLE_CONFIG #环境变量,注意:指定目录下的ansible.cfg文件必须存在才能生效,此变量中指向的文件必须存在才生效，指向的文件要以.cfg 结尾./ansible.cfg #当前目录下的ansible.cfg,一般一个项目对应一个专用配置文件,推荐使用~/.ansible.cfg #当前用户家目录下的.ansible.cfg/etc/ansible/ansible.cfg #系统默认配置文件 Ansible 的默认配置文件 /etc/ansible/ansible.cfg ，其中大部分的配置内容无需进行修改 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[defaults]#inventory = /etc/ansible/hosts #主机列表配置文件#library = /usr/share/my_modules/ #库文件存放目录#remote_tmp = $HOME/.ansible/tmp #临时py命令文件存放在远程主机目录#local_tmp = $HOME/.ansible/tmp #本机的临时命令执行目录#forks = 5 #默认并发数#sudo_user = root #默认sudo（管理员）用户#ask_sudo_pass = True #每次执行ansible命令是否询问ssh密码#ask_pass = True #连接远程主机时，询问是否需要输入密码，不询问走ssh key校验#remote_port = 22#host_key_checking = False #检查对应服务器的host_key，建议取消此行注释,实现第一次连接自动信任目标主机#module_name = command #默认模块，可以修改为shell模块-------------------------------------------------------------------------------------------其他配置项keep_remote_files=False #是否保留远程主机上的py脚本文件，默认不保留log_path=/var/log/ansible.log #日志文件，建议启用executable=/bin/sh #生成shell命令时用指定的bash执行gathering=smart|implicit|explicit #smart表示如果有缓存则使用缓存,implicit每次收集,explicit不收集,除非指定fact_caching_timeout=86400 #远程主机信息缓存时长，默认 86400fact_caching=memeory|jsonfile|redis #默认缓存到内存，可以写文件和redisfact_caching_connection=/path/to/cachedir #如果写json 文件，此处写保存路径，如果是redis此处写redis连接地址interpreter_python=auto #指定远程主机python版本和路径#普通用户提权配置[privilege_escalation] #become=True #启用权限提升功能#become_method=sudo #指定权限提升的方法，默认sudo#become_user=root #指定以root用户身份运行#become_ask_pass=False #是否提示输入提权密码-------------------------------------------------------------------------------------------其他配置项agnostic_become_prompt=True #是否对密码提示符进行宽松匹配（适用于多语言环境）become_allow_same_user=False #当前用户和提权用户一样时，是否执行提权操作become_exe= #指定提权工具的可执行文件路径（如 /usr/bin/sudo）become_flags= #为提权命令添加额外的标志或参数（如 -n 或 -l）#连接持久化配置[persistent_connection] #connect_timeout = 30 #连接超时时长#command_timeout = 30 #命令执行超时时长-------------------------------------------------------------------------------------------其他配置项connect_retry_timeout=15 #超时重试时长control_path_dir=~/.ansible/pc #socket文件保存目录#ansible命令执行状态的颜色配置[colors] #highlight = white#verbose = blue#warn = bright purple#error = red#debug = dark gray#deprecate = purple#skip = cyan#unreachable = red#ok = green#changed = yellow #执行成功并且对目标主机做变更#diff_add = green #执行成功并且对目标主机不需要做改变的操作#diff_remove = red #执行失败#diff_lines = cyan 配置文件优先级生效顺序 123456789101112131415161718192021222324252627[root@ubuntu ~]# ansible --version | grep cfgconfig file = /etc/ansible/ansible.cfg#在家目录下创建配置文件[root@ubuntu ~]# touch .ansible.cfg#再次查看,配置文件己经发生了改变[root@ubuntu ~]# ansible --version | grep cfgconfig file = /root/.ansible.cfg#创建目录，在当前目录下创建配置文件[root@ubuntu ~]# mkdir app1[root@ubuntu ~]# cd app1[root@ubuntu app1]# touch ansible.cfg[root@ubuntu app1]# ansible --version | grep cfgconfig file = /root/app1/ansible.cfg#设置环境变量，要以 cfg 结尾[root@ubuntu app1]# export ANSIBLE_CONFIG=/tmp/test.cfg#不生效是因为文件不存在[root@ubuntu app1]# ansible --version | grep cfgconfig file = /root/app1/ansible.cfg[root@ubuntu app1]# touch /tmp/test.cfg[root@ubuntu app1]# ansible --version | grep cfgconfig file = /tmp/test.cfg 主配置文件中又间接定义了其它的配置项，在使用时，可以为不同的项目建立不同的配置文件放到不同的目录中，再去到该目录下执行 ansible，或者用变量指定不同的配置文件用来区分不同的项目配置，对于不同用户的配置，可以写在相应的家目录中。总结来说，除了环境变量，当前目录下的ansible的配置文件优先生效，在哪个目录下，哪个配置文件生效，如果该目录没有，就默认系统配置文件 2.3 Inventory主机清单文件官方文档 ansible的主要功能在于批量主机操作，为了便捷地使用其中的部分主机，可以在inventory主机清单文件中将其进行分组组织 默认的 inventory file为 /etc/ansible/hosts ，inventory file可以有多个，且也可以通过Dynamic Inventory来动态生成 生产建议在每个项目目录下创建项目独立的hosts文件，通过项目目录下的 ansible.cfg 文件中的 inventory = ./hosts 实现 主机清单文件格式 inventory文件遵循INI文件风格，中括号中的字符为组名。可以将同一个主机同时归并到多个不同的组中 此外，如若目标主机使用了非默认的SSH端口，还可以在主机名称之后使用冒号加端口号来标明SSH端口 如果主机名称遵循相似的命名模式，还可以使用列表的方式标识各主机 主机定义支持 ip 地址和主机名两种方式，写法多种多样 Inventory 参数说明 12345678910ansible_ssh_host=IP|hostname #指定远程主机，可用IP或主机名ansible_ssh_port=PORT #指定SSH端口,如果不是默认的端口号,通过此变量设置.这种可以使用ip:端口ansible_ssh_user=UNAME #指定SSH用户名ansible_ssh_pass=PWD #指定SSH密码(这种方式并不安全,我们强烈建议使用 --ask-pass或SSH密钥)ansible_sudo_pass=PWD #指定SUDO密码(这种方式并不安全,我们强烈建议使用 --ask-sudo-pass)ansible_sudo_exe=/PATH/CMD #sudo命令路径(适用于1.8及以上版本)ansible_connection=local|ssh|paramiko|docker #与主机的连接类型ansible_ssh_private_key_file=/PATH/FILE #SSH私钥文件,适用于有多个密钥,而你不想使用 SSH 代理的情况ansible_shell_type=sh|csh|fish #目标系统的shell类型.默认shansible_python_interpreter=/PATH/PYTHON #目标主机的python路径,用于系统中有多个Python版本,或者命令路径不是/usr/bin/python,或者默认/usr/bin/python不存在 范例： 1234567891011ntp.wang.org[webservers]www1.wang.org:2222www2.wang.org[dbservers]db1.wang.orgdb2.wang.orgdb3.wang.org#或者区间写法db[1:3].wang.org 范例: 组嵌套和区间写法 12345678910111213[webservers]www[1:100].example.com[dbservers]db-[a:f].example.com[appservers]10.0.0.[1:100]#定义testsrvs组中包括两个其它分组,实现组嵌套[testsrvs:children]webserversdbservers 范例: 基于用户名和密码的ssh连接主机清单 123456789101112131415161718192021[test]10.0.0.8 ansible_connection=local #指定本地连接,无需ssh配置#每个主机分别指定用户和密码,ansible_connection=ssh需要StrictHostKeyChecking no 或者 host_key_checking = False10.0.0.7 ansible_connection=ssh ansible_ssh_port=2222 ansible_ssh_user=wang ansible_ssh_password=123456 10.0.0.6 ansible_ssh_user=root ansible_ssh_password=123456 #对每个分组的所有主机统一定义用户和密码,执行ansible命令时显示别名,如web01[webservers]web01 ansible_ssh_host=10.0.0.101web02 ansible_ssh_host=10.0.0.102#为webservers组中的所有主机统一定义变量，如果有主机单独定义，优先使用该主机单独定义的[webservers:vars]ansible_ssh_password=magedu#其他主机的配置参考some_host ansible_ssh_port=2222 ansible_ssh_user=manageraws_host ansible_ssh_private_key_file=/home/example/.ssh/aws.pemfreebsd_host ansible_python_interpreter=/usr/local/bin/pythonruby_module_host ansible_ruby_interpreter=/usr/bin/ruby.1.9.3 范例：批量写入100台机器 1for i in &#123;1..100&#125;;do echo 10.0.0.$i &gt;&gt; /etc/ansible/hosts;done","categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"}]},{"title":"架构概述","slug":"CICD/ansible/introduce","date":"2025-09-13T10:07:24.000Z","updated":"2025-09-13T10:24:31.063Z","comments":true,"path":"CICD/ansible/introduce/","permalink":"https://aquapluto.github.io/CICD/ansible/introduce/","excerpt":"","text":"1 Ansible介绍Ansible 是一个自动化运维工具，基于Python开发，集合了众多运维工具（puppet、cfengine、chef、func、fabric）的优点，实现了批量系统配置、批量程序部署、批量运行命令等功能。官网 官方文档 GitHub 批量执行远程命令，可以对远程的多台主机同时进行命令的执行 批量安装和配置软件服务，可以对远程的多台主机进行自动化的方式配置和管理各种服务 编排高级的企业级复杂的IT架构任务，Ansible的Playbook和role可以轻松实现大型的IT复杂架构 提供自动化运维工具的开发API，有很多运维工具，如 jumpserver 就是基于 ansible 实现自动化管理功能 有代理和无代理 有代理是指需要在被管理的机器上预先安装客户端软件，类似于C&#x2F;S架构。 无代理是指预先不需要在被管理的机器上做任何操作，只需要被管理机器的远程连接用户名和密码即可。 优点 缺点 提供了多达数千个的各种功能的模块，完成特定任务只需调用特定模块即可，还支持自定义模块，可使用任何编程语言写模块 如果管理的主机较多时，执行效率不如saltstack高 无需安装专用代理软件，基于python和SSH(默认已安装)实现 当前还不支持像MySQL数据库一样的事务回滚 幂等性：一个任务执行1遍和执行n遍效果一样，不因重复执行带来意外情况 基于OpenSSH实现安全通讯无需专用协议 1.1 Ansible 命令执行来源1、系统用户直接执行 系统用户登录终端后，在命令行直接运行单条 Ansible 命令 示例：ansible webservers -m yum -a &quot;name=httpd state=latest&quot; 2、Playbooks 编排执行 将多个 Ansible 任务写入 playbook.yml 文件中，由 Ansible 按顺序依次执行 示例：ansible-playbook site.yml 3、CMDB API 调用 通过配置 API 接口，让第三方应用（如 CMDB、运维平台）调用 Ansible 执行任务 例如：CMDB 中修改主机配置后，自动触发 Ansible 更新服务器配置 4、公有云&#x2F;私有云 API 调用 在云环境中，通过调用公有云（如 AWS、Azure、阿里云）或私有云（如 OpenStack）的 API 动态获取主机列表，并结合 Ansible 实现自动化 Ansible 支持动态 Inventory，可直接对接云平台 API 5、Web 管理界面执行 通过图形化 Web 平台执行 Ansible 任务 用户可在界面上选择 playbook、设置参数、查看执行日志 1.2 注意事项执行ansible的主机一般称为管理端，主控端，中控，master或堡垒机 主控端Python版本需要2.6或以上 被控端Python版本小于2.4，需要安装python-simplejson 被控端如开启SELinux需要安装libselinux-python windows 不能做为主控端，只能做为被控制端 2 Ansible架构 组合INVENTORY、API、MODULES、PLUGINS的绿框，ansible命令工具，其为核心执行工具 INVENTORY：Ansible管理主机的清单文件，默认为 &#x2F;etc&#x2F;ansible&#x2F;hosts MODULES：Ansible执行命令的功能模块，多数为内置核心模块，也可自定义 PLUGINS：模块功能的补充，如连接类型插件、循环插件、变量插件、过滤插件等，该功能不常用 API：供第三方程序调用的应用程序编程接口 3 Ansible 工作原理 执行流程 加载配置文件，读取配置； 加载对应的模块文件； 根据命令或模块，生成一个临时的 py 文件，再将该文件传送至对应的被管理的机器上($HOME/.ansible/tmp/ansible-tmp-xxx/xx.py) 给远程的 py 文件加可执行权限 执行该文件，并将结果抓取回当前的主机显示； 删除远端的临时 py 文件 4 Ansible Tower介绍公司中实现运维自动化的架构中主要用到ansible，但是ansible脚本在部署服务器指令行中显得不太直观。而 Ansible Tower 是一个图形化基于 WEB 的任务调度，复杂服务部署，IT自动化的一个管理平台，属于发布配置管理系统，支持 Api 及界面操作，基于 Django 编写。Tower 允许对用户进行权限控制，即使某用户不能传送某 SSH 凭证，你也可以通过 Tower 来对该用户共享该凭证。我们可以通过图形化界面来管理 Inventory，也可以对各种各样的云资源做同步。Tower可以记录所有 job 的日志，也可以与LDAP集成，并且拥有强大的可浏览的 REST API。Tower 也提供了命令行工具，可以与 Jenkins 轻松集成。Provisioning回调对自动伸缩拓扑图提供了强大的支持。 可以在 Ansible Tower 页面 了解Tower更多的功能，并且了解如何下载使用。Tower的免费版本最多支持10个节点，并且Ansible公司会提供强大的支持。可以支持用Ansible playbook来安装Tower。下载地址 官方文档 安装完成后的登录初始界面,需要订阅获取 license 文件才能使用 5 公有云中的批量管理公有云中也有以 Web 界面提供的批量管理主机的功能。 以阿里云为例，在 ECS 实例列表菜单页面，可以点击 “批量管理” 导航进入该功能。","categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"}]},{"title":"高可用架构","slug":"Monitor/alertmanager/ha","date":"2025-09-13T09:55:52.000Z","updated":"2025-09-13T10:06:12.249Z","comments":true,"path":"Monitor/alertmanager/ha/","permalink":"https://aquapluto.github.io/Monitor/alertmanager/ha/","excerpt":"","text":"1 使用nginx负载均衡实现高可用最简单的方案是，多部署几个alertmanager，采用nginx或者lvs组件进行负载均衡 效果：如果其中一个Alertmanager出现故障，负载均衡组件会自动屏蔽掉故障组件，走正常的、其他的Alertmanager进行服务 步骤 部署两个alertmanager服务 部署nginx服务 修改Promentheus配置 基本功能测试 测试alertmanager1挂了告警能否发出 1.1 配置文件内容创建目录 12345678910111213#创建alertmanager目录cd /datamkdir docker-compose -pmkdir docker-compose/alertmanager1 -pmkdir docker-compose/alertmanager2 -pmkdir docker-compose/alertmanager1/template -pmkdir docker-compose/alertmanager2/template -p#创建nginx目录cd /data/docker-composemkdir nginx -pmkdir nginx/conf.d -p 创建alertmanager的配置文件 12345678910111213141516171819202122232425#vi docker-compose/alertmanager1/config.yml#vi docker-compose/alertmanager2/config.ymlglobal: smtp_smarthost: &#x27;smtp.163.com:465&#x27; smtp_from: &#x27;你的邮箱@163.com&#x27; smtp_auth_username: &#x27;你的邮箱@163.com&#x27; smtp_auth_password: &#x27;你的授权码&#x27; smtp_require_tls: falsetemplates: - &#x27;./template/*.tmpl&#x27;route: group_by: [&#x27;alertname&#x27;] group_wait: 10s group_interval: 10s repeat_interval: 10m receiver: emailreceivers:- name: &#x27;email&#x27; email_configs: - to: &#x27;你的邮箱@163.com&#x27; html: &#x27;&#123;&#123; template &quot;email.html&quot; .&#125;&#125;&#x27; send_resolved: true 创建告警邮件模版文件 12345678910111213141516171819202122232425#vi docker-compose/alertmanager1/template/email.tmpl#vi docker-compose/alertmanager2/template/email.tmpl&#123;&#123; define &quot;email.html&quot; &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;&lt;h2&gt;@告警通知&lt;/h2&gt;告警程序: prometheus_alert &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; 级&lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;故障主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;触发时间: &#123;&#123; .StartsAt.Local.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;&lt;h2&gt;@告警恢复&lt;/h2&gt;告警程序: prometheus_alert &lt;br&gt;告警主机：&#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题：&#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;告警时间: &#123;&#123; .StartsAt.Local.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;恢复时间: &#123;&#123; .EndsAt.Local.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- end &#125;&#125; 创建nginx配置文件 12345678910111213141516cd nginx/conf.dcat &gt; default.conf&lt;&lt;&quot;EOF&quot; #创建alert的服务器分组，指向alertmanager1/2 upstream alert&#123; server 192.168.28.111:9093; server 192.168.28.111:9094; &#125; #执行反向代理的策略，访问到nginx根目录时，访问alert服务器分组 server&#123; # alertmanager location / &#123; proxy_pass http://alert/; &#125; &#125;EOF 1.2 部署两个alertmanager服务和nginx服务创建docker-compose.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#vi /data/docker-compose/docker-compose.yamlversion: &#x27;3.3&#x27;services: alertmanager1: image: prom/alertmanager:v0.27.0 container_name: alertmanager1 restart: always volumes: - /etc/localtime:/etc/localtime:ro - ./alertmanager1/:/etc/alertmanager/ command: - &#x27;--config.file=/etc/alertmanager/config.yml&#x27; - &#x27;--storage.path=/alertmanager&#x27; expose: - &#x27;9093&#x27; #9093端口映射为alertmanager1 ports: - 9093:9093 alertmanager2: image: prom/alertmanager:v0.27.0 container_name: alertmanager2 restart: always volumes: - /etc/localtime:/etc/localtime:ro - ./alertmanager2/:/etc/alertmanager/ command: - &#x27;--config.file=/etc/alertmanager/config.yml&#x27; - &#x27;--storage.path=/alertmanager&#x27; expose: - &#x27;9093&#x27; #9094端口映射为alertmanager2 ports: - 9094:9093 nginx: container_name: nginx image: nginx:1.18 #访问9998端口时，能访问到nginx，进而负载均衡到alertmanager1/2 ports: - 9998:80 volumes: - ./nginx/html:/etc/nginx/html - ./nginx/conf.d:/etc/nginx/conf.d - ./nginx/log:/var/log/nginx restart: always 启动服务 1docker-compose up -d 访问alertmanager1&#x2F;2 12http://192.168.28.111:9093/#/alertshttp://192.168.28.111:9094/#/alerts 访问nginx 1234http://192.168.28.111:9998/#查看负载均衡效果http://192.168.28.111:9998/#/status 1.3 修改Promentheus配置12345678910111213141516#cd /data/docker-prometheus/prometheus#vi prometheus.yml#Alertmanager 配置指向alertmanager1或者2alerting: alertmanagers: - static_configs: - targets: [&#x27;192.168.28.111:9998&#x27;]#搜刮配置，增加nginxscrape_configs: - job_name: &#x27;nginx-alert&#x27; scrape_interval: 15s static_configs: - targets: [&#x27;192.168.28.111:9998&#x27;] #curl -X POST http://localhost:9090/-/reload 2 使用Gossip协议实现高可用2.1 Gossip协议简介消息传递机制Gossip协议 多Alertmanager实例的场景中，告警信息可能会被重复发送； Gossip机制可为多个Alertmanager之间提供了信息传递的机制，以确保在多个Alertmanager分别接收到相同告警信息的情况下，只会有一个告警通知被发送给Receiver； Gossip是分布式系统中被广泛使用的协议，用于实现分布式节点之间的信息交换和状态同步； Gossip有两种实现方式分别为： Push-based 在Push-based当集群中某一节点A完成一个工作后，随机的从其它节点B并向其发送相应的消息，节点B接收到消息后在重复完成相同的工作，直到传播到集群中的所有节点 Pull-based Pull-based的实现中节点A会随机的向节点B发起询问是否有新的状态需要同步，如果有则返回 2.2 alertmanager结合Gossip协议如何实现告警状态同步当Alertmanager接收到来自Prometheus的告警消息后，会按照以下流程对告警进行处理 在第一个阶段Silence中，Alertmanager会判断当前通知是否匹配到任何的静默规则，如果没有则进入下一个阶段，否则则中断流水线不发送通知。 在第二个阶段Wait中，Alertmanager会根据当前Alertmanager在集群中所在的顺序(index)等待index * 5s的时间。 当前Alertmanager等待阶段结束后，Dedup阶段则会判断当前Alertmanager数据库中该通知是否已经发送，如果已经发送则中断流水线，不发送告警，否则则进入下一阶段Send对外发送告警通知。 告警发送完成后该Alertmanager进入最后一个阶段Gossip，Gossip会通知其他Alertmanager实例当前告警已经发送。其他实例接收到Gossip消息后，则会在自己的数据库中保存该通知已发送的记录 Gossip机制的关键在于两点 Silence设置同步：Alertmanager启动阶段基于Pull-based从集群其它节点同步Silence状态，当有新的Silence产生时使用Push-based方式在集群中传播Gossip信息。 通知发送状态同步：告警通知发送完成后，基于Push-based同步告警发送状态。Wait阶段可以确保集群状态一致 2.3 部署两个alertmanager服务 服务名 容器端口&#x2F;职责 宿主机占据端口&#x2F;职责 alertmanager1 8001&#x2F;集群端口+广播端口 8001&#x2F;集群端口 alertmanager2 8002&#x2F;广播端口 8002&#x2F;广播端口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#vi /data/docker-compose/docker-compose.yamlversion: &#x27;3.3&#x27;#增加网络配置networks: monitoring: driver: bridgeservices: alertmanager1: image: prom/alertmanager:v0.27.0 container_name: alertmanager1 restart: always volumes: - /etc/localtime:/etc/localtime:ro - ./alertmanager1/:/etc/alertmanager/ command: - &#x27;--config.file=/etc/alertmanager/config.yml&#x27; - &#x27;--storage.path=/alertmanager&#x27; #集群端口 - &#x27;--cluster.listen-address=0.0.0.0:8001&#x27; #广播 - &#x27;--cluster.advertise-address=0.0.0.0:8001&#x27; #额外暴露容器的8001端口给外部，且做tcp+udp映射 expose: - &#x27;9093&#x27; - &#x27;8001&#x27; ports: - 9093:9093 - 8001:8001 - 8001:8001/udp networks: - monitoring alertmanager2: image: prom/alertmanager:v0.27.0 container_name: alertmanager2 restart: always volumes: - /etc/localtime:/etc/localtime:ro - ./alertmanager2/:/etc/alertmanager/ command: - &#x27;--config.file=/etc/alertmanager/config.yml&#x27; - &#x27;--storage.path=/alertmanager&#x27; #指定集群端口 - &#x27;--cluster.listen-address=0.0.0.0:8002&#x27; - &#x27;--cluster.peer=192.168.28.111:8001&#x27; #广播端口 - &#x27;--cluster.advertise-address=0.0.0.0:8002&#x27; expose: - &#x27;9093&#x27; - &#x27;8002&#x27; #额外暴露自身的广播端口给外部，且做tcp+udp映射 ports: - 9094:9093 - 8002:8002 - 8002:8002/udp networks: - monitoring links: - alertmanager1 #docker-compose up -d 查看alertmanager1&#x2F;2的日志 123456789101112#提示gossip已启用docker logs -f alertmanager1ts=2024-06-03T08:59:23.229Z caller=cluster.go:708 level=info component=cluster msg=&quot;gossip not settled&quot; polls=2 before=2 now=1 elapsed=6.002067278sts=2024-06-03T08:59:31.232Z caller=cluster.go:700 level=info component=cluster msg=&quot;gossip settled; proceeding&quot; elapsed=14.004375558s#提示alertmanager2已加入集群docker logs -f alertmanager2ts=2024-06-03T08:59:23.442Z caller=cluster.go:708 level=info component=cluster msg=&quot;gossip not settled&quot; polls=2 before=2 now=1 elapsed=6.002208738sts=2024-06-03T08:59:31.447Z caller=cluster.go:700 level=info component=cluster msg=&quot;gossip settled; proceeding&quot; elapsed=14.007193252s 2.4 修改Promentheus配置由于Gossip机制的实现，在Promthues和Alertmanager实例之间不要使用任何的负载均衡，需要确保Promthues将告警发送到所有的Alertmanager实例中 12345678910111213141516#vi /data/docker-prometheus/prometheus/prometheus.ymlalerting: alertmanagers: - static_configs: - targets: - 192.168.28.111:9093 - 192.168.28.111:9094scrape_configs: - job_name: &#x27;alertmanager&#x27; scrape_interval: 15s static_configs: - targets: - 192.168.28.111:9093 - 192.168.28.111:9094","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Alertmanager","slug":"Alertmanager","permalink":"https://aquapluto.github.io/tags/Alertmanager/"}]},{"title":"集群与高可用","slug":"Monitor/prometheus/cluster-ha","date":"2025-09-13T03:19:25.000Z","updated":"2025-09-13T09:35:44.295Z","comments":true,"path":"Monitor/prometheus/cluster-ha/","permalink":"https://aquapluto.github.io/Monitor/prometheus/cluster-ha/","excerpt":"","text":"1 Prometheus的本地存储机制1.1 概述Prometheus内置了一个基于本地存储的时间序列数据库。在Prometheus设计上，使用本地存储可以降低Prometheus部署和管理的复杂度同时减少高可用（HA）带来的复杂性。 在默认情况下，用户只需要部署多套Prometheus，采集相同的Targets即可实现基本的HA。同时由于Promethus高效的数据处理能力，单个Prometheus Server基本上能够应对大部分用户监控规模的需求。 Prometheus 2.x 采用自定义的存储格式将样本数据保存在本地磁盘当中。如下所示，按照两个小时为一个时间窗口，将两小时内产生的数据存储在一个块(Block)中，每一个块中包含该时间窗口内的所有样本数据(chunks)，元数据文件(meta.json)以及索引文件(index)。 以每2小时为一个时间窗口，并存储为一个单独的block block会压缩、合并历史数据块，随着压缩合并，其block数量会减少 block的大小并不固定，但最小会保存两个小时的数据 12345678910t0 t1 t2 now ┌───────────┐ ┌───────────┐ ┌───────────┐ │ │ │ │ │ │ ┌────────────┐ │ │ │ │ │ mutable │ &lt;─── write ──── ┤ Prometheus │ │ │ │ │ │ │ └────────────┘ └───────────┘ └───────────┘ └───────────┘ ^ └──────────────┴───────┬──────┘ │ │ query │ │ merge ──────────────────────────────────┘ 当前时间窗口内正在收集的样本数据，Prometheus则会直接将数据保存在内存当中。为了确保此期间如果Prometheus发生崩溃或者重启时能够恢复数据，Prometheus启动时会从写入日志(WAL)进行重播，从而恢复数据。此期间如果通过API删除时间序列，删除记录也会保存在单独的逻辑文件当中(tombstone)。 在文件系统中这些块保存在单独的目录当中，Prometheus保存块数据的目录结构如下所示： 12345678910111213141516./data |- 01BKGV7JBM69T2G1BGBGM6KB12 # 块 |- meta.json # 元数据 |- wal # 写入日志 |- 000002 |- 000001 |- 01BKGTZQ1SYQJTR4PB43C8PD98 # 块 |- meta.json #元数据 |- index # 索引文件 |- chunks # 样本数据 |- 000001 |- tombstones # 逻辑数据 |- 01BKGTZQ1HHWHV8FBJXW1Y3W0K |- meta.json |- wal |-000001 通过时间窗口的形式保存所有的样本数据，可以明显提高Prometheus的查询效率，当查询一段时间范围内的所有样本数据时，只需要简单的从落在该范围内的块中查询数据即可。同时该存储方式可以简化历史数据的删除逻辑。只要一个块的时间范围落在了配置的保留范围之外，直接丢弃该块即可。 1234567 |┌────────────┐ ┌────┼─────┐ ┌───────────┐ ┌───────────┐ │ 1 │ │ 2 | │ │ 3 │ │ 4 │ . . .└────────────┘ └────┼─────┘ └───────────┘ └───────────┘ | | retention boundary 1.2 TSDB Block每个block都有单独的目录，里面包含该时间窗口内所有的chunk、index、tombstones、meta.json 文件&#x2F;目录 作用 详细说明 特点 chunks/ 存储实际的时序数据样本 - 每个 chunk 默认大小为 512MB- 超出大小时会截断并创建新的 Chunk- Chunk 按数字编号（如 000001, 000002）- 每个 Chunk 包含多个时间序列的压缩样本数据 - 数据以列式存储，高效压缩- 写入时追加（append-only）- 是 TSDB 的“数据本体” index 索引文件，支持高效查询 - 记录 指标名称（metric name）和标签（labels） 到时间序列的映射- 可通过 metric name + labels 快速定位该时间序列在 chunks 中的位置- 支持反向索引（从标签值查序列） - 查询性能的核心- 使用 LevelDB 格式存储（倒排索引）- 查询时先查 index，再读 chunk tombstones 软删除标记文件 - 记录被删除的时间序列或时间范围- 删除操作不立即清除数据，而是“标记”为已删除- 读取时会根据 tombstones 过滤掉已删除的数据 - 降低删除开销- 实际清理发生在 block 合并（compaction） 时- 文件小，但关键 meta.json Block 的元数据信息 - 包含 block 的起止时间（minTime, maxTime）- 版本信息、ULID（唯一标识）- 样本数量、序列数量、chunk 文件数等统计信息 - 是 compaction、保留策略、备份恢复 的依据- 人类可读的 JSON 格式 1.3 WAL （Write-Ahead Logging）WAL 是数据库操作的顺序事件日志，用于保障数据持久化的可靠性： 写入流程：在修改数据库数据（写入、修改、删除）前，先将操作事件追加记录到 WAL，再执行磁盘 Block 的实际修改。 文件组织：WAL 被分割为默认 128MB 大小的文件段，集中存储在 WAL 目录下。 可靠性保障：WAL 支持回滚、重试等操作（如写入失败时，可通过 WAL 恢复或重试）。WAL 日志的数量及截断位置记录在 checkpoint 文件中 当WAL 中对应的数据已不再需要，就会被Checkpoint截断（即哪些日志已持久化），Checkpoint本质是一个磁盘文件，用于定期压缩旧 WAL 并记录截断位置，以减少 WAL 文件数量，提升启动速度。Checkpoint文件需同步写入磁盘，确保 WAL 元数据的可靠性。 1.4 总结TSDB数据存储与持久化机制的流程 如上图，Head块是数据库位于内存中的部分，Block（灰色块）是磁盘上不可更改的持久块，而预写日志（WAL）用于辅助完成持久写入 传入的样本（t,v）首先会进入Head，并在内存中停留一会儿，然后即会被刷写到磁盘并映射进内存中（M-map），同时将该写入事件追加到 WAL 日志文件段中，确保即使 Prometheus 崩溃，重启后也能通过 WAL 恢复未持久化的数据 当这些内存映射的块或Head 中的数据老化到一定程度时，它会将作为持久块刷写到磁盘 随着它们的老化进程，将合并更多的块，最终在超过保留期限后被删除 1.5 存储配置用户可以通过命令行启动参数的方式修改本地存储的配置。 启动参数 默认值 含义 --storage.tsdb.path data/ 指标数据存储的根路径。Prometheus 的 TSDB 数据块（block）将存储在此目录下。 --storage.tsdb.retention 15d 数据保留时间，即样本数据在存储中保留的最长时间。超过此时间的旧数据块将被自动删除。例如 15d 表示保留 15 天。 --storage.tsdb.min-block-duration 2h 内存中的 Head Block 在累积了至少该时间范围的数据后，才会被持久化为磁盘上的数据块。值越小，数据落盘越频繁，磁盘 I&#x2F;O 可能越高。 --storage.tsdb.max-block-duration 36h 压缩（compaction）后生成的持久化数据块的最大时间范围。它也是任何持久化块的最小持续时间（通常用于控制块大小和合并策略）。该值应大于等于 min-block-duration。 --storage.tsdb.no-lockfile false 是否不在数据目录中创建锁文件。启用后（设为 true），Prometheus 不会在数据目录中创建 lock 文件，允许多个实例同时访问同一目录（危险，可能导致数据损坏，仅在明确需要时使用）。 在一般情况下，Prometheus中存储的每一个样本大概占用1-2字节大小。如果需要对Prometheus Server的本地磁盘空间做容量规划时，可以通过以下公式计算： 1needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample 从上面公式中可以看出在保留时间(retention_time_seconds)和样本大小(bytes_per_sample)不变的情况下，如果想减少本地磁盘的容量需求，只能通过减少每秒获取样本数(ingested_samples_per_second)的方式。因此有两种手段，一是减少时间序列的数量，二是增加采集样本的时间间隔。考虑到Prometheus会对时间序列进行压缩效率，减少时间序列的数量效果更明显。 1.6 从失败中恢复如果本地存储由于某些原因出现了错误，最直接的方式就是停止Prometheus并且删除data目录中的所有记录。当然也可以尝试删除那些发生错误的块目录，不过相应的用户会丢失该块中保存的大概两个小时的监控记录。 2 Prometheus的远端存储机制当然本地存储也带来了一些不好的地方，首先就是数据持久化的问题，特别是在像Kubernetes这样的动态集群环境下，如果Promthues的实例被重新调度，那所有历史监控数据都会丢失。 其次本地存储也意味着Prometheus不适合保存大量历史数据(一般Prometheus推荐只保留几周或者几个月的数据)。最后本地存储也导致Prometheus无法进行弹性扩展。 为了适应这方面的需求，Prometheus提供了remote_write和remote_read的特性，支持将数据存储到远端和从远端读取数据。通过将监控与数据分离，Prometheus能够更好地进行弹性扩展。Prometheus可通过基于gRPC的适配器对接到远程存储，适配器主要处理“读”和“写”两种数据操作，它们可分别通过不同的URL完成 2.1 Remote Write用户可以在Prometheus配置文件中指定Remote Write(远程写)的URL地址，一旦设置了该配置项，Prometheus将采集到的样本数据通过HTTP的形式发送给适配器(Adaptor)。而用户则可以在适配器中对接外部任意的服务。外部服务可以是真正的存储系统，公有云的存储服务，也可以是消息队列等任意形式。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879remote_write: # 远程写入目标的接收端点 URL url: http://your-remote-write-endpoint/api/v1/write # （可选）此远程写配置的名称，必须唯一 # 用于日志和监控指标中区分多个写入目标 name: remote-write-01 # 请求远程写端点的超时时间 # 默认为 30s remote_timeout: 30s # 写入前的重新标记配置 # 可用于过滤、修改、删除要发送的指标或标签 write_relabel_configs: # 示例：删除 __param_ 开头的内部标签 - source_labels: [__name__] regex: &#x27;job:.+&#x27; action: drop # 注释：丢弃所有 job: 开头的指标 # 示例：添加统一标签 - target_label: cluster replacement: us-west-1-prod action: replace # 基本身份认证配置 # password 和 password_file 互斥 basic_auth: username: prometheus-writer password: your-write-secret # password_file: /etc/prometheus/remote_write_password.txt # Bearer Token 认证 # bearer_token 和 bearer_token_file 互斥 # bearer_token: abcdef123456 bearer_token_file: /etc/prometheus/write_token.txt # TLS 配置 tls_config: insecure_skip_verify: false # ca_file: /etc/prometheus/ssl/ca.crt # cert_file: /etc/prometheus/ssl/client.crt # key_file: /etc/prometheus/ssl/client.key # （可选）代理 URL proxy_url: http://proxy.internal:8080 # 队列配置：控制并发写入行为，防止压垮远程存储 queue_config: # 每个分片（shard）缓冲区能容纳的最大样本数 # 建议足够大以应对短暂延迟 capacity: 2500 # 最小并发分片数（最小并发量） min_shards: 1 # 最大并发分片数（最大并发量） max_shards: 200 # 每次发送最多携带的样本数量 max_samples_per_send: 500 # 批量发送的截止时间（超时） batch_send_deadline: 5s # 初始重试延迟（指数退避） min_backoff: 30ms # 最大重试延迟 max_backoff: 100ms # 元数据发送配置（实验性，未来可能变更或移除） metadata_config: # 是否发送指标元数据（如 HELP, TYPE） send: true # 发送元数据的间隔 send_interval: 1m 2.2 Remote Read如下图所示，Promthues的Remote Read(远程读)也通过了一个适配器实现。在远程读的流程当中，当用户发起查询请求后，Promthues将向remote_read中配置的URL发起查询请求(matchers,ranges)，Adaptor根据请求条件从第三方存储服务中获取响应的数据。同时将数据转换为Promthues的原始样本数据返回给Prometheus Server。 当获取到样本数据后，Promthues在本地使用PromQL对样本数据进行二次处理。需要注意的是，启用远程读设置后，只在数据查询时有效，对于规则文件的处理，以及Metadata API的处理都只基于Prometheus本地存储完成。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354remote_read: # 要查询的远程读端点的 URL url: http://your-remote-read-endpoint/api/v1/read # （可选）此远程读配置的名称，必须唯一 # 将用于日志和监控指标中，便于识别不同配置 name: remote-read-01 # （可选）必须匹配的标签列表，用于过滤查询请求 # 只有当查询包含这些 label=value 的等值匹配时，才会发送到此远程端点 required_matchers: env: production tenant: acme # 请求远程读端点的超时时间 # 默认为 1m（1分钟） remote_timeout: 30s # 是否对本地存储应覆盖的时间范围内的查询， # 也向远程端点发起查询（即合并本地 + 远程数据） # 设置为 true 时可用于迁移或双写阶段 # 默认为 false read_recent: false # 基本身份认证配置（basic auth） # password 和 password_file 互斥，只能设置其一 basic_auth: username: prometheus-reader password: your-secret-password # 或使用文件方式： # password_file: /etc/prometheus/remote_read_password.txt # Bearer Token 身份认证 # bearer_token 和 bearer_token_file 互斥，只能设置其一 # bearer_token: your-bearer-token bearer_token_file: /etc/prometheus/token.jwt # TLS 配置（用于 HTTPS 连接） tls_config: # 是否验证服务器证书 insecure_skip_verify: false # （可选）客户端证书和密钥（双向 TLS） # cert_file: /etc/prometheus/ssl/client.crt # key_file: /etc/prometheus/ssl/client.key # （可选）CA 证书，用于验证服务器 # ca_file: /etc/prometheus/ssl/ca.crt # （可选）服务器名称（SNI） # server_name: remote-storage.example.com # （可选）代理 URL，用于通过 HTTP/HTTPS 代理访问远程端点 proxy_url: http://proxy.internal:8080 2.3 使用InfluxDB作为Remote StorageInfluxDB是一款开源的时序数据库，且内置了强大的 UI 平台。时序数据库通常被用在监控场景，比如。我们可以写一个程序将服务器上 CPU 的使用情况每隔 10 秒钟向 InfluxDB 中写入一条数据。接着，我们写一个查询语句，查询过去 30 秒 CPU 的平均使用情况，然后让这个查询语句也每隔 10 秒钟执行一次。最终，我们配置一条报警规则，如果查询语句的执行结果&gt;xxx，就立刻触发报警。 下面演示如何用 InfluxDB 作为 Prometheus 的 Remote Storage，从而确保当Prometheus发生宕机或者重启之后能够从 InfluxDB 中恢复和获取历史数据。 2.3.1 安装部署InfluxDB二进制安装 1234wget https://dl.influxdata.com/influxdb/releases/influxdb2-2.4.0-linux-amd64.tar.gztar -zxvf influxdb2-2.4.0-linux-amd64.tar.gz -C /opt/module# 开启InfluxDB服务进程./influxd 容器化安装 1234567891011121314151617version: &#x27;2&#x27;services: influxdb: image: influxdb:1.8.0 command: -config /etc/influxdb/influxdb.conf container_name: influxdb volumes: - /data/InfluxDB/data:/var/lib/influxdb ports: - &quot;8086:8086&quot; environment: - INFLUXDB_DB=prometheus - INFLUXDB_ADMIN_ENABLED=true - INFLUXDB_ADMIN_USER=admin - INFLUXDB_ADMIN_PASSWORD=admin - INFLUXDB_USER=prom - INFLUXDB_USER_PASSWORD=prom 进入InfluxDB的容器 123456789docker exec -it influxdb influxConnected to http://localhost:8086 version 1.8.0 #提示influxdb的版本号 InfluxDB shell version: 1.8.0&gt; auth #用初始安装的用户名和密码登录，prom/promusername: prompassword:&gt; use prometheus #显示初始的prometheus数据库信息&gt; SHOW MEASUREMENTS #显示该数据库中收集到的数据表(prometheus server对接后有数据) 2.3.2 安装remote_storage_adapter12345678go get github.com/prometheus/prometheus/documentation/examples/remote_storage/remote_storage_adapterchmod +x /usr/local/bin/remote_storage_adapter# 运行数据采集器remote_storage_adapter --influxdb-url=http://localhost:8086/ --influxdb.database=prometheus --influxdb.retention-policy=autogen &amp;# 默认端口为9201，查看端口状态可看到实例已正常启动netstat -lnpt |grep 9201 2.3.3 配置Prometheus添加远程读写的配置内容 1234remote_write: - url: &quot;http://192.168.28.111:9201/write&quot;remote_read: - url: &quot;http://192.168.28.111:9201/read&quot; 确认InfluxDB中是否有数据 12345docker exec -it influxdb influx&gt;use prometheus&gt;show measurements#如果后续展现很多measurement数据表数据，说明prometheus的数据已经写入InfluxDB 当数据写入成功后，停止Prometheus服务。同时删除Prometheus的data目录，模拟Promthues数据丢失的情况后重启Prometheus。打开Prometheus UI如果配置正常，说明Prometheus可以正常查询到本地存储以删除的历史数据记录。 3 VictoriaMetrics3.1 VictoriaMetrics简介VictoriaMetrics 是一个高性能、低成本、完全开源（MIT 许可证）的时序数据库（TSDB），专为 Prometheus 生态设计，同时支持独立部署与大规模扩展，是云原生监控场景下的理想选择。核心定位如下 原生支持高可用、经济高效、易于扩展的时序数据存储系统。 可作为 Prometheus 的远程长期存储数据库，也可独立运行并替代 Prometheus，直接抓取指标。 设计目标：高性能、低资源消耗、Prometheus 兼容性强。 特性 说明 Prometheus 兼容 完全兼容 remote_write &#x2F; remote_read 协议，无缝集成 Prometheus 支持 Relabeling 支持 Prometheus 风格的 relabel_configs，可实现数据过滤与标签重写 多协议支持 支持 Prometheus、InfluxDB Line Protocol、Graphite、OpenTelemetry 等多种摄取协议 Push &amp; Pull 模式 支持 Push（如 remote_write）和 Pull（主动 scrape）两种数据采集方式 数据缓冲 内置 WAL 和队列机制，支持写入缓冲与故障恢复 分布式设计 从架构上原生支持分布式部署，可水平扩展 资源占用低 极高的压缩率（通常 8-15x），内存和 CPU 消耗远低于同类系统 多租户支持 集群模式支持多租户（通过 accountID 隔离） 高并发能力 支持百万级时间序列的高并发读写 3.2 VictoriaMetrics架构VictoriaMetrics集群模式由三个核心无状态&#x2F;有状态服务组成 1234567891011121314151617181920 +----------------+ | Clients | | (Prometheus, | | Grafana, etc)| +-------+--------+ | +-----------------+------------------+ | | |+---------v-------+ +-------v------+ +---------v-------+| vminsert | | vmselect | | vmstorage || (Stateless) | | (Stateless) | | (Stateful) || 数据写入入口 | | 查询入口 | | 数据存储节点 |+--------+--------+ +-------+------+ +--------+--------+ | | | +------------------+-----------------+ | +-------v--------+ | Storage Layer | | (Persistent Disk)| +------------------+ 3.2.1 vmstorage（有状态，数据存储）是真正的数据存储节点，负责持久化时间序列数据。 监听端口： 8482/tcp：HTTP API 服务（由 -httpListenAddr 控制） 8400/tcp：接收来自 vminsert 的写入数据（由 -vminsertAddr 控制） 8401/tcp：响应来自 vmselect 的查询请求（由 -vmselectAddr 控制） 数据目录：通过 -storageDataPath 指定，默认为 ./vmstorage-data/ 特点： vmstorage节点间不进行任何交互，都是独立的个体，只靠上层的 vminsert，产生副本和分片 一个vmstorage节点故障时会丢失约 1/N 的数据（N 为 vmstorage 节点数），因此依赖副本机制保障可靠性 支持单节点和集群模式，集群模式下支持多租户，通过 accountID 实现隔离 3.2.2 vminsert（无状态，写入入口）负责接收来自客户端的数据写入请求，并将数据分片（shard）到多个 vmstorage 节点。 监听端口：8480/tcp（由 -httpListenAddr 控制） 核心功能 实现 Prometheus remote_write 协议 根据指标名称和标签进行哈希计算，实现数据分片 若接入的是VM存储集群时，其调用端点的URL格式： 1http://&lt;vminsert&gt;:8480/insert/&lt;accountID&gt;/&lt;suffix&gt; &lt;accountID&gt;：租户 ID（多租户场景） &lt;suffix&gt;：协议后缀，如 /prometheus 或 /prometheus/api/v1/write，它们作用相同，都用于接收写入请求 3.2.3 vmselect（无状态，查询入口）负责接收来自客户端的数据查询请求，向所有 vmstorage 节点广播查询，并合并结果返回。 监听端口：8481/tcp（由 -httpListenAddr 控制） 核心功能： 实现 Prometheus remote_read 协议 完全支持 PromQL 查询语言，可以作为 Grafana 中的 Prometheus 数据源使用 Prometheus 查询 URL 格式 1http://&lt;vmselect&gt;:8481/select/&lt;accountID&gt;/prometheus 3.3 告警组件vmalertVictoriaMetrics 使用 vmalert 组件实现告警与记录规则。 完全兼容 Prometheus Recording Rules（记录规则） Alerting Rules（告警规则） 支持与 Alertmanager 集成，发送告警通知 支持模板化告警消息，灵活定制通知内容 告警模板变量 变量名 说明 示例 $value 或 .Value 当前告警的值。避免在标签中使用值，否则可能会导致标签基数爆炸 Number of connections is &#123;&#123; $value &#125;&#125; $activeAt 或 .ActiveAt 警报变为 pending 或 firing 的时间 http://vm-grafana.com/panelld=xx?from=&#123;&#123;(activeAt.Add (parseDurationTime &quot;1h&quot;).Unix&#125;j&amp;to:&#123;&#123;(activeAt.Add (parseDurationTime \"-1h\")).Unix&#125;&#125; $labels 或 .Labels 告警的标签集合，以 “.Labels.” 形式使用 Too high number of connections for &#123;&#123; .Labels.instance &#125;&#125; $alertId 或 .AlertId vmalert生成的当前警报的 lD，用于构建告警详情链接 Link: vmalert/alert?group id=&#123;&#123;.GrouplD]&#125;&alert id=&#123;&#123;.AlertlD&#125;&#125; $groupId 或 .GroupId vmalert生成的当前警报的组 lD，用于分组管理 Link: vmalert/alert?group id=f&#123;.GrouplD&#125;&#125;&amp;alert id=&#123;&#123;.AlertlD&#125;&#125; $expr 或 .Expr 触发告警的 PromQL 表达式，可用于生成到 Grafana 或其他系统的链接 /api/v1/query?query=&#123;&#123; $exprlqueryEscape &#125;&#125; $externalLabels 或 .ExternalLabels 通过 -external.label 配置的全局标签 lssues with &#123;&#123; labels.instance &#125;&#125; (datacenter-&#123;&#123; externalLabels.dc &#125;&#125;) $externalUrl 或 .ExternalUrl 通过 -external.url 配置的外部访问地址，用于 vmalert 隐藏在代理后面的情况。 Visit &#123;&#123; $externalURL &#125;&#125; for more details 3.4 单体部署架构对于小规模场景，VictoriaMetrics 可以使用单体部署模式（VMSingle），所有功能集成在一个二进制中。 组件 配置说明 默认端口 vmstorage 单实例存储节点配置：-retentionPeriod=30d -storageDataPath=/data 8482 vminsert 写入入口配置：-storageNode=&lt;vmstorage-host&gt;:8400 8480 vmselect 查询入口配置：-storageNode=&lt;vmstorage-host&gt;:8401 8481 vmalert 告警组件配置：-datasource.url=http://vmstorage:8482-notifier.url=http://alertmanager:9093 8880 Grafana 可视化工具数据源配置为：http://vmstorage:8482（兼容 Prometheus API） 3000 4 Prometheus高可用架构除了数据持久化问题以外，影响 Promthues 性能表现的另外一个重要因素就是数据采集任务量，以及单台 Promthues 能够处理的时间序列数，由于Prometheus基于Pull模型，当有大量的Target需要采样本时，单一Prometheus实例在数据抓取时可能会出现一些性能问题，联邦集群的特性可以让Prometheus将样本采集任务划分到不同的Prometheus实例中，并且通过一个统一的中心节点进行聚合，从而可以使Prometheuse可以根据规模进行扩展。 4.1 联邦集群4.1.1 概述对于大部分监控规模而言，我们只需要在每一个数据中心(例如：EC2 可用区，Kubernetes 集群)安装一个 Prometheus Server 实例，就可以在各个数据中心处理上千规模的集群。同时将 Prometheus Server 部署到不同的数据中心可以避免网络配置的复杂性。 如上图所示，在每个数据中心部署单独的Prometheus Server，用于采集当前数据中心监控数据。并由一个中心的Prometheus Server负责聚合多个数据中心的监控数据。这一特性在Promthues中称为联邦集群。 原理：每个数据中心的 Prometheus Server 对特定区域或业务的监控数据进行本地采集和初步处理，比如只采集和存储特定服务的指标数据。中心 Prometheus Server 通过配置远程读取规则，定期从每个数据中心的 Prometheus Server 获取数据，将不同区域或业务模块的数据整合起来，实现对整个系统的全局监控。 4.1.2 配置联邦集群的核心在于一个Prometheus Server可以通过 /federate 接口，从另一个 Prometheus 实例中拉取它已经采集到的样本数据。对于中心Prometheus Server而言，无论是从其他的Prometheus实例还是Exporter实例中获取数据实际上并没有任何差异。 1234567891011121314scrape_configs: - job_name: &#x27;federate&#x27; scrape_interval: 15s honor_labels: true metrics_path: &#x27;/federate&#x27; # 从/federate接口拉取聚合数据 params: &#x27;match[]&#x27;: # 指定要拉取哪些时间序列 - &#x27;&#123;job=&quot;prometheus&quot;&#125;&#x27; # 获取所有 job 标签等于 prometheus 的指标 - &#x27;&#123;__name__=~&quot;job:.*&quot;&#125;&#x27; # 获取指标名称以 job: 开头的所有指标 - &#x27;&#123;__name__=~&quot;node.*&quot;&#125;&#x27; # 获取指标名称以 node. 开头的所有指标 static_configs: - targets: # 列出要拉取数据的其他Prometheus实例地址 - &#x27;192.168.77.11:9090&#x27; - &#x27;192.168.77.12:9090&#x27; 为了有效的减少不必要的时间序列，通过params参数可以用于指定只获取某些时间序列的样本数据，例如 12# 获取所有标签为 job=&quot;prometheus&quot; 的指标数据http://192.168.77.11:9090/federate?match[]=&#123;job=&quot;prometheus&quot;&#125; 通过URL中的 match[] 参数指定我们可以指定需要获取的时间序列。match[] 参数必须是一个瞬时向量选择器，例如 up 或者&#123;job=”api-server”&#125;。配置多个 match[] 参数，用于获取多组时间序列的监控数据。 honor_labels: true 参数说明：当 Prometheus 发现拉取的数据与本地已有的时间序列标签冲突时，如果该参数设置为 true，能够自动忽略冲突的监控数据；如果为 false 时，prometheus会自动将冲突的标签替换为 ”exported_“ 的形式。 123456789本地已有：up&#123;instance=&quot;a&quot;, job=&quot;api&quot;&#125;新拉取：up&#123;instance=&quot;a&quot;, job=&quot;prometheus&quot;&#125;冲突发生在 job 标签上如果 honor_labels: false，Prometheus 会把拉取的数据改为：up&#123;instance=&quot;a&quot;, job=&quot;prometheus&quot;, exported_job=&quot;api&quot;&#125;如果 honor_labels: true，忽略本地已有的数据，使用新拉取的数据 4.1.3 按照实例进行功能分区这时再考虑另外一种极端情况，即单个采集任务的Target数也变得非常巨大。这时简单通过联邦集群进行功能分区，Prometheus Server也无法有效处理时。这种情况只能考虑继续在实例级别进行功能划分。如下图所示，在各个数据中心中部署多个Prometheus Server实例，每一个Prometheus Server实例只负责采集当前数据中心中的一部分任务(Job)，将不同的监控任务分离到不同的Prometheus实例当中，再有中心Prometheus实例进行聚合 将同一任务的不同实例的监控数据采集任务划分到不同的Prometheus实例。通过relabel设置，我们可以确保当前Prometheus Server只收集当前采集任务的一部分实例的监控指标 12345678910111213141516171819202122global: # 给当前 Prometheus 实例打上一个全局标签 slave=&quot;1&quot; # 作用：在后续联邦聚合时，可以区分数据来自哪个分片（shard） # 防止标签冲突：当多个分片的数据汇聚到中心 Prometheus 时，slave 标签能保证时间序列唯一性 external_labels: slave: 1scrape_configs: - job_name: some_job relabel_configs: # 对每个目标实例的 __address__（即 IP:Port）进行哈希，然后对 4 取模，结果写入临时标签 __tmp_hash。 # 假设有 1000 个目标，它们的地址会被哈希后分配到 0, 1, 2, 3 四个桶中 # 这样就实现了均匀分片，每个 Prometheus 实例处理约 1/4 的目标 - source_labels: [__address__] modulus: 4 target_label: __tmp_hash action: hashmod # 只保留 __tmp_hash == 1 的目标，其余全部丢弃 # 所以当前 Prometheus 实例只采集哈希值为 1 的那部分目标 - source_labels: [__tmp_hash] regex: ^1$ action: keep 并且通过当前数据中心的一个中心Prometheus Server将监控数据进行聚合到任务级别 12345678910111213- scrape_config: - job_name: slaves honor_labels: true metrics_path: /federate params: match[]: - &#x27;&#123;__name__=~&quot;^slave:.*&quot;&#125;&#x27; # 只拉取名称以 slave: 开头的指标 static_configs: - targets: # 从所有分片 Prometheus 实例拉取数据 - slave0:9090 - slave1:9090 - slave3:9090 - slave4:9090 4.2 基本HA：服务可用性由于Promthues的Pull机制的设计，为了确保Promthues服务的可用性，用户只需要部署多套Prometheus Server实例，并且采集相同的Exporter目标即可。基本的HA模式只能确保Promthues服务的可用性问题，但是不解决Prometheus Server之间的数据一致性问题以及持久化问题(数据丢失后无法恢复)，也无法进行动态的扩展。因此这种部署方式适合监控规模不大，Promthues Server也不会频繁发生迁移的情况，并且只需要保存短周期监控数据的场景。 4.3 基本HA + 远程存储在基本HA模式的基础上通过添加Remote Storage存储支持，将监控数据保存在第三方存储服务上。在解决了Promthues服务可用性的基础上，同时确保了数据的持久化，当Promthues Server发生宕机或者数据丢失的情况下，可以快速的恢复。 同时Promthues Server可能很好的进行迁移。因此，该方案适用于用户监控规模不大，但是希望能够\b将监控数据持久化，同时能够确保Promthues Server的可迁移性的场景。 4.4 基本HA + 远程存储 + 联邦集群当单台Promthues Server无法处理大量的采集任务时，用户可以考虑基于Prometheus联邦集群的方式将监控采集任务划分到不同的Promthues实例当中即在任务级别功能分区。 这种部署方式一般适用于两种场景： 场景一：单数据中心 + 大量的采集任务 这种场景下Promthues的性能瓶颈主要在于大量的采集任务，因此用户需要利用Prometheus联邦集群的特性，将不同类型的采集任务划分到不同的Promthues子服务中，从而实现功能分区。例如一个Promthues Server负责采集基础设施相关的监控指标，另外一个Prometheus Server负责采集应用监控指标。再有上层Prometheus Server实现对数据的汇聚。 场景二：多数据中心 这种模式也适合与多数据中心的情况，当Promthues Server无法直接与数据中心中的Exporter进行通讯时，在每一个数据中部署一个单独的Promthues Server负责当前数据中心的采集任务是一个不错的方式。这样可以避免用户进行大量的网络配置，只需要确保主Promthues Server实例能够与当前数据中心的Prometheus Server通讯即可。 中心Promthues Server负责实现对多数据中心数据的聚合。","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"使用Pushmanager实现跨机房采集","slug":"Monitor/prometheus/pushmanager","date":"2025-09-12T08:52:33.000Z","updated":"2025-09-12T08:58:06.155Z","comments":true,"path":"Monitor/prometheus/pushmanager/","permalink":"https://aquapluto.github.io/Monitor/prometheus/pushmanager/","excerpt":"","text":"1 场景介绍跨机房监控服务器时，网络开销过大会导致Prometheus pull收集数据的耗时过长，或者Prometheus收集的数据较多，需要将各个Prometheus节点收集的数据进行统一汇总，此时可以使用pushgateway的架构采集数据； 可能由于不在一个子网或者防火墙原因，导致 Prometheus 无法直接拉取各个 target 数据。此时可以使用pushmanager组件来得到各个target数据，再由Prometheus拉取pushgateway pushgateway方案的架构与作用解析 Pushgateway就是个数据中转站。提供API，支持数据生产者随时将数据推送过来。 Pushgateway提供exporter功能，在promethus server拉取数据时，将自己保存的数据反馈给promethus server端 优点 Prometheus 默认采用定时 pull 模式拉取targets数据，但是如果不在一个子网或者防火墙，prometheus就拉取不到targets数据，所以可以采用各个target往pushgateway上push数据，然后prometheus去pushgateway上定时pull数据。 在监控业务数据的时候，需要将不同数据汇总，汇总之后的数据可以由pushgateway统一收集，然后由 Prometheus 统一拉取，起到给Prometheus 减压的作用。 自定义采集指标简单 缺点 Pushgateway出现问题，整个采集到的数据都会出现问题。 Pushgateway 可以持久化推送给它的所有监控数据。因此，即使你的监控已经下线，prometheus 还会拉取到旧的监控数据，需要手动清理 pushgateway 不要的数据 2 架构图123456789101112131415161718+----------------+ +----------------+ +----------------+| 北京机房 | | 上海机房 | | 深圳机房 || | | | | || Job/Service | | Job/Service | | Job/Service || | | | | | | | || v | | v | | v || Pushgateway | | Pushgateway | | Pushgateway || (北京) | | (上海) | | (深圳) || | http | | | http | | | http |+------|----------+ +------|----------+ +------|----------+ | | | +-----------+-----------+-----------+-----------+ | | v v +-------------------------------+ | 中心 Prometheus Server | | (统一拉取所有 Pushgateway) | +-------------------------------+ 3 实现步骤3.1 在每个机房部署一个Pushgateway12345678910111213141516171819202122232425262728293031323334apiVersion: apps/v1kind: Deploymentmetadata: name: pushgateway namespace: monitoring labels: app: pushgatewayspec: replicas: 1 selector: matchLabels: app: pushgateway template: metadata: labels: app: pushgateway spec: containers: - name: pushgateway image: prom/pushgateway:v1.8.0 ports: - containerPort: 9091---apiVersion: v1kind: Servicemetadata: name: pushgateway namespace: monitoringspec: selector: app: pushgateway ports: - port: 9091 targetPort: 9091 3.2 被监控服务将指标推送到本地机房的Pushgateway123456# 在北京机房的脚本中echo &quot;job_duration_seconds 120&quot; | \\curl --data-binary @- http://pushgateway-beijing:9091/metrics/job/batch-job/instance/beijing-worker-01echo &quot;job_success 1&quot; | \\curl --data-binary @- http://pushgateway-beijing:9091/metrics/job/batch-job/instance/beijing-worker-01 3.3 Prometheus从所有Pushgateway拉取指标1234567891011121314151617181920212223242526272829303132# prometheus.ymlscrape_configs: # 采集北京 Pushgateway - job_name: &#x27;pushgateway-beijing&#x27; scrape_interval: 30s static_configs: - targets: [&#x27;pushgateway-beijing.monitoring.svc.cluster.local:9091&#x27;] metric_relabel_configs: - source_labels: [] replacement: &#x27;beijing&#x27; target_label: &#x27;datacenter&#x27; # 采集上海 Pushgateway - job_name: &#x27;pushgateway-shanghai&#x27; scrape_interval: 30s static_configs: - targets: [&#x27;pushgateway-shanghai.monitoring.svc.cluster.local:9091&#x27;] metric_relabel_configs: - source_labels: [] replacement: &#x27;shanghai&#x27; target_label: &#x27;datacenter&#x27; # 采集深圳 Pushgateway - job_name: &#x27;pushgateway-shenzhen&#x27; scrape_interval: 30s static_configs: - targets: [&#x27;pushgateway-shenzhen.monitoring.svc.cluster.local:9091&#x27;] metric_relabel_configs: - source_labels: [] replacement: &#x27;shenzhen&#x27; target_label: &#x27;datacenter&#x27;","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"基于流处理的可观测性架构","slug":"Monitor/streaming","date":"2025-09-12T08:52:20.000Z","updated":"2025-09-12T09:48:34.427Z","comments":true,"path":"Monitor/streaming/","permalink":"https://aquapluto.github.io/Monitor/streaming/","excerpt":"","text":"1 架构组件说明 组件 角色 Prometheus 指标采集器负责从目标（如服务、Node Exporter、应用）拉取（scrape）指标数据。 Kafka 消息缓冲与解耦中间件作为高吞吐、可持久化的消息队列，接收来自 Prometheus 的指标流，实现生产者（Prometheus）与消费者（Flink）的解耦。 Flink 实时流处理引擎从 Kafka 消费原始指标数据，进行清洗、聚合、降采样、丰富（enrich）、异常检测等复杂计算。处理完数据后可以将数据存储到TSDB或InfluxDB Grafana 可视化展示平台从 Flink 处理后的结果存储（如数据库、时序数据库、或通过自定义数据源）读取数据并展示图表。 2 实现步骤1、配置Prometheus以推送数据到Kafka 由于Prometheus本身并不直接支持向Kafka推送数据，我们可以使用一个叫做 prometheus-kafka-adapter 的工具或者通过配置Prometheus的remote_write指向该适配器的服务地址，以remote_write的方式将数据发送给Kafka。 2、设置Kafka集群 确保你的Kafka集群已经正确安装并配置好，创建相应的topic用于接收Prometheus的metrics数据。 3、Flink消费Kafka中的Metrics数据 数据接入：使用Flink的Kafka connector来订阅Prometheus写入Kafka的数据。 数据处理 清洗：去除无效或错误的数据。 聚合与降采样：根据业务需求对数据进行聚合计算。 丰富：添加额外的信息，如地理位置信息、用户信息等。 异常检测：利用机器学习算法或其他方法识别异常行为。 输出：处理后的数据可以被写入到不同的存储系统中，如时序数据库(InfluxDB, TimescaleDB)，关系型数据库(MySQL, PostgreSQL)，或者其他适合长期存储和快速查询的系统。 4、Grafana集成 在Grafana中添加相应的数据源","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[]},{"title":"Service Account","slug":"Kubernetes/certification-authentication/service-account","date":"2025-09-12T07:30:23.000Z","updated":"2025-09-12T08:01:39.433Z","comments":true,"path":"Kubernetes/certification-authentication/service-account/","permalink":"https://aquapluto.github.io/Kubernetes/certification-authentication/service-account/","excerpt":"","text":"1 为何需要Service AccountKubernetes原生（kubernetes-native）托管运行于Kubernetes之上，通常需要直接与API Server进行交互以获取必要的信息API Server同样需要对这类来自于Pod资源中客户端程序进行身份验证，Service Account也就是设计专用于这类场景的账号，专供集群上的Pod中的进程访问API Server时使用，是API Server支持的标准资源类型之一 在Pod上使用Service Account 自动设定：Service Account通常由API Server自动创建并通过 ServiceAccount准入控制器自动关联到集群中创建的每个Pod上 ServiceAccount Admission Controller（服务账户准入控制器）：当 Pod 被创建时，该组件会自动检查 Pod 是否指定了 ServiceAccount 若未指定，自动为 Pod 关联当前命名空间的默认 ServiceAccount（通常名为 default） 同时自动将 ServiceAccount 的认证信息（令牌、CA 证书等）通过卷挂载到 Pod 的 /var/run/secrets/kubernetes.io/serviceaccount 目录，供 Pod 内进程使用 Token Controller（令牌控制器）：负责管理 ServiceAccount 的认证令牌 当新的 ServiceAccount 创建时，自动生成对应的令牌并存储到 Secret 中， 当 ServiceAccount 被删除时，自动清理关联的令牌 ServiceAccount Controller（服务账户控制器）：负责维护 ServiceAccount 资源的生命周期 当新的命名空间被创建时，自动在该命名空间中创建 default 服务账户 自定义：需要用到特殊权限时，在Pod规范上使用serviceAccountName指定要使用的特定ServiceAccount 2 ServiceAccount专用的Secret类型ServiceAccount使用专用的Secret类型存储相关的敏感信息 类型标识为 “kubernetes.io/serviceaccount” 有三个固定的数据项，键名称分别为 ca.crt：Kubernetes CA的数字证书 namespace：该ServiceAccount可适用的名称空间 token：认证到API Server的令牌 ServiceAccount Admission Controller负责完成Pod上的ServiceAccount的自动化 为每个名称空间生成一个默认的default ServiceAccount及其依赖到的Secret对象 为未定义serviceAccountName的Pod资源自动附加名称空间下的serviceaccounts/default 为定义了serviceAccountName的Pod资源检查其引用的目标对象是否存在 3 创建和使用ServiceAccount创建ServiceAccount 命令式命令 1kubectl create serviceaccount NAME [--dry-run=server|client|none] 资源配置 12345678apiVersion: v1kind: ServiceAccountmetadata: name: … namespace: …imagePullSecrets &lt;[]Object&gt; # 引用的用于下载Pod中容器镜像的Secret对象列表secrets &lt;[]Object&gt; # 以该SA运行的Pod所要使用的Secret对象组成的列表automountServiceAccountToken &lt;boolean&gt; #是否让Pod自动挂载API令牌 Pod上引用ServiceAccount对象，仅支持引用同一名称空间下的对象 123456789apiVersion: v1kind: Podmetadata: name: … namespace: …spec: serviceAccountName &lt;string&gt; # 指定sà，注意如果sa找不到，pod无法创建成功 automountServiceAccountToken &lt;boolean&gt; … 容器内部的账号身份信息挂载到容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录下了 12345678910111213141516171819[root@master1 ~]#kubectl exec -it demoapp-5b79574789-bk9n4 -- /bin/sh[root@demoapp-5b79574789-bk9n4 /]# cd /var/run/secrets/kubernetes.io/serviceaccount/[root@demoapp-5b79574789-bk9n4 /run/secrets/kubernetes.io/serviceaccount]# ls -ltotal 0lrwxrwxrwx 1 root root 13 Mar 21 03:59 ca.crt -&gt; ..data/ca.crtlrwxrwxrwx 1 root root 16 Mar 21 03:59 namespace -&gt; ..data/namespacelrwxrwxrwx 1 root root 12 Mar 21 03:59 token -&gt; ..data/token#但是容器中没有kubectl命令，可以使用kubectl镜像测试[root@master1 ~]#kubectl run kubeclient --image=bitnami/kubectl --restart=Never -it --rm --command -- /bin/sh$ cd /var/run/secrets/kubernetes.io/serviceaccount/#也可以直接kubectl get pods，因为会自动读取/var/run/secrets/kubernetes.io/serviceaccount/里面的信息#按道理来说会通过这些信息，认证到当前pod所在的集群，也就是kubectl get svc里面的kubernetes，但是这里的kubectl是pod内部的，跟系统上的kubectl没有关联关系，所以认证就会识别到自己所属的default.serviceaccount下$ kubectl --token=&quot;$(cat ./token)&quot; --server=https://kubernetes.default.svc.cluster.local --certificate-authority=./ca.crt get podsError from server (Forbidden): pods is forbidden: User &quot;system:serviceaccount:default:default&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;[root@master1 ~]#kubectl get pods kubeclient -o yamlserviceAccountName: default #其权限受限于分配给default.serviceaccount的角色和角色绑定 4 绑定角色跟User Account的操作差不多 范例：ClusterRole和RoleBinding 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@master1 ~]#kubectl create serviceaccount kubectl-user -n demo#serviceaccount必须以&lt;namespace&gt;:&lt;name&gt;来定义#这里我们不自定义角色，直接使用内置角色[root@master1 ~]#kubectl create rolebinding kubectl-user-admin --clusterrole=admin --serviceaccount=demo:kubectl-user -n demo# 测试方式一命令语法：kubectl auth can-i get pods --as=system:serviceaccount:&lt;namespace&gt;:&lt;service-account-name&gt;[root@master1 ~]#kubectl auth can-i get pods -n --as=system:serviceaccount:demo:kubectl-useryes[root@master1 ~]#kubectl auth can-i get pods -n --as=system:serviceaccount:default:kubectl-userno# 测试方式二[root@master1 ~]#vim kubeclient.yaml apiVersion: v1kind: Podmetadata: name: kubeclient namespace: demospec: containers: - image: bitnami/kubectl:latest name: kubectl imagePullPolicy: IfNotPresent command: - &quot;/bin/sh&quot; - &quot;-c&quot; - &quot;sleep 99999&quot; serviceAccountName: kubectl-user[root@master1 ~]#kubectl apply -f kubeclient.yaml[root@master1 ~]#kubectl exec -it -n demo kubeclient -- /bin/sh$ bashI have no name!@kubeclient:/$ kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-9g92g 1/1 Running 2 (36h ago) 14ddemoapp-7c58cd6bb-mzgqk 1/1 Running 2 (36h ago) 14ddemodb-0 1/1 Running 0 36hdemodb-1 1/1 Running 0 36hkubeclient 1/1 Running 0 4m13sI have no name!@kubeclient:/$ kubectl get pods -n defaultError from server (Forbidden): pods is forbidden: User &quot;system:serviceaccount:demo:kubectl-user&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;[root@master1 ~]#kubectl delete rolebindings -n demo kubectl-user-admin 范例：ClusterRole和ClusterRoleBinding 123456789101112131415161718[root@master1 ~]#kubectl create clusterrolebinding kubectl-user-admin --clusterrole=admin --serviceaccount=demo:kubectl-user[root@master1 ~]#kubectl exec -it -n demo kubeclient -- /bin/sh$ bashI have no name!@kubeclient:/$ kubectl get podsNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-9g92g 1/1 Running 2 (36h ago) 14ddemoapp-7c58cd6bb-mzgqk 1/1 Running 2 (36h ago) 14ddemodb-0 1/1 Running 0 36hdemodb-1 1/1 Running 0 36hkubeclient 1/1 Running 0 9m44sI have no name!@kubeclient:/$ kubectl get pods -n defaultNAME READY STATUS RESTARTS AGEconfigmaps-volume-demo 1/1 Running 2 (36h ago) 14ddemoapp-5b79574789-4594g 1/1 Running 0 116mdemoapp-5b79574789-n8k98 1/1 Running 5 (36h ago) 22dvolumes-nfs-csi-demo 1/1 Running 2 (36h ago) 15d 5 综合案例5.1 Jenkins运行于Kubernetes之上的 Jenkins，为了能够动态创建 Jenkins-slave 相关的Pod，也需要以拥有特定权限的ServiceAccount运行 创建 jenkins 名称空间下的ServiceAccount&#x2F;jenkins-master，并授予其具有 jenkins 名称空间的管理权限 12345678910111213141516171819202122232425262728293031323334353637383940414243444546apiVersion: v1kind: Namespacemetadata: name: jenkins---apiVersion: v1kind: ServiceAccountmetadata: name: jenkins-master namespace: jenkins---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: jenkins-master namespace: jenkinsrules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;pods/exec&quot;] verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;pods/log&quot;] verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;events&quot;] verbs: [&quot;watch&quot;]- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: jenkins-master-rolebinding namespace: jenkinsroleRef: kind: ClusterRole name: jenkins-master apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: jenkins-master namespace: jenkins 5.2 Prometheus将Prometheus部署运行于Kubernetes之上并监控集群时，需要使用专用的ServiceAccount运行prometheus-server相关的Pod 创建monitoring名称空间的ServiceAccount&#x2F;prometheus，授予它读取整个集群上所有Pod、Service、Endpoints、Nodes资源的权限 12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: v1kind: Namespacemetadata: name: monitoring---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: monitoring---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [&quot;&quot;] resources: - pods - services - endpoints - nodes - nodes/proxy verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: [&quot;extensions&quot;] resources: - ingresses verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- nonResourceURLs: [&quot;/metrics&quot;] verbs: [&quot;get&quot;]---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheus-bindingsubjects:- kind: ServiceAccount name: prometheus namespace: monitoringroleRef: kind: ClusterRole name: prometheus apiGroup: rbac.authorization.k8s.io 5.3 Kubernetes Dashboard部署Kubernetes Dashboard，使用指定的ServiceAccount来访问Dashboard，测试使用该SA的token认证，以及将该token创建为kubeconfig文件进行认证 对于dashboard来说，他的认证方法主要有两种：令牌认证和文件认证。 令牌认证：基于认证用户的唯一令牌来进行认证，有默认的超时机制，一会儿就失效了 文件认证：基于账号的token信息，创建kubeconfig文件，实现长久的认证方式 根据我们之前对serviceaccount的工作流程的学习，对于dashboard的配置也应该遵循相应的操作流程 创建专用的serviceaccount 创建对应的权限角色 将serviceaccount和权限角色进行关联绑定 范例：基于令牌认证（集群级别） 1234567891011121314151617181920212223242526272829303132333435[root@kubernetes-master1 ~]# vim kubernetes_secure_dashboard_cluster.yamlkind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: dashboard-admin annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot;roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.iosubjects:- kind: ServiceAccount name: dashboard-admin namespace: kube-system---apiVersion: v1kind: ServiceAccountmetadata: name: dashboard-admin namespace: kube-system labels: kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcile [root@kubernetes-master1 ~]# kubectl apply -f kubernetes_secure_dashboard_cluster.yaml#获取token信息[root@kubernetes-master1 ~]# kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | awk &#x27;/dashboard-admin/&#123;print $1&#125;&#x27;)Name: dashboard-admin-token-4cdrd...token: eyJhbG...qU3__b9ITbLHEytrA复制token到浏览器查看效果 范例：基于令牌认证（用户级别） 12345678910111213141516171819202122[root@kubernetes-master1 ~]# vim kubernetes_secure_dashboard_namespace.yamlapiVersion: v1kind: ServiceAccountmetadata: name: dashboard-ns namespace: default---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: dashboard-ns namespace: defaultroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: adminsubjects:- kind: ServiceAccount name: dashboard-ns namespace: default [root@kubernetes-master1 ~]# kubectl apply -f kubernetes_secure_dashboard_namespace.yaml 范例：基于kubeconfig文件（集群级别） 12345678910111213141516#设置集群信息kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --server=&quot;https://10.0.0.200:6443&quot; --embed-certs=true --kubeconfig=/root/dashboard-cluster.conf#获取秘钥信息CLUSTER_TOKEN=$(kubectl get secret -n kube-system $(kubectl get secret -n kube-system | awk &#x27;/dashboard-admin/&#123;print $1&#125;&#x27;) -o jsonpath=&#123;.data.token&#125; |base64 -d)#设置用户信息kubectl config set-credentials dashboard-cluster --token=$CLUSTER_TOKEN --kubeconfig=/root/dashboard-cluster.conf#配置上下文信息kubectl config set-context dashboard-cluster@kubernetes --cluster=kubernetes --user=dashboard-cluster --kubeconfig=/root/dashboard-cluster.conf#切换用户kubectl config use-context dashboard-cluster@kubernetes --kubeconfig=/root/dashboard-cluster.conf 然后下载dashboard-cluster.conf到windows主机，然后在浏览器上，以kubeconfig文件方式登录dashboard 范例：基于kubeconfig文件（用户级别） 12345678910111213141516#设置集群信息kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/pki/ca.crt --server=&quot;https://10.0.0.200:6443&quot; --embed-certs=true --kubeconfig=/root/dashboard-ns.conf#获取秘钥信息NAMESPACE_TOKEN=$(kubectl get secret $(kubectl get secret | awk &#x27;/dashboard-ns/&#123;print $1&#125;&#x27;) -o jsonpath=&#123;.data.token&#125; |base64 -d)#设置用户信息kubectl config set-credentials dashboard-ns --token=$NAMESPACE_TOKEN --kubeconfig=/root/dashboard-ns.conf #配置上下文信息kubectl config set-context dashboard-ns@kubernetes --cluster=kubernetes --user=def-ns-admin --kubeconfig=/root/dashboard-ns.conf#切换用户kubectl config use-context dashboard-ns@kubernetes --kubeconfig=/root/dashboard-ns.conf 然后下载dashboard-ns.conf到windows主机，然后在浏览器上，以kubeconfig文件方式登录dashboard","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"User Account","slug":"Kubernetes/certification-authentication/user-account","date":"2025-09-12T07:30:15.000Z","updated":"2025-09-12T08:00:11.556Z","comments":true,"path":"Kubernetes/certification-authentication/user-account/","permalink":"https://aquapluto.github.io/Kubernetes/certification-authentication/user-account/","excerpt":"","text":"1 静态令牌认证静态令牌认证的基础配置 令牌信息保存于文本文件中 文件格式为CSV，每行定义一个用户，由“令牌、用户名、用户ID和所属的用户组”四个字段组成，用户组为可选字段 格式：token,user,uid,&quot;group1,group2,group3&quot; 由kube-apiserver在启动时通过 --token-auth-file 选项加载 加载完成后的文件变动，仅能通过重启程序进行重载，因此，相关的令牌会长期有效 客户端在HTTP请求中，通过 “Authorization Bearer TOKEN” 标头附带令牌令牌以完成认证 做静态令牌认证有些麻烦，原因如下 因为API Server是以Pod形式运行的，我们是在Pod之外做了静态令牌认证文件，那么Pod中如何加载呢？要把这个文件定义成卷，然后把卷挂载在容器内，最后才能通过容器中的进程加载这个文件 当我们做高可用master时，比如有三个master节点，那么我们三个节点都要做这个配置，比较麻烦，所以不适用于生产环境 1.1 配置示例① 生成token命令：echo &quot;$(openssl rand -hex 3).$(openssl rand -hex 8)&quot; ② 生成static token文件 ③ 配置kube-apiserver加载该静态令牌文件以启用相应的认证功能 ④ 测试命令：curl -k -H &quot;Authorization: Bearer TOKEN&quot; https://API_SERVER:6443/api/v1/namespaces/default/pods/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@master1 ~]#echo &quot;$(openssl rand -hex 3).$(openssl rand -hex 8)&quot;b986e0.3ec5c9d82b5e7503[root@master1 ~]#echo &quot;$(openssl rand -hex 3).$(openssl rand -hex 8)&quot;6fd20a.fde3a95eb81d12c4[root@master1 ~]#cd /etc/kubernetes/[root@master1 kubernetes]#mkdir authfiles[root@master1 kubernetes]#vim authfiles/token.csvb986e0.3ec5c9d82b5e7503,tom,1001,kubeadmin6fd20a.fde3a95eb81d12c4,jerry,1002,kubeuser#不能直接打开manifests/kube-apiserver.yaml做改动，因为kubernetes会间隔自动加载这个文件，如果我们边改，它就会边加载，会造成不好的结果，所以需要复制到其他路径下改，改完直接覆盖[root@master1 kubernetes]#cp manifests/kube-apiserver.yaml /tmp[root@master1 kubernetes]#vim /tmp/kube-apiserver.yaml spec: containers: - command: - kube-apiserver - --authorization-mode=Node,RBAC - --token-auth-file=/etc/kubernetes/authfiles/token.csv volumeMounts: - mountPath: /etc/kubernetes/authfiles/token.csv name: static-tokens readOnly: true volumes: - hostPath: path: /etc/kubernetes/authfiles/token.csv type: FileOrCreate name: static-tokens[root@master1 kubernetes]#cp /tmp/kube-apiserver.yaml /etc/kubernetes/manifests/[root@master1 kubernetes]#kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGEkube-apiserver-master1.wu.org 1/1 Running 0 29s#说明：原来我们在master节点执行kubectl get pods时，是依赖于~/.kube/config的信息做认证和鉴权才能实现的#没有证书所以失败[root@node1 ~]#kubectl --server=https://10.0.0.183:6443 --token=&quot;b986e0.3ec5c9d82b5e7503&quot; get podsUnable to connect to the server: tls: failed to verify certificate: x509: certificate signed by unknown authority#认证成功了，只是没有权限而已，不指定证书的话，也可以用--insecure-skip-tls-verify=true忽略证书[root@node1 ~]#kubectl --server=https://10.0.0.183:6443 --token=&quot;b986e0.3ec5c9d82b5e7503&quot; --certificate-authority=/etc/kubernetes/pki/ca.crt get podsError from server (Forbidden): pods is forbidden: User &quot;tom&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;#curl命令做测试也行[root@node1 ~]#curl -k -H &quot;Authorization: Bearer b986e0.3ec5c9d82b5e7503&quot; https://10.0.0.183:6443/api/v1/namespaces/default/pods/&#123; &quot;kind&quot;: &quot;Status&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123;&#125;, &quot;status&quot;: &quot;Failure&quot;, &quot;message&quot;: &quot;pods is forbidden: User \\&quot;tom\\&quot; cannot list resource \\&quot;pods\\&quot; in API group \\&quot;\\&quot; in the namespace \\&quot;default\\&quot;&quot;, &quot;reason&quot;: &quot;Forbidden&quot;, &quot;details&quot;: &#123; &quot;kind&quot;: &quot;pods&quot; &#125;, &quot;code&quot;: 403&#125; 1.2 设定kubeconfig文件① 定义Cluster：提供包括集群名称、API Server URL和信任的CA的证书相关的配置；clusters配置段中的各列表项名称需要惟一； 1234# 如果是高可用集群，有负载均衡，--server应该指定负载均衡所在的地址或域名 # --kubeconfig不指定的话，自动创建在了.kube/config# --embed-certs=true表示将证书隐藏起来kubectl config set-cluster kube-test --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=&quot;https://kubeapi.wu.com:6443&quot; --kubeconfig=$HOME/.kube/kubeusers.conf ② 定义User：添加身份凭据，使用静态令牌文件认证的客户端提供令牌令牌即可 1kubectl config set-credentials jerry --token=&quot;$JERRY_TOKEN&quot; --kubeconfig=$HOME/.kube/kubeusers.conf ③ 定义Context：为用户jerry的身份凭据与kube-test集群建立映射关系 1kubectl config set-context jerry@kube-test --cluster=kube-test --user=jerry --kubeconfig=$HOME/.kube/kubeusers.conf ④ 设定Current-Context 1kubectl config use-context jerry@kube-test --kubeconfig=$HOME/.kube/kubeusers.conf 操作范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@node1 ~]#kubectl config set-cluster kube-test --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=&quot;https://kubeapi.wu.org:6443&quot;[root@node1 ~]#kubectl config get-clusters NAMEkube-test#token是之前在/etc/kubernetes/authfiles/token.csv定义的[root@node1 ~]#kubectl config set-credentials jerry --token=&quot;6fd20a.fde3a95eb81d12c4&quot;[root@node1 ~]#kubectl config get-usersNAMEjerry#创建context[root@node1 ~]#kubectl config set-context jerry@kube-test --cluster=kube-test --user=jerry#指定当前默认使用的context[root@node1 ~]#kubectl config use-context jerry@kube-test[root@node1 ~]#kubectl config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: https://kubeapi.magedu.com:6443 name: kube-testcontexts:- context: cluster: kube-test user: jerry name: jerry@kube-testcurrent-context: jerry@kube-testkind: Configpreferences: &#123;&#125;users:- name: jerry user: token: REDACTED#配置完kubeconfig文件，就不用像上例那样加一堆选项了，因为会自动读取.kube/config里的信息[root@node1 ~]#kubectl get podError from server (Forbidden): pods is forbidden: User &quot;jerry&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;#假如我们又创建了tom@mykube，如果我们现在的current-context不是tom@mykube的话，直接kubectl get pod会报错，要么指定current-context为tom@mykube，要么可以加--context选项[root@node1 ~]#kubectl get pods --context=&#x27;tom@mykube&#x27;# 如果还想在其他机器上使用，将jerry@kube-test导出，然后写入那台机器上的~/.kube/config中就可以，或者其他路径kubectl config view --minify --flatten --context=&#x27;jerry@kube-test&#x27; &gt; jerry-context.kubeconfig.yam] 2 X509数字证书认证X509客户端认证依赖于PKI证书体系，kubeadm部署Kubernetes集群时会自动生成所需要的证书，它们位于 /etc/kubernetes/pki 目录下 路径 默认通用名称（CN） 描述 ca.crt,key kubernetes-ca Kubernetes 通用 CA etcd&#x2F;ca.crt,key etcd-ca 用于所有与 etcd 相关的功能 front-proxy-ca.crt,key kubernetes-front-proxy-ca 用于前端代理 下图是依赖到的PKI体系。另外，对Service Account的token进行签名还需要用到一个可选的密钥对儿 所有的证书 各kubelet的证书可在Bootstrap过程中自动生成证书签署请求，而后由Kubernetes CA予以签署 各kube-proxy、kube-scheduler和kube-controller-manager也都有相应的数字证书以完成向API Server的身份认证 X509数字证书认证相较于静态令牌认证的好处是，证书都是由Kubernetes CA签署的，而Kubernetes CA在任何一个master节点上都是一样的，就不需要每个master节点都做一次。而证书其实只需要客户端持有就行，服务器端可以没有，只要服务器端持有CA，对于客户端发来的证书进行验证就行 2.1 配置示例一、创建客户端私钥和证书签署请求，以下操作在master节点上以 /etc/kubernetes/ 为工作目录 ① 生成私钥： 12cd /etc/kubernetes/pki/(umask 077; openssl genrsa -out mason.key 4096) ② 根据私钥创建证书签署请求文件： 12# CN表示用户名，O表示组openssl req -new -key ./mason.key -out ./mason.csr -subj &quot;/CN=mason/O=developers&quot; ③ 由Kubernetes CA签署证书： 12345openssl x509 -req -days 365 -CA ./ca.crt -CAkey ./ca.key -CAcreateserial -in ./mason.csr -out ./mason.crt# 如果是二进制安装，k8s的CA证书的可能在/opt/kubernetes/ssl/ca.pem ------&gt; 即ca.crt/opt/kubernetes/ssl/ca-key.pem -----&gt; 即ca.key ④ 将pki目录下的 mason.crt、mason.key 和 ca.crt 复制到某部署了kubectl的主机上，即可进行测试，这里以k8s-node01为示例；只需要复制mason.crt 和 mason.key 即可，因为集群工作节点上已经有ca.crt文件 1scp -rp ./&#123;mason.crt,mason.key&#125; k8s-node01:/etc/kubernetes/pki 二、在k8s-node01上发起访问测试 ① 使用kubectl测试： 1kubectl get pods --client-certificate=$HOME/.certs/mason.crt --client-key=$HOME/.certs/mason.key --server=https://kubeapi.magedu.com:6443/ --certificate-authority=/etc/kubernetes/pki/ca.crt ② 也可以使用curl命令进行测试 1curl --cert ./mason.crt --key ./mason.key --cacert ./ca.crt https://172.29.6.1:6443/api/v1/namespaces/default/pods 2.2 设定kubeconfig文件① 定义Cluster：我们利用之前的Cluster，使用不同的身份凭据访问同一集群时，集群相关的配置无须重复定义 ② 定义User：添加身份凭据，基于X509客户端证书认证时，需要提供客户端证书和私钥 1kubectl config set-credentials mason --embed-certs=true --client-certificate=/etc/kubernetes/pki/mason.crt --client-key=/etc/kubernetes/pki/mason.key --kubeconfig=$HOME/.kube/kubeusers.conf ③ 定义Context：为用户mason的身份凭据与kube-test集群建立映射关系 1kubectl config set-context mason@kube-test --cluster=kube-test --user=mason --kubeconfig=$HOME/.kube/kubeusers.conf ④ 设定Current-Context 1kubectl config use-context mason@kube-test --kubeconfig=$HOME/.kube/kubeusers.conf 3 Role和RoleBinding1kubectl create role NAME --verb=verb --resource=resource.group/subresource --namespace=namespace [--resource-name=resourcename] verb：允许在资源上使用的操作（verb）列表 resources.group/subresource：操作可施加的目标资源类型或子资源列表 resourcename：特定的资源对象列表，可选 1kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME --namespace=namespace [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] 可以绑定到Role，也可以绑定到ClusterRole，后者会将ClusterRole的权限缩减至当前名称空间之内 Subject可以是User、Group或者ServiceAccount 范例：基于此前基于静态令牌创建的 jerry@kube-test 来做鉴权测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120[root@node1 ~]#kubectl config current-context jerry@kube-test#--namespace不指定默认就是限定于default名称空间下[root@master1 ~]#kubectl create role pods-role --verb=&quot;get,list,watch&quot; --resource=&quot;pods&quot; --namespace=default --dry-run=client -o yaml &gt; pods-role.yaml[root@master1 ~]#cat pods-role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: creationTimestamp: null name: pods-role namespace: defaultrules:- apiGroups: # 对哪些api组内的资源进行操作 - &quot;&quot; # pod资源属于核心API组，所以留空字符即可 resources: # 对哪些资源定义 - pods verbs: # 操作权限定义，也可以用*，表示全部权限 - get - list - watch [root@master1 ~]#kubectl apply -f pods-role.yamlrole.rbac.authorization.k8s.io/pods-role created[root@master1 ~]#kubectl get roleNAME CREATED ATpods-role 2025-03-22T12:25:59Z[root@master1 ~]#kubectl create rolebinding jerry-pods-role --role=pods-role --user=jerry --namespace=default --dry-run=client -o yaml &gt; jerry-pods-role.yaml[root@master1 ~]#cat jerry-pods-role.yaml apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: creationTimestamp: null name: jerry-pods-role namespace: defaultroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: pods-rolesubjects: # 用户实体- apiGroup: rbac.authorization.k8s.io kind: User name: jerry [root@master1 ~]#kubectl apply -f jerry-pods-role.yaml rolebinding.rbac.authorization.k8s.io/jerry-pods-role created[root@master1 ~]#kubectl get rolebindingsNAME ROLE AGEjerry-pods-role Role/pods-role 9s[root@node1 ~]#kubectl get podsNAME READY STATUS RESTARTS AGEconfigmaps-volume-demo 1/1 Running 2 (32h ago) 14ddemoapp-5b79574789-bk9n4 1/1 Running 5 (32h ago) 21ddemoapp-5b79574789-n8k98 1/1 Running 5 (32h ago) 21dvolumes-nfs-csi-demo 1/1 Running 2 (32h ago) 15d#只能看dafault名称空间[root@node1 ~]#kubectl get pods -n demoError from server (Forbidden): pods is forbidden: User &quot;jerry&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;demo&quot;#只能看pods资源[root@node1 ~]#kubectl get svcError from server (Forbidden): services is forbidden: User &quot;jerry&quot; cannot list resource &quot;services&quot; in API group &quot;&quot; in the namespace &quot;default&quot;#只能查，不能增删改[root@node1 ~]#kubectl delete pods demoapp-5b79574789-bk9n4Error from server (Forbidden): pods &quot;demoapp-5b79574789-bk9n4&quot; is forbidden: User &quot;jerry&quot; cannot delete resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;[root@master1 ~]#vim pods-role.yamlapiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: creationTimestamp: null name: pods-role namespace: defaultrules:- apiGroups: - &quot;&quot; resources: - pods verbs: - get - list - watch- apiGroups: - &quot;apps&quot; # Deployment、Replicast属于apps这个api组 resources: - deployments verbs: - get - list - watch [root@node1 ~]#kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEdemoapp 2/2 2 2 22d[root@master1 ~]#vim pods-role.yamlrules:- apiGroups: - &quot;&quot; resources: - pods - persistentvolumes #增加PV这个集群资源 verbs: - get - list - watch [root@master1 ~]#kubectl apply -f pods-role.yaml#role不能对集群资源做授权[root@node1 ~]#kubectl get pvError from server (Forbidden): persistentvolumes is forbidden: User &quot;jerry&quot; cannot list resource &quot;persistentvolumes&quot; in API group &quot;&quot; at the cluster scope 4 ClusterRole和ClusterRoleBindingClusterRole 命令式命令 1kubectl create clusterrole NAME --verb=verb --resource=resource.group [--resource-name=resourcename] ClusterRoleBinding 命令式命令 12kubectl create clusterrolebinding NAME --clusterrole=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] 范例：基于此前基于静态令牌创建的 jerry@kube-test 来做鉴权测试 12345678910111213141516171819202122232425262728293031323334353637383940414243#就算指定--namespace，也可以访问所有名称空间下的资源[root@master1 ~]#kubectl create clusterrole pods-clusterrole --verb=&quot;get,list,watch&quot; --resource=&quot;pods&quot; --dry-run=client -o yaml &gt; pods-clusterrole.yaml[root@master1 ~]#kubectl apply -f pods-clusterrole.yaml [root@master1 ~]#kubectl create clusterrolebinding jerry-clusterrole --clusterrole=pods-clusterrole --user=jerry --dry-run=client -o yaml &gt; jerry-clusterrole.yaml[root@master1 ~]#kubectl apply -f jerry-clusterrole.yaml #相较于Role，ClusterRole是可以看到所有名称空间下的资源[root@node1 ~]#kubectl get podsNAME READY STATUS RESTARTS AGEconfigmaps-volume-demo 1/1 Running 2 (34h ago) 14ddemoapp-5b79574789-4594g 1/1 Running 0 16mdemoapp-5b79574789-n8k98 1/1 Running 5 (34h ago) 22dvolumes-nfs-csi-demo 1/1 Running 2 (34h ago) 15d[root@node1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-9g92g 1/1 Running 2 (34h ago) 14ddemoapp-7c58cd6bb-mzgqk 1/1 Running 2 (34h ago) 14ddemodb-0 1/1 Running 0 34hdemodb-1 1/1 Running 0 34h[root@master1 ~]#vim pods-clusterrole.yamlrules:- apiGroups: - &quot;&quot; resources: - pods - persistentvolumes #增加PV这个集群资源 verbs: - get - list - watch #也可以给集群资源授权[root@node1 ~]#kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-2cf16f3f-9fba-45cc-b0e3-9579add5196d 2Gi RWO Delete Bound demo/data-demodb-1 nfs-csi 34hpvc-372404ed-8c31-4ffc-bbf1-5844e3bbf923 2Gi RWO Delete Bound demo/data-demodb-0 nfs-csi 34hpvc-a9a84173-7308-46ac-99e8-c140cc7f012e 10Gi RWX Delete Bound default/pvc-nfs-dynamic nfs-csi 15dpvc-e2030cb4-39f7-4ea3-840c-7069b66cb949 2Gi RWO Delete Bound demo/data-demodb-2 nfs-csi 34h 5 ClusterRole和RoleBinding这里不需要我们创建Role了，直接用的是ClusterRole的权限，但是我们上面讲过，即便将Subject使用RoleBinding关联到了ClusterRole上，该角色赋予到Subject的权限也会降级到RoleBinding所属的Namespace范围之内 12345678910111213141516171819202122232425262728293031[root@master1 ~]#kubectl create rolebinding jerry-cluster-admin --clusterrole=cluster-admin --user=jerry --namespace=default --dry-run=client -o yaml &gt; jerry-cluster-admin.yaml[root@master1 ~]#kubectl apply -f jerry-cluster-admin.yaml rolebinding.rbac.authorization.k8s.io/jerry-cluster-admin created[root@master1 ~]#kubectl get rolebindingsNAME ROLE AGEjerry-cluster-admin ClusterRole/cluster-admin 12s#啥都能做[root@node1 ~]#kubectl get podsNAME READY STATUS RESTARTS AGEconfigmaps-volume-demo 1/1 Running 2 (34h ago) 14ddemoapp-5b79574789-bk9n4 1/1 Running 5 (34h ago) 22ddemoapp-5b79574789-n8k98 1/1 Running 5 (34h ago) 22dvolumes-nfs-csi-demo 1/1 Running 2 (34h ago) 15d[root@node1 ~]#kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEdemoapp 2/2 2 2 22d[root@node1 ~]#kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-nfs-dynamic Bound pvc-a9a84173-7308-46ac-99e8-c140cc7f012e 10Gi RWX nfs-csi 15d[root@node1 ~]#kubectl delete pods demoapp-5b79574789-bk9n4pod &quot;demoapp-5b79574789-bk9n4&quot; deleted#但仅限于其名称空间下[root@node1 ~]#kubectl get pods -n demoError from server (Forbidden): pods is forbidden: User &quot;jerry&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;demo&quot;","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"kubeconfig","slug":"Kubernetes/certification-authentication/kubeconfig","date":"2025-09-12T07:30:08.000Z","updated":"2025-09-13T14:10:13.282Z","comments":true,"path":"Kubernetes/certification-authentication/kubeconfig/","permalink":"https://aquapluto.github.io/Kubernetes/certification-authentication/kubeconfig/","excerpt":"","text":"1 kubeconfig概念kubeconfig是YAML格式的文件，用于存储身份认证信息，以便于客户端加载并认证到API Server，进入到kubernetes集群内部 kubeconfig保存有认证到一至多个Kubernetes集群的相关配置信息，并允许管理员按需在各配置间灵活切换 clusters：记录你需要访问的Kubernetes集群访问端点（API Server）列表（k8s集群的地址） users：记录认证到API Server的身份凭证列表（基于证书和秘钥信息创建登录k8s集群的用户） contexts：将每一个user可认证到的cluster建立关联的上下文列表（将登录用户和目标k8s集群关联在一起，形成k8s集群入口） current-context：指定当前默认使用的context（设定默认的k8s集群入口） 2 设定kubeconfig文件kubectl config SUBCOMMAND [options] 123456# SUBCOMMAND说明打印加载的kubeconfig：viewcluster相关的子命令：get-clusters；set-cluster；delete-clusteruser相关的子命令：get-users；set-credentials；delete-usercontext相关的子命令：get-contexts；set-context；delete-context；rename-contextcurrent-context相关的子命令：current-context；use-context 范例：查看各个组件kubeconfig信息 12[root@master1 ~]#kubectl config view --kubeconfig=/etc/kubernetes/scheduler.conf[root@master1 ~]#kubectl config view --kubeconfig=/etc/kubernetes/admin.conf 3 合并kubeconfig文件客户端能够通过下列途径获取到 kubeconfig 文件，加载的kubeconfig文件的次序如下（这里优先级从高到低） 加上 --kubeconfig=/etc/kubernetes/admin.conf 选项 KUBECONFIG环境变量：其值是包含有kubeconfig文件的列表，export KUBECONFIG=&#39;/etc/kubernetes/admin.conf&#39; ，支持指定多个kubeconfig文件，使用 : 隔开 默认路径：$HOME/.kube/config 其中若设置了KUBECONFIG环境变量，是一个文件列表 file1:file2:file3 ，且都定义了 kube-test,jerry,jerry@kube-test ，那么谁生效呢？就要进行合并，处理规则如下 忽略不存在的文件 遇到内容无法反序列化的文件时，将生成错误信息 若都是有效的文件，按照文件列表中自左而右，第一个出现的配置生效，例如如果 file1 和 file2 都定义了同一个 context（ jerry@kube-test），则使用 file1 中的配置 context的判定机制 若使用了 --context 选项，则加载该选项指定要使用的上下文 否则，将使用合并后的 kubeconfig 文件中的 current-context 的设定 验证KUBECONFIG环境变量合并kubeconfig文件的方法 123456echo $KUBECONFIGexport KUBECONFIG=&quot;/root/.kube/mykube.conf:/etc/kubernetes/admin.conf&quot;kubectl config viewkubectl get podskubectl --context=&quot;tom@mykube&quot; get podskubectl --context=&quot;kubernetes-admin@kubernetes&quot; get pods","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"认证与鉴权体系概论","slug":"Kubernetes/certification-authentication/conspect","date":"2025-09-12T07:30:02.000Z","updated":"2025-09-12T07:34:58.756Z","comments":true,"path":"Kubernetes/certification-authentication/conspect/","permalink":"https://aquapluto.github.io/Kubernetes/certification-authentication/conspect/","excerpt":"","text":"1 API Server内置的访问控制机制API Server是Kubernetes集群的网关，是能够与etcd通信唯一入口，确保对API Server的安全访问至关重要，验证流程如下 认证(Authentication)：基于凭证用于确认用户身份。、 均不成功，则失败，或以“匿名者”身份访问 建议禁用“匿名者” 授权(Authorization： 验证该凭证关联的权限（apiGroup、resources、verbs），支持的模块如下 Node：专用的授权模块，它基于kubelet将要运行的Pod向kubelet进行授权 ABAC：通过将属性（包括资源属性、用户属性、对象和环境属性等）组合 在一起的策略，将访问权限授予用户 RBAC：基于企业内个人用户的角色来管理对计算机或网络资源的访问的鉴权方法 Webhook：用于支持同Kubernetes外部的授权机制进行集成 AlwaysDeny和AlwaysAllow 准入控制(Admission Controllers)：将数据写入etcd前，触发两种webhook，validating（校验）和 mutating（补全或订正），负责检查内容的有效性，因此仅对“写”操作有效 2 认证体系2.1 Kubernetes上的用户Kubernetes系统的用户分为两类 Service Account：服务账户，给Pod用的，让Pod内的进程拥有访问API Server时使用的身份信息及操作权限 API Server使用ServiceAccount类型的资源对象来保存该类账号 认证到API Server的认证信息称为Service Account Token，它们保存于同名的专用类型的Secret对象中 名称空间级别，不需要提供集群内部证书的认证签名，即凭证，会自动关联 User Account：用户账户，指非Pod类的客户端访问API Server时使用的身份标识，一般是现实中的“人” API Server没有为这类账户提供保存其信息的资源类型，相关的信息通常保存于外部的文件或认证系统中 身份核验操作可由API Server进行，也可能是由外部身份认证服务完成 本身非由Kubernetes管理，因而作用域为整个集群级别 如果需要操作k8s资源，需要集群内部证书的认证签名，即凭证 不能被识别为Service Account，也不能被识别为User Account的用户，即“匿名用户” 2.2 身份认证策略X.509数字证书认证：本质上就是TLS双向认证 在双向TLS通信中，客户端持有数字证书，而API Server信任客户端证书的颁发者 信任的CA，需要在kube-apiserver程序启动时，通过 --client-ca-file 选项传递 认证通过后，客户端数字证书中的CN（Common Name）即被识别为用户名，而O（Organization）被识别为组名 kubeadm部署的Kubernetes集群，默认使用 /etc/kubernetes/pki/ca.crt 进行客户端认证 /etc/kubernetes/pki/ca.crt 是kubeadm为Kubernetes各组件间颁发数字证书的CA 静态令牌文件 令牌信息保存于文本文件中 由kube-apiserver在启动时通过 --token-auth-file 选项加载 加载完成后的文件变动，仅能通过重启程序进行重载，因此，相关的令牌会长期有效 客户端在HTTP请求中，通过 “Authorization Bearer TOKEN” 标头附带令牌令牌以完成认证 Service Account令牌 该认证方式将由kube-apiserver程序内置直接启用，所以Service Account不需要提供凭证 它借助于经过签名的Bearer Token来验证请求 签名时使用的密钥可以由 --service-account-key-file 选项指定，也可以默认使用API Server的tls私钥 用于将Pod认证到API Server之上，以支持集群内的进程与API Server通信 Kubernetes可使用ServiceAccount准入控制器自动为Pod关联ServiceAccount OpenID Connect（OIDC）令牌 OAuth2认证机制，通常由底层的IaaS服务所提供 Webhook令牌认证 是一种用于验证Bearer Token的回调机制 能够扩展支持外部的认证服务，例如LDAP等 身份认证代理 由kube-apiserver从请求报文的特定HTTP标头中识别用户身份，相应的标头名称可由特定的选项配置指定 kube-apiserver应该基于专用的CA来验证代理服务器身份 2.3 API Server启用的身份认证机制基于认证插件支持多种认证方式，而相应认证插件的启用需要经由kube-apiserver上的专用选项完成 kubeadm v1.22 部署的集群默认启用的认证机制如右图红框中的选项 (/etc/kubernetes/manifests/kube-apiserver.yaml)，它们依次是 X509客户端证书认证 Bootstrap令牌认证 身份认证代理 Service Account认证 注意：API Server并不保证各认证插件的生效次序与定义的次序相同 2.4 kubelet启用的身份认证机制各kubelet也会监听一些套接字，提供一个小型的REST API 10250是具有所在节点上Pod管理权限的读写端口，应谨慎管理 10255仅提供只读操作，是REST API的子集，一般是禁用的，有信息泄露的风险 另外，10248是本地healthz端点使用的端口 kubelet的REST API端点默认通过TCP协议的10250端口提供，支持管理操作 需要对客户端身份进行认证（/var/lib/kubelet/config.yaml） 启用的身份认证 显式禁用匿名用户（anonymous.enabled） webhook x509客户端证书认证 API Server是该API端点的客户端，API Server向kubelet证明身份时，会出示由某个CA颁发的数字证书，kubelet需要在验证客户端身份时信任给API Server颁发数字证书的CA，否则无法确认对方是否为真实的 API Server 3 鉴权体系在kube-apiserver上使用 --authorization-mode 选项进行定义授权模块（kube-apiserver.yaml），多个模块彼此间使用逗号分隔，kubeadm部署的集群，默认启用了Node和RBAC 3.1 RBAC概念实体（Entity）：在RBAC也称为Subject，通常指的是User Account、Group或者是Service Account 角色（Role）：承载资源操作权限的容器； 资源（Resource）：在RBAC中也称为Object，指代Subject期望操作的目标，例如Secret、Pod及Service对象等； 仅限于 /api/v1/… 及 /apis/&lt;group&gt;/&lt;version&gt;/… 起始路径的资源型对象 其它路径对应的端点均被视作 “非资源类请求（Non-Resource Requests）”，例如 &#x2F;healthz 等端点； 动作（Actions、verbs）：Subject可以于Object上执行的特定操作，具体的可用动作取决于Kubernetes的定义； 资源型对象 只读操作：get、list、watch等 读写操作：create、update、patch、delete、deletecollection等 非资源型端点仅支持get操作 角色绑定（RoleBinding）：将角色关联至实体上，它能够将角色具体的操作权限赋予给实体 3.2 RBAC的基本工作逻辑Role、RoleBinding、ClusterRole和ClusterRoleBinding是API Server上的标准资源类型 角色有两类 —&gt; 相当于职位 role（私有角色）：此类角色只能被单个名称空间看到并加以引用 —–&gt; 即该类角色是私有的 clusterrole（公共的角色）：此类角色能够被所有名称空间看到并加以引用 —-&gt; 即该类角色是公有的 用户需要绑定角色 —&gt; 相当于任职 rolebinding：绑定role私有角色，授予的权限只能在一个名称空间中用 clusterrolebingding：绑定clusterrole角色，授予的权限能在所有名称空间中用 rolebinding绑定clusterrole：拿到一个公共的权限声明，然后用rolebingding绑定死在一个名称空间中使用 多个namespace中的role角色都一致，如果都使用内部的RoleBingding的话，每个namespace都必须单独创建role，而使用ClusterRole的话，只需要一个就可以了，大大的减轻批量使用namespace中的RoleBingding操作 强调：角色本身声明的名称空间，作用只是声明自己这个角色能够被一个还是所有名称空间看到，至于说角色下的rules规则能在哪个还是所有名称空间执行，取决于绑定方式 3.3 默认的ClusterRole及ClusterRoleBinding启用RBAC鉴权模块时，API Server会自动创建一组ClusterRole和ClusterRoleBinding对象 多数都以 system: 为前缀，也有几个面向用户的ClusterRole未使用该前缀，如cluster-admin(所属system:masters组)、admin等 它们都默认使用 “kubernetes.io/bootstrapping: rbac-defaults” 这一标签 默认的ClusterRole大体可以分为如下5个类别 API发现相关的角色 包括 system:basic-user、system:discovery 和 system:public-info-viewer 面向用户的角色 包括cluster-admin、admin、edit和view 核心组件专用的角色 包括 system:kube-scheduler、system:volume-scheduler、system:kube-controller-manager、system:node和system:node-proxier等 其它组件专用的角色 包括 system:kube-dns、system:node-bootstrapper、system:node-problem-detector 和 system:monitoring 等 内置控制器专用的角色 3.4 面向用户的角色某些默认角色没有 system: 前缀。这些角色旨在成为面向用户的角色。 1234567891011121314151617181920212223[root@master1 ~]#kubectl get clusterrole cluster-admin -o yamlapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; creationTimestamp: &quot;2025-02-28T08:45:50Z&quot; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: &quot;111&quot; uid: 94e8518b-f66d-44bd-be72-65ad898ed0e3rules:- apiGroups: - &#x27;*&#x27; resources: - &#x27;*&#x27; verbs: - &#x27;*&#x27;- nonResourceURLs: - &#x27;*&#x27; verbs: - &#x27;*&#x27;","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"kubectl命令","slug":"Kubernetes/conspect/kubectl","date":"2025-09-12T06:49:07.000Z","updated":"2025-09-12T07:22:53.612Z","comments":true,"path":"Kubernetes/conspect/kubectl/","permalink":"https://aquapluto.github.io/Kubernetes/conspect/kubectl/","excerpt":"","text":"1 语法介绍集群中的管理操作几乎都可以使用kubectl命令完成 12https://kubernetes.io/zh-cn/docs/reference/kubectl/kubectl [command] [TYPE] [NAME] [flags] command：指定在一个或多个资源上要执行的操作。例如：create、get、describe、delete、apply等 TYPE：指定资源类型(如：pod、node、services、deployments等)。资源类型大小写敏感，可以指定单数、复数或缩写形式 NAME：指定资源的名称。名称大小写敏感。如果省略名称空间，则显示默认名称空间资源的详细信息或者提示：No resources found in default namespace(在默认命名空间中没有找到该资源) flags：指定可选的标记。例如，可以使用 -s 或 –server 标识来指定Kubernetes API服务器的地址和端口；-n 指定名称空间；等等。 在多个资源上执行操作时，可以通过类型 [TYPE] 和名称 [NAME] 指定每个资源，也可以指定一个或多个yml文件。按类型和名称指定资源 2 kubectl命令补全12kubeadm completion bash &gt; /etc/profile.d/kubeadm_completion.shsource /etc/profile.d/kubeadm_completion.sh 3 create命令12345678910111213kubectl create deployment --name=&lt;deployment_name&gt; --image=&lt;image&gt; --replicas=&lt;number&gt; --dry-run=client|server--dry-run #表示生成配置文件而不实际创建资源，client表明仅在客户端进行模拟操作，不会与 Kubernetes API 服务器进行交互，server表明会将请求发送到 Kubernetes API 服务器--image=&lt;image&gt; #指定容器镜像--replicas=&lt;number&gt; #指定副本数量kubectl create service --name=&lt;service_name&gt; --tcp=&lt;service-port&gt;:&lt;pod-Port&gt; --type=&lt;type&gt;--name=&lt;service_name&gt; #指定服务名称--tcp=&lt;service-port&gt;:&lt;pod-Port&gt; #指定服务端口和目标pod端口映射--type=&lt;type&gt; #指定服务类型，如ClusterIP、NodePort等。kubectl create namespace NAME [--dry-run=server|client|none] [options]--dry-run #必须是“无”、“服务器”或“客户端”。 如果是客户端策略，则只打印要发送的对象，而不发送。 如果是服务器策略，则提交服务器端请求，不持久化资源 范例：创建Deployment和Service资源 1234567891011#根据文件[root@master ~]# kubectl create -f demo-deployment.yaml[root@master ~]# kubectl create -f demo-service.yaml#根据输入[root@master ~]# kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 -n stage#仅打印资源清单（--dry-run=client）[root@master ~]#kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 --port=80 --dry-run=client --replicas=3 -o yaml[root@master ~]#kubectl create service clusterip demoapp --tcp=80:80 --dry-run=client -o yaml 4 run命令在命令行中通过参数指定资源的属性，此方式简单直观，比较适合临时测试或实验使用 12345# 该命令只能创建pod，创建了一个名为pod-test的pod资源kubectl run pod-test --image=nginx:1.8.1 # 也可以启用临时容器进行测试，exit退出后则删除kubectl run alpine --rm -ti --image=alpine -- /bin/sh 5 cp命令拷贝文件或者目录到pod容器中 用于pod和外部的文件交换，类似于docker的cp，就是将容器中的内容和外部的内容进行交换。 1kubectl cp &lt;file-spec-src&gt; &lt;file-spec-dest&gt; [options] 拷贝宿主机本地文件夹到pod 1[root@master ~]# kubectl cp /tmp/foo_dir &lt;some-pod&gt;:/tmp/bar_dir 指定namespace的拷贝pod文件到宿主机本地目录 1[root@master ~]# kubectl cp &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/foo /tmp/bar 对于多容器pod，用 -c 指定容器名 1[root@master ~]# kubectl cp /tmp/foo &lt;some-pod&gt;:/tmp/bar -c &lt;specific-container&gt; 6 cordon命令用于标记某个节点不可调度（打上污点） 标记 my-node 为 unschedulable，禁止pod被调度过来。注意这时现有的pod还会继续运行，不会被驱逐。 1[root@master ~]# kubectl cordon my-node 7 drain命令用于在维护期间排除节点 drain字面意思为排水，实际就是把my-node的pod平滑切换到其他node，同时标记pod为unschedulable，也就是包含了cordon命令。 1[root@master ~]# kubectl drain my-node 但是直接使用命令一般不会成功，建议在要维护节点时，加上以下参数： 1kubectl drain my-node --ignore-daemonsets --force --delete-local-data –ignore-daemonsets 忽略daemonset部署的pod –force 直接删除不由workload对象（Deployment、Job等）管理的pod –delete-local-data 直接删除挂载有本地目录(empty-dir方式）的pod 8 scale命令扩容或缩容 Deployment、ReplicaSet、Replication Controller、Statefulset或 Job 中Pod数量 scale也可以指定多个前提条件，如：当前副本数量或 –resource-version ，进行伸缩比例设置前，系统会先验证前提条件是否成立。这个就是弹性伸缩策略。只针对工作负载型控制器才能扩容和缩容 1kubectl scale TYPE/NAME [--resource-version=version] [--current-replicas=count] --replicas=COUNT (-f FILENAME | TYPE NAME) 将名为foo中的pod副本数设置为3。 1[root@master ~]# kubectl scale --replicas=3 rs/foo 将由 foo.yaml 配置文件中指定的资源对象和名称标识的Pod资源副本设为3 1[root@master ~]# kubectl scale --replicas=3 -f foo.yaml 如果当前副本数为2，则将其扩展至3。 1[root@master ~]# kubectl scale --current-replicas=2 --replicas=3 deployment/mysql 设置多个RC中Pod副本数量 1[root@master ~]# kubectl scale --replicas=5 rc/foo rc/bar rc/baz 9 taint命令给这个节点打上污点，该节点就不会被调度 NoSchedule：一定不被调度 PreferNoSchedule：尽量不被调度（也有被调度的几率） NoExecute：不会调度，并且还会驱逐Node已有Pod 1234# 打污点kubectl taint nodes 10.1.1.104 node-role.kubernetes.io/control-plane:NoSchedule# 去掉污点kubectl taint nodes 10.1.1.104 node-role.kubernetes.io/control-plane:NoSchedule- 10 proxy命令kubernetes中，API Server本身就是一个web服务器，它使用的是https协议，要求客户端提供证书做双向https认证，所以直接使用curl命令访问的时候会失败 123456789101112#-k忽略错误，因为我们客户端现在没有证书[root@master1 ~]curl -k https://127.0.0.1:6443&#123; &quot;kind&quot;: &quot;Status&quot;, &quot;apiVersion&quot;: &quot;v1&quot;, &quot;metadata&quot;: &#123;&#125;, &quot;status&quot;: &quot;Failure&quot;, #失败的 &quot;message&quot;: &quot;forbidden: User \\&quot;system:anonymous\\&quot; cannot get path \\&quot;/\\&quot;&quot;, #匿名用户 &quot;reason&quot;: &quot;Forbidden&quot;, &quot;details&quot;: &#123;&#125;, &quot;code&quot;: 403&#125; 那怎么与它交互呢？就要使用 kubectl proxy 命令，主要用于在本地与 Kubernetes API 服务器建立安全的代理连接，它会在本地启动一个 HTTP 代理服务器 那么我们就可以使用curl命令向本地代理发起请求，然后将本地的 HTTP 请求转发到 Kubernetes API 服务器，因为kubectl自己会自动使用当前用户的 Kubernetes 配置（ /etc/kubernetes/admin.conf 当中的认证信息，或者是 ~/.kube/config），进行身份验证和授权，认证到API Server上 123456789101112[root@master1 ~]#kubectl proxyStarting to serve on 127.0.0.1:8001[root@master1 ~]#curl 127.0.0.1:8001&#123; &quot;paths&quot;: [ &quot;/.well-known/openid-configuration&quot;, &quot;/api&quot;, &quot;/api/v1&quot;, ... ]&#125; 11 log命令123kubectl logs pod-name -n namespace#如果要查看之前的日志，加-p选项","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"kubectl","slug":"kubectl","permalink":"https://aquapluto.github.io/tags/kubectl/"}]},{"title":"Namespace","slug":"Kubernetes/conspect/namespace","date":"2025-09-12T06:49:02.000Z","updated":"2025-09-12T07:21:26.084Z","comments":true,"path":"Kubernetes/conspect/namespace/","permalink":"https://aquapluto.github.io/Kubernetes/conspect/namespace/","excerpt":"","text":"1 名称空间概念名称空间（Namespace） Namespace是对一组资源和对象的抽象集合，是Kubernetes集群提供对内部资源进行“软隔离”的机制，以方便用户管理和组织集群资源，可以将其想像成虚拟的“子集群” 名称空间不允许嵌套，使用kubectl api-resources可以查看哪些资源是属于名称空间资源，哪些是集群资源，集群资源可以被名称空间下的所有资源所引用 用来隔离pod的运行环境（同个命名空间里的pod可以相互访问），大多数情况下用于实现多租户的资源隔离，namespace通过将集群内部的资源对象分配到不同的namespace中，形成逻辑上分组的不同项目、小组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。 namespace的定义很简单，如下所示的yaml定义了名为development的namespace 12metadata: name: development 一旦创建了Namespace，我们在创建资源对象时就可以指定这个资源对象属于哪个namespace，比如可以定义名为busybox的Pod，放入development这个namespace里 Kubernetes的名称空间可以划分为两种类型 系统级名称空间：由Kubernetes集群默认创建，主要用来隔离系统级的资源对象 自定义名称空间：由用户按需创建 系统级名称空间 default：默认的名称空间，为任何名称空间级别的资源提供的默认设定 kube-system：Kubernetes集群自身组件及其它系统级组件使用的名称空间，Kubernetes自身的关键组件均部署在该名称空间中 kube-public：公众开放的名称空间，所有用户（包括Anonymous【匿名用户】）都可以读取内部的资源 kube-node-lease：节点租约资源所用的名称空间 分布式系统通常使用“租约（Lease）”机制来锁定共享资源并协调集群成员之间的活动 Kubernetes上的租约概念由API群组coordination.k8s.io群组下的Lease资源所承载，以支撑系统级别的功能需求，例如节点心跳（node heartbeats）和组件级的领导选举（多个Controller-Manager在该名称空间下创建分布式锁，哪个Controller-Manager抢到就是领导，抢的资源类型就是Lease）等 Kubernetes集群的每个节点，在该名称空间下都有一个与节点名称同名的Lease资源对象，即该节点通过这个Lease资源对象进行健康监测，向Controller-Manager和Scheduler说明是健康的，可以调度 1kubectl get lease -n kube-node-lease 所有的系统级名称空间均不能进行删除操作，而且除default外，其它三个也不应该用作业务应用的部署目标 2 管理名称空间查看名称空间 12345678[root@master1 ~]#kubectl get nsNAME STATUS AGEdefault Active 20h # 所有未指定Namespace的对象都会被默认分配在default命名空间kube-flannel Active 20hkube-node-lease Active 20hkube-public Active 20h # 此命名空间下的资源可以被所有人访问kube-system Active 20h # 所有由Kubernetes系统创建的资源都处于这个命名空间openelb-system Active 11h 查看名称空间里的资源 12kubectl get all --namespace=命名空间名称 # 可以查看此命名空间下的所有资源kubectl get 资源类型 --namespace=命名空间名称 # 可以查看此命名空间下的对应的资源 创建名称空间 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@master1 ~]#kubectl create namespace demo --dry-run=client -o yaml &gt; namespace-demo.yaml[root@master1 ~]#vim namespace-demo.yamlapiVersion: v1 #表示资源的API版本，v1是Kubernetes核心API组的版本kind: Namespace #表明资源类型是Namespacemetadata: name: stage # 命名空间的名称 labels: environment: stage #添加标签 #应用[root@master1 ~]#kubectl apply -f namespace-demo.yaml namespace/stage created[root@master1 ~]#kubectl get nsNAME STATUS AGEdefault Active 21hkube-flannel Active 21hkube-node-lease Active 21hkube-public Active 21hkube-system Active 21hopenelb-system Active 12hstage Active 68s # 对应 name: stage#显示标签[root@master1 ~]#kubectl get ns --show-labelsNAME STATUS AGE LABELSdefault Active 21h kubernetes.io/metadata.name=defaultkube-flannel Active 21h k8s-app=flannel,kubernetes.io/metadata.name=kube-flannel,pod-security.kubernetes.io/enforce=privilegedkube-node-lease Active 21h kubernetes.io/metadata.name=kube-node-leasekube-public Active 21h kubernetes.io/metadata.name=kube-publickube-system Active 21h kubernetes.io/metadata.name=kube-systemopenelb-system Active 12h kubernetes.io/metadata.name=openelb-systemstage Active 111s environment=stage,kubernetes.io/metadata.name=stage#根据标签过滤[root@master1 ~]#kubectl get ns -l environment=stageNAME STATUS AGEstage Active 119s#创建资源[root@master1 ~]#kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 -n stagedeployment.apps/demoapp created[root@master1 ~]#kubectl get pods -n stage NAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-qq4kn 1/1 Running 0 16s#扩容[root@master1 ~]#kubectl scale deployment/demoapp --replicas=3 -n stagedeployment.apps/demoapp scaled[root@master1 ~]#kubectl get pods -n stage NAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-cd47m 1/1 Running 0 42sdemoapp-7c58cd6bb-qq4kn 1/1 Running 0 3m38sdemoapp-7c58cd6bb-v8bgd 1/1 Running 0 42s#缩容[root@master1 ~]#kubectl scale deployment/demoapp --replicas=2 -n stagedeployment.apps/demoapp scaled[root@master1 ~]#kubectl get pods -n stage NAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-cd47m 1/1 Running 0 102sdemoapp-7c58cd6bb-qq4kn 1/1 Running 0 4m38s 删除名称空间：删除名称空间会将其下的所有资源一并删除，此行为危险 12[root@master1 ~]#kubectl delete namespaces stagenamespace &quot;stage&quot; deleted 3 需要使用名称空间的情形环境管理：需要在同一Kubernetes集群上隔离研发、预发和生产等一类的环境时，可以通过名称空间进行 隔离：多个团队的不同产品线需要部署于同一Kubernetes集群时，可以使用名称空间进行隔离 资源控制 名称空间可用作资源配额的承载单位，从而限制其内部所有应用可以使用的CPU&#x2F;Memory&#x2F;PV各自的资源总和 需要在产品线或团队等隔离目标上分配各自总体可用的系统资源时，可通过名称空间实现 权限控制：基于RBAC鉴权体系，能够在名称空间级别进行权限配置 提高集群性能：进行资源搜索时，名称空间有利于Kubernetes API缩小查找范围，从而对减少搜索延迟和提升性能有一定的帮助 4 k8s Namespace和容器Namespace的区别虽然都叫 “Namespace”，但 Kubernetes 的 Namespace 和 容器的 Namespace 是完全不同层次的概念 Kubernetes Namespace是逻辑上的资源隔离机制，用于组织和管理集群中的对象（如 Pod、Service） 容器Namespace是操作系统内核级别的隔离机制，用于实现容器的“隔离”本身 对比项 Kubernetes Namespace 容器Namespace 所属系统 Kubernetes Linux 内核 作用层次 API 层 &#x2F; 控制平面 操作系统层 &#x2F; 运行时 主要用途 多租户、环境隔离、资源组织 实现容器隔离（进程、网络、文件系统等） 隔离能力 逻辑隔离（API 可见性） 强隔离（内核级） 是否影响容器运行 否 是（容器依赖它才能“隔离”） 常见类型 default, kube-system, prod, dev pid, net, mnt, uts, ipc, user 查看方式 kubectl get namespaces ls /proc/&lt;pid&gt;/ns","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"Kubernetes对象","slug":"Kubernetes/conspect/object","date":"2025-09-12T06:48:57.000Z","updated":"2025-09-12T07:17:17.198Z","comments":true,"path":"Kubernetes/conspect/object/","permalink":"https://aquapluto.github.io/Kubernetes/conspect/object/","excerpt":"","text":"1 Kubernetes对象Kubernetes对象是 Kubernetes 系统中的持久性实体。 Kubernetes使用这些实体表示你的集群状态，它们持久存储于API Server上 哪些容器化应用正在运行（以及在哪些节点上运行） 可以被应用使用的资源 关于应用运行时行为的策略，比如重启策略、升级策略以及容错策略 Kubernetes 对象（Kubernetes API Primitive）是一种“意向表达”。一旦创建该对象， Kubernetes 系统将不断工作以确保该对象存在。通过创建对象，你本质上是在告知 Kubernetes 系统，你想要的集群工作负载状态看起来应是什么样子的， 这就是 Kubernetes 集群所谓的期望状态 以应用为中心，Kubernetes API Primitive基本都是围绕一个核心目的而设计：如何更好地运行和丰富Pod资源，从而为容器化应用提供更灵活和更完善的操作与管理组件 操作 Kubernetes 对象 —— 无论是创建、修改或者删除 —— 需要使用 Kubernetes API 2 API的设计和组织形式API Server的三大核心功能 对组件的贡献：对下封装了对etcd数据库的操作、对上提供了list-watch机制 对客户端的贡献：提供了RESTFul风格的API接口 对安全的贡献：客户端访问的时候要经过一系列权限相关处理，包括 检查凭证：验证用户是否是一个合法账号，凭证代表的是一个人的身份 检验授权：验证你有没有对请求中指定的api有操作权限 准入控制器（两种webhook：mutate与validate） API 的设计：https://ip:端口/api/apps/v1/namespace/default/pods 顶级大类：/api（核心组API），/apis（非核心组API），/metrics （指标）等等 细分的组名：apps等等，对应 apiVersion 中的GROUP 具体的版本，对应 apiVersion 中的VERSION alpha版：尝鲜版本，新特性一定最先出现在这里 Beta版：稳定性提高版本，相对alpha版更稳定一些 稳定级别：比如v1版本表示是最稳定版了 具体的资源：job，pods等等 核心组API（pod等）构成：/api/版本/namespaces/具体的名称空间/资源类型 非核心组API（deployment等）构成：/顶级大类/细分组名/版本/namespaces/具体的名称空间/资源类型 12345# 查看顶级大类和细分的组名[root@k8s-master-01 ~]# kubectl get --raw /# 查看具体的细节信息[root@k8s-master-01 ~]# kubectl get --raw /apis/apps/v1 | python -m json.tool 3 API资源类型依据资源的主要功能作为分类标准，Kubernetes的API对象大体可分为如下几个类别 工作负载（Workload） 服务发现和负载均衡（Discovery &amp; LB） 配置和存储（Config &amp; Storage） 集群（Cluster） 元数据（Metadata） 工作负载型资源负责应用编排；服务发现和负载均衡型资源完成服务注册、发现及流量调度 4 API资源规范Kubernetes API 中是如何表示 Kubernetes 对象的？ 绝大多数的Kubernetes对象都包含spec和status两个嵌套字段 spec字段存储对象的期望状态（或称为应有状态），即用户期望的状态 由用户在创建时提供，随后也可按需进行更新（但有些属性并不支持就地更新机制） 不同资源类型的spec格式不尽相同 对于 Namespace 资源，spec 通常为空，因为命名空间没有需要配置的复杂属性。 status字段存储对象的实际状态（或称为当前状态） 由Kubernetes系统控制平面相关的组件负责实时维护和更新 在任何时刻，Kubernetes控制平面都一直在积极地管理着对象的实际状态，以使之达成期望状态。 对于Namespace，status在资源创建前是空的，因为它是由Kubernetes API 服务器填充的动态字段。 status字段要和spec字段的状态一致 Controller 会隔段时间检查status字段和spec字段状态一致不一致，不一致就会让他们一致（loop循环） 有些资源当中是没有status字段和spec字段的，只有对象元数据 apiVersion 和 kind 两个字段负责指明对象的类型（资源类型）元数据 前者负责标明该类型所隶属的API群组（API Group），即创建该对象所使用的 Kubernetes API 的版本，kubectl api-versions 后者用于指定资源类型标识，即创建的资源对象的类别，kubectl api-resources 对象元数据 metadata：名称、标签、注解和隶属的名称空间（不包括集群级别的资源）等 123456789101112apiVersion: &lt;GROUP/VERSION&gt;kind: &lt;RESOURCE KIND&gt;metadata: name: … #名称空间级别的资源：同一名称空间下，同一类型中，必须唯一 namespace: … #集群级别的资源：不需要定义名称空间字段 labels: &#123; key1: val1, ... &#125; annotations: &#123; key1: val1, ... &#125;spec: &#123; … &#125; #保存在etcd中的期望状态，可以使用kubectl explain KIND.spec查看格式status: &#123; … &#125; #实际状态，由API服务自动填充--- #使用 --- 隔离，表明创建多个资源对象apiVersion: &lt;GROUP/VERSION&gt;... 资源规范的具体格式 内建文档：kubectl explain 命令获取 123kubectl explain namespace # 查看namespace相关语法参数kubectl explain namespace.metadata # 查看namespace下级metadata的相关语法参数kubectl explain namespace.metadata.name # 查看namespace下级metadata再下级name的相关语法参数 参考现有资源对象：kubectl get TYPE/NAME -o &#123;yaml|json&#125; 5 API资源管理之前的介绍中，我们知道API Server 基于HTTP(S)协议暴露了一个RESTful风格的API kubectl命令或其它UI通过该API查询或请求变更API对象的状态 施加于对象之上的基本操作包括增、删、改、查等 通过HTTP协议的GET、POST、DELETE和PUT等方法完成，而对应于kubectl命令，它们则是create、get、describe、delete、patch和edit等子命令 所以资源对象管理的基本操作，就是CRUD，来进行API对象管理 创建对象时，必须向API Server提供描述其所需状态的对象规范、对象元数据及类型元数据 需要在请求报文的body中以 JSON 格式提供，用户也能够以YAML格式定义对象，提交给API Server后由其自行完成格式转换 使用客户端程序，对API Server的REST服务端点发请求 请求的body部分要遵循API资源规范 kubectl命令提供了三种类型的对象管理机制 指令式命令（Imperative commands） 直接作用于集群上的活动对象（Live objects） 适合在开发环境中完成一次性的操作任务 指令式对象配置（Imperative object configuration） 指令式：要么创建，要么删除，不具有幂等性，即创建一个Pod，如果存在就报错 基于资源配置文件执行对象管理操作，但只能独立引用每个配置清单文件 可用于生产环境的管理任务 kubectl create 声明式对象配置（Declarative object configuration） 声明式：有创建，删除等功能，具有幂等性，即创建一个Pod，如果存在就不执行该操作 基于配置文件执行对象管理操作 可直接引用目录下的所有配置清单文件，也可直接作用于单个配置文件 kubectl apply","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"List-Watch机制","slug":"Kubernetes/conspect/list-watch","date":"2025-09-12T06:48:52.000Z","updated":"2025-09-12T07:15:21.658Z","comments":true,"path":"Kubernetes/conspect/list-watch/","permalink":"https://aquapluto.github.io/Kubernetes/conspect/list-watch/","excerpt":"","text":"1 概念在Kubernetes中，list和watch是两种主要的API操作、是一种异步消息处理机制，实现 控制器（Controller）模式的核心通信机制，它们用于让客户端获取和观察资源的状态，使得 Kubernetes 的各种组件（如 kube-controller-manager、kube-scheduler、kubelet 等）能够实时感知集群状态的变化，并做出相应反应 用户通过 kubectl 创建、更新、删除资源 控制器（如 Deployment Controller）需要知道这些变化，并确保实际状态（Real State）与期望状态（Desired State）一致 1.1 List操作List 操作是短连接的操作，用于一次性获取集群中某种类型资源的当前状态列表 短连接：请求发出后，API 服务器返回结果即断开连接，适用于初始化资源状态或定期全量校验。 版本标识：返回的每个资源项均包含 resourceVersion 字段，这是 Kubernetes 用于标记资源状态版本的唯一标识符（类似乐观锁机制），每次资源更新（创建、修改、删除）都会生成新的 resourceVersion。 灵活筛选：支持通过 namespace、标签选择器（labelSelector）、字段选择器（fieldSelector）等参数过滤结果，例如 kubectl get pods -n default -l app=nginx 就是通过 list 操作实现的。 1.2 Watch操作watch操作是客户端与 API 服务器建立的长连接操作，用于订阅资源的更新事件。 长连接：客户端发起请求后，API Server会保持连接处于打开状态，API Server一旦检测到资源变化（如 ADDED、MODIFIED、DELETED），会立即将事件推送给客户端 基于版本的增量同步：客户端可通过 resourceVersion 参数指定监听的起始版本（通常使用 list 操作返回的最新版本），避免漏掉在 list 和 watch 操作之间发生的更新。若连接中断，客户端可通过上次记录的 resourceVersion 重新发起 watch 通过curl模拟watch 12345678910$ kubectl proxy# 进行listwatch default名称空间下的pods$ curl &quot;127.0.0.1:8001/api/v1/namespaces/default/pods?watch=true&quot;# list出defaullt名称空间下的pods$ curl &quot;127.0.0.1:8001/api/v1/namespaces/default/pods&quot;# 创建pod进行观察$ kubectl run nginx --image=nginx 1.3 有了List为什么还需要Watch既然 List 已经获取了所有资源的当前状态，为什么还需要 Watch？因为List 只能获取某一时刻的“静态快照”，而 Watch 提供了“持续的实时变化流”，没有 Watch，系统就无法实时感知变化，只能靠低效的轮询，导致延迟高、响应慢、资源浪费。 所以 k8s 中各组件间协同都采用 list-watch 机制进行通信，这种机制可以确保客户端获取到最新的资源状态，与简单的短轮询相比，这种机制可以更有效地获取资源状态的更新，且避免了频繁的轮询造成的开销，减少了网络流量和API服务器的负载。大致流程如下 客户端首先执行一次list操作来初始化资源状态 然后通过watch操作来持续接收后续的更新 2 使用场景 组件 使用场景 ReplicaSet Controller 监听 Pod 变化，确保副本数正确 Deployment Controller 监听 ReplicaSet 变化，实现滚动更新 Node Controller 监听 Node 状态，处理节点失联 Service Controller 监听 Pod 变化，更新 Endpoints Scheduler 监听 Pod 创建，为 Pending Pod 分配节点 kubelet 监听 Pod 分配给自己，执行创建 3 设计理念一个异步消息的系统时，对消息机制有至少如下四点要求 3.1 消息可靠性首先消息必须是可靠的，list和watch一起保证了消息的可靠性，避免因消息丢失而造成状态不一致场景。 List API可以查询当前的资源及其对应的状态(即期望的状态)，客户端通过拿期望的状态和实际的状态进行对比，纠正状态不一致的资源。 Watch API和apiserver保持一个长链接，接收资源的状态变更事件并做相应处理。 如果仅调用watch API，若某个时间点连接中断，就有可能导致消息丢失，所以需要通过list API解决消息丢失的问题 从另一个角度出发，我们可以认为list API获取全量数据，watch API获取增量数据。虽然仅仅通过轮询list API，也能达到同步资源状态的效果，但是存在开销大，实时性不足的问题。 3.2 消息实时性消息必须是实时的，list- watch机制下，每当apiserver的资源产生状态变更事件，都会将事件及时的推送给客户端，从而保证了消息的实时性。 3.3 消息顺序性消息的顺序性也是非常重要的，在并发的场景下，客户端在短时间内可能会收到同一个资源的多个事件，对于关注最终一致性的K8S来说， 它需要知道哪个是最近发生的事件，并保证资源的最终状态如同最近事件所表述的状态一样。 K8S在每个资源的事件中都带一个resourceVersion的标签，这个标签是递增的数字，所以当客户端并发处理同一个资源的事件时，它就可以对比resourceVersion来保证最终的状态和最新的事件所期望的状态保持一致。 3.4 高性能虽然仅通过周期性调用List API也能达到资源最终一致性的效果， 但是周期性频繁的轮询大大的增大了开销，增加apiserver的压力。 而watch作为异步消息通知机制，复用一条长链接，保证实时性的同时也保证了性能","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"容器运行时接口CRI","slug":"Kubernetes/conspect/cri","date":"2025-09-12T06:48:46.000Z","updated":"2025-09-12T06:57:47.513Z","comments":true,"path":"Kubernetes/conspect/cri/","permalink":"https://aquapluto.github.io/Kubernetes/conspect/cri/","excerpt":"","text":"1 CRI介绍容器运行时接口（CRI）是 kubelet 和容器运行时之间通信的主要协议。定义了在节点组件kubelet和容器运行时之间通信的主要gRPC协议，使 kubelet 能够使用各种容器运行时，无需重新编译集群组件。 CRI 包括两类服务： 镜像服务（Image Service） 镜像服务提供下载、检查和删除镜像的远程程序调用。 运行时服务（Runtime Service） 运行时服务包含用于管理容器生命周期，以及与容器交互的调用的远程程序调用。 docker是不支持 CRI 接口的，最初是使用Docker-shim来让docker支持 CRI 接口，但是自 kubernetes 1.24 版起，Docker-shim被抛弃了，转而使用 cri-dockerd 来接管 Docker-shim 的工作 2 Kubernetes的CRI方案 3 Kubernetes的容器运行时https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/ Kubernetes如何接入Docker和Containerd？ 调用链一：kubelet通过CRI调用dockershim，而后dockershim调用docker，再由docker通过containerd管理容器 用户基础好 Kubernetes自1.24版开始，正式从kubelet中移除dockershim相关的代码，dockershim被弃用 但Mirantis又提供了cri-docker项目，以kubelet外部独立运行的CRI服务替代dockershim 1.24之前：dockershim –&gt; docker daemon –&gt; containerd –&gt; containerd shim –&gt; runc 1.24之后：cri-docker –&gt; docker daemon –&gt; containerd –&gt; containerd shim –&gt; runc 调用链二：kubelet通过CRI调用Containerd，而后Containerd直接管理容器 性能损耗低 不再支持docker客户端管理容器 containerd –&gt; containerd shim –&gt; runc","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"CRI","slug":"CRI","permalink":"https://aquapluto.github.io/tags/CRI/"}]},{"title":"Kubernetes架构","slug":"Kubernetes/conspect/architecture","date":"2025-09-12T06:48:38.000Z","updated":"2025-09-12T06:57:12.320Z","comments":true,"path":"Kubernetes/conspect/architecture/","permalink":"https://aquapluto.github.io/Kubernetes/conspect/architecture/","excerpt":"","text":"1 Kubernetes介绍https://kubernetes.io/ https://github.com/kubernetes Kubernetes是一个完备的分布式系统支撑平台。Kubernetes具有完备的集群管理能力，包括多层次的安全防护和准入机制&#x2F;多租户应用支撑能力、透明的服务注册和服务发现机制、内建智能负载均衡器、强大的故障发现和自我修复功能、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。同时kubernetes提供了完善的管理工具，这些工具覆盖了包括开发、测试部署、运维监控在内的各个环节；整合了各节点上的计算和内存资源统一分配，因此kubernetes是一个全新的基于容器技术的分布式架构解决方案，并且是一个一站式的完备的分布式系统开发和支撑平台。 kubernetes的功能 自动化上线和回滚：分步骤地将针对应用或其配置的更改上线，同时监视应用程序运行状况以确保你不会同时终止所有实例。如果出现问题，会为你回滚所作更改。 自我修复：自动重启崩溃的容器，在必要时替换整个 Pod，在发生更大范围的故障时重新挂载存储，并且能够与节点自动扩缩容器集成，实现节点级别的自我修复能力。 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行扩缩 服务发现：服务可以通过自动发现的形式找到它所依赖的服务 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡，将传入的流量分发到不同的容器实例中 存储编排：自动挂载所选存储系统，包括本地存储、公有云提供商所提供的存储或者诸如 iSCSI 或 NFS 这类网络存储系统。 2 扩展接口 Kubernetes的设计初衷是支持可插拔架构，从而利于扩展kubernetes的功能。所以Kubernetes提供了三个特定功能的接口，kubernetes通过调用这几个接口，来完成相应的功能。 容器运行时接口CRI: Container Runtime Interface CRI 首次发布于2016年12月的Kubernetes 1.5 版本。在此版本之前，Kubernetes 直接与 Docker 通信，没有标准化的接口。 从 Kubernetes 1.5 开始，CRI 成为 Kubernetes 与容器运行时交互的标准接口，使得 Kubernetes可以与各种容器运行时进行通信，从而增加了灵活性和可移植性。 kubernetes 对于容器的解决方案，只是预留了容器接口，只要符合CRI标准的解决方案都可以使用 容器网络接口CNI: Container Network Interface Kubernetes 的网络插件接口，允许用户选择或开发自己的网络解决方案，以实现Pod之间的网络通信。 kubernetes 对于网络的解决方案，只是预留了网络接口，只要符合CNI标准的解决方案都可以使用 容器存储接口CSI: Container Storage Interface CSI 是 Kubernetes 的存储插件接口，使得第三方存储解决方案可以集成到 Kubernetes 中，为有状态应用提供持久化存储。 kubernetes 对于存储的解决方案，只是预留了存储接口，只要符合CSI标准的解决方案都可以使用 此接口非必须 3 集群的节点类型Kubernetes 是一个分布式容器编排系统，是由很多主机节点组成，且各个节点的分工不同，主要由Master和Node两类节点组成 Master：管理(控制)节点，相当于公司的管理层，接收请求 Node：工作worker节点或者Minion节点，相当于公司具体完成工作的基层员工，node节点上的docker承载运行各类应用容器 运行逻辑 Kubernetes将所有工作节点的资源集结在一起形成一台更加强大的“服务器” 计算和存储接口通过Master之上的API Server暴露 客户端通过API提交应用程序的运行请求，而后由Master通过调度算法将其自动指派至某特定的工作节点以Pod对象的形式运行 Master会自动处理因工作节点的添加、故障或移除等变动对Pod的影响 4 系统组件1https://kubernetes.io/zh/docs/concepts/overview/components/ kubernetes组件分成三种 Control Plane Components 控制平台组件 Master主要由API Server、Controller-Manager和Scheduler三个组件，以及一个用于集群状态存储的Etcd存储服务组成 Node Components 节点组件 每个Node节点则主要包含Kubelet、Kube Proxy及容器运行时三个组件组成，它们承载运行各类应用容器 Addons 附件（插件） 工作节点托管Pods 是应用程序工作负载的组成部分。该控制平面管理集群中的工作节点和Pod。 在生产环境中，控制平面通常在多台计算机上运行，而集群通常在多个节点上运行，从而提供了容错能力和高可用性。 4.1 Master 组件API Server 是整个集群的声明式API网关，基于http&#x2F;https协议以REST风格提供，把kubernetes所有功能抽象成了“资源”及相关的“对象”，相应应用程序为kube-api-server 作为用户而言，只需要声明对象的“终态”，具体的业务逻辑由各资源相关的Controller负责完成。 负责处理所有增删改查的请求，无状态，数据都存储在etcd中。 对于请求(kubectl)，API会做RBAC鉴权，鉴权后，再通过设置的 mutation webhook 和 validation webhook 做修改和审计，最后才会把终态声明存入etcd中。 Cluster Store 集群状态数据存储系统，通常指的就是etcd，仅会同API Server交互 etcd是一个分布式键值存储系统，存储K8S 集群的所有配置数据和状态信息，如节点信息、容器的期望状态(如副本数量、资源请求等)等关键数据。 etcd集群的性能决定了集群的规模，采用RAFT算法，超过半数以上的etcd节点挂掉后，整个etcd集群才不可用 Controller Manager 负责实现客户端通过API提交的终态声明，相应应用程序为kube-controller-manager，由相关代码通过一系列步骤驱动API对象的“实际状态”接近或等同“期望状态”，工作于loop(循环)模式。 通过 list-watch 机制获取 etcd 对应资源的信息，不断地检查系统的实际状态是否与期望状态一致。 Scheduler 调度器，负责实现pod的调度，相关程序为kube-scheduler。 根据节点的状态；CPU、内存、PV等资源是否可用；端口、IP是否被占用或耗尽，来为pod调度到最合适的节点上。 Cloud Manager 用于管理和监控云资源的综合工具或平台，通常由云服务提供商或第三方提供。它的主要作用包括资源管理、监控、自动化、成本控制和安全管理 把k8s部署在云环境，可以调用底层云的虚拟机的弹性创建能力，比如集群只有三个node节点，跑了很多Pod，资源耗尽，那么Cloud Manager可以联动外部云自动请求创建一个虚拟机，加入到集群中，该组件只有在云端可以实现 4.2 Node 组件Kubelet 运行在Kubernetes集群中每个node节点上的代理，负责Pod对应容器的创建，启动和删除，保证容器都运行在 Pod 中，相应程序为kubelet 通过API Server接收Pod资源定义，或从节点本地目录中加载静态Pod配置 借助于兼容CRI的容器运行时管理和监控Pod相关的容器 需要依赖于CNI（虚拟网络）和CRI（容器运行时）插件 调度成功后，调用容器引擎先创建pause容器，主要是为了打通网络，之后的容器都共享这个网络空间，如果有 init，会串行执行每一个 init 容器，做初始化，最后就会开始多个业务容器并行创建。 完成一系列操作后，kubelet上报节点状态给apiserver，写入etcd。 kubelet的GC机制会负责定期清理工作节点上的镜像以及退出的容器 Kube Proxy 运行于每个Worker节点上，专用于负责将Service资源的定义转为本地节点的实现，实现服务的负载均衡和网络代理功能 iptables模式：将Service资源的定义转为适配当前节点视角的iptables规则，但是如果应用比较多，就不适合了 ipvs模式：将Service资源的定义转为适配当前节点视角的ipvs和少量iptables规则 是打通Pod网络在Service网络的关键所在，实现Kubernetes Service的通信与负载均衡机制的重要组件 例如，当外部请求访问某个服务时，Kube-Proxy会根据负载均衡策略将请求转发到后端的一个或多个容器。 容器运行时 负责运行容器的软件，Kubelet使用该软件来管理容器 负责管理 Kubernetes 环境中容器的执行和生命周期 4.3 附加组件负责扩展Kubernetes集群的功能的应用程序，通常以Pod形式托管运行于Kubernetes集群之上 必选插件 Network Plugin：网络插件，经由CNI接口，负责为Pod提供专用的通信网络，有多种实现 CoreOS Flannel ProjectCalico Cluster DNS：集群DNS服务器，负责服务注册、发现和名称解析，当下的实现是CoreDNS 重要插件 Ingress Controller：Ingress控制器，负责为Ingress资源提供具体的实现，实现http&#x2F;https协议的七层路由和流量调度，有多种实现，例如Ingress-Nginx、Contour等 Metrics Server：Node和Pod等相关组件的核心指标数据收集器，它接受实时查询，但不存储指标数据 Kubernetes Dashboard&#x2F;Kuboard&#x2F;Rainbond：基于Web的UI Prometheus：指标监控系统 ELK&#x2F;PLG：集中式日志系统 OpenELB：适用于非云端部署的Kubernetes环境的负载均衡器，可利用BGP和ECMP协议达到性能最优和高可用性 5 集群架构集群：整合了各节点上的计算和内存资源，统一分配 容器编排 容器的创建等管理操作：第三方组件，例如Docker, Containerd, CRI-O CRI: Container Runtime Interface docker-shim（已抛弃） cri-dockerd Docker Daemon –&gt; Containerd –&gt; containerd-shim –&gt; RunC 网络编排 第三方组件：CoreOS Flannel, ProjectCalico, … CNI: Container Network Interface 存储编排 kebulet内部实现：In-Tree Driver 第三方组件：CSI: Container Storage Interface Kubernetes属于典型的Server-Client形式的二层架构，Master节点和Node节点之间的协作如下： 用户或自动化工具通过API server提交请求（如创建一个新的Pod），这些请求首先经过认证和授权过程。 如果请求被批准，kube-scheduler会选择一个合适的Node节点来运行Pod，并将此信息写入etcd。 kube-controller-manager中的Deployment Controller会监视etcd中的变化，确保Pod的数量和状态符合预期。 当kubelet接收到新的Pod创建请求时，它会调用container runtime来启动相应的容器，并通过CNI设置网络，CSI设置存储。 kube-proxy会在Node节点上监听API server的变化，当新的Service被创建时，它会相应地调整网络规则以提供负载均衡和内部DNS解析。 6 工作逻辑部署并访问应用 依照编排需求，选定合适类型的工作负载型控制器 创建工作负载型控制器对象，由其确保运行合适数量的Pod对象 创建Service对象，为该组Pod对象提供固定的访问入口 请求访问Service对象上的服务 集群内部的通信流量也称为东西向流量，客户端也是集群上的Pod对象 Service同集群外部的客户端之间的通信流量称为南北向流量，客户端是集群外部的进程；另外，集群上的Pod也可能会与集群外部的服务进程通信","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"Pod的驱逐","slug":"Kubernetes/pod/eviction","date":"2025-09-12T06:18:50.000Z","updated":"2025-09-12T06:44:02.802Z","comments":true,"path":"Kubernetes/pod/eviction/","permalink":"https://aquapluto.github.io/Kubernetes/pod/eviction/","excerpt":"","text":"1 驱逐的概念驱逐(Eviction)是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程，在以下两种情况下会发生pod的驱逐。节点压力驱逐 当节点不可用时则会触发，负责驱逐的组件是Kube-controller-manager，它会会周期性检查所有节点状态 当某个节点遇到不可压缩资源剩余量达到预设的阈值时则会触发，负责驱逐的组件是kubelet，驱逐的依据是Pod的Qos等级 2 服务的质量等级服务的质量等级（Qos）：质量等级越低的会成为被驱逐的目标，按照优先级从高到低的顺序排列如下 Guranteed(完全可靠的)：Pod中的所有容器对所有资源类型（cpu与内存）都定义了Limits与requests，且二者值相等、且大于0 Burstable(弹性被动、较可靠的)：具体来说此级别涉及两种情况 Pod中的一部分容器在一种或者多种类型的资源配置中定义了Requests值和Limits值（都不为0），且Requests值小于Limits值 Pod中的一部分容器未定义资源配置（Requests值和Limits值都未定义） BestEffort(尽力而为、不太可靠的)：Pod中的所有容器都未定义资源配置（Requests与Limits都未定义） Qos使用建议 如果资源充足，可将 QoS pods 类型均设置为 Guaranteed。用计算资源换业务性能和稳定性，减少排查问题时间和成本 如果想更好的提高资源利用率，关键业务服务可以设置为Guaranteed，而其他服务根据重要程度可分别设置为Burstable或 BestEffort 3 Kube-controller-manger发起的驱逐Kube-controller-manager （包含的node controller控制器代码）周期性检查节点资源的状态信息，从检查节点状态并判断为 NotReady异常到完成工作负载pod的驱逐，总周期默认约等于为6分钟，由下列三个参数的处理时间加起来。更多涉及到的参数可参考官网 node-monitor-period 节点控制器每5秒进行一次节点状态检测，主要是通过检查 kube-apiserver 中节点对象的状态信息。这些状态信息(包含节点的健康状态、资源使用情况等)是由每个节点上的 kubelet 定期（默认间隔 10 秒）向 kube-apiserver 上报的 节点控制器是 Kubernetes 控制平面的一部分，它运行在 kube-controller-manager 中 node-monitor-grace-period 节点控制器判断节点故障的时间窗口，默认40秒。即40 秒没有收到kubelet发来的节点消息则判断节点为故障，节点控制器会将节点的状态变更为notready，同时会为该节点打上NoExecute的污点 pod-eviction-timeout 当节点故障时，kubelet允许pod在此故障节点的保留时间，默认300秒。即当节点故障5分钟后，kubelet开始在其他可用节点重建pod 在v1.27 版本已废弃，取而代之的是节点挂掉后k8s为节点添加NoExecute的污点，并且在此之前默认为每个pod都添加了容忍且设置了多久后才会驱逐（tolerationSeconds参数） kubernetes节点失效后pod的调度&#x2F;驱逐过程 前提：kubelet 会将自己节点的状态信息（节点健康信息、资源使用情况等）定期更新状态到apiserver，通过参数–node-status-update-frequency指定上报频率，默认是 10s 上报一次。 kube-controller-manager中的节点控制器会每隔–node-monitor-period时间去检查 kubelet汇报上来的节点状态，默认是 5s。 当node失联一段时间后，默认通过node-monitor-grace-period参数配置默认值为40s，kubernetes 判定 node 为 notready 状态。 当node失联后一段时间后，kubernetes开始删除原node上的pod，这段时长配置项为 pod-eviction-timeout ，默认5m0s，即300s，这5分钟包含判定失联的1min，即在判定失联后再过4min即可以驱逐 4 Kubelet发起的驱逐4.1 节点压力驱逐可压缩资源不会导致pod驱逐，因为在资源紧缺时系统内核会重新分配权重。如果不可压缩资源不足，那系统中很多系统进程、用户进程随时可能申请更多的内存或磁盘资源，内存不足，则会触发系统级的OOM Killer，k8s进程都可能被干掉，就会导致k8s不稳定，为了避免出现这种系统级别的OOM而导致k8s集群崩溃掉，k8s设计和实现了一套自动化的Pod驱逐机制 节点压力驱逐是 kubelet 主动终止 Pod 以回收节点上资源的过程。kubelet通过cAdvisor提供的资源使用指标监控集群节点的内存、磁盘空间和文件系统的 inode 等资源。 当这些资源中的一个或者多个达到特定的消耗水平， kubelet可以根据参数配置主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。 在节点压力驱逐期间，kubelet 将所选 Pod 的阶段设置为 Failed 并终止 Pod（其内部的容器会被全部停止）。kubelet会上报自己的节点资源的真实使用情况不足这一些信息给apiserver存入etcd，节点控制器会watch这一信息会变更node的状态：MemoryPressure: True，一旦节点被标注为该状态，那用一些调度规则将pod调度到该节点，该pod不会正常启动会处于Evicted 4.2 驱逐信号驱逐信号是特定资源在特定时间点的当前状态。 kubelet使用驱逐信号与驱逐条件进行比较来做出驱逐决定 pkg/kubelet/eviction/api/types.go 源码中定义了memory、pid和file system（nodefs，imagefs）三种驱逐信号 4.2.1 内存驱逐信号 节点状态 驱逐信号 系统获取驱逐信号值的计算方式 MemoryPressure memory.available node.status.capacity[memory] - node.stats.memory.workingSet MemoryPressure allocatableMemory.available pod.allocatable - pod.workingSet 注意： memory.available 的值来自 cgroupfs（/sys/fs/cgroup/...），而不是 free -m，因为 free -m 在容器中不起作用，它看到的是宿主机全局内存，不能反映容器的资源。 当内存不足时，Kubelet会根据Pod的QoS（质量服务等级）来进行排序并逐出。优先逐出的是BestEffort，接着是Burstable，最后是Guaranteed，总的来说优先驱逐低优先级的Pod。这种排序方式确保了资源消耗较少且非关键性的Pod先被逐出，从而保障关键性服务 4.2.2 磁盘驱逐信号 节点状态 驱逐信号 系统获取驱逐信号值的计算方式 DiskPressure nodefs.available node.stats.fs.available 表示该文件系统可用的磁盘空间 DiskPressure nodefs.inodesFree node.stats.fs.inodesFree 表示该文件系统可用的 inode 数量 DiskPressure imagefs.available node.stats.runtime.imagefs.available 同上 DiskPressure imagefs.inodesFree node.stats.runtime.imagefs.inodesFree 同上 k8s关于磁盘的容量监测，主要就是监测就监测两种文件系统，本质就是对应两个文件夹 nodefs：/var/lib/kubelet（pod运行相关的本地磁盘卷、emptyDir 卷、日志存储） 如果某个节点的nodefs达到驱逐阈值，kubelet会首先删除所有已经失效的pod及其容器实例对应的磁盘文件 如果还不够用，在这种情况下引发的驱逐，kubelet并不会参考pod的Qos等级，而是按照pod对nodefs的使用量来进行排序来驱逐，所以即使是Qos等级为Guaranteed的Pod，在这个阶段也有可能被驱逐（例如nodefs使用量最大） images：/var/lib/containerd（容器运行时的只读层、可写层） 如果某个节点的images达到驱逐阈值，删掉停掉的容器、没有被使用的镜像 4.2.3 PID驱逐信号 节点状态 驱逐信号 系统获取驱逐信号值的计算方式 PIDPressure pid.available node.stats.rlimit.maxpid - node.stats.rlimit.curproc 4.3 驱逐条件驱逐条件是节点上应该可用资源的最小量&#x2F;阈值，达到驱逐条件则会按照优先级驱逐 pod 驱逐条件的格式为 [eviction-signal][operator][quantity] ，其中 eviction-signal 驱逐信号 operator 关系运算符 quantity 阈值，必须与Kubernetes使用的数值表示相匹配，例如 1Gi，可以使用数值或百分比 例如如果一个节点的总内存为 10GiB 并且你希望在可用内存低于 1GiB 时触发驱逐， 则可以将驱逐条件定义为 memory.available &lt; 10% 或 memory.available &lt; 1G （你不能同时使用二者） 驱逐条件也分为软驱逐和硬驱逐两种 达到预设阈值之后 软驱逐会有宽限期，如果改善到低于阈值就不进行驱逐，若这段时间一直高于阈值就进行驱逐 硬驱逐没有宽限期，一旦达到阈值配置，kubelet立马回收关联的短缺资源，将pod kill掉，而不是优雅终止 当真的发起驱逐的时候 驱逐到什么程度停下来：对于软驱逐会一直驱逐到资源使用量在软阈值以下，硬驱逐会一直驱逐到 --eviction-minimum-reclaim 规定的资源指标 驱逐采取的是强制杀还是平滑的杀：软驱逐是平滑的杀（本身不会强制杀，但是超过最大grace时间也会强制杀死），硬驱逐是直接强制杀，没有grace时间 4.3.1 soft软驱逐配置参数第一种方式：加入到 /etc/kubernetes/kubelet.env ，即kublet启动参数即可 12345678910# 触发软驱逐策略的阀值--eviction-soft=memory.available&lt;10%,nodefs.available&lt;15%,imagefs.available&lt;15%# 宽限期，表示达到阈值后到开始驱逐动作之前的时间，如果在此时间内还未恢复到阀值以下，则会开始驱逐pod--eviction-soft-grace-period=memory.available=2m,nodefs.available=2m,imagefs.available=2m # 满足驱逐条件需要Kill Pod时，等待Pod优雅退出的时间。超过这个时间会强制杀死--eviction-max-pod-grace-period=30注意：软驱逐的目的是尽量恢复资源到正常状态，而不是确保回收特定的资源量，所以没有专门的参数像硬驱逐中的 --eviction-minimum-reclaim 一样来控制软驱逐停止时的回收量。 第二种方式：如果是kubeadm安装的集群，在kubelet的配置文件中修改，可以用 ps aux |grep kubelet 或者 systemctl status kubelet 查看配置文件路径 12345678910111213$ vim /var/lib/kubelet/config.yaml # 顶头写，不用缩进evictionSoft: memory.available: &quot;256Mi&quot; nodefs.available: &quot;1Gi&quot; imagefs.available: &quot;1Gi&quot;evictionSoftGracePeriod: memory.available: &quot;2m&quot; nodefs.available: &quot;5m&quot; imagefs.available: &quot;5m&quot;$ systemctl daemon-reload$ systemctl restart kubelet 4.3.2 hard硬驱逐配置参数第一种方式：加入到 /etc/kubernetes/kubelet.env ，即kublet启动参数即可 123456# 触发硬驱逐策略的阀值--eviction-hard=memory.available&lt;256Mi,nodefs.available&lt;1Gi,imagefs.available&lt;1Gi# 最小驱逐回收策略，表示每一次驱逐必须至少回收多少资源，所以一直驱逐直到达到此行策略的阀值为止。该参数可以避免在某些情况下，驱逐Pod只会回收少量的资源，驱逐完之后没一会资源又涨上来了，导致反复触发驱逐# 要注意的是，这个值设置的过高会导致陷入节点状况为True无法恢复，如果一直处于这个状态，那么这个节点的pod会一直被evicted驱逐，所以建议可以根据实际情况设置的小一些合理一些，确保资源回收之后，达到一个合理的指标即可恢复正常--eviction-minimum-reclaim=memory.available=512Mi,nodefs.available=1Gi,imagefs.available=1Gi 第二种方式：如果是kubeadm安装的集群，在kubelet的配置文件中修改，可以用 ps aux |grep kubelet 或者 systemctl status kubelet 查看配置文件路径 12345678910111213$ vim /var/lib/kubelet/config.yaml # 顶头写，不用缩进evictionHard: memory.available: &quot;100Mi&quot; nodefs.available: &quot;500Mi&quot; imagefs.available: &quot;500Mi&quot;evictionMinimumReclaim: memory.available: &quot;100Mi&quot; nodefs.available: &quot;200Mi&quot; imagefs.available: &quot;200Mi&quot; $ systemctl daemon-reload$ systemctl restart kubelet 4.4 节点状况上报kubelet 报告节点状况以反映节点处于压力之下， 原因是满足硬性的或软性的驱逐条件，与配置的宽限期无关。 kubelet 根据下表将驱逐信号映射为节点状况 节点状况 驱逐信号 描述 MemoryPressure memory.available 节点上的可用内存已满足驱逐条件 DiskPressure nodefs.available、nodefs.inodesFreeimagefs.available、imagefs.inodesFree 节点的根文件系统、镜像文件系统或容器文件系统上的可用磁盘空间和 inode 已满足驱逐阈值 PIDPressure pid.available （Linux）节点上的可用进程标识符已低于驱逐条件 控制平面还将这些节点状况映射为其污点，例如节点挂掉之后自动打上NoExecute的污点 kubelet 根据配置的 --node-status-update-frequency 更新节点条件&#x2F;状态，默认为 10s 4.5 节点状况波动在某些情况下，节点资源压力（如内存、磁盘）可能在驱逐阈值附近波动，导致节点状况（如 MemoryPressure）频繁在 True 和 False 之间切换。这会使调度器误判节点状态，反复调度新 Pod，引发“驱逐-调度-再驱逐”的恶性循环。 为了防止振荡，可以使用 eviction-pressure-transition-period 标志，该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间，过渡期的默认值为 5m。注意它与软策略里的grace时间是不同的概念 但它不能解决根本的资源不足问题，eviction-pressure-transition-period 只能是帮助我们避免节点陷入“高频震荡”，大量 Pod 被反复创建&#x2F;销毁，因此应利用这个窗口期解决根本问题（扩容节点、优化资源请求） 第一种方式：加入到 /etc/kubernetes/kubelet.env ，即kublet启动参数即可 12# 当节点压力解除后，kubelet必须等待30s，才能将节点状况（如MemoryPressure）从True改为False--eviction-pressure-transition-period=30s 第二种方式：kubelet的配置文件 12$ vim /var/lib/kubelet/config.yamlevictionPressureTransitionPeriod: &quot;30s&quot; 4.6 驱逐监测间隔kubelet 根据其配置的 housekeeping-interval （默认为 10s ）评估驱逐条件 4.7 限制最大保留的Evicted pod数当资源不够或发生争夺有pod被驱逐后，pod的状态会变为Eviction，该状态pod并不会被立即删除掉，需要等到垃圾回收机制（周期性执行）触发时，才会进行回收，垃圾回收机制执行时会检查被驱逐的pod数量，如果没有达到限制则不会立即清理 于是问题来了！如果资源一直无法协调过来，或者资源真的不够用了，那么会产生大量的Eviction状态的Pod，会影响整个集群的使用，所以需要限制Eviction的Pod的数量 此配置是由 kube-contoller-manager 来管理的，所以在/etc/kubernetes/manifests/kube-controller-manager.yaml 添加 123# 集群中最多只允许存在1个Eviction状态的Pod，一旦超过这个数量，GC就会开始批量删除这些Pod，最终只保留1个# 默认为12500，最小可以设置为1，注意如果你设置为0，则表示没有限制--terminated-pod-gc-threshold=1 注意：Kubernetes的GC机制针对多种资源，包括终止的Pod、已完成的Job、以及没有所有者引用的对象进行清理，系统会自动进行垃圾回收以保持集群资源的健康管理，但具体的回收时机和周期是由系统自动管理的，不提供直接配置用于精确控制被驱逐Pod的垃圾回收时间的参数 4.8 驱逐示例测试pod如下：创建2个pod，分别调到到两个不同节点 12345678910111213141516171819202122232425262728# qos-demo1.yamlapiVersion: v1kind: Podmetadata: name: qos-demo1spec: nodeName: k8s-node-02 containers: - name: nginx image: centos:7 command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1000000&quot;] securityContext: privileged: true # 开启特权模式，一会测试脚本内挂载tmps文件系统挂载需要权限---# qos-demo.yamlapiVersion: v1kind: Podmetadata: name: qos-demospec: nodeName: k8s-node-01 containers: - name: nginx image: centos:7 command: [&quot;sh&quot;, &quot;-c&quot;, &quot;sleep 1000000&quot;] securityContext: privileged: true # 开启特权模式，一会测试脚本内挂载tmps文件系统挂载需要权限 在两个节点上设置内存的硬阈值设置 12345678[root@k8s-node-02 ~]# cat /var/lib/kubelet/config.yamlevictionHard: memory.available: &quot;500Mi&quot; evictionMinimumReclaim: memory.available: &quot;800Mi&quot; evictionPressureTransitionPeriod: &quot;30s&quot; 写一个脚本 increase-mem.sh 来人为制造内存不足 12345678#!/bin/bashmkdir /tmp/memorymount -t tmpfs -o size=1G tmpfs /tmp/memory dd if=/dev/zero of=/tmp/memory/blocksleep 120rm -rf /tmp/memory/blockumount /tmp/memoryrmdir /tmp/memory 产生两个被驱逐的pod，过一会GC机制触发后，会清理到一个，然后保留一个，因为最大保留限制1个 5 总结Pod驱逐流程当资源使用情况触发了驱逐条件时，kubelet会启动一个任务去轮流停止运行中的pod，直到资源使用状况恢复到阈值以下。以硬驱逐为例，整体流程是： 1、每隔一段时间从kubelet内部的cadvisor组件中获取资源使用情况，和定义的阀值进行对比 1在 kubelet 中 --node-status-update-frequency 参数来定义获取及上报的频率，默认为10s。 2、从运行中的pod里按QoS策略优先级进行驱逐 当资源紧缺时，驱逐pod的优先级大体为 ： BestEffort &gt; Burstable &gt; Guaranteed ，这是大前提，在遵循大前提的情况下，谁耗 的多就驱逐谁 当 pod qos&#x3D;BestEffort 时，消耗最多紧缺资源的 Pod 最先驱逐。 当 pod qos&#x3D;Burstable 时，请求（request的值）最多紧缺资源的 Pod 会被驱逐，如果没有Pod 超出他们的请求(比如说mem request的值为1G,但实际使用量并没有超过1G)，会驱逐资源消耗量最大的 Pod。 当 pod qos&#x3D;Guaranteed 时，请求（request的值）最多紧缺资源的 Pod 被驱逐，如果没有Pod 超出他们的请求，会驱逐资源消耗量最大的 Pod。 如果当磁盘空间 &#x2F;inodes 紧缺时，就会通过 QoS 的等级基础上，选择消耗最多磁盘空间 inodes 的 Pod 进行驱逐。 3、检查资源是否到达阀值以内，若还未满足，则再进行第二步 6 节点资源紧缺情况下的操作系统行为在资源紧缺情况下，系统级的OOM killer与k8s的组件kubelet的pod驱逐要混在一起考虑 &#x3D;&#x3D;1、调度器的行为&#x3D;&#x3D; 在节点资源紧缺时，节点上的kubelet组件会想Master报告这一情况，在Master上进行的调度器不会再向该节点调度Pod &#x3D;&#x3D;2、Node的OOM行为（kubelet无法及时观测到内存压力，可能会出现系统级OOM提前触发的情况）&#x3D;&#x3D; kubelet是周期性执行驱逐pod来释放资源的，如果在周期抵达之前遭遇到了系统的OOM（内存不足），节点则依赖oom_killer的设置来杀掉某些进程，杀掉哪些进程看评分OOMScore，可以使用 cat /proc/$PID/oom_score 命令查看进程的OOMScore kubelet根据pod的Qos为每个容器都设置了一个 oom_score_adj 值 QoS等级 oom_score_adj Guaranteed -998 Burstable min(max(2, 1000 - (1000 * memoryRequestBytes) &#x2F; machineMemoryCapacityBytes), 999) BestEffort 1000 如果kubelet无法在系统OOM killer之前回收足够的内存的话，则oom_killer会根据下列公式来计算OOMScore，得分最高的Pod首先被杀掉：系统总的可用页面数 * oom_score_adj值 + 进程已经使用的物理页面数 也就是说当系统 OOM 时，Qos最低且相对于调度的Request来说消耗最多内存的Pod会首先被杀掉，来保障内存的回收 要注意的是，节点触发的OOM杀掉了某个Pod这种策略，与kubelet驱逐Pod是不同的，如果一个Pod的容器被系统OOM“杀掉了”，则可能被kubelet根据RestartPolicy重启 &#x3D;&#x3D;3、对DaemonSet类型的Pod驱逐的考虑&#x3D;&#x3D; 通过DaemonSet创建Pod具有在节点上自动重启的特性，因此我们不希望kubelet驱逐这种Pod，然而kubelet目前并没有能力分辨DaemonSet的Pod，所以无法单独为其定制驱逐策略。 所以强烈建议不要在DaemonSet中创建BestEffort类型的Pod，最好是Guaranteed级，避免产生驱逐方面的问题，然后Daemonset又自动重启Pod，反复横跳&#x2F;震荡，导致不稳定 7 落地经验先说结论：kube-controller-manager的驱逐时间不要过短 对于 kubelet 发起的驱逐，往往是资源不足导致，它优先驱逐 BestEffort 类型的容器，这些容器多为离线批处理类业务，对可靠性要求低。驱逐后释放资源，减缓节点压力，弃卒保帅，保护了该节点的其它容器。无论是从其设计出发，还是实际使用情况，该特性非常 nice。 对于由 kube-controller-manager 发起的驱逐，效果需要商榷。正常情况下，计算节点周期上报心跳给master，如果心跳超时，则认为计算节点NotReady，当NotReady状态达到一定时间后，kube-controller-manager发起驱逐。然而造成心跳超时的场景非常多，例如 原生 bug：kubelet 进程彻底阻塞 误操作：误把 kubelet 停止 基础设施异常：如交换机故障演练，NTP 异常，DNS 异常 节点故障：硬件损坏，掉电等 从实际情况看，真正因计算节点故障造成心跳超时概率很低，反而由原生 bug，基础设施异常造成心跳超时的概率更大，造成不必要的驱逐。理想的情况下，驱逐对无状态且设计良好的业务方影响很小。但是并非所有的业务方都是无状态的，也并非所有的业务方都针对 Kubernetes 优化其业务逻辑。例如，对于有状态的业务，如果没有共享存储，异地重建后的 pod 完全丢失原有数据；即使数据不丢失，对于 Mysql 类的应用，如果出现双写，重则破坏数据。对于关心 IP 层的业务，异地重建后的 pod IP 往往会变化，虽然部分业务方可以利用service 和 dns 来解决问题，但是引入了额外的模块和复杂性。 除非满足如下需求，不然请尽量关闭 kube-controller-manager 的驱逐功能，即把驱逐的超时时间设置非常长，同时把一级／二级驱逐速度设置为 0。否则，非常容易就搞出大大小小的故障，血泪的教训。 业务方要用正确的姿势使用容器，如数据与逻辑分离，无状态化，增强对异常处理等 分布式存储 可靠的 Service／DNS 服务或者保持异地重建后的 IP 不变 通常情况下，从架构设计的角度来说，管理模块的不可用并不会影响已有的实例。例如：OpenStack 计算节点的服务异常，管理节点不会去变动该节点上已经运行虚拟机。但是 kubernetes 的 pod eviction则打破了这种设计，出发点很新颖，但是落地时，需要充分的考虑业务方的特点，以及业务方的设计是否充分考虑了 kubernetes 的特色。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Pod","slug":"Pod","permalink":"https://aquapluto.github.io/tags/Pod/"}]},{"title":"Pod的调度","slug":"Kubernetes/pod/schedule","date":"2025-09-12T06:18:44.000Z","updated":"2025-09-13T15:51:44.769Z","comments":true,"path":"Kubernetes/pod/schedule/","permalink":"https://aquapluto.github.io/Kubernetes/pod/schedule/","excerpt":"","text":"1 调度的概念调度指的就是创建新pod的时候，应该分配给哪个节点来创建该pod，具体负责创建的是该节点上的kubelet。要注意的是静态pod不参与pod的调度流程，静态固定在某个节点上，只有非静态pod的创建，都会有调度流程 什么时候会创建新pod 使用者提交一个全新的创建pod的请求 某个节点挂掉、资源不足了，导致该节点上pod被驱逐到其他节点 当副本数与预期不符，引发调谐过程也会发起创建新pod的请求 Pod进行调度时会进行预选和优选 预选：根据资源申请，选择器、亲和与反亲和、污点与容忍等规则，先筛选出一些机器 —&gt; 符合基本条件的 优选：从筛选出的节点中进行打分，选择资源最优 —&gt; 谁最优 节点选定：从优先级排序结果中挑选出优先级最高的节点运行Pod，当这类节点多于1个时，则进行随机选择 2 调度策略1、Pod声明的requests和limits initContainers与Containers包含多个容器的情况下，k8s如何调度？步骤如下 找出initContainers多个容器中对资源需求requests最大的那一个 –&gt; 得到一个值假设为x 求出Containers下多个容器中对资源需求requests的综合 –&gt; 得到一个值假设为y k8s会取x与y中较大的那一个作为预选阶段的筛选依据，确保pod被调度某个节点上之后 无论是initContainer还是业务Containers容器都有足够的剩余资源来确保拉起 2、节点标签选择器 会选择符合标签的节点进行调度 3、节点亲和性 分为硬亲和和软亲和，前者必须满足，后者尝试满足，不强制 4、污点 Pod 默认不会调度到带有污点（Taint）的节点上，除非该 Pod 配置了匹配的容忍（Toleration），才能被调度到该节点 2.1 资源需求（requests）和限制（limits）资源申请和资源限制，requests和limits定义在容器级别，主要围绕cpu、memory和hugepages三种资源 资源申请（requests） 定义需要系统预留给该容器使用的资源最小可用值 预选阶段的参考指标，并没有实际占住资源，容器运行时可能用不到这些额度的资源，但用到时必须确保有相应数量的资源可用，也有可能超过这个额度，如果是内存就会触发OOM，所以有了资源限制 资源申请的定义会影响调度器Scheduler的决策 不定义requests，默认和 limits 的值一样 资源限制（limits） 定义该容器可以申请使用的资源最大可用值，超出该额度的资源使用请求将被拒绝 该限制需要大于等于requests的值（request &lt;&#x3D; limit &lt;&#x3D; 节点可用内存） 系统在其某项资源紧张时，会从容器那里回收其使用的超出其requests值的那部分 安全起见，内存、磁盘的limits要和requests一样，CPU不做硬性要求，因为CPU是可压缩资源 2.1.1 原理在k8s中，会为每个容器所调度到的节点中，在cgroup的子系统中创建一个控制组，然后把容器进程的pid写入控制组的 cgroup.procs 中。我们为Kubernetes设置CPU requests实际上是设置了cpu.shares cgroup属性。就像内存requests对调度器的意义一样，CPU requests会让调度器选择至少拥有那么多可用CPU分片的节点。不同于内存requests，设置CPU requests也会给cgroup设置相应的属性，帮助内核实际给进程分配一样数量的CPU核心分片。Limits的处理也与内存不一样。超出内存limits会让你的容器进程成为oom-kill的选项，但是你的进程基本上不可能超出设置的cpu配额，并且永远不会因为试着使用更多CPU而被驱逐。系统在调度器那里加强了配额的使用，所以进程在到达limits后只会被限流。 limit控制的就是cpu.cfs_quota_us/cpu.cfs_period_us的比值，得到是资源使用上限，cpu.cfs_period_us通常固定，k8s不去修改它，只需要修改cpu.cfs_quota_us即可。 request指的是无论其他容器申请多少个cpu，即便是当物理机cpu打满，request也能保证获取需要的cpu，并不一定够用，但一定是按比例获取，cpu.cfs_shares与同一控制组目录树下的其他控制组的cpu.cfs_share的比值决定了谁可以抢到对应的比例的资源，cpu.cfs_shares只在cpu被打满的情况下才有效 总的来说，limit为4代表你无论如何都无法超过4，request 2的意思是你申请资源，在资源够用时，就给你2，但在资源不够用的情况下，你可能还不足2 2.1.2 资源指标分类与单位内存申请资源的单位 1234567891011在计算机科学中，内存和存储的单位通常有两种不同的计量方式：一种是基于1000的十进制系统，另一种是基于1024的二进制系统。 在十进制系统中，1 MB（Megabyte）等于1000 KB（Kilobyte） 在二进制系统中，1 MiB（Mebibyte）等于1024 KiB（Kibibyte）。当你提到128Mi时，这里的Mi通常指的是Mebibytes，即使用二进制计量方式。因此，要将128 MiB转换为MB，我们需要使用以下转换公式：MB= (MiB/1024)×1000将128 MiB代入公式中：MB= (128/1024)×1000≈125所以，128 MiB大约等于125 MB。这个计算显示了在使用基于1024的单位（如在大多数操作系统和计算机硬件中常见）与基于1000的单位（如在某些软件和网络带宽描述中常见）之间的差异。 具体的资源指标 123456789cpu: &quot;500m&quot; # 请求 0.5 核 CPUmemory: &quot;256Mi&quot; # 请求 256 MiB 内存ephemeral-storage: &quot;1Gi&quot; # 请求 1 GiB 临时存储,非持久化数据nvidia.com/gpu: &quot;1&quot; # 请求 1 个 GPU hugepages-2Mi: &quot;64Mi&quot; # 请求 64 MiB 大页内存,容器可以使用大页内存example.com/special-resource: &quot;2&quot; # 请求 2 个自定义扩展资源 # 允许你为节点上安装的特殊硬件或软件资源 #（例如特殊硬盘）定义自定义资源类型。 # 你可以在容器中指定这些资源的请求和限制 2.1.3 配置示例范例：资源需求 1234567891011121314151617[root@master1 pods]#vim resource-requests-demo.yamlapiVersion: v1kind: Podmetadata: name: stress-podspec: containers: - name: stress image: ikubernetes/stress-ng command: [&quot;/usr/bin/stress-ng&quot;, &quot;-c 1&quot;, &quot;-m 1&quot;, &quot;--metrics-brief&quot;] resources: requests: memory: &quot;128Mi&quot; cpu: &quot;200m&quot; #1000毫核=1个CPU核心，这里是0.2个CPU核心 limits: memory: &quot;512Mi&quot; cpu: &quot;400m&quot; 范例：资源限制 12345678910111213141516171819[root@master1 pods]#vim resource-limits-demo.yamlapiVersion: v1kind: Podmetadata: name: memleak-pod labels: app: memleakspec: containers: - name: simmemleak image: ikubernetes/simmemleak imagePullPolicy: IfNotPresent resources: requests: memory: &quot;64Mi&quot; cpu: &quot;1&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;1&quot; 2.1.4 所有容器都应该设置 requestrequest 的值并不是指给容器实际分配的资源大小，它仅仅是给调度器看的，调度器会“观察”每个节点可以用于分配的资源有多少，也知道每个节点已经被分配了多少资源，被分配资源的大小就是节点上所有 Pod 中定义的容器 request 之和，它可以计算出节点剩余多少资源可以被分配（可分配资源减去已分配的request 之和）。如果发现节点剩余可分配资源大小比当前要被调度的 Pod 的reugest还小，那么就不会考虑调度到这个节点，反之才可能调度。 所以如果不配置 request，那么调度器就不能知道节点大概被分配了多少资源出去，调度器得不到准确信息也就无法做出合理的调度决策，很容易造成调度不合理，有些节点可能很闲，而有些节点可能很忙，甚至NotReady。使用 kubectl describe node 可以看到节点已经被分配了多少资源 CPU request与 limit 的一般性建议 如果不确定应用最佳的 CPU 限制，可以不设置CPU limit 如果要设置 CPU request，大多可以设置到不大于1核，除非是CPU密集型应用 合理设置 Request 与 Limit | Kubernetes 实践指南 2.2 NodeSelectorNodeSelector：节点选择器，把pod调度到指定的物理节点上，选择依据是节点的标签，所以必须事先给目标节点打上标签。这个物理节点一定有其特殊之处，例如有GPU、有固态盘 1234567891011121314151617181920212223242526# 查看所有节点标签信息kubectl get node --show-labels# 查看特定节点标签信息kubectl get nodes k8s-master1 --show-labels# 设置节点标签，为节点k8s-worker1打一个region=huanai的标签kubectl label node k8s-worker1 region=huanai# 设置多维度标签，用于不同的需要区分的场景，如把`k8s-master3`标签为华南区,A机房,测试环境,游戏业务kubectl label node k8s-master3 zone=A env=test bussiness=game# 查看所有节点带region的标签kubectl get nodes -L region# 查看带多个标签的节点kubectl get nodes -L region,zone# 查看带有特定标签值的节点kubectl get nodes -l region=huanai# 修改标签，--overwrite=true覆盖原标签的value进行修改操作kubectl label node k8s-master3 bussiness=ad --overwrite=true# 删除标签，使用key加一个减号的写法来取消标签kubectl label node k8s-worker1 region- 场景举例 把运行有人工智能应用的pod调度到包含GPU的物理节点 把mysql调度到一个具有SSD磁盘的目标节点 1234567891011121314151617181920# 1、带有sshd磁盘的物理节点打上自定义标签disk=ssdkubectl label nodes k8s-node-01 disk=ssd# 2、pod模版中设定NodeSelector的值为disk=ssd，则会调度到指定节点apiVersion: v1kind: Podmetadata: labels: app: centos-pod name: test-podspec: containers: - name: test-pod image: centos imagePullPolicy: Always command: - sleep - &quot;3600&quot; nodeSelector: disk: ssd 注意 如果我们给多个Node都定义了相同的标签（disk&#x3D;ssd），则kube-scheduler会根据调度算法从这组Node中挑选一个可用的Node NodeSelector一定会调度到包含执行标签的节点，没有这种节点则调度失败，pod无法创建 物理节点上有一些预定义标签，你也可以直接选中这些标签 12345[root@k8s-master-01 ~]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSk8s-node-01 Ready &lt;none&gt; 5d14h v1.30.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-01,kubernetes.io/os=linux,disk=ssdk8s-node-02 Ready &lt;none&gt; 5d14h v1.30.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node-02,kubernetes.io/os=linuxnode Ready control-plane 5d14h v1.30.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers= 了解：NodeSelector将继续被使用，但是随着节点亲和性与反亲和性越来越能够体现nodeSelector所具备的功能，最终NodeSelector会被遗弃掉 2.3 亲和性与反亲和性亲和性：一些彼此依赖的服务需要调度到同一个“地方” 反亲和性：考虑到冗余性，需要将一些服务调到不同的“地方” 亲和性与反亲和性，有两种维度 节点维度：标签选择器定位的是节点的标签 pod维度：标签选择器定位的是pod的标签，而不是节点 无论何种维度的亲和性与反亲和性，都对应有两种具体的策略 软策略(preferredDuringSchedulingIgnoredDuringExecution)：在Pod调度时可以尽量满足其规则，在无法满足规则时，可以调度到一个不匹配规则的节点之上。并且可以设置权重 硬策略(requiredDuringSchedulingIgnoredDuringExecution)：是Pod调度时必须满足的规则，否则Pod对象的状态会一直是Pending，但不能设置权重 需要注意的是，软硬策略名字中后半段字符串 IgnoredDuringExecution 表示的是，在Pod资源基于节点亲和性规则调度到某个节点之后，如果节点的标签发生了改变，该pod不再符合该节点的亲和性要求，调度器不会将Pod从该节点上移除，因为该规则仅对新建的Pod对象有效。 2.3.1 节点维度2.3.1.1 节点亲和nodeAffinity是用于替换nodeSelector的全新调度策略，硬策略与软策略可以一起用，也可以单独用，NodeAffinity语法支持的操作符 In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 12345678910111213141516171819202122232425262728293031323334353637383940# node-affinity-demo.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: node-affinity labels: app: node-affinityspec: replicas: 8 selector: matchLabels: app: node-affinity template: metadata: labels: app: node-affinity spec: containers: - name: nginx image: nginx:1.18.0 ports: - containerPort: 80 name: nginxweb affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略:必须满足 nodeSelectorTerms: - matchExpressions: - key: node_type operator: In values: - gpu preferredDuringSchedulingIgnoredDuringExecution: # 软策略：尽量满足 - weight: 1 # 软策略可以定义权重，执行时权重值高的优先匹配 preference: matchExpressions: - key: com operator: In values: - wu.com 节点打上标签 123456[root@k8s-master-01 ~]# kubectl label nodes k8s-node-01 node_type=gpunode/k8s-node-01 labeled[root@k8s-master-01 ~]# kubectl label nodes k8s-node-02 node_type=gpunode/k8s-node-02 labeled[root@k8s-master-01 ~]# kubectl label nodes k8s-node-01 com=wu.comnode/k8s-node-01 labeled 部署上述yaml：规则是必须调度到 node_type=gpu 的节点上，如果这些节点里满足 com=wu.com 的话就尽量调度到该节点上（说的是尽量，而不是必须，要是资源都不足了，那肯定是不会搭理软策略的，所以如下所示大多数都调到了k8s-node-01上，也有例外调度到k8s-node-02上 12345678910[root@k8s-master-01 ~ ]# kubectl get pods -o wideNAME READY STATUS NODE node-affinity-76b7f5d9-2xh4s 1/1 Running k8s-node-01 node-affinity-76b7f5d9-bdp89 1/1 Running k8s-node-01 node-affinity-76b7f5d9-btbl4 1/1 Running k8s-node-01 node-affinity-76b7f5d9-f8kl8 1/1 Running k8s-node-01 node-affinity-76b7f5d9-fllfs 1/1 Running k8s-node-01 node-affinity-76b7f5d9-sv5fl 1/1 Running k8s-node-02 # 注意这里node-affinity-76b7f5d9-t4j5j 1/1 Running k8s-node-01 node-affinity-76b7f5d9-x774m 1/1 Running k8s-node-01 nodeAffinity节点亲和的规则设置注意如下 如果同时设置了节点选择器nodeSeletor与nodeAffinity，那么必须两个条件都得到满足才行 如果nodeAffinity中设置了多个nodeSelectorTerms满足任一即可（逻辑或） 如果nodeSelectorTerms中设置了多个matchExpression，必须同时满足才可以（逻辑与） 2.3.1.2 节点反亲和节点维度本身并没有反亲和这种排斥功能，但是用NotIn、DoesNotExist就可以实现排斥的功能了 2.3.2 Pod维度2.3.2.1 Pod亲和2.3.2.1.1 硬策略准备好参照物pod，该pod拥有标签 security=s1 和 app=nginx，后面的例子将使用该pod作为亲和的参照物 123456789101112# pod-flag.yamlapiVersion: v1kind: Podmetadata: labels: security: &quot;s1&quot; app: &quot;nginx&quot; name: pod-flagspec: containers: - name: myapp image: nginx:1.18 123456789101112131415161718192021222324252627282930313233# required-podAffinity-pod1.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: pod-affinity namespace: default labels: app: pod-affinityspec: replicas: 3 selector: matchLabels: app: pod-affinity template: metadata: labels: app: pod-affinity spec: containers: - name: nginx image: nginx:1.18 ports: - containerPort: 80 name: nginxweb affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 - labelSelector: matchExpressions: - &#123;key: app, operator: In, values: [&quot;nginx&quot;, &quot;mysql&quot;]&#125; topologyKey: kubernetes.io/hostname # 指定拓扑域 #namespaces: #- kube-system # 会去该名称空间下查找带有标签app=nginx的pod与其亲和，不设置则默认从本pod当前的名称空间下找 PodAffinity规则设置注意事项：与Label Selector和topologyKey同级，还可以设置namespaces来指定去跨名称空间查找pod来进行亲和。如果namespaces省略、或者被设置为空null或空列表，那么 Pod Affinity 规则将仅应用于与当前 Pod 在同一命名空间中的 Pods。 上述亲和性—硬策略表示：新pod必须被调度到一个带有app&#x3D;nginx标签的pod所在的拓扑域为 kubernetes.io/hostname 中，这里的拓扑域是主机名，每台机器上打的这个标签对应的值均不同，都是自己的主机名，所以拓扑域中只包含唯一一台机器，所以简单理解为新pod必须被调度到一个带有app&#x3D;nginx标签的pod所在的那一台机器上 2.3.2.1.2 软策略准备好两个参照物pod 123456789101112131415161718192021222324# pod-flag1.yamlapiVersion: v1kind: Podmetadata: labels: app: &quot;nginx&quot; name: pod-flag1spec: containers: - name: myapp image: nginx:1.18 # pod-flag2.yamlapiVersion: v1kind: Podmetadata: labels: security: &quot;s1&quot; app: &quot;nginx&quot; name: pod-flag2spec: containers: - name: myapp image: nginx:1.18 1234567891011121314151617181920212223242526272829303132333435363738# deploy-with-preferred-podAffinity.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: pod-affinity labels: app: pod-affinityspec: replicas: 3 selector: matchLabels: app: pod-affinity template: metadata: labels: app: pod-affinity spec: containers: - name: nginx image: nginx:1.18 ports: - containerPort: 80 name: nginxweb affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 80 podAffinityTerm: labelSelector: matchExpressions: - &#123;key: app, operator: In, values: [&quot;nginx&quot;]&#125; topologyKey: kubernetes.io/hostname - weight: 20 podAffinityTerm: labelSelector: matchExpressions: - &#123;key: app, operator: In, values: [&quot;db&quot;]&#125; topologyKey: kubernetes.io/hostname 上述的清单配置当中，pod的软亲和调度需要将Pod调度到标签为 app=nginx 的pod所在的拓扑域当中，或者调度到 app=db 标签的pod所在的拓扑域中，若不存在此类标签，调度器会根据软亲和调度进行随机调度到一个节点之上 123456789101112131415161718192021$ kubectl get pods -o wide -l app=pod-affinitypod-affinity-6f98f9fcf6-9vxb9 1/1 Running 0 k8s-node-01 pod-affinity-6f98f9fcf6-qvn89 1/1 Running 0 k8s-node-01 pod-affinity-6f98f9fcf6-sb7md 1/1 Running 0 k8s-node-01 # 修改上面软亲和的权重，下面的权重变80，上面的变20 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 20 # 原来为80，改为20 .... - weight: 80 # 原来为20，改为80 ..... # 删除后重新部署$ kubectl delete -f deploy-with-preferred-podAffinity.yaml$ kubectl apply -f deploy-with-preferred-podAffinity.yaml$ kubectl get pods -o wide -l app=pod-affinitypod-affinity-7b5cc4f784-5nzmx 1/1 Running k8s-node-02pod-affinity-7b5cc4f784-mp7lp 1/1 Running k8s-node-02pod-affinity-7b5cc4f784-qwbf5 1/1 Running k8s-node-02 2.3.2.2 Pod反亲和pod 反亲和性则是反着来的，比如一个节点上运行了某个 pod，那么我们的 pod 则希望被调度到其他节点上去，同样我们把上面的 podAffinity 直接改成 podAntiAffinity，创建参照物pod略，直接用之前创建好的 1234567891011121314151617181920212223242526272829303132# pod-antiaffinity-demo.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: pod-affinity labels: app: pod-affinityspec: replicas: 3 selector: matchLabels: app: pod-affinity template: metadata: labels: app: pod-affinity spec: containers: - name: nginx image: nginx:1.18 ports: - containerPort: 80 name: nginxweb affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 - labelSelector: matchExpressions: - &#123;key: app, operator: In, values: [&quot;nginx&quot;]&#125; topologyKey: zone # 用pod的标签选择器选出的pod，然后避免调度到这些pod所在拓扑域（zone）的节点上 # 即：确保带有 app=nginx 标签的 Pod 不会调度到同一个 zone 中 3个新的pod副本没有一个被调度到 app&#x3D;nginx 标签的pod所在的主机域 k8s-node-01 上 123456789$ kubectl get pods -l app=nginx -o widepod-flag1 1/1 Running 0 14m 10.244.1.39 k8s-node-01 $ kubectl apply -f pod-antiaffinity-demo.yaml$ kubectl get pods -o wide |grep pod-affinitypod-affinity-855b988c76-f7jgq 1/1 Running ...... k8s-node-02pod-affinity-855b988c76-hchwt 1/1 Running ...... k8s-node-02 pod-affinity-855b988c76-kvd9k 1/1 Running ...... k8s-node-02 2.3.3 拓扑域一个拓扑域可以是 一个地区Zone（如华北地区、华南地区） 一个机房 一个机架 甚至是单独一台机器 我们把Node物理节点归属的区域称之为拓扑域，而Node物理节点归属的区域通常按照物理服务器的摆放位置来进行划分 从最小的维度看，单个Node就独自构成一个拓扑域，该拓扑域里只有它自己这台机器 范围扩大，同一个机架上摆放的n个Node也是一个拓扑域 范围继续扩大，一个机房里摆放的n个Node是一个更大的拓扑域 范围继续扩大，一个地域（Zone）中摆放的n个Node是一个更更大的拓扑域，例如华北地区、华南地区 通过这种方式，我们就可以将各个 Pod 进行跨集群、跨机房、跨地区的调度了 在前面的基础上，我们更进一步，pod亲和与反亲和真正准确的解释如下：pod亲和性调度需要各个相关的pod对象运行于”同一位置”，即n个pod需要被调度到同一个拓扑域中；而反亲和性调度则要求他们不能运行于”同一位置”，即n个pod不能被调度到同一个拓扑域中，在同一个拓扑域中相互排除 当我们给机器打上标签，比如1～5节点打上zone&#x3D;A标签，6～10节点有zone&#x3D;B标签，就表示机器所归属的拓扑域，所以可以认为拓扑域的本质就是打标签。当我们创建新pod时，通过 topologyKey 来指定，对应的值是 node 上的一个标签名称，该名称对应的值相同的节点就属于同一个拓扑域，那么pod affinity topologyKey定义为zone时，1～5节点属于一个位置，6～10属于另外一个位置 示例需求：当前有两个机房（ beijing，shanghai），需要部署一个nginx产品，副本为两个，为了保证机房容灾高可用场景，需要在两个机房分别部署一个副本 前提： beijing机房的物理节点已经为node打上标签zone&#x3D;beijing shanghai机房的物理节点已经为node打上标签zone&#x3D;shanghai 1234567891011121314151617181920212223242526272829303132333435apiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-affinity-testspec: serviceName: &quot;nginx-service&quot; replicas: 2 selector: matchLabels: service: nginx template: metadata: name: nginx labels: service: nginx spec: affinity: # 下面的 pod 反亲和策略总结一句话就是：新 pod 不能调度到带有标签 service=nginx 的 pod 所在的 zone。 # 创建第一个 pod 时，service=nginx 标签的 pod 是全新的，尚未存在于任何一个拓扑域中，所以可以调度。 # 创建第二个 pod 时，service=nginx 标签的 pod 已经存在，已存在于某一个拓扑域中，所以第二个 pod 排斥该拓扑域。 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: service operator: In values: - nginx topologyKey: zone containers: - name: test image: contos7:latest command: - sleep - &quot;360000000&quot; 解释：两个node上分别有zone标签，来标注自己属于哪个机房，topologyKey定义为zone，pod所以在调度的时候，会根据node上zone标签来区分拓扑域，当前用的上 反亲和性调度 根据拓扑纬度调度，beijing机房调度完一个pod后，然后控制器判断beijing 拓扑域已经有server&#x3D;nginx标签的pod，就在下一个拓扑域的node上调度了 k8s中内置了一些常用的默认拓扑域标签&#x2F;标识，图省事，可以直接用这些标签 1234567891011121314151617# 1、该拓扑域由k8s自动打上kubernetes.io/hostname=物理节点的主机名# 2、公有云厂商还会为Node节点打上下面两种标签，来标识拓扑域topology.kubernetes.io/region=topology.kubernetes.io/zone=华北三区具体使用的时候需要在pod中指定key为topologyKey，value为上面的拓扑域的名字例1：topologyKey: kubernetes.io/hostname # 如果用于pod亲和，则代表必须亲和到kubernetes.io/hostname这个标签的值相同的域中，而kubernetes.io/hostname这个标签的值相同，只有一种情况，那就是同一个节点例2：topologyKey: topology.kubernetes.io/zone # 如果用于pod亲和，则代表必须亲和到topology.kubernetes.io/zone这个标签的值相同的域中，而topology.kubernetes.io/zone这个标签的值相同的机器有非常非常多，然后你再配合具体的labelSelector来进一步选中这个zone中的某些机器即可例3：topologyKey: topology.kubernetes.io/region # 同上 2.4 污点与容忍1、什么是污点（taints） 对于 nodeAffinity 无论是硬策略还是软策略方式，都是调度 pod 到预期节点上，而 Taints污点 恰好与之相反，如果一个节点标记为 Taints ，除非 pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 pod。 2、什么是容忍（tolerations） 污点（taints）是定义在节点上的一组键值型属性数据，用来让节点拒绝将Pod调度到该节点上，除非该Pod对象具有容纳节点污点的容忍度。而容忍度（tolerations）是定义在Pod对象上的键值型数据，用来配置让Pod对象可以容忍节点的污点。 3、污点与容忍的应用场景 比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊硬件设备的节点预留给某些 pod，或者让一部分节点专门给一些特定应用使用&#x2F;独占，则污点就很有用了，pod 不会再被调度到 taint 标记过的节点。 注意：我们使用 kubeadm 搭建的集群默认就给 master 节点添加了一个污点标记，所以我们看到我们平时的 pod 都没有被调度到 master 上去 1234567891011$ kubectl describe node masterName: masterRoles: masterLabels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=master node-role.kubernetes.io/master=......Taints: node-role.kubernetes.io/control-plane:NoScheduleUnschedulable: false...... 一些系统级别的应用在创建时，就会添加相应的容忍度来确保被创建时可以调度到master节点上，如flannel插件 123456789[root@master01 ~]# kubectl describe pods kube-flannel-ds-amd64-2p8wm -n kube-system......Tolerations: node-role.kubernetes.io/master:NoSchedule node.kubernetes.io/disk-pressure:NoSchedule node.kubernetes.io/memory-pressure:NoSchedule node.kubernetes.io/not-ready:NoExecute node.kubernetes.io/unreachable:NoExecute...... 2.4.1 为节点打上和去除污点污点是定义是在节点上的，而容忍度的定义是在Pod中的podSpec，都属于键值型数据，两种方式都支持一个 effect 标记，语法格式为 key=value: effect ，其中 effect 是用来定义对Pod对象的排斥等级，主要包含以下3种类型 NoSchedule：一定不被调度。属于强制约束，节点现存的Pod对象不受影响。 PreferNoSchedule：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去 NoExecute：该选项意味着一旦 Taint 生效，如该节点内正在运行的 pod 没有对应 Tolerate 设置，会直接被逐出 ，属于强制约束 12kubectl taint nodes node1 key=value1:NoSchedulekubectl taint nodes node1 key=:NoSchedule # value可以省略，代表空值 去掉污点的语法：kubectl taint nodes &lt;node-name&gt; &lt;key&gt;[: &lt;effect&gt;]- 12345678#删除node01上的node-type标识为NoSchedule的污点$ kubectl taint nodes k8s-node01 node-type:NoSchedule-#删除指定键名的所有污点$ kubectl taint nodes k8s-node01 node-type-#补丁方式删除节点上的全部污点信息$ kubectl patch nodes k8s-node01 -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;taints&quot;:[]&#125;&#125;&#x27; 2.4.2 为pod定义容忍在Pod对象上定义容忍度时，其支持2种operator操作符： Equal 和 Exists Equal：等值比较，表示容忍度和污点必须在key、value、effect三者之上完全匹配。 Exists：存在性判断，表示二者的key和effect必须完全匹配，而容忍度中的value字段使用空值 如果不指定 operator 属性，则默认值为 Equal 另外，还有两个特殊值： 空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也是是能容忍所有 node 的所有 Taints 空的 effect 匹配所有的 effect 范例 1、给节点打上污点 1234$ kubectl taint nodes k8s-node-02 test=k8s-node-02:NoSchedule$ kubectl describe node k8s-node-02 |grep -i taintTaints: test=k8s-node-02:NoSchedule 2、给Pod配置容忍 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: Deploymentmetadata: name: taint labels: app: taintspec: replicas: 6 selector: matchLabels: app: taint template: metadata: labels: app: taint spec: containers: - name: nginx image: nginx:1.18 ports: - name: http containerPort: 80 tolerations: - key: &quot;test&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; 123456789101112$ kubectl apply -f taint-demo.yaml# 可以看到pod也会被调度到node02节点上$ kubectl get pods -o wide -l app=taintNAME READY STATUS RESTARTS AGE IP NODE taint-66d7f7bb-2vfdv 1/1 Running 0 5s 10.244.2.25 k8s-node-02 taint-66d7f7bb-46xn4 1/1 Running 0 5s 10.244.0.17 k8s-node-01taint-66d7f7bb-6vfkb 1/1 Running 0 5s 10.244.2.24 k8s-node-02 taint-66d7f7bb-htd2f 1/1 Running 0 5s 10.244.1.61 k8s-node-01 taint-66d7f7bb-qcjzd 1/1 Running 0 5s 10.244.1.60 k8s-node-01 taint-66d7f7bb-sh57j 1/1 Running 0 5s 10.244.2.23 k8s-node-02 2.4.3 定义Pod驱逐时间NoExecute配置后会把该节点上正在运行的Pod立即驱逐，指定驱逐时间(tolerationSeconds)，该时间过后Pod才会被驱逐，那么NoExecute这个Taint的效果对节点上正在运行的pod有以下影响 没有设置Toleration容忍的Pod会被立即驱逐 配置了对应Toleration的Pod，并且没有为tolerationSeconds赋值的pod，则会一直保留在该节点上 配置了对应Toleration的Pod，并且为tolerationSeconds赋了值的Pod，则会在指定的时间后被驱逐 注意：如果使用了tolerationSeconds，那么effect必须为NoExecute，不能是其他的 123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: taint labels: app: taintspec: replicas: 8 selector: matchLabels: app: taint template: metadata: labels: app: taint spec: containers: - name: nginx image: nginx:1.18 ports: - name: http containerPort: 80 tolerations: - key: &quot;test&quot; operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationSeconds: 30 有一种情况：指定时间过后，Pod被驱逐后会重新调度，因为配置容忍了，所以还是可能重新调度到被打上 NoExecute 的节点上，然后又指定时间后被驱逐，再次调度，进入一个死循环。这看起来不合理，但其实是合理的，因为此时的NoExecute是我们人为打上的，而通常情况下，NoExecute都不是我们手动打的，而是在节点挂掉或者节点上的kubelet挂掉时，k8s自动添加的 123# 在node02上停掉kubelet$ kubectl describe node k8s-node-02 |grep -i taintTaints: node.kubernetes.io/unreachable:NoExecute 此时，你为你的pod设置了tolerationSeconds就很有意义了，默认300s，你可以缩短该时间，让其在一定时间内快速驱逐掉，重新拉起的pod不会调度到挂掉的节点，因为挂掉的节点不会响应你，此时不会进入死循环了 1、节点挂掉或者kubelet挂掉后被自动添加污点 node.kubernetes.io/unreachable:NoExecute，effect为NoExecute驱逐 当节点被打上 node.kubernetes.io/unreachable:NoExecute 污点时，Kubernetes 开始驱逐该节点上 Pod 的默认时间是 300秒（5分钟）。 达到时间后该节点上的 Pod 被驱逐（Eviction）到其他健康的节点，除非这些 Pod 显式地容忍（Tolerate）了这个污点。 2、k8s会自动为Pod添加下面几种Toleration key为 node.kubernets.io/not-ready，并配置tolerationSeconds&#x3D;300 key为 node.kubernets.io/unreachable，并配置tolerationSeconds&#x3D;300 以上添加的这种自动机制保证了在某些节点发生 一些临时性问题时，Pod默认能够继续留在当前节点运行5min等待节点恢复，而不是立即被驱逐，从而避免系统的异常波动 上述的默认的tolerationSeconds时间设置都是在apiserver启动时默认设置的，更多apiserver的启动参数见官网 1234567$ vim /etc/kubernetes/manifests/kube-apiserver.yamlspec: containers: - command: - kube-apiserver - --default-not-ready-toleration-seconds=30 - --default-unreachable-toleration-seconds=30 综合练习：Deployment+污点容忍+pod反亲和，实现每个节点各分配一个pod 前提：我们的集群三个节点，master与两个node，其中master默认打的污点，需要设置容忍才可以调度过去 12345678910111213141516171819202122232425262728293031apiVersion: apps/v1kind: Deploymentmetadata: name: ds-demospec: replicas: 3 # 副本数为3，正好对应三个物理节点，每个节点分一个 selector: matchLabels: app: ds-demo template: metadata: labels: app: ds-demo spec: tolerations: # 容忍master节点的污点 - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; containers: - name: nginx image: nginx:1.18 affinity: podAntiAffinity: # pod反亲和性：根据pod的标签来排斥调度 requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 - labelSelector: matchExpressions: - key: app # Pod的标签 operator: In values: - ds-demo topologyKey: kubernetes.io/hostname # 以hostname为拓扑域 3 Pod的均匀调度pod亲和与反亲和都是两种极端的方案，前者是极端的”集中”，集中在同一个拓扑域；后者是极端的”分散”，打撒到不同拓扑域，且每个拓扑域中只允许有一个，不能有多个。两种方式都无法做到让pod均匀分布在所有拓扑域中，均匀的意思是例如有9个pod、4个拓扑域，那每个拓扑域中用应该有2~3个pod 均匀分布是实现容灾和高可用的核心，将业务 Pod 尽可能均匀的分布在不同可用拓扑域中是非常重要的。所以均匀调度是k8s引入的一种判定机制，可以用标签将每个域中包含的带有此标签的pod给筛选出来，这便得到了每个域中带有指定标签的pod的分布情况，然后接下来就以此作为判定依据，来决定新pod应该创建到哪个域中，实现均匀分布，即同一标签的pod副本（代表同一服务）均匀分散到不同的拓扑域中 在Kubernetes 1.16版本中引入了Even Pod Spreading特性，用于通过 topologyKey 属性识别Zone，并通过设置新的参数topologySpreadConstraints 来将Pod均匀调度到不同的Zone。参见官方文档 3.1 实现均匀调度的步骤pod.spec.topologySpreadConstraints 字段定义如下所示 123456spec: topologySpreadConstraints: - maxSkew: &lt;integer&gt; # 代表Skew的最大值，该最大值是控制均匀调度的关键，它描述了Pod在不同拓扑域中不均匀分布的最大程度，maxSkew的值越大越不均匀，越小则越均匀，但必须大于0 topologykey: &lt;string&gt; # 指定以以何种拓扑域为单位来统计pod的分布情况，指的就是每个域中带有指定标签的pod数量 whenUnsatisfiable: &lt;string&gt; # 硬策略(DoNotschedule)/软策略(ScheduleAnyway) labelSelector: &lt;object&gt; # 用该选择器来筛选pod 实现均匀调度的步骤 创建出不同域 –&gt; 打标签 为新pod声明均匀调度相关字段 计算出新pod应该调度到哪个域中才均匀的 先基于：topologyKey拓扑域 + labelSelector标签选择器，来统计出每个域中包含的带有指定标签的pod数 然后计算出每个域的skew值：当前拓扑域中被标签选出的 Pod 个数 - min&#123;所有拓扑域中被标签选中的 Pod 个数的最小值&#125; 最后要让每个域的skew值 &lt;&#x3D; maxSkew 3.2 单条均匀调度策略123456789101112&#123;% mermaid %&#125;graph TD subgraph zoneB Node3[Node3] --&gt; Pod3[Pod] Node4[Node4] end subgraph zoneA Node1[Node1] --&gt; Pod1[Pod] Node2[Node2] --&gt; Pod2[Pod] end&#123;% endmermaid %&#125; 假设拥有一个4节点集群，其中标记为 foo:bar 的3个Pod分别位于node1、node2和node3中 &lt;pre class&#x3D;&quot;mermaid&quot;&gt;graph TD subgraph zoneB Node3[Node3] --&gt; Pod3[Pod] Node4[Node4] end subgraph zoneA Node1[Node1] --&gt; Pod1[Pod] Node2[Node2] --&gt; Pod2[Pod] end&lt;&#x2F;pre&gt; 如果希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其规约 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: mypod labels: foo: barspec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 计算步骤： 1234567891、假设该pod调度到了zoneA，根据topologyKey拓扑域 + labelSelector标签选择器，计算出 zoneA：3个pod zoneB：1个pod 2、计算出每个域的skew值 zoneA：3-1=2 zoneB：1-1=0 3、会发现zoneA的skew值大于maxSkew，不符合条件 1234567891、假设该pod调度到了zoneB，根据topologyKey拓扑域 + labelSelector标签选择器，计算出 zoneA：2个pod zoneB：2个pod 2、计算出每个域的skew值 zoneA：2-2=O zoneB：2-2=0 3、会发现zoneA和zoneB的skew值都小于maxSkew，符合条件 3.3 多条均匀调度策略上述的单条均匀调度策略，只能确保pod调度到zoneB，但是调度到node03还是node04，是随机的，不能保证。由此我们需要定义多条均匀调度策略，在此基础上添加 topologyKey: kubernetes.io/hostname，以便pod不仅均匀分布在区域上，也均匀分布在节点上 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: mypod labels: foo: barspec: topologySpreadConstraints: # 多个约束都需要满足，可以是硬策略也可以是软策略 - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar containers: - name: pause image: k8s.gcr.io/pause:3.1 上述的例子中的多个约束之间并无冲突，但实际情况多个约束之间可能存在冲突。假设有一个跨越2个区域的3节点集群 graph TD subgraph zoneB Node3[Node3] Pod4[Pod] Pod5[Pod] Node3 --> Pod4 Node3 --> Pod5 end subgraph zoneA Node1[Node1] Node2[Node2] Pod1[Pod] Pod2[Pod] Pod3[Pod] Node1 --> Pod1 Node1 --> Pod2 Node2 --> Pod3 end 如果将上述代码应用到这个集群，会发现pod处于pending状态，这是因为第一条策略会让pod只能在zoneB，第二条策略会让pod只能在node2上，pod调度无法满足两种约束，为了克服这种情况，可以增加 maxSkew 或修改其中一个约束的策略为软策略，现在来实现 1、按照zone作为key分成两个拓扑域 123kubectl label node k8s-master-01 zone=zoneAkubectl label node k8s-node-01 zone=zoneAkubectl label node k8s-node-02 zone=zoneB 2、把一些pod事先调度到指定节点，以便测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# pod_demo.yamlkind: PodapiVersion: v1metadata: name: mypod1 labels: foo: pod-demospec: nodeName: k8s-master-01 containers: - name: nginx image: nginx:1.18---kind: PodapiVersion: v1metadata: name: mypod2 labels: foo: pod-demospec: nodeName: k8s-master-01 containers: - name: nginx image: nginx:1.18 ---kind: PodapiVersion: v1metadata: name: mypod3 labels: foo: pod-demospec: nodeName: k8s-node-01 containers: - name: nginx image: nginx:1.18---kind: PodapiVersion: v1metadata: name: mypod4 labels: foo: pod-demospec: nodeName: k8s-node-02 containers: - name: nginx image: nginx:1.18 ---kind: PodapiVersion: v1metadata: name: mypod5 labels: foo: pod-demospec: nodeName: k8s-node-02 containers: - name: nginx image: nginx:1.18 3、部署新pod，并且zone维度的硬性均匀，与主机维度的软性均匀 1234567891011121314151617181920212223kind: PodapiVersion: v1metadata: name: mypod6 labels: foo: pod-demospec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: pod-demo - maxSkew: 1 topologyKey: kubernetes.io/hostname whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: foo: pod-demo containers: - name: nginx image: nginx:1.18 部署后发现pod6会被调度到zoneB中的Node3机器上 3.4 约定这里有一些值得注意的隐式约定 只有与新的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。 物理节点上的标签必须与 topologySpreadConstraints[*].topologyKey 所指定的保持一致，否则该节点将会被均匀调度算法忽略。这意味着 位于该节点上的 Pod 不影响 maxSkew 的计算。在上面的例子中，假设”Node1”没有标签”zone”，那么2个 Pod 将被忽略，因此传入的 Pod 将被调度到”zoneA”中。 总结：没有topologyKey指定的标签的节点上的pod数不参与skew的计算，也不会被选中 新 Pod 的标签必须与 topologySpreadConstraints[*],labelSelector 设置的标签保持一致，且只有被 topologySpreadConstraints[*].labelSelector 选中的pod才会参与计算，未被选中的pod对计算结果无影响 如果新 Pod 定义了 spec.nodeSelector 或 spec.affinity.nodeAffinity，他们的优先级更高，会先依据他们来选择或排除一些节点，然后在剩下的节点里进行均匀调度，如果你有多套环境 env=prod、env=staging、env=qa 时，先定位到某一个环境中，然后再进行均匀调度 示例：与NodeSelector&#x2F;NodeAffinity结合在一起使用，假设有一个跨越 zoneA到zoneC的5节点集群 graph TD subgraph zoneC Node5[Node5] end subgraph zoneB Node3[Node3] Pod3[Pod] --> Node3 Node4[Node4] end subgraph zoneA Node1[Node1] Pod1[Pod] --> Node1 Node2[Node2] Pod2[Pod] --> Node2 end 需求：zoneC必须被排除在外，然后再进行均匀调度，以便将pod放置在zoneB上，而不是zoneC上 1234567891011121314151617181920212223242526apiVersion: v1kind: Podmetadata: name: mypod labels: foo: barspec: topologySpreadConstraints: - maxSkew: 1 topologyKey: zone whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: foo: bar affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: zone operator: NotIn values: - zoneC containers: - name: pause image: k8s.gcr.io/pause:3.1 3.5 均匀调度的局限性当 Pod 被移除时，无法保证约束仍被满足。例如缩减某 Deployment 的规模时，Pod 的分布可能不再均衡，即均匀分布机制只负责创建时的均匀，不负责运行时的均匀 具有污点的节点上通过添加容忍而运行的pod，这种pod属于特殊情况的pod，按理说不应该参与计算，但实际情况是也会被匹配到并参与均匀调度计算 3.6 完整示例同一个deployment控制器下管理多个pod副本，要求让每个节点上运行它的2个pod副本 前提：我们的集群有3个节点，一个master、两个node，master上有污点、想调度的话需要加容忍 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: name: my-topo-demo-podspec: replicas: 6 # 副本数为6，正好对应三个物理节点，每个节点分2个 selector: matchLabels: app: topo-demo template: metadata: labels: app: topo-demo spec: tolerations: # 容忍master节点的污点 - key: &quot;node-role.kubernetes.io/control-plane&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; containers: - image: nginx:1.18 name: nginx topologySpreadConstraints: - maxSkew: 1 topologyKey: kubernetes.io/hostname # 使用该拓扑域，即每个节点就独立构成一个拓扑域 whenUnsatisfiable: DoNotSchedule labelSelector: matchLabels: app: topo-demo 4 Deschedulerk8s集群是非常动态的，例如为了停机维护某一个物理节点，先执行驱逐&#x2F;排空操作，使得该节点上的pod都被驱逐到了其他物理节点上。但是当我们停机维护完成后，之前的pod并不会自动回到该节上，因为Pod一旦被绑定了节点是不会自动触发重新调度的，这就会导致集群在一段时间内出现不均衡的状态。 为了解决这个问题，可能会想到手动去删掉之前驱逐走的那些pod，从而触发其重新调度，这确实可以让当中的某些pod重新被调度回来，但这种做法并不是解决问题的根本，真正的解决这个问题的办法，是引入一个均衡器Descheduler来重新平衡集群，用来解决k8s运行过程中节点pod分布的不均匀问题。 Descheduler的核心原理：Descheduler本身并不会参与调度，它只是根据其策略配置计算出应该驱逐的pod将其进行驱逐，驱逐后重建新pod的调度还是会交给默认的调度器kube-scheduler来完成 4.1 部署Descheduler通过 Helm Chart 我们可以配置 descheduler 以 cronJob 或者 Deployment 方式运行，默认情况下descheduler会以一个 critical pod 运行，以避免被自己或者 kubelet 驱逐了，需要确保集群中有 system-cluster-critical 这个Priorityclass 123$ kubectl get priorityclass system-cluster-criticalNAME VALUE GLOBAL-DEFAULT AGEsystem-cluster-critical 2000000000 false 7d17h 使用 Helm Chart 安装默认情况下会以 CronJob 的形式运行，执行周期为 schedule:&quot;*/2 * * * *&quot;，这样每隔两分钟会执行一次 descheduler任务 12$ helm repo add descheduler https://kubernetes-sigs.github.io/descheduler/$ helm upgrade --install descheduler descheduler/descheduler --set podSecurityPolicy.create=false -n kube-system 4.2 Descheduler配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@k8s-master-01 ~/selector]# kubectl -n kube-system get cm descheduler -o yamlapiVersion: v1kind: ConfigMapmetadata: ....data: policy.yaml: | apiVersion: &quot;descheduler/v1alpha2&quot; kind: &quot;DeschedulerPolicy&quot; profiles: - name: default pluginConfig: - args: evictLocalStoragePods: true ignorePvcPods: true name: DefaultEvictor - name: RemoveDuplicates - args: includingInitContainers: true podRestartThreshold: 100 name: RemovePodsHavingTooManyRestarts - args: nodeAffinityType: - requiredDuringSchedulingIgnoredDuringExecution name: RemovePodsViolatingNodeAffinity - name: RemovePodsViolatingNodeTaints - name: RemovePodsViolatingInterPodAntiAffinity - name: RemovePodsViolatingTopologySpreadConstraint - args: targetThresholds: cpu: 50 memory: 50 pods: 50 thresholds: cpu: 20 memory: 20 pods: 20 name: LowNodeUtilization plugins: # 启用的相关插件/策略 balance: enabled: - RemoveDuplicates # 删除调度到同一节点上的相同 Pod（通常指具有相同 Pod Template Hash 的 Pod），以避免不必要的冗余和资源浪费。 - RemovePodsViolatingTopologySpreadConstraint # 移除违反拓扑分布约束（Topology Spread Constraints）的 Pod，以确保 Pod 在集群中的分布更加均匀。 - LowNodeUtilization # 将工作负载从资源利用率低的节点迁移到其他节点，以释放资源或关闭低利用率节点。 deschedule: enabled: - RemovePodsHavingTooManyRestarts # 移除因频繁重启而可能存在问题的 Pod。 - RemovePodsViolatingNodeTaints # 移除违反节点污点（Node Taints）规则的 Pod。 - RemovePodsViolatingNodeAffinity # 移除违反节点亲和性（Node Affinity）规则的 Pod。 - RemovePodsViolatingInterPodAntiAffinity # 移除违反 Pod 间反亲和性（Inter-Pod Anti-Affinity）规则的 Pod。 4.3 Deschduler示例1、先部署pod 123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: name: descheduler-demo-podspec: replicas: 6 selector: matchLabels: app: descheduler-demo template: metadata: labels: app: descheduler-demo spec: tolerations: - key: &quot;node-role.kubernetes.io/control-plane&quot; # 容忍master节点的污点,允许调度到master上 operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; - key: &quot;node.kubernetes.io/unreachable&quot; # 节点挂掉后，k8s默认为集群打上该污点 operator: &quot;Exists&quot; effect: &quot;NoExecute&quot; tolerationSeconds: 10 # 设置容忍多久后驱逐 containers: - image: nginx:1.18 name: nginx 2、模拟某个节点故障，驱逐其上的pod 12# 去node02停掉kubeletsystemctl stop kubelet 3、过一会后发现node02上的pod被驱逐，状态如下 123456789[root@k8s-master-01 /test5]# kubectl get pods -o wide -w |grep deschdescheduler-demo-pod-6b84dcfcb-5c2mw 1/1 Running 0 113s 10.244.0.191 k8s-master-01 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-9l8pm 1/1 Running 0 42s 10.244.0.193 k8s-master-01 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-dxxvq 1/1 Running 0 113s 10.244.1.51 k8s-node-01 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-f2zmw 1/1 Running 0 42s 10.244.1.53 k8s-node-01 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-jhqfq 1/1 Terminating 0 113s 10.244.2.254 k8s-node-02 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-lw5zg 1/1 Running 0 113s 10.244.0.192 k8s-master-01 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-vsxcs 1/1 Running 0 113s 10.244.1.52 k8s-node-01 &lt;none&gt; &lt;none&gt;descheduler-demo-pod-6b84dcfcb-zq7k5 1/1 Terminating 0 113s 10.244.2.2 k8s-node-02 &lt;none&gt; &lt;none&gt; 4、模拟node02恢复 1systemctl start kubelet 4.4 添加PDB策略防止单点引入descheduler之后，它会根据策略对pod进行驱逐以便进行重新调度，使k8s的资源达到一个平衡状态。但是会有问题 如果一个服务只有一个副本，你为了资源平衡，把它驱逐走了，虽然会在新节点上拉起来，但过程中会导致业务中断 如果有多个副本，也打撒在多个节点上，但是这只是你的副本均匀，并不代表节点的资源分配是均匀的，当descheduler判定节点资源不均衡时，就会触发pod驱逐，也就是说你的pod副本是有可能被全都驱逐的，同样会导致业务中断 针对上述pod副本被全部驱逐导致业务中断的问题，我们可以通过配置PDB（PodDisruptionBudget） 对象来避免所有副本同时被删除，比如我们可以设置在驱逐的时候，设置某应用最多只有一个副本不可用，创建如下所示的资源清单即可，用标签选中你要保护的pod 1234567891011apiVersion: policy/v1kind: PodDisruptionBudgetmetadata: name: pdb-demo namespace: defaultspec: # minAvailable: 1 # 设置最少处于健康状态的副本数量，可以使用整数或百分比 maxUnavailable: 1 # 设置最多不可用的副本数量，可以使用整数或百分比 selector: matchLabels: # 匹配Pod标签------------》针对被选中的pod，最大不可用副本为1 app: demo 关于 PDB 的更多详细信息可以查看官方文档 4.5 注意事项当使用 descheduler 驱除 Pods 的时候，需要注意以下几点： 关键性 Pod 不会被驱逐，比如 priorityClassName 设置为 system-cluster-critical 或 system-node-critical 的 Pod 12[root@k8s-master-01 ~]# kubectl -n kube-system get cronjobs.batch descheduler -o yaml |grep -i priority priorityClassName: system-cluster-critical 不属于 RS、Deployment 或 Job 管理的 Pods 不会被驱逐 DaemonSet 创建的 Pods 不会被驱逐 使用 LocalStorage 的 Pod 不会被驱逐，除非设置 evictLocalStoragePods: true 具有 PVC 的 Pods 不会被驱逐，除非设置 ignorePvcPods: true 在 LowNodeUtilization 和 RemovePodsViolatingInterPodAntiAffinity 策略下，Pods 按优先级从低到高进行驱逐，如果优先级相同，Besteffort 类型的 Pod 要先于 Burstable 和 Guaranteed 类型被驱逐 annotations 中带有 descheduler.alpha.kubernetes.io/evict 字段的 Pod 都可以被驱逐，该注释用于覆盖阻止驱逐的检查，用户可以选择驱逐哪个 Pods 如果 Pods 驱逐失败，可以设置 --v=4 从 descheduler 日志中查找原因，如果驱逐违反 PDB 约束，则不会驱逐这类 Pods 5 Pod的优先级调度对于大规模集群来说，我们需要尽可能地提高集群的资源利用率，如何提高呢？cpu、内存等资源都是超配的，因为大多数情况下所有pod都是错峰使用这些资源的，但一旦遇到节点可用资源不足的情况，总不能让那些新建的、更重要的pod一直在原地pending，需要把那些没用的、优先级低的pod驱逐掉，以便腾出资源给重要的pod使用。为此k8s推出了优先级抢占调度。Pod优先级和抢占 优先级调度是优先级高的pod会抢占优先级低的pod的地盘，即终止低优先级的pod以便高优先级的pod可以调度运行的过程，当创建一个高优先级的pod，但是因为资源不足而迟迟无法被调度时，就会发生抢占，抢占的依据是pod的priorityClassName优先级类里规定的优先级值来作为判定优先级高低的用依据，而不是Qos，注意和驱逐的依据的区别，而负责按优先级抢占的组件是kube-scheduler 如果一个Pod没有指定priorityclassName，通常会被视为低优先级，抢占会考虑Pod优先级并尝试选择一组优先级最低的目标。仅当移除优先级最低的Pod不足以让调度程序调度抢占式Pod，或者最低优先级的Pod受PodDisruptionBudget（PDB策略）保护时，才会考虑优先级较高一些的Pod 注意：优先级抢占的优先程度高于其他调度规则。例如一个低优先级的PodA在NodeA（属于拓扑域：机架R），并且PodA设置了基于机架R这个拓扑域的pod反亲和规则，此时一个高优先级的PodB等待调度，目标节点是机架R这个拓扑域中的NodeB，因为之前的PodA设置的pod反亲和规则不允许调度到同一个机架上（当然也可以是PodB设置的该规则），由于优先级调度的优先程度高于其他调度规则，此时Scheduler调度器会驱逐低优先级的PodA，以满足更高优先级的PodB的调度需求 优先级抢占调度示例 1、创建优先级类 1234567apiVersion: scheduling.k8s.io/v1kind: PriorityClassmetadata: name: high-priorityvalue: 1000000#globalDefault: falseglobalDefault: true 2、随便部署几个pod把k8s的资源申请完（关键字段是requests申请的资源，这里的Qos为Guranteed） 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: StatefulSetmetadata: name: nginx-affinity-testspec: serviceName: nginx-service replicas: 6 selector: matchLabels: service: nginx template: metadata: name: nginx labels: service: nginx spec: containers: - name: test image: busybox:1.27 command: - sleep - &quot;360000000&quot; resources: requests: cpu: 1 memory: 1Gi limits: cpu: 1 memory: 1Gi 3、下述优先级高的抢占式pod会虽然Qos等级比上面的低，但是抢占行为不看Qos，而是看priority值 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: nginx labels: app: nginxspec: containers: - name: nginx image: nginx:1.18 imagePullPolicy: IfNotPresent resources: requests: cpu: 1 memory: 2Gi priorityClassName: high-priority","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Pod","slug":"Pod","permalink":"https://aquapluto.github.io/tags/Pod/"}]},{"title":"Pod的配置","slug":"Kubernetes/pod/configuration","date":"2025-09-12T06:18:37.000Z","updated":"2025-09-12T08:19:34.812Z","comments":true,"path":"Kubernetes/pod/configuration/","permalink":"https://aquapluto.github.io/Kubernetes/pod/configuration/","excerpt":"","text":"1 钩子函数Hook钩子函数是由kubelet发起的，提供了两种hook函数 1.1 postStart在容器创建后立即执行，注意由于是异步执行，它无法保证一定在容器的 ENTRYPQINT 前运行 如果一些初始化工作一定要在业务容器运行前运行，那就用 init containers来定制 如果这些初始化工作没有要求必须在业务容器运行前运行，那就用postStart来定制 主要用于资源部署、环境准备等。不过需要注意的是如果钩子花费时间过长以及于不能运行或者挂起，容器将不能达到Running状态 postStart失败，容器会被杀死，并根据RestartPolicy决定是否重启 1234567891011121314apiVersion: v1kind: Podmetadata: name: poststart namespace: defaultspec: containers: - name: poststart image: nginx:1.15-alpine imagePullPolicy: IfNotPresent lifecycle: # 生命周期事件 postStart: exec: command: [&quot;mkdir&quot;,&quot;-p&quot;,&quot;/usr/share/nginx/html/haha&quot;] 1.2 preStop当用户请求删除含有 Pod 的资源对象时(如 Deployment等)，K8S 为了让应用程序优雅关闭(即让应用程序完成正在处理的请求后，再关闭软件)，K8S 提供两种信息通知 默认：K8S 通知 node 执行容器 stop 命令，容器运行时会先向容器中 PID 为1的进程发送系统信号SIGTERM，然后等待容器中的应用程序终止执行，如果等待时间达到设定的超时时间，或者默认超时时间(30s)，会继续发送 SIGKILL 的系统信号强行 kill 掉进程 在发送终止信号之前执行PreStop钩子函数，它是阻塞的，意味着它是同步的 主要用于资源清理、优雅关闭应用程序、通知其他系统等。 当 Pod 开始终止时，Kubernetes 会执行 prestop 钩子。钩子有一个超时时间(默认为 30 秒)。如果钩子在超时时间内没有成功完成，Kubernetes 会终止Pod，而不再继续等待钩子完成。 如果钩子运行失败，也会强制删除pod 1234567891011121314apiVersion: v1kind: Podmetadata: name: prestop namespace: defaultspec: containers: - name: prestop image: nginx:1.15-alpine imagePullPolicy: IfNotPresent lifecycle: # 生命周期事件 preStop: exec: command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;sleep 60000000&quot;] # 容器终止前sleep 60000000秒 2 配置Pod2.1 指定容器启动命令command： 用于指定容器启动时运行的命令，相当于 Docker 的 ENTRYPOINT。 如果没有设置 command，则使用容器镜像中的默认入口点。如果设置将覆盖 Docker 镜像的 ENTRYPOINT command 通常是一个字符串数组，第一个元素是命令，后续元素是命令的参数（可以为空）。 args： 用于指定传递给 command 的参数，相当于 Docker 的 CMD。 如果没有设置 args，则使用容器镜像中的默认命令参数。如果设置将覆盖 Docker 镜像的 CMD args 也是一个字符串数组，每个元素是一个参数。 范例 123456789101112131415161718192021222324252627282930313233343536[root@master1 ~]#vim pod-demoapp.yamlapiVersion: v1kind: Podmetadata: name: pod-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent command: - &quot;/bin/sh&quot; - &quot;-c&quot; args: - &quot;sleep 9999&quot; restartPolicy: OnFailure#另一种写法command: [&quot;/bin/sh&quot;]args: [&quot;-c&quot;, &quot;sleep 9999&quot;] #不用argscommand: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 9999&quot;]command: - &quot;/bin/sh&quot; - &quot;-c&quot; - &quot;sleep 9999&quot;[root@master1 ~]#kubectl apply -f pod-demoapp.yaml pod/pod-demo created[root@master1 ~]#kubectl exec pod-demo -- ps auxPID USER TIME COMMAND 1 root 0:00 sleep 9999 7 root 0:00 ps aux 2.2 通过环境变量向容器传递参数环境变量是容器化应用的常用配置方式之一 12345678假如有一个nginx镜像，我们想要启动两个容器，一个做web服务器，一个做负载均衡，怎么做？1.基于基础的nginx镜像，调试好各自的配置文件，各自做各自的镜像 a.强耦合方式 b.一般来说有测试环境，预生产环境和生产环境，每个环境所需的资源不同，就要每个环境都要做一个镜像 b.不推荐使用，除非环境很简单2.基于卷的方式，在宿主机中调试好配置文件，让两个容器加载各自所需要的配置文件 a.在kubernetes中，pod如果崩掉会创建在另一个节点，那怎么加载配置文件呢？就要去多使用一个服务器做共享存储了3.通过环境变量向容器传递参数 在容器上嵌套使用env字段 每个环境变量需要通过name给出既定的名称 传递的值则定义在value字段上 查看帮助 1[root@master1 ~]#kubectl explain pods.spec.containers 注意：用 env 传递参数时，可能因为版本的不同，yaml 文件里的 value 的写法不同 12345678#正常可以这么写value: wpdb#如果报错Error from server (BadRequest): error when creating &quot;pod-mysql.yaml&quot;: Pod in version &quot;v1&quot; cannot be handled as a Pod: json: cannot unmarshal number into Go struct field EnvVar.spec.containers.env.value of type string#报错说要从数字类型更改为字符串类型，就要这么写value: &quot;wpdb&quot; 范例：通过创建service资源部署MySQL和WordPress 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[root@master1 ~]#kubectl create service clusterip mysql --tcp=3306:3306 --dry-run=client -o yaml &gt;&gt; pod-mysql.yaml #最终的内容[root@master1 ~]#vim pod-mysql.yamlapiVersion: v1kind: Namespacemetadata: name: blog---apiVersion: v1kind: Podmetadata: name: mysql labels: app: db namespace: blogspec: containers: - name: mysql image: mysql:8.0 #默认从docker-hub上拉取镜像，将来被调度到哪个node节点，哪个就有mysql镜像 imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD value: &quot;123456&quot; - name: MYSQL_USER value: &quot;wpuser&quot; - name: MYSQL_PASSWORD value: &quot;wppass&quot; - name: MYSQL_DATABASE value: &quot;wordpress&quot; restartPolicy: OnFailure---apiVersion: v1kind: Service #Service资源用于抽象Pod的访问方式，这里用于访问MySQL数据库的Podmetadata: labels: app: mysql name: mysql namespace: blogspec: ports: - name: 3306-3306 port: 3306 protocol: TCP targetPort: 3306 selector: app: db #这里的选择器要和上面pod指定的label一致，不然service无法根据标签进行调度 type: ClusterIP [root@master1 ~]#kubectl apply -f pod-mysql.yaml namespace/blog createdpod/mysql createdservice/mysql created#同时查看不同的资源类型要用,隔开，不能用空格[root@master1 ~]#kubectl get pods,services -n blog -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/mysql 1/1 Running 0 10m 10.244.1.10 node1.wang.org &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/mysql ClusterIP 10.107.97.149 &lt;none&gt; 3306/TCP 10m app=db[root@master1 ~]#kubectl describe services mysql -n blogName: mysqlNamespace: blogLabels: app=mysqlAnnotations: &lt;none&gt;Selector: app=dbType: ClusterIPIP Family Policy: SingleStackIP Families: IPv4IP: 10.107.97.149IPs: 10.107.97.149Port: 3306-3306 3306/TCPTargetPort: 3306/TCPEndpoints: 10.244.1.10:3306Session Affinity: NoneEvents: &lt;none&gt; 说明：WordPress Pod 要访问 MySQL Service，当它们处于同一名称空间时，使用 WORDPRESS_DB_HOST: &quot;mysql&quot; 就能正确解析到 MySQL Service。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@master1 ~]#kubectl create service loadbalancer wordpress --tcp=80:80 --dry-run=client -o yaml &gt;&gt; wordpress.yaml [root@master1 ~]#vim wordpress.yamlapiVersion: v1kind: Podmetadata: name: wordpress labels: app: wordpress namespace: blogspec: containers: - name: wordpress image: wordpress:6 env: - name: WORDPRESS_DB_HOST value: &quot;mysql&quot; #这里要写的是pod-mysql.yaml中service里面的name，不然WordPress无法连接数据库 - name: WORDPRESS_DB_USER value: &quot;wpuser&quot; - name: WORDPRESS_DB_PASSWORD value: &quot;wppass&quot; - name: WORDPRESS_DB_NAME value: &quot;wordpress&quot;---apiVersion: v1kind: Servicemetadata: labels: app: wordpress name: wordpress namespace: blog #这里要指定名称空间，如果不指定，即不和mysql在同一个名称空间，那么WORDPRESS_DB_HOST的value要写成mysql.blog，并且在Pod上面要创建namespacespec: ports: - name: 80-80 port: 80 protocol: TCP targetPort: 80 selector: app: wordpress type: LoadBalancer [root@master1 ~]#kubectl apply -f wordpress.yaml pod/wordpress createdservice/wordpress created#因为本环境安装不了OpenELB，所以EXTERNAL-IP没有，我们要使用 任一节点ip:31079 进行访问[root@master1 ~]#kubectl get pods,services wordpress -n blog -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/wordpress 1/1 Running 0 11m 10.244.2.11 node2.wang.org &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/wordpress LoadBalancer 10.109.206.0 &lt;pending&gt; 80:31079/TCP 11m app=wordpress[root@master1 ~]#kubectl describe services wordpress -n blogName: wordpressNamespace: blogLabels: app=wordpressAnnotations: &lt;none&gt;Selector: app=wordpressType: LoadBalancerIP Family Policy: SingleStackIP Families: IPv4IP: 10.109.206.0IPs: 10.109.206.0Port: 80-80 80/TCPTargetPort: 80/TCPNodePort: 80-80 31079/TCPEndpoints: 10.244.2.11:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 2.3 镜像拉取策略由imagePullPolicy参数控制 Always : 不管本地有没有镜像，都要从仓库中下载镜像 好处：如果有人侵入将镜像换成了带有病毒的，而Always不会使用原有镜像，可以规避这种情况- 坏处：消耗大量的网络带宽 Never : 从来不从仓库下载镜像, 只用本地镜像,本地没有就算了,适用于无法使用网络的机器 IfNotPresent: 如果本地存在就直接使用, 不存在才从仓库下载, 但 Tag 为 latest 的 Image 除外，这种将自动使用Always策略 默认的策略是： 当镜像标签版本是latest，默认策略就是Always 如果指定特定版本默认拉取策略就是IfNotPresent。 2.4 Pod的重启策略决定了容器终止后是否应该重启，在spec字段的 restartPolicy 中定义 重启策略 解释 适用场景 说明 Always 无论容器如何终止（成功或失败【exit code】），都会重新启动容器。这是 Kubernetes Pod 的默认重启策略。 适用于长期运行的服务（例如 Web 服务器），需要在任何情况下保持运行。 容器只要挂了，就会立即重启，这样是很耗费资源的。所以Always重启策略是这么做的：第一次容器挂了立即重启，如果再挂了就要延时10s重启，第三次挂了就等20s重启…… 依次类推 OnFailure 只有在容器非正常退出（退出状态码不为 0，即错误退出）时，才会重新启动容器。如果容器正常结束（退出状态码为 0），不会重新启动。 适用于批处理任务（非服务）或作业（Job），希望在任务失败时重试，但任务成功完成后不再重启。 Never 容器终止后，不会重新启动，无论其退出状态码如何。 适用于希望一次性运行的任务，任务结束后不需要重启的情况。 查看重启策略 1~# kubectl get pod my-pod -o yaml | grep restartPolicy 重启策略在静态Pod和普通Pod中的应用 静态pod：不允许设置RestartPolicy 裸Pod：可以设置三种策略Always、OnFailure、Never Deployment 和 ReplicaSet：只能使用 restartPolicy: Always，确保 Pod 总是保持运行，适用于长期运行的应用。 Job 和 CronJob：只能使用 restartPolicy: OnFailure/Never，适用于一次性或定期运行的任务，确保任务失败时重试。 2.5 探针-Pod健康检查探针就是用来检测容器或容器的服务是否存活的一种机制，帮我们周期性检测服务状态，自动完成上下线或者重启 2.5.1 探针类型Pod支持的监测类型 启动探针 startup Probe：专门用于处理容器启动时间较长的场景。它仅在容器启动期间执行，一旦成功通过后，启动探针将不再执行，再运行后续的探针。这有助于避免由于应用程序初始化时间过长而导致的其他探针误判。 存活探针 liveness Probe：用于确定容器内的应用程序是否仍在运行，用于判断Pod是否需要重启。如果探测失败，kubelet将根据配置的重启策略对容器进行处理。 就绪探针 readiness Probe：用于检测容器内的程序是否健康，即是否准备好接受流量，控制Pod的网络流量流向。如果就绪探针返回成功，表明容器已准备就绪，可以接受流量。当就绪探针检测失败后，Pod的 IP:Port 会从对应的EndPoint列表中删除，即service不会再去访问该pod，防止流量被发送到该容器，直到探针再次成功为止 存活探针和就绪探针的区别 存活探针主要用于恢复容器故障；就绪探针主要用于控制流量，确保只有健康的容器才会接收到流量。 在生产环境中，推荐同时使用存活探针和就绪探针，以确保应用程序的健康、可用和可靠。 监测机制 Exec Action：根据指定命令结果的状态码判定 TcpSocket Action：根据相应TCP套接字连接建立状态判定，实际上就是检查端口 HTTPGet Action：根据指定https&#x2F;http服务URL的响应状态码判定 配置参数 initialDelaySeconds：首次监测的延迟时间，为了给予容器足够的时间来完成启动和初始化过程，避免因为过早的探测导致误报 periodSeconds：监测的间隔时间 timeoutSeconds：监测失败的超时时间，如果在指定的时间内探针没有收到响应，则认为该次探测失败 successThreshold：监测成功的阈值，即监测成功几次才能认为是成功 failureThreshold：监测失败的阈值，即监测失败几次才能认为是失败 引入 startup Probe 是专门应对容器启动时间不固定的场景，需要把 failureThreshold 尽量设置大点，检测时间长没关系；而 liveness Probe 和 readiness Probe 启动之后检测要短平快，及时检测到问题 2.5.2 探针示例范例：同时定义了三种探针 startup使用了Exec Action liveness和readiness使用HTTPGet Action 测试效果 liveness：URL “/livez” 支持以POST方法为livez参数设定不同值，非OK值都以5xx响应码响应； readiness：URL “/readyz” 支持以POST方法为readyz参数设定不同值，非OK值都以5xx响应码响应； 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: Podmetadata: name: pod-probe-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent startupProbe: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;test&#x27;,&#x27;&quot;$(curl -s 127.0.0.1/livez)&quot;==&quot;OK&quot;&#x27;] initialDelaySeconds: 0 failureThreshold: 3 periodSeconds: 2 livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 scheme: HTTP initialDelaySeconds: 3 timeoutSeconds: 2 readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 80 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 2restartPolicy: Always 范例：startup 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: startup-exec-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent startupProbe: exec: command: [&#x27;/bin/sh&#x27;, &#x27;-c&#x27;, &#x27;[ &quot;$(/usr/bin/curl -s http://127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;] initialDelaySeconds: 0 failureThreshold: 3 periodSeconds: 5 #说明[ &quot;$(/usr/bin/curl -s http://127.0.0.1/livez)&quot; == &quot;OK&quot; ]/usr/bin/curl -s http://127.0.0.1/livez #结果为OK或FALIS$(/usr/bin/curl -s http://127.0.0.1/livez) #变量值，取决于/usr/bin/curl -s http://127.0.0.1/livez的结果[ &quot;$(/usr/bin/curl -s http://127.0.0.1/livez)&quot; == &quot;OK&quot; ] #[]条件测试，$(/usr/bin/curl -s http://127.0.0.1/livez)的值如果为OK，则为0,否则非0 范例：liveness-exec 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@master1 pods]#vim liveness-exec-demo.yamlapiVersion: v1kind: Podmetadata: name: liveness-exec-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent livenessProbe: exec: #command: [&#x27;/bin/sh&#x27;, &#x27;-c&#x27;, &#x27;[ &quot;$(curl -s 127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;] command: [&#x27;/bin/sh&#x27;, &#x27;-c&#x27;, &#x27;[ &quot;$(/usr/bin/curl -s http://127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;] initialDelaySeconds: 5 timeoutSeconds: 1 periodSeconds: 5 [root@master1 pods]#kubectl apply -f liveness-exec-demo.yamlpod/liveness-exec-demo created[root@master1 pods]#kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-demo 1/1 Running 0 31m 10.244.2.19 node2.wang.org &lt;none&gt; &lt;none&gt;[root@master1 pods]#kubectl describe pods liveness-exec-demoRestart Count: 0 #重启次数Liveness: exec [/bin/sh -c [ &quot;$(/usr/bin/curl -s http://127.0.0.1/livez)&quot; == &quot;OK&quot; ]] delay=5s timeout=1s period=5s #success=1 #failure=3Events: #发生事件 Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m19s default-scheduler Successfully assigned default/liveness-exec-demo to node2.wang.org Normal Pulled 3m18s kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 3m18s kubelet Created container demo Normal Started 3m18s kubelet Started container demo Warning Unhealthy 3m8s kubelet Liveness probe failed: command &quot;/bin/sh -c [ \\&quot;$(/usr/bin/curl -s http://127.0.0.1/livez)\\&quot; == \\&quot;OK\\&quot; ]&quot; timed out #这里失败，但是没有重启，所以推断它只是首次监测失败 #模拟失败[root@master1 pods]#curl -X POST -d &#x27;livez=FALI&#x27; 10.244.2.19/livez[root@master1 pods]#curl 10.244.2.19/livezFALI[root@master1 pods]#kubectl describe pods liveness-exec-demoEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m19s default-scheduler Successfully assigned default/liveness-exec-demo to node2.wang.org Normal Pulled 3m18s kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 3m18s kubelet Created container demo Normal Started 3m18s kubelet Started container demo Warning Unhealthy 3m8s kubelet Liveness probe failed: command &quot;/bin/sh -c [ \\&quot;$(/usr/bin/curl -s http://127.0.0.1/livez)\\&quot; == \\&quot;OK\\&quot; ]&quot; timed out Warning Unhealthy 3S(x3 over 13s) kubelet Liveness probe failed: Normal Killing 3s kubelet Container demo faild liveness prode,will be resarted [root@master1 pods]#kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-demo 1/1 Running 1(8s ago) 31m 10.244.2.19 node2.wang.org &lt;none&gt; &lt;none&gt;[root@master1 pods]#curl 10.244.2.19/livezOK 范例：liveness-tcpsocket 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@master1 pods]#vim liveness-tcpsocket-demo.yamlapiVersion: v1kind: Podmetadata: name: liveness-tcpsocket-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 securityContext: #设置安全上下文 capabilities: add: - NET_ADMIN #允许在容器内执行iptables命令，一般来说iptables只能在宿主机执行，这里写主要是为了模拟失败 livenessProbe: tcpSocket: port: http #使用ports定义的name来引用，也可以写80 periodSeconds: 5 initialDelaySeconds: 5 [root@master1 pods]#kubectl apply -f liveness-tcpsocket-demo.yaml[root@master1 pods]#kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-demo 1/1 Running 0 40m 10.244.2.19 node2.wang.org &lt;none&gt; &lt;none&gt;liveness-tcpsocket-demo 1/1 Running 0 24s 10.244.1.16 node1.wang.org &lt;none&gt; &lt;none&gt;[root@master1 pods]#kubectl describe pods liveness-tcpsocket-demoLiveness: tcp-socket :http delay=5s timeout=1s period=5s #success=1 #failure=3Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m17s default-scheduler Successfully assigned default/liveness-tcpsocket-demo to node1.wang.org Normal Pulled 2m17s kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 2m17s kubelet Created container demo Normal Started 2m16s kubelet Started container demo #模拟失败[root@master1 pods]#kubectl exec -it liveness-tcpsocket-demo -- /bin/sh[root@liveness-tcpsocket-demo /]# iptables -A INPUT -p tcp --dport=80 -j REJECT[root@liveness-tcpsocket-demo /]# iptables -vnLChain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 3 180 REJECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachableChain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 3 packets, 264 bytes) pkts bytes target prot opt in out source destination [root@master1 pods]#kubectl describe pods liveness-tcpsocket-demoEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m4s default-scheduler Successfully assigned default/liveness-tcpsocket-demo to node1.wang.org Warning Unhealthy 2m13s kubelet Liveness probe failed: dial tcp 10.244.1.16:80: connect: connection refused Normal Killing 83s (x3 over 3m3s) kubelet Container demo failed liveness probe, will be restarted Normal Pulled 53s (x4 over 9m4s) kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 53s (x4 over 9m4s) kubelet Created container demo Normal Started 53s (x4 over 9m3s) kubelet Started container demo Warning Unhealthy 38s (x10 over 3m13s) kubelet Liveness probe failed: dial tcp 10.244.1.16:80: i/o timeout [root@master1 pods]#kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-demo 1/1 Running 0 47m 10.244.2.19 node2.wang.org &lt;none&gt; &lt;none&gt;liveness-tcpsocket-demo 1/1 Running 3 (50s ago) 7m22s 10.244.1.16 node1.wang.org &lt;none&gt; &lt;none&gt; 范例：liveness-httpget 1234567891011121314151617181920212223242526272829303132333435363738[root@master1 pods]#vim liveness-httpget-demo.yamlapiVersion: v1kind: Podmetadata: name: liveness-httpget-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent livenessProbe: httpGet: path: &#x27;/livez&#x27; #指定向本地哪个路径发起请求，默认为根 port: 80 scheme: HTTP initialDelaySeconds: 5 [root@master1 pods]#kubectl apply -f liveness-httpget-demo.yaml[root@master1 pods]#kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-demo 1/1 Running 0 52m 10.244.2.19 node2.wang.org &lt;none&gt; &lt;none&gt;liveness-httpget-demo 1/1 Running 0 10s 10.244.2.20 node2.wang.org &lt;none&gt; &lt;none&gt;liveness-tcpsocket-demo 1/1 Running 6 (102s ago) 12m 10.244.1.16 node1.wang.org &lt;none&gt; &lt;none&gt;[root@master1 pods]#kubectl describe pods liveness-httpget-demoLiveness: http-get http://:80/livez delay=5s timeout=1s period=10s #success=1 #failure=3Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 56s default-scheduler Successfully assigned default/liveness-httpget-demo to node2.wang.org Normal Pulled 56s kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 56s kubelet Created container demo Normal Started 56s kubelet Started container demo Warning Unhealthy 45s kubelet Liveness probe failed: Get &quot;http://10.244.2.20:80/livez&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) #模拟失败方法跟exec一样 范例：readiness 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117[root@master1 pods]#kubectl create service clusterip readiness-httpget-demo --tcp=80:80 --dry-run=client -o yaml &gt;&gt; readiness-httpget-demo.yaml[root@master1 pods]#vim readiness-httpget-demo.yamlapiVersion: v1kind: Podmetadata: name: readiness-httpget-demo namespace: default labels: name: readiness-httpget-demospec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 80 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 2 periodSeconds: 5 failureThreshold: 3 restartPolicy: Always---apiVersion: v1kind: Servicemetadata: labels: app: readiness-httpget-demo name: readiness-httpget-demospec: ports: - name: 80-80 port: 80 protocol: TCP targetPort: 80 selector: name: readiness-httpget-demo type: ClusterIP [root@master1 pods]#kubectl apply -f readiness-httpget-demo.yaml [root@master1 pods]#kubectl get pods,services -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/readiness-httpget-demo 1/1 Running 0 44s 10.244.1.17 node1.wang.org &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/readiness-httpget-demo ClusterIP 10.99.186.245 &lt;none&gt; 80/TCP 44s name=readiness-httpget-demo[root@master1 pods]#kubectl describe service readiness-httpget-demoName: readiness-httpget-demoNamespace: defaultLabels: app=readiness-httpget-demoAnnotations: &lt;none&gt;Selector: name=readiness-httpget-demoType: ClusterIPIP Family Policy: SingleStackIP Families: IPv4IP: 10.99.186.245IPs: 10.99.186.245Port: 80-80 80/TCPTargetPort: 80/TCPEndpoints: 10.244.1.17:80 # 有说明可以作为可用后端Session Affinity: NoneEvents: &lt;none&gt;[root@master1 pods]#kubectl describe pods readiness-httpget-demoReadiness: http-get http://:80/readyz delay=15s timeout=2s period=5s #success=1 #failure=3Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m47s default-scheduler Successfully assigned default/readiness-httpget-demo to node1.wang.org Normal Pulled 2m47s kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 2m47s kubelet Created container demo Normal Started 2m47s kubelet Started container demo Warning Unhealthy 2m15s (x3 over 2m25s) kubelet Readiness probe failed: Get &quot;http://10.244.1.17:80/readyz&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) #模拟失败[root@master1 pods]#curl -X POST -d &#x27;readyz=FALI&#x27; 10.244.1.17/readyz[root@master1 pods]#curl 10.244.1.17/readyzFALI[root@master1 pods]#kubectl describe pods readiness-httpget-demoReadiness: http-get http://:80/readyz delay=15s timeout=2s period=5s #success=1 #failure=3Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m47s default-scheduler Successfully assigned default/readiness-httpget-demo to node1.wang.org Normal Pulled 2m47s kubelet Container image &quot;ikubernetes/demoapp:v1.0&quot; already present on machine Normal Created 2m47s kubelet Created container demo Normal Started 2m47s kubelet Started container demo Warning Unhealthy 111s(x3 over 2mls) kubelet Readiness probe failed: Get &quot;http://10.244.1.17/readyz&quot;: context deadline exceeded (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 3s(x8 over 33s) kubelet Readiness probe failed: HTTp probe failed with statuscode: 507 [root@master1 pods]#kubectl describe service readiness-httpget-demoName: readiness-httpget-demoNamespace: defaultLabels: app=readiness-httpget-demoAnnotations: &lt;none&gt;Selector: name=readiness-httpget-demoType: ClusterIPIP Family Policy: SingleStackIP Families: IPv4IP: 10.99.186.245IPs: 10.99.186.245Port: 80-80 80/TCPTargetPort: 80/TCPEndpoints: #被移除Session Affinity: NoneEvents: &lt;none&gt;# READY表示就绪状态[root@master1 pods]#kubectl get podsNAME READY STATUS RESTARTS AGEreadiness-httpget-demo 0/1 Running 7 (2d20h ago) 2d20h 2.6 Security Context2.6.1 Security Context介绍在容器中，里面的root用户是虚拟的，并不像宿主机的root用户具有所有权限，但有时我们需要给它们赋予一些权限，但我们不可能给它们所有的权限，而容器的本质就是宿主机上运行的一个进程，为了运行安全，必须要给进程的权限加以控制，就需要用到Security Context来设置 Security Context（安全上下文）是Kubernetes 中用于定义 Pod 或容器安全设置的配置，允许用户为Pod或容器定义一系列特权与访问控制设置。Pod和容器的Security Context设置主要包括以下几个方面： 用户和组ID 可以设置容器内进程的用户(runAsUser)和组(runAsGroup)ID，以控制进程的权限 有效避免以root用户身份运行容器进程 特权设置 可以启用特权模式( privileged )，让容器内的root变成真正的管理员账号 只能在容器级别设置 访问控制 Linux Capabilities：给某个特定的进程赋予root用户的部分特权而非全部特权 AppArmor：使用程序文件来限制单个程序的权限，即在安全配置文件中指定特定程序被允许执行的操作 SELinux：为对象分配 SELinux 标签 Seccomp：使用配置文件来限制容器中进程的系统调用(system call) AllowPrivilegeEscalation(允许特权扩大) 此项配置是一个布尔值，定义了一个进程是否可以比其父进程获得更多的特权，直接效果是容器的进程上是否被设置no_new_privs 标记。当出现如下情况时，AllowPrivilegeEscalation 的值始终为 true 容器以 privileged 模式运行 容器拥有 CAP_SYS_ADMIN 的 Linux capability Kubernetes支持在Pod及容器级别分别使用安全上下文，提供了三种配置安全上下文的方法： 容器级别：仅应用到指定的容器 Pod级别：应用到 Pod 内所有容器以及 volume Pod Security Policy(PSP)：应用到集群内部所有 Pod 以及 Vo]ume（已弃用） 查看帮助 12345#pod级别[root@master1 pods]#kubectl explain pod.spec.securityContext#容器级别[root@master1 pods]#kubectl explain pod.spec.containers.securityContext 安全上下文的应用场景 自定义对象访问权限：通过设置安全上下文，你可以控制容器对其他Kubernetes对象的访问权限。例如，你可以限制容器只能访问特定的存储卷或不允许容器访问某些API端点。 控制容器的运行方式：通过设置容器的安全上下文，你可以决定容器以特权模式还是非特权模式运行。特权模式允许容器执行一些特权操作，而非特权模式则限制容器的权限以增强安全性。 SELinux和Linux Capabilities：安全上下文可以与SELinux和Linux Capabilities结合使用，为容器提供更细粒度的权限控制。SELinux为对象分配Security标签，而Linux Capabilities则允许为容器分配部分特权，而不是root用户的所有特权。 进程能力限制：通过设置容器的安全上下文，可以限制进程的系统调用能力，以减少潜在的安全风险。例如，使用Seccomp可以过滤容器的进程系统调用，从而降低潜在的安全风险。 防止特权提升：通过配置安全上下文的AllowPrivilegeEscalation选项，可以防止容器内的进程获得比其父进程更多的特权。这有助于防止潜在的特权提升攻击。 查看capabilities的帮助 1https://man7.org/linux/man-pages/man7/capabilities.7.html 常用选项 1234567891011121314151617#CAP_NET_ADMIN执行各种与网络相关的操作：接口配置；管理IP防火墙、伪装和记帐；修改路由表；绑定到任何地址以进行透明代理；设置服务类型 (TOS)；清除驱动程序统计信息；设置混杂模式；启用多播；使用setsockopt(2)设置以下套接字选项：SO_DEBUG、SO_MARK、SO_PRIORITY（对于 0 到 6 范围之外的优先级）、SO_RCVBUFFORCE和 SO_SNDBUFFORCE。#CAP_NET_BIND_SERVICE将套接字绑定到Internet域特权端口（端口号小于 1024），即普通用户具有使用特权端口的权限。CAP_NET_ADMIN里也具有这个功能#CAP_CHOWN对文件UID和GID进行任意更改 2.6.2 设置Pod级的Security Context123456789101112131415161718192021apiVersion: v1kind: Podmetadata: name: security-context-pod-demospec: securityContext: # Pod级别的安全上下文配置，作用于所有容器 runAsUser: 1000 # 所有容器内的进程以UID 1000身份运行 runAsGroup: 3000 # 所有容器内的进程以GID 3000身份运行（省略时默认GID为0，即root组） fsGroup: 2000 # Pod中所有容器挂载的卷（支持的卷类型）及卷内文件的GID为2000 volumes: - name: my-volume emptyDir: &#123;&#125; containers: - name: sec-ctx-demo image: busybox command: [&#x27;sh&#x27;, &#x27;-c&#x27;, &#x27;sleep 60m&#x27;] volumeMounts: - name: my-volume mountPath: /pod/demo # 卷挂载路径，受fsGroup影响，该路径的GID及该路径下创建的子文件的GID都为2000 securityContext: allowPrivilegeEscalation: false 2.6.3 设置Container级的Security ContextContainer级与Pod级冲突时，容器级会覆盖Pod级，并且容器级的Security Context设置不会影响Pod中的数据卷 范例：使用capabilities 1234567891011121314151617[root@master1 pods]#vim securitycontext-capabilities-demo.yamlapiVersion: v1kind: Podmetadata: name: securitycontext-capabilities-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;,&quot;-c&quot;] args: [&quot;/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80 &amp;&amp; /usr/bin/python3 /usr/local/bin/demo.py&quot;] securityContext: capabilities: #用于设置容器安全上下文的一个字段，它允许你为容器添加或删除特定的Linux功能，比如网络接口数据 add: [&#x27;NET_ADMIN&#x27;] #为容器添加了NET_ADMIN功能 #drop: [&#x27;CHOWN&#x27;] #从容器中删除CHOWN这个Linux功能 范例：指定普通用户来运行 12345678910111213141516171819202122[root@master1 pods]#vim liveness-tcpsocket-demo.yamlapiVersion: v1kind: Podmetadata: name: liveness-tcpsocket-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 securityContext: runAsUser: 1001 #指定用户 runAsGroup: 1001 #指定用户组 livenessProbe: tcpSocket: port: http #使用ports定义的name来引用，也可以写80 periodSeconds: 5 initialDelaySeconds: 5 测试的时候，发现普通用户可以监听80端口，按道理来说普通用户不可以监听1023以内的端口的，估计是新版kubernetes默认可以监听，可以进行禁用 123456789101112131415161718192021222324[root@master1 pods]#vim liveness-tcpsocket-demo.yamlapiVersion: v1kind: Podmetadata: name: liveness-tcpsocket-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 securityContext: capabilities: drop: [&#x27;NET_BIND_SERVICE&#x27;] #从容器中删除NET_BIND_SERVICE这个Linux功能 runAsUser: 1001 #指定用户 runAsGroup: 1001 #指定用户组 livenessProbe: tcpSocket: port: http #使用ports定义的name来引用，也可以写80 periodSeconds: 5 initialDelaySeconds: 5 范例：开启特权模式 场景：k8s部署ES的时候需要初始化很多linux的内核参数。但是文件系统挂载到pod容器中就会变成read-only，难以进行操作实现需求。所以需要给Pod privileged权限，然后在容器的初始化脚本或代码中去修改sysctl参数。 1234567891011apiVersion: v1kind: Podmetadata: name: demo3spec: containers: - name: demo3 image: busybox command: [&#x27;sh&#x27;, &#x27;-c&#x27;, &#x27;sleep 60m&#x27;] securityContext: # #只能在container级设置，pod级无法设置 privileged: true 容器内修改的内核参数，不会影响宿主机上的内核参数，因为容器使用的是宿主机的内核，但隔离了部分内核的功能，即使容器是在 privileged 模式下运行，某些行为还是受到限制的，特别是那些能够影响到宿主机本身的操作，所以这些修改是隔离在容器层面的，并不会应用到宿主机 123456789[root@k8s-master-01~]# kubectl exec -ti demo3 sh/ # cat/proc/sys/net/ipv4/ip_forward1/ # echo 0 &gt; /proc/sys/net/ipv4/ip_forward/ # cat/proc/sys/net/ipv4/ip_forward0[root@k8s-master-01~]# cat /proc/sys/net/ipv4/ip_forward1 3 Downward API基于Downward API机制可以把容器所在pod的一些状态信息（pod名字、pod的ip、调度的节点，pod的注释、Label、Annotation）注入到容器中（在容器体现的形式可以是环境变量、也可以某个文件中的文件），这些信息可能会被容器内的应用用到，比如用Pod名字作为日志记录的一个字段用于表示日志来源 注意： Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。 而如果你想要获取 Pod 容器运行后才会出现的信息，比如，容器进程的 PID，那就肯定不能使用 Downward API 了，而应该考虑在 Pod 里定义一个 sidecar 容器来获取了。 Downward API可以通过两种方式将Pod和容器的元数据注入容器内部 环境变量：将Pod或Container信息设置为容器内的环境变量 Volume挂载：将Pod或Container的信息以文件形式挂载到容器内部 3.1 Doward APl支持的Pod与Container信息1234567891011121314151617181.使用 fieldRef 可以声明使用:metadata.name # Pod的名字metadata.namespace # Pod的Namespacemetadata.uid # Pod的UIDmetadata.labels[&#x27;&lt;KEY&gt;&#x27;] # Pod某个Label的值，通过&lt;KEY&gt;进行引用metadata.annotations[&#x27;&lt;KEY&gt;&#x27;] # Pod某个Annotation的值，通过&lt;KEY&gt;进行引用metadata.labels # Pod的所有Labe1metadata.annotations # Pod的所有Annotationspec.nodeName # 宿主机名字spec.serviceAccountName # Pod的Service Account的名字status.hostIp # 宿主机IPstatus.podIP # Pod的IP2.使用resourceFieldRef 可以声明使用:容器的 CPU limit容器的 CPU request容器的 memory limit容器的 memory request 3.2 环境变量fieldRef 获取Pod的基本信息 123456789101112131415161718192021222324252627282930313233apiVersion: v1 kind: Pod metadata: name: env-pod spec: containers: - name: env-pod image: registry.cn-shanghai.aliyuncs.com/egon-k8s-test/busybox:v1.0 command: [&quot;/bin/sh&quot;, &quot;-c&quot;] args: - while true;do echo -en &quot;\\n&quot;; env; echo &quot;===============&quot;; sleep 300; done; env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP resourceFieldRef 获取容器的资源申请和资源限制的信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: v1 kind: Pod metadata: name: dapi-envars-resourcefieldref spec: containers: - name: test-container image: registry.cn-shanghai.aliyuncs.com/egon-k8s-test/busybox:v1.0 command: [ &quot;sh&quot;, &quot;-c&quot;] args: - while true; do echo -en &#x27;\\n&#x27;; printenv MY_CPU_REQUEST MY_CPU_LIMIT; printenv MY_MEM_REQUEST MY_MEM_LIMIT; sleep 10; done; resources: requests: memory: &quot;32Mi&quot; cpu: &quot;1&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;2&quot; env: - name: MY_CPU_REQUEST # 1 valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu - name: MY_CPU_LIMIT # 2 valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu - name: MY_MEM_REQUEST # 32Mi valueFrom: resourceFieldRef: containerName: test-container resource: requests.memory - name: MY_MEM_LIMIT # &quot;64Mi&quot; valueFrom: resourceFieldRef: containerName: test-container resource: limits.memory restartPolicy: Never 3.3 Volume挂载12345678910111213141516171819202122232425262728293031apiVersion: v1 kind: Pod metadata: name: volume-pod labels: k8s-app: test-volume node-env: test annotations: user: egon gender: male build: test spec: volumes: - name: podinfo downwardAPI: items: - path: labels fieldRef: fieldPath: metadata.labels - path: annotations fieldRef: fieldPath: metadata.annotations containers: - name: volume-pod image: busybox args: - sleep - &quot;3600&quot; volumeMounts: - name: podinfo mountPath: /etc/podinfo 4 Pod的设计模式基于容器的分布式系统中常用的3类设计模式 单容器模式：单一容器形式运行的应用，适合于那些小型且独立的应用，不需要与其他服务紧密协作 单节点模式：由强耦合的多个容器协同共生，比如Redis实例要运行两个容器，一个提供服务，另一个暴露指标给Prometheus监控 多节点模式：基于特定部署单元（Pod）实现分布式算法，比如Redis集群要实现的读写分离 单节点多容器模式 一种跨容器的设计模式，有以下四种模式 目的是在单个节点之上同时运行多个共生关系的容器，因而容器管理系统需要由将它们作为一个原子单位进行统一调度 Pod概念就是这个设计模式的实现之一，将一组容器（具有超亲密关系）组合在一起，并以原子方式进行管理。这种模式使得容器能够共享IP地址和端口空间，以及存储卷等资源 4.1 Sidecar模式：服务辅助Pod中的应用由主应用程序（通常是基于HTTP协议的应用程序）以及一个Sidecar容器组成 辅助容器用于为主容器提供辅助服务以增强主容器的功能，是主应用程序是必不可少的一部分，但却未必非得运行为应用的一部分，可以认为是用来扩展主容器的功能而不必修改其代码 这种模式主要是为了解耦主应用，让主应用专注于核心业务逻辑，与辅助功能分离。例如监控Redis，主应用容器负责业务逻辑处理，辅助容器负责暴露指标给Prometheus。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# demoapp负责自身业务处理，nginx为他做反向代理[root@master1 pods]#vim sidecar-container-demo.yamlapiVersion: v1kind: Podmetadata: name: sidecar-container-demo namespace: defaultspec: containers: - name: nginx image: nginx:1.24-alpine - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: HOST value: &quot;127.0.0.1&quot; - name: PORT value: &quot;8080&quot;[root@master1 pods]#kubectl apply -f sidecar-container-demo.yaml[root@master1 pods]#kubectl get podsNAME READY STATUS RESTARTS AGEsidecar-container-demo 2/2 Running 0 73s#-c表示指定容器[root@master1 pods]#kubectl exec -it sidecar-container-demo -c demo -- /bin/sh[root@sidecar-container-demo /]# ps auxPID USER TIME COMMAND 1 root 0:00 python3 /usr/local/bin/demo.py 8 root 0:00 /bin/sh 14 root 0:00 ps axu [root@master1 pods]#kubectl exec -it sidecar-container-demo -c nginx -- /bin/sh/ # ps auxPID USER TIME COMMAND 1 root 0:00 nginx: master process nginx -g daemon off; 30 nginx 0:00 nginx: worker process 31 nginx 0:00 nginx: worker process 32 nginx 0:00 nginx: worker process 33 nginx 0:00 nginx: worker process 40 root 0:00 /bin/sh 47 root 0:00 ps aux / # cd /etc/nginx//etc/nginx # lsconf.d fastcgi_params modules scgi_paramsfastcgi.conf mime.types nginx.conf uwsgi_params 4.2 Ambassador模式：网络代理Pod中的应用由主应用程序和一个Ambassador容器组成 辅助容器代表主容器发送网络请求至外部环境中，即作为主应用与外部服务之间的代理，因此可以将其视作主容器应用的“大使” 应用场景：云原生应用程序需要诸如断路、路由、计量和监视等功能，但更新已有的应用程序或现有代码库以添加这些功能可能很困难，甚至难以实现，进程通信代理便成了一种有效的解决方案 例如，一个微服务应用需要调用多个第三方的天气、地图等 API，大使容器可以统一处理这些 API 的调用。 1234567891011121314151617181920212223242526272829303132333435[root@master1 pods]#vim ambassador-container-demo.yamlapiVersion: v1kind: Podmetadata: name: ambassador-container-demospec: containers: - name: curl image: ikubernetes/admin-box:v1.2 command: [&quot;sleep&quot;, &quot;999999&quot;] - name: ambassador image: bitnami/kubectl:latest command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;kubectl proxy&quot;] #创建一个代理服务器，curl容器通过它来访问本地的Kubernetes API服务器 args: # 传递给 kubectl proxy 的选项，若需要改变默认监听的tcp/8001端口，可以额外附加“--port=NUM”选项； - --server=&quot;https://kubernetes.default.svc&quot; - --certificate-authority=&quot;/var/run/secrets/kubernetes.io/serviceaccount/ca.crt&quot; - --token=&quot;$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)&quot; - --accept-paths=&#x27;^.\\*&#x27; [root@master1 pods]#kubectl exec -it ambassador-container-demo -c curl -- /bin/shroot@ambassador-container-demo # curl localhost:8001/api/&#123; &quot;kind&quot;: &quot;APIVersions&quot;, &quot;versions&quot;: [ &quot;v1&quot; ], &quot;serverAddressByClientCIDRs&quot;: [ &#123; &quot;clientCIDR&quot;: &quot;0.0.0.0/0&quot;, &quot;serverAddress&quot;: &quot;10.0.0.183:6443&quot; &#125; ]&#125; 4.3 Adapter模式：接口转换（适配器）Pod中的应用由主应用程序和一个Adapter容器组成 Adapter容器为主应用程序提供一致的接口，实现了模块重用，支持标准化和规范化主容器应用程序的输出，即将主应用的输出或输入转换为其他系统能够理解的格式，以便于外部服务进行聚合 允许一个容器充当其他容器或外部工具与主应用容器之间交互的适配器。这在需要将遗留系统或外部服务与现代微服务架构集成时非常有用。适配器容器可以转换数据格式、协议或者处理认证等任务，使得主应用容器可以更容易地与其他系统进行通信。 例如，主应用输出的是 XML 格式的数据，而外部系统需要 JSON 格式，适配器容器可以完成 XML 到 JSON 的转换；主应用使用 HTTP 协议，而外部系统使用 gRPC 协议，适配器容器可以在两者之间进行协议转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@master1 pods]#vim adapter-container-demo.yamlapiVersion: v1kind: ConfigMapmetadata: name: nginx-confdata: default.conf: | server &#123; listen 80; server_name localhost; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; location /nginx_status &#123; stub_status; allow 127.0.0.1; deny all; &#125; &#125;---apiVersion: v1kind: Podmetadata: name: adapter-container-demospec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 #表明nginx容器内部使用的是80端口 volumeMounts: - mountPath: /etc/nginx/conf.d/ name: nginx-conf readOnly: true - name: adapter image: nginx/nginx-prometheus-exporter:latest #暴露nginx的指标，并转换为Prometheus兼容的格式给Prometheus监控 args: [&quot;-nginx.scrape-uri&quot;,&quot;http://localhost/nginx_status&quot;] ports: # nginx-prometheus-exporter默认监听tcp/9113端口 - name: exporter containerPort: 9113 volumes: - name: nginx-conf configMap: name: nginx-conf items: - key: default.conf path: default.conf 4.4 Init Container模式：初始化容器一个Pod中可以同时定义多个Init容器，在containers定义的业务容器启动之前先启动，串行运行以完成初始化 Init容器负责初始化包括在文件系统上设置必要的特殊权限、数据库模式设置或为主应用程序提供初始数据等，但这些初始化逻辑无法包含在应用程序的镜像文件中，或者出于安全原因，应用程序镜像没有执行初始化活动的权限等等 Init容器重启 整个pod只要不重启，init容器就会只执行一次，并且init容器并不一定会重新运行 如果是自主式pod 只更新了containers业务容器的镜像，不会触发pod的重启，那自然也不会不会重新执行init容器 更新initcontainer的镜像，也不会触发pod的重启，，那自然也不会不会重新执行init容器 如果是由控制器管控的pod，那必然会运行init容器 init容器运行失败，会依据pod的重启策略进行重启；init容器通常无需设置探针，探测通常设置在containers业务容器中 init container应用场景 在启动前检测依赖的上下游服务端口是否就绪 做启动前的初始化配置，例如数据库初始化 启动前将pod信息注册到配置中心等场景 1234567891011121314151617181920212223242526[root@master1 pods]#vim init-container-demo.yaml apiVersion: v1kind: Podmetadata: name: init-container-demo namespace: defaultspec: #在demo程序创建之前配置完成iptables规则 initContainers: - name: iptables-init image: ikubernetes/admin-box:latest imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80&#x27;] securityContext: capabilities: add: - NET_ADMIN containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 5 标签和标签选择器5.1 附加标签标签：附加在资源对象上的键值型元数据 键标识：由“键前缀（可选）”和“键名”组成，格式为“key_prefix/key_name” 键前缀必须使用DNS域名格式 键名的命名格式：支持字母、数字、连接号、下划线和点号，且只能以字母或数字开头；最长63个字符； Pod 和 Node 的 label pod 的 label 与 node 的 label 操作方式几乎相同 node 的 label 用于 pod 调度到指定 label 的 node 节点 pod 的 label 用于controller关联控制的 pod 通过YAML创建Pod时添加标签 12345678apiVersion: v1kind: Podmetadata: name: pod-stress namespace: default labels: env: dev app: nginx kubectl get pods --show-labels 查看标签 123456789[root@master1 ~]#kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELSconfigmaps-env-demo 1/1 Running 0 25h &lt;none&gt;pod-669649fb6-l6xdc 1/1 Running 0 25h app=pod,pod-template-hash=669649fb6pod-669649fb6-v2f5h 1/1 Running 0 25h app=pod,pod-template-hash=669649fb6pod-test-b896f97bf-lc7dv 1/1 Running 0 25h app=pod-test,pod-template-hash=b896f97bfpod-test-b896f97bf-nx4lf 1/1 Running 0 25h app=pod-test,pod-template-hash=b896f97bfsecrets-env-demo 1/1 Running 0 23h &lt;none&gt;secrets-volume-demo 1/1 Running 0 23h &lt;none&gt; kubectl label 命令可管理对象的标签 12345678910111213141516171819202122232425kubectl label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] [options]#赋予标签[root@master1 ~]#kubectl label pods secrets-env-demo app=mysql version=8.0[root@master1 ~]#kubectl get pods secrets-env-demo --show-labels NAME READY STATUS RESTARTS AGE LABELSsecrets-env-demo 1/1 Running 0 23h app=mysql,version=8.0#改标签，直接改会报错[root@master1 ~]#kubectl label pods secrets-env-demo version=8.1error: &#x27;version&#x27; already has a value (8.0), and --overwrite is false#--overwrite 表示覆盖[root@master1 ~]#kubectl label --overwrite pods secrets-env-demo version=8.1[root@master1 ~]#kubectl get pods secrets-env-demo --show-labels NAME READY STATUS RESTARTS AGE LABELSsecrets-env-demo 1/1 Running 0 23h app=mysql,version=8.1#删除标签[root@master1 ~]#kubectl label pods secrets-env-demo version-[root@master1 ~]#kubectl get pods secrets-env-demo --show-labels NAME READY STATUS RESTARTS AGE LABELSsecrets-env-demo 1/1 Running 0 23h app=mysql kubectl annotate 用于给Kubernetes对象添加注解。注解是一种元数据，可以用于描述对象的状态、行为或其他相关信息。通过注解，用户可以在不修改原始对象的情况下，为对象添加额外的信息，以便于管理和监控。 1234567kubectl annotate &lt;资源类型&gt; &lt;资源名称&gt; &lt;注解键&gt; [=&lt;注解值&gt;] [--overwrite][root@master1 ~]#kubectl annotate pods secrets-env-demo app=mysql[root@master1 ~]#kubectl get pods secrets-env-demo -o yamlmetadata: annotations: app: mysql 5.2 标签选择器Label Selector：用于对pod进行分类，同一类pod会拥有相同的标签，一个label是一个key&#x3D;value的键值组合，然后可以通过label selector（标签选择器）查询和筛选拥有某些label的资源对象。 标签选择器：基于标签筛选对象的过滤条件，支持两种类型 基于等值关系的选择器 操作符：&#x3D;或&#x3D;&#x3D;、!&#x3D; 基于集合关系的选择器 操作符：in、notin和exists 使用格式：KEY in (VALUE1, VALUE2, …)、 KEY notin (VALUE1, VALUE2, …)、KEY 和 !KEY 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@master1 ~]#kubectl label pods secrets-volume-demo app=mysql volume_type=nfspod/secrets-volume-demo labeled[root@master1 ~]#kubectl label pods configmaps-env-demo app=nginx volume_type=nfs pod/configmaps-env-demo labeled[root@master1 ~]#kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELSconfigmaps-env-demo 1/1 Running 0 26h app=nginx,volume_type=nfspod-669649fb6-l6xdc 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-669649fb6-v2f5h 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-test-b896f97bf-lc7dv 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bfpod-test-b896f97bf-nx4lf 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bfsecrets-env-demo 1/1 Running 0 24h app=mysqlsecrets-volume-demo 1/1 Running 0 23h app=mysql,volume_type=nfs[root@master1 ~]#kubectl get pods -l app=mysql --show-labels NAME READY STATUS RESTARTS AGE LABELSsecrets-env-demo 1/1 Running 0 24h app=mysqlsecrets-volume-demo 1/1 Running 0 23h app=mysql,volume_type=nfs[root@master1 ~]#kubectl get pods -l app!=mysql --show-labels NAME READY STATUS RESTARTS AGE LABELSconfigmaps-env-demo 1/1 Running 0 26h app=nginx,volume_type=nfspod-669649fb6-l6xdc 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-669649fb6-v2f5h 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-test-b896f97bf-lc7dv 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bfpod-test-b896f97bf-nx4lf 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bf[root@master1 ~]#kubectl get pods -l &#x27;app in (mysql,nginx)&#x27; --show-labels NAME READY STATUS RESTARTS AGE LABELSconfigmaps-env-demo 1/1 Running 0 26h app=nginx,volume_type=nfssecrets-env-demo 1/1 Running 0 24h app=mysqlsecrets-volume-demo 1/1 Running 0 23h app=mysql,volume_type=nfs[root@master1 ~]#kubectl get pods -l &#x27;app notin (mysql,nginx)&#x27; --show-labels NAME READY STATUS RESTARTS AGE LABELSpod-669649fb6-l6xdc 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-669649fb6-v2f5h 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-test-b896f97bf-lc7dv 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bfpod-test-b896f97bf-nx4lf 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bf[root@master1 ~]#kubectl get pods -l volume_type --show-labels NAME READY STATUS RESTARTS AGE LABELSconfigmaps-env-demo 1/1 Running 0 26h app=nginx,volume_type=nfssecrets-volume-demo 1/1 Running 0 23h app=mysql,volume_type=nfs[root@master1 ~]#kubectl get pods -l &#x27;!volume_type&#x27; --show-labels NAME READY STATUS RESTARTS AGE LABELSpod-669649fb6-l6xdc 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-669649fb6-v2f5h 1/1 Running 0 26h app=pod,pod-template-hash=669649fb6pod-test-b896f97bf-lc7dv 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bfpod-test-b896f97bf-nx4lf 1/1 Running 0 26h app=pod-test,pod-template-hash=b896f97bfsecrets-env-demo 1/1 Running 0 24h app=mysql 6 实现Pod的平滑关闭6.1 思路在k8s中发起pod删除指令后，pod并不会立即删除，而是先将pod 的DeleteTimestap置位，让pod先进入Terminating态 接下来containerd-shim将向容器首进程发送SIGTERM信号，然后会等待10s(默认可改terminationGracePeriodSeconds参数)后发送SIGKILL信号。中间的等待时间给用户提供了优雅退出(graceful stop)机制，当然在此期间你会看到pod一直处于Terminating状态，我们可以为应用捕获SIGTERM注册handler，利用 terminationGracePeriodSeconds 的时间内执行完清理资源操作。 这里有两个问题需要注意: 全程只看到给1号进程发送信号，但实际上现象是容器退出后相关进程会全部消失 查阅资料后，了解到由于PID=1进程的特殊性，1号进程退出后，由其而生的PID-namespace被销毁，内核将向该namespace下所有子进程发送SIGKILL信号。 注意这里 子进程们是直接被kill的，不存在优雅结束的机会，所以这就用到了我们上一小节演示的tini进程来转发term信号给容器内的其他进程。 进程被kill后，如何被回收 dockerDaemon发起创建容器请求，由containerd接收并创建containerd-shim，containerd-shim即上面提到的0号进程。所以实际的创建容器、容器内执行指令等都是此进程在做。 同时，containerd-shim具有回收僵尸进程的功能，容器1号进程退出后，内核清理其下子孙进程，这些子孙进程被containerd-shim收养并清理。 注意：如果1号进程不被Kill，那么其下进程如果有僵尸进程，是无法被处理的。所以用户开发的容器首进程要注意回收退出进程。 注意： 一个pod被删除需要等到pod内所有容器都被清理掉之后才行 (当然pod删除过程也包含preStop的执行等) 每次发版过程中，pod会在Terminating状态停留很久，可能是由于deployment滚动更新时，旧版本可删除pod会被立刻置位DeleteTimestamp，至于删除还需要等待一段平滑退出的时间，然后pod内所有容器都退出后，pod才会被删除，这可能是造成退出过慢的系统性原因。但退出慢并不影响更新速度。 docket退出的容器不会被清理，但是k8s中的kubelet会清理 6.2 正确使用tini在k8s中使用 tini 跟你直接用docker启动 tini 容器还是有区别的 你直接用docker run –init启动容器，dockerfile文件里定制的CMD可以是一条用&amp;&amp;符号链接的shell命令，此时你的容器内1号进程是tini，你的这条shell命令也就是你的业务进程直接就是挂在tini下面、是它的儿子进程 而在k8s里为POD定制COMMAND，正常来说支持单独一条命令，你无法使用&amp;&amp;或分号来链接多条命令，如果要支持这种普通的shell命令，你需要用bash -c 包裹，例如 bash -c &quot;命令1 &amp;&amp; 命令2 ;命令3&quot;，此时，当你把tini命令打到了容器里，启动时，进程的父子关系就成了这个样子 tini为1号进程 bash进程成了1号进程的直接子进程，它若退出，tini则随即结束 业务进程则是bash进程的子进程 此时，当删除pod时，容器内的1号进程tini收到的是SIGTERM信号，然后tini默认只会把该信号转发给它的儿子即bash进程，而不会发给它的孙子即业务进程 而tini的儿子bash收到SIGTERM后会立即退出，随即引起tini的退出，tini退出则容器结束，然后开始清理容器的pids namespace空间，容器所有其他进程都会收到强制杀死的SIGKILL信号。如何解决这个问题呢 在dockerfile里为容器定义变量ENV TINI_KILL_PROCESS_GROUP 1，配置后，SIGTREM信号将传递给子进程所在进程组的所有进程(即由bash而生的进程可收到信号)。 在bash -c的基础上，再添一个参数 -i，即bash -ic，-i 代表开启交互式模式，开启后bash会忽略SIGTERM信号，等到它的子进程都挂掉后，它会随即一起结束。 例如开启 init 是，启动命令为[&quot;bash&quot;, &quot;-ic&quot;, &quot;cd . &amp;&amp; sleep 10d&quot;]，此时进程视图为： 正常启动时，init作为1号进程，bash进程作为1号子进程，业务进程又作为bash进程的子进程 容器正常退出时，init收到SIGTERM信号，传递信号给其子进程(6号)所在进程组的所有进程(6和16)，bash处于交互模式忽略信 号不作为, 业务容器接受SIGTERM信号，处理后退出，bash紧随业务进程退出。 容器异常退出时，业务进程(16)异常退出，bash紧随业务进程退出。 init进程接受到子进程(6号bash)退出信号SIGCHILD，退出容器。 想在k8s中使用tini，由于k8s目前还未提供init开关参数，若想让k8s支持init，我们有三种方式实现，推荐第三种方式 方式1：配置全局使用init 可以修改docker的配置文件，开启全局使用init，如此K8s创建的所有容器都将开启init，所以该方法并不是特别推荐 1234可在 /etc/docker/daemon.json 文件中添加:&#123; &quot;init&quot;: true,&#125; 方式2：开关模式 需要修改K8s代码，最终决定使用container.Env来设置init开关，原因：annotation和label均为pod级别，而pod下支持多个容器，全局设置不够灵活。故写入环境变量，作为container级别的配置。(理想状态是将 init 作为pod.spec.containers[n].init字段交由使用者配置) 注意：如果想用label或annotation做init标记，需要注意代码修改比env多一些，因为在构造容器config时，label和annotation不会继承pod的，而env是会完整复制pod内定义的 代码修改比较简单，在pkg/kubelet/dockershim/docker_container.go文件中添加 12345678910111213init := falsefor i, _ := range config.Envs &#123; if config.Envs[i].Key == &quot;CONTAINER_S_INIT&quot; &#123; init = true &#125;&#125;createConfig := dockertypes.ContainerCreateConfig&#123; ... // 一些容器参数的设置 HostConfig: &amp;dockercontainer.HostConfig&#123; ... Init: &amp;init, &#125;,&#125; 方式3：虽然不能像docker启动容器那样，直接用 –init 参数，但是我们可以在编写dockerfile文件时，把tini命令打包到容器内 12345678910111213141516171819202122# ===============需要先下载好一个tini命令，然后编写dockerfile文件如下，制作成一个容器===============FROM centos:7 ENV TINI_VERSION v0.19.0 # 1、从网上下载tini，官网地址https://github.com/krallin/tini/releases/# ADD https://github.com/krallin/tini/releases/download/$&#123;TINI_VERSION&#125;/tini /tini # 2、基本本地下载好的tini命令ADD tini /tini RUN chmod +x /tini ADD test.py /opt ENTRYPOINT [&quot;/tini&quot;,&quot;--&quot;] # Run your program under Tini# CMD [&quot;/your/program&quot;, &quot;-and&quot;, &quot;-its&quot;, &quot;arguments&quot;] # ===============然后启动时，就不需要指定--init参数了===============docker run -d --name test1 t1:v1 设置环境变量TINI_KILL_PROCESS_GROUP，这里又有两种方式 方式1：在构建容器的镜像时，在dockerfile文件中设置变量：ENV TINI_KILL_PROCESS_GROUP 1 方式2：在pod的yaml文件里设置该ENV 在编写yaml时， 如果有COMMAND涉及到多条业务指令，可使用bash -ic包裹业务指令，例如COMMAND: [&quot;bash&quot;, &quot;-ic&quot;, &quot;cd . &amp;&amp; sleep 10d&quot;]，如此便可以使： bash负责启动业务进程，把它们作为自己的儿子 pod退出时业务进程可处理SIGTERM后很快完成容器退出 总结 k8s的kubelet不创建容器，它是调用containerd&#x2F;func来创建一个shim进程，然后由shim来创建容器的init进程 containerd在中止容器时，会向它的init发送一个term信号，这个代表平滑退出，而向init的子进程发送的是强制杀死的sigkill信号，如果子进程有一些数据正在写有网络连接正在操作比如数据库数据呢，那么强制杀死就可能导致数据错误 所以我们需要将init进程收到的term信号转发给子进程，这就用到了工具tini，用该工具取代容器内的init可以达到这一目标，我们更希望进程收到 SIGTERM 信号而不是 SIGKILL 信号 7 使用Cgroups限制Pod的PID数7.1 场景介绍Kubernetes 里面的 Pod 资源是最小的计算单元，抽象了一组（一个或多个）容器。容器也是 Linux 系统上的进程，但基于 Namespace 和 Cgroups(Control groups) 等技术实现了不同程度的隔离。 简单来说 Namespace 可以让每个进程有独立的 PID, IPC 和网络空间。Cgroups 可以控制进程的资源占用，比如 CPU ，内存和允许的最大进程数等等。 之前遇到过这样一个问题，我们的服务会调用执行外部的命令，每调用一次外部命令就会 fork 产生子进程。但是由于代码上的 bug ，没有及时对子进程回收，然后这个容器不断 fork 产生子进程，耗尽了宿主机的进程表空间，最终导致整个系统不响应，影响了其它的服务。这种问题除了让开发人员修复 bug 外，也需要在系统层面对进程数量进行限制。所以，如果一个容器里面运行的服务会 fork 产生子进程，就很有必要使用 Cgroups 的 pids 控制器限制这个容器能运行的最大进程数量。 7.2 Kubelet开启PodPidsLimit功能Kubernetes 里面的每个节点都会运行一个叫做 Kubelet 的服务，负责节点上容器的状态和生命周期，比如创建和删除容器。根据 Kubernetes 的官方文档 Process ID Limits And Reservations 内容，可以设置 Kubelet 服务的 --pod-max-pids 配置选项，之后在该节点上创建的容器，最终都会使用 Cgroups pid 控制器限制容器的进程数量。 说明：在某些 Linux 安装环境中，操作系统会将 PID 约束设置为一个较低的默认值，例如 32768。这时可以考虑提升 /proc/sys/kernel/pid_max 的设置值。 1234567891011# kubelet 使用 systemd 启动的，可以通过编辑 /etc/sysconfig/kubelet# 添加额外的启动参数，设置 pod 最大进程数为 1024$ vim /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=&quot;--pod-max-pids=1024 # 重启 kubelet 服务$ systemctl restart kubelet # 查看参数是否生效$ ps faux | grep kubelet | grep pod-max-pidsroot 104865 10.5 0.6 1731392 107368 ? Ssl 11:56 0:30 /usr/bin/kubelet ... --pod-max-pids=10 --feature-gates=SupportPodPidsLimit=true 7.3 验证PodPidsLimit现在来测试下如果容器内不断 fork 子进程，数目到达 1024 个时会触发什么行为。 参考 Fork bomb 的内容，可以创建一个 pod，不断 fork 子进程。 123456789101112131415161718192021# 创建普通的 nginx pod yaml$ cat &lt;&lt;EOF &gt; test-nginx.yamlapiVersion: v1kind: Podmetadata: name: test-nginxspec: containers: - name: nginx image: nginxEOF # 创建到 Kubernetes 集群$ kubectl apply -f test-nginx.yaml # 进入 nginx 容器模拟 fork bomb $ kubectl exec -ti test-nginx bashroot@test-nginx:/# bash -c &quot;fork() &#123; fork | fork &amp; &#125;; fork&quot;environment: fork: retry: Resource temporarily unavailableenvironment: fork: retry: Resource temporarily unavailableenvironment: fork: retry: Resource temporarily unavailable 通过进入一个 nginx 容器里面使用 bash 运行 fork bomb 命令，我们会发现当 fork 的子进程达到限制的上限数目后，会报 retry: Resource temporarily unavailable 的错误，这个时候再看下宿主机的 fork 进程数目。 123# 通过在外部宿主机执行下面的命令，会发现 fork 的进程数目接近 1024 个$ ps faux | grep fork | wc -l1019 通过以上的实验，发现能够通过设置 Kubelet 的 --pod-max-pids 选项，限制容器类的进程数，避免容器进程数不断上升最终耗尽宿主机资源，拖垮整个宿主机系统。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Pod","slug":"Pod","permalink":"https://aquapluto.github.io/tags/Pod/"}]},{"title":"Pod概论","slug":"Kubernetes/pod/conspect","date":"2025-09-12T06:18:30.000Z","updated":"2025-09-12T06:31:46.941Z","comments":true,"path":"Kubernetes/pod/conspect/","permalink":"https://aquapluto.github.io/Kubernetes/pod/conspect/","excerpt":"","text":"1 Pod概念Kubernetes本质上是“以应用为中心”的现代应用基础设施，Pod是其运行应用及应用调度的最小逻辑单元，即容器运行的环境，最小单元表示了无论Pod有多少容器，都必须运行在同一个节点上，k8s为每个pod分配了唯一的IP地址endpoint（IP+port），一个pod里的多个容器共享pod IP 本质上是共享Network、IPC和UTS名称空间以及存储资源的容器集 可将其想象成一台物理机或虚拟机，各容器就是该主机上的进程 依赖于底层基础设施pause容器事先创建出可被各应用容器共享的基础环境，各容器共享网络协议栈、网络设备、路由、IP地址、端口、IPC和UTS名称空间等，但Mount、PID和USER仍隔离，PID名称空间也可以共享，但需要用户显式定义 每个Pod上还可附加一个“存储卷（Volume）”，作为该“主机”的外部存储，独立于Pod的生命周期，可由Pod内的各容器共享 模拟“不可变基础设施”，Pod删除后可通过资源清单重建 具有动态性，可容忍误删除或主机故障等异常 存储卷可以确保数据能超越Pod的生命周期 Pod资源类型： 静态Pod：直接通过配置文件或HTTP的方式交给kubelet自动在节点级创建的pod，不走API Server，配置文件路径/etc/kubernetes/manifests/，注意它并不存放在etcd存储中，并且只在此Node上启动运行，由kubelet直接监控每个 Pod kubelet无法对其进行健康检测，但pod崩溃时kubelet会重启该静态pod 静态pod始终与某个Node节点的kubelet绑定，即始终运行在同一个节点，不会被调度到其他节点上 普通pod：创建pod的请求是提交给Api server的，一旦被创建就会被放入etcd中存储。随后被master调度到某个具体的Node上并进行绑定，随后该pod被对应的Node上的kubelet进程实例化成一组相关的docker容器并启动起来 自主式Pod：由用户直接定义并提交给API Server创建的Pod，不由Controller管控，Pod内的容器挂掉可以根据RestartPolicy自动重启，但是Pod副本本身没了（例如删除、或者Pod所在的节点挂掉），无法拉起新副本 由Workload Controller管控的Pod，控制器可以控制pod的副本数，扩容与裁剪，版本更新与回滚等，某个pod被删掉或者说所在的物理节点挂掉了，导致副本数变少，控制器会发起调谐过程，在一个新节点上拉起Pod 1.1 pause容器pause容器：基于sidecar设计模式，初始化Pod环境并为后续加入的容器提供共享网络的名称空间 共享存储：一个pod内包含两个容器，业务容器负责写数据到共享卷，另外一个容器(称之为sidecar容器)从共享的卷里读出数据来进行处理，如果读职的是日志数据可以发送到ES中存储起来，如此便完成了最基础的日志采集工作，而不需要侵入业务 共享网络：启动一个sidecar容器对外暴漏端口，然后sidecar容器将流量代理给业务容器，业务容器不对外暴漏端口，如此便实现了对流量的截流，可以进行染色处理 如下图，Pod里有两个容器container，它们共享相同的网络命名空间，可以使用 localhost 互相通信，pause是Pod的基础镜像，用于提供网络和其他功能，eth0是Pod的网络接口，用于连接到集群的网络 每个pod会被分配一个独立的IP地址，也就是每个pod都提供一个独立的endpoint（IP+port），由于pod内的IP不是固定的，所以集群外不能直接访问pod，所以多个pod将由service的负载均衡功能进行调度访问。pod的Endpoint地址会随着Pod的销毁和重新创建而发生改变，因为新的Pod地址与之前的旧的Pod不同 1.2 划分Pod每个pod中都运行着一个特殊的被称为Pause的容器，其他容器则称为业务容器&#x2F;用户容器，这些业务容器共享Pause容器的网络和Volume存储卷，因此它们之间的通信和数据交换更为高效，在设计时我们可以充分利用这特性，将一组密切相关的服务进程放入同一个Pod中，而彼此独立且有单独扩容场景的服务应该放到不同的Pod中 所以在设计上，仅应该将具有超亲密关系的应用分别以不同容器的形式运行于同一Pod内部，“超亲密”关系通常指的是两个或多个应用组件之间存在非常紧密的交互和依赖，以至于它们需要共享资源或者进行频繁的通信。 举例说明 wordpress 与 mysql 就不应该放在一个pod中，因为高并发来时，可以分别单独扩容 截胡流量、流量染色，需要用到sidecar模式，必须放在一个pod内 日志处理，必须放在一个pod内 1.3 静态Pod1234ps aux | grep kubelet ---&gt; 可以看到配置文件路径 --config=/var/lib/kubelet/config.yaml[root@k8s-master-01 ~]# cat /var/lib/kubelet/config.yaml | grep staticstaticPodPath: /etc/kubernetes/manifests 创建静态Pod的两种方式 静态Pod适用于系统级别的服务，想让哪个节点运行该服务，就在该节点的 /etc/kubernetes/manifests 创建yaml文件，kubelet就会自动创建 需要为kubelet设置启动参数 -manifest-url= 指定url地址，kubelet会周期性地去该地址下载pod的定义文件，并以 JSON&#x2F;YAML 格式的进行解析，当文件变化时对应着去终止或启动静态 pod 删除静态Pod：删除目录下或url下的文件就行 2 Pod的生命周期 2.1 创建Pod的流程静态Pod：kubelet组件定期扫描 /etc/kubernetes/manifests/ 下的 yaml 文件来在当前机器上创建Pod 普通pod： controller-manager组件、scheduler组件、kubelet组件都list-watch监听apiserver、监听自己的关注的资源状态，只要有更新，apiserver就会上报该更新给对应的组件 客户端命令kubectl提交创建pod的请求(假如该pod的控制器采用Replicaset) apiserver收到该请求，会把创建replicaset的请求写入etcd，上报事件replicaset created controller-mamanger收到replicaset create事件，进入调谐阶段，controller-mamanger发现当前该pod是0副本，而预期是要创建1个副本，所以controller-mamanger发起要创建一个pod的请求给apiserver apiserver将创建pod的事件存入etcd中，上报Pod created事件 scheduler组件收到Pod created事件后，经过预选与优选来选出一台合适的物理节点来创建新pod，然后把调度结果(要在某一台物理节点上创建新pod)发送给apiserver apiserver将调度事件存入etcd中，上报该事件 某一个物理节点上的kubelet收到该事件后进入pod创建环节，kubelet会调用容器引擎以及网络插件创建出pod kubelet会先调用容器引擎创建出一个pause容器，默认的网络模式设置为none 先为pause容器创建一个veth对，一端放在pause容器内，另外一端接到网桥上（cni0） 最后kubelet会调用容器引擎创建出业务容器，并且业务容器采用的是container网络模式，与pause容器共享网络 最后kubelet将创建后的结果返回到给apiserver，更新etcd数据库中数据状态 2.2 删除Pod的流程删除Pod的两件事 去目标节点终止进程 清理掉该资源在etcd中的数据 如果是强制删除Pod，那么只是清理掉该资源在etcd中的数据，但kubelet所在节点上管理的pod进程依然存在，迫不得已不要强制删除 流程： 用户或者控制器通过kubectl，rest api或其他客户端向api server提交pod删除请求。 pod对象不会立刻被api server删除。api server会在pod添加deleteiontimestamp和deletiongraceperiodseconds（默认值10s字段），并将pod的spec更改回写到edcd中。 api server将pod已删除的信息返回给用户，此时用户通过kubectl查看pod状态，发现pod已经被标记为terminating了。 当kubelet监听到api server处的pod对象的deleteiontimestamp被设置时，就会准备删除这个pod（killpod）。 kubelet首先会停止pod内的所有容器，调用 CRI 的stopcontainer接口向容器运行时发起停止容器的请求。这里我们同样以containerd为例子，containerd会先调用 runC 向容器发送SIGTERM信号（具体来说是向容器的1号进程发送信号），容器停止或者deleteiongraceperiodseconds超时，再发送SIGKILL信号去杀死容器内的所有进程，完成容器的停止操作。 当容器被停止后，容器运行时会向kubelet发送消息，表示容器状态发送了变化，kubelet这时会把容器被停止的信息更新到pod的status当中。 endpoint控制器监听到这个pod的状态变化，接着将pod的ip地址从相关的endpoint对象中删除。 kube-proxy检测到endopint的变化，将根据新的endpoint设置的转发规则，从而移除pod ip的转发规则。 当pod内的容器被停止后，kubelet可以通过stoppodsandbox停止podsandbox，containerd首先会调用 CNI 将容器内的网络插件删除，然后停止podsandbox，podsandbox停止后，kubelet会进行一些清理工作，例如清理pod的cgroup等。 如果pod上有finalizer，即使pod中的容器和podsandbox被全部停止，这个pod也不能消失，需要等到其他关联控制器完成相关的清理工作，并将pod上的finalizer删除。 当kubelet再次监听到pod的变化时，finalizer被清理干净了，canbedeleted方法返回true，发送deletiongraceperiodseconds为0的删除请求。 api server将这个pod从etcd中彻底删除。 2.3 总结Pod完整的生命周期 创建pod，并调度到合适节点 创建pause基础容器，提供共享名称空间 串行业务容器容器初始化—》init容器 启动业务容器，启动那一刻会同时运行主容器上定义的postStart钩子事件 startupProbe健康状态监测，判断容器是否启动成功 持续检测livenessProbe、readnessProbe 结束时，在容器结束之前会先执行preStop钩子事件，然后才终止容器 kubelet会把当前节点的pod信息上报给API Server将Pod信息存储到etcd系统中。 在etcd确认写入操作完成，API Server将确认信息发送到相关的kubelet。 3 Pod的状态3.1 Pod 的相位（phase）和容器状态Pod 的相位：关注的是 Pod 整体的情况，包括 Pod 是否被调度、容器镜像是否下载完成、Pod 中的所有容器作为一个整体的最终结果等。例如处于挂起相位，可能是 Pod 正在等待调度资源或者下载容器镜像 123# 查看 Pod 的相位kubectl get pod &lt;pod-name&gt; -o jsonpath=&#x27;&#123;.status.phase&#125;&#x27;kubectl describe pod &lt;pod-name&gt; 容器的状态：侧重于容器自身的运行状态，比如容器是否正在正常运行、是否因为某些原因而停止、是否还在启动过程中等。即使 Pod 处于运行中相位，其中的某个容器也可能因为程序错误等原因处于终止状态 123456789101112131415161718# 查看容器状态kubectl describe pod &lt;pod-name&gt;# 在输出的 Containers 部分，可以找到每个容器的状态信息Containers: my-container: Container ID: docker://abcdef123456 Image: nginx:latest Image ID: docker-pullable://nginx@sha256:abcdef123456 Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Thu, 25 May 2023 12:34:56 +0000 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-abcdef (ro) 容器的详细状态信息 State：包括 waiting、running 或 terminated 的详细信息。 Last State：容器的前一个状态，例如上次运行时的终止信息。 Ready：表示容器是否已准备好接受流量。 Restart Count：容器重启的次数，反映容器的稳定性。 Start Time：容器启动的时间戳。 3.2 Pod异常问题排查 3.3 Pod常见状态 状态 状态说明 解决办法 pending Pod 等待被调度 资源不足等原因导致，通过 kubectl describe 命令查看 Pod 事件。 Init:Error、Init:CrashLoopBackOff Init容器运行失败或反复重启 借助日志排查具体的错误原因 ImagePullBackOff、ErrImagePull Image拉取失败 检查Image URL是否正确，或手动测试拉取是否能够正常完成。 unkown 状态不知道 Pod所在的节点有问题（节点notready了：网络出问题、kubelet挂掉、节点宕机） evicted Pod被节点驱逐 某个物理节点资源不足（不可压缩资源：磁盘、内存；文件系统的inode、PID等）而驱逐 OOMkilled 针对某个Pod，该pod的容器对内存的使用达到 limits 的限定 这是k8s级别的OOM，可以增加 limits 的数值系统级别的OOM：杀进程的范围是系统中所有的进程 Terminating Pod 正在被销毁 如果一直处于Terminating状态，可能是目标节点kubelet挂掉，可使用命令强制删除该Pod（命令：kubectl delete pod [$Pod] -n [$namespace] --grace-period=0 --force）。 CrashLoopBackOff Pod启动失败，且处于反复重启过程中 一般是由于容器启动命令、参数配置错误所致。可通过查看Pod事件、容器日志和Pod配置等几方面联合排查问题。 ErrImageNeverPull 策略禁止拉取镜像 拉取镜像失败，确认 imagePullSecret 是否正确。 PostStartHookError 执行 postStart hook 报错 postStart 命令有误 NetworkPluginNotReady 网络插件还没有完全启动 CNI 插件异常，可检查 CNI 状态。 RegistryUnavailable 连接不到镜像仓库 联系仓库管理员 Init:N&#x2F;M Pod中定义了M个Init容器，其中N个已经运行完成，目前仍处于初始化过程中。 Completed Pod中的各容器已经正常运行完毕，且返回的状态码为0","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Pod","slug":"Pod","permalink":"https://aquapluto.github.io/tags/Pod/"}]},{"title":"ConfigMap和Secret","slug":"Kubernetes/data-storage/configmap-secret","date":"2025-09-12T05:45:57.000Z","updated":"2025-09-12T06:08:31.304Z","comments":true,"path":"Kubernetes/data-storage/configmap-secret/","permalink":"https://aquapluto.github.io/Kubernetes/data-storage/configmap-secret/","excerpt":"","text":"1 应用配置和密钥管理ConfigMap和Secret是Kubernetes系统上两种特殊类型的存储卷，用于将配置和机密信息与应用程序分开，并在容器中以环境变量或卷的形式提供。他们不依赖外部的存储系统，通过API Server自己所暴露出来的存储能力，抽象成一个又一个的卷，存储在etcd中 ConfigMap用于为容器中的应用提供配置数据以定制程序的行为，而敏感的配置信息，例如密钥、证书等则通常由Secret来配置 ConfigMap和Secret将相应的配置信息保存于资源对象中，而后在Pod对象上支持以存储卷的形式将其挂载并加载相关的配置，从而降低了配置与镜像文件的耦合关系，提高了镜像复用能力 Kubernetes借助于ConfigMap对象实现了将配置文件从容器镜像中解耦，从而增强了工作负载的可移植性，使其配置更易于更改和管理，并避免了将配置数据硬编码到Pod配置清单中 此二者都属于名称空间级别，只能被同一名称空间中的Pod引用 ConfigMap和Secret资源都是数据承载类的组件，是Kubernetes API的标准资源类型，是一等公民，其他方式创建的（比如静态 Pod）不能使用 主要负责提供key-value格式的数据项，其值支持 单行字符串：常用于保存环境变量值，或者命令行参数等 多行字串：常用于保存配置文件的内容 资源规范中不使用spec字段，而是直接使用特定的字段嵌套定义key-value数据 ConfigMap支持使用data或binaryData字段嵌套一至多个键值数据项 Secret支持使用data或stringData（非base64编码的明文格式）字段嵌套一至多个键值数据项 从Kubernetes v1.19版本开始，ConfigMap和Secret支持使用immutable字段创建不可变实例 2 文件符号说明竖线符| : 在yaml中代表保留换行，但是每行的缩进和行尾空白都会被去掉，而额外的缩进会被保留。 123456789lines: | 我是第一行 我是第二行 我是Egon 我是第四行 我是第五行# JSON&#123;&quot;lines&quot;: &quot;我是第一行\\n我是第二行\\n 我是Egon\\n 我是第四行\\n我是第五行\\n&quot;&#125; 竖线符搭配 + 或 - 号： + 表示保留文字块末尾的换行， - 表示删除字符串末尾的换行 1234567891011121314value: | hello world # &#123;&quot;value&quot;: &quot;hello\\nworld\\n&quot;&#125;value: |- hello world # &#123;&quot;value&quot;: &quot;hello\\nworld&quot;&#125;value: |+ hello world # &#123;&quot;value&quot;: &quot;hello\\nworld\\n\\n&quot;&#125; (有多少个回车就有多少个\\n) 大于号 &gt;： 在yaml中表示折叠换行，内容最末尾的换行会保留，但文中部分只有空白行才会被识别为换行，原来的换行符都会被转换成空格。 1234567891011lines: &gt; 我是第一行 我也是第一行 我仍是第一行 我依旧是第一行 我是第二行 这么巧我也是第二行 # JSON&#123;&quot;lines&quot;: &quot;我是第一行 我也是第一行 我仍是第一行 我依旧是第一行\\n我是第二行 这么巧我也是第二行\\n&quot;&#125; 3 ConfigMap3.1 创建ConfigMap对象创建ConfigMap对象的方法有两种 1、命令式命令 字面量 1kubectl create configmap NAME --from-literal=key1=value1 [--from-literal=key2=value2] 从文件加载 12#不写key，FILE的name当作keykubectl create configmap NAME --from-file=[key=]/PATH/TO/FILE 从目录加载： 12#key是每个FILE的namekubectl create configmap NAME --from-file=/PATH/TO/DIR/ 2、配置文件 命令式 1kubectl create -f 声明式 1kubectl apply -f 提示：基于文件内容生成时，可以使用命令式命令以dry-run模式生成并保存 范例：字面量 1234567891011121314151617[root@master1 ~]#kubectl create configmap demoapp --from-literal=host=&#x27;127.0.0.1&#x27; --from-literal=port=&#x27;8080&#x27;configmap/demoapp created#DATA表示两个键值对[root@master1 ~]#kubectl get cmNAME DATA AGEdemoapp 2 55s[root@master1 ~]#kubectl create configmap demoapp --from-literal=host=127.0.0.1 --from-literal=port=8080 --dry-run=client -o yamlapiVersion: v1data: host: 127.0.0.1 port: &quot;8080&quot;kind: ConfigMapmetadata: creationTimestamp: null name: demoapp 范例：文件加载 1234567[root@master1 nginx-conf.d]#pwd/root/learning-k8s/examples/configmaps_and_secrets/nginx-conf.d[root@master1 nginx-conf.d]#lsmyserver.conf myserver-gzip.cfg myserver-status.cfg[root@master1 nginx-conf.d]#kubectl create configmap nginx-confs --from-file=./myserver.conf --from-file=status.cfg=./myserver-status.cfg --from-file=gzip.cfg=./myserver-gzip.cfg 配置文件 123456789101112131415161718192021222324252627apiVersion: v1data: gzip.cfg: | #“|”是键名及多行键值的分割符，多行键值要进行固定缩进 gzip on; #该缩进范围内的文本块即为多行键值 gzip_comp_level 5; gzip_proxied expired no-cache no-store private auth; gzip_types text/plain text/css application/xml text/javascript; myserver.conf: | server &#123; listen 8080; server_name www.ik8s.io; include /etc/nginx/conf.d/myserver-*.cfg; location / &#123; root /usr/share/nginx/html; &#125; &#125; status.cfg: | location /nginx-status &#123; stub_status on; access_log off; &#125;kind: ConfigMapmetadata: creationTimestamp: null #可以省略不要 name: nginx-confs 范例：目录加载 1234[root@master1 nginx-conf.d]#lsmyserver.conf myserver-gzip.cfg myserver-status.cfg[root@master1 nginx-conf.d]#kubectl create configmap nginx-confs --from-file=./ --dry-run=client -o yaml 3.2 引用ConfigMap对象ConfigMap资源对象中以key-value保存的数据，在Pod中引用的方式有两种 环境变量 引用ConfigMap对象上特定的key，以 valueFrom 赋值给Pod上指定的环境变量 在Pod上使用 envFrom 一次性导入ConfigMap对象上的所有key-value，key（也可以统一附加特定前缀）即为环境变量名，value自动成为相应的变量值（不常用） 通过环境变量引用，是一次性的，假如修改了ConfigMap对象，是不会自动将变更的配置推送到pod上 configMap卷（热更新） 在Pod上将ConfigMap对象引用为存储卷，仅引用其中定义的key，而后整体由容器mount至某个目录下 key转为文件名，value即为相应的文件内容 通过存储卷的方式应用，是可变的，假如修改了ConfigMap对象，会自动将变更的配置推送到pod上 范例：在Pod上配置使用ConfigMap 1、通过环境变量引用 1234567891011121314151617181920212223242526272829303132#configmaps-env-demo.yaml ---apiVersion: v1kind: ConfigMapmetadata: name: demoapp-config namespace: defaultdata: demoapp.port: &quot;8080&quot; demoapp.host: 127.0.0.1---apiVersion: v1kind: Podmetadata: name: configmaps-env-demospec: containers: - image: ikubernetes/demoapp:v1.0 name: demoapp env: #定义环境变量 - name: PORT #定义了一个名为 &quot;PORT&quot; 的环境变量 valueFrom: #表示从其他地方获取环境变量的值 configMapKeyRef: #表示从 ConfigMap 中获取值 name: demoapp-config #指定要使用的 ConfigMap 的名称 key: demoapp.port #指定要从 ConfigMap 中获取的键 optional: false #表示如果找不到指定的键，则抛出错误。在这里设置为 &quot;false&quot;，表示必须找到该键。 - name: HOST valueFrom: configMapKeyRef: name: demoapp-config key: demoapp.host optional: true #表示如果找不到指定的键，就使用容器内的默认值 2、通过存储卷引用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576[root@master1 configmaps_and_secrets]#kubectl create configmap nginx-config-files --from-file=./nginx-conf.d/ --dry-run=client -o yaml#把configmap的配置加入进去（这里省略）[root@master1 configmaps_and_secrets]#vim configmaps-volume-demo.yamlapiVersion: v1kind: Podmetadata: name: configmaps-volume-demospec: containers: - image: nginx:alpine name: nginx-server volumeMounts: - name: ngxconfs mountPath: /etc/nginx/conf.d/ readOnly: true volumes: - name: ngxconfs configMap: #指定卷的类型为 ConfigMap name: nginx-config-files #指定使用的 ConfigMap 的名称 optional: false [root@master1 configmaps_and_secrets]#kubectl apply -f configmaps-volume-demo.yaml[root@master1 configmaps_and_secrets]#kubectl get podsNAME READY STATUS RESTARTS AGEconfigmaps-volume-demo 1/1 Running 0 3m15s[root@master1 configmaps_and_secrets]#kubectl get cmNAME DATA AGEkube-root-ca.crt 1 7d20hnginx-config-files 3 5s[root@master1 configmaps_and_secrets]#kubectl exec -it configmaps-volume-demo -- /bin/sh/ # nginx -T# configuration file /etc/nginx/conf.d/myserver.conf:server &#123; listen 8080; server_name www.ik8s.io; include /etc/nginx/conf.d/myserver-*.cfg; location / &#123; root /usr/share/nginx/html; &#125;&#125;# configuration file /etc/nginx/conf.d/myserver-gzip.cfg:gzip on;gzip_comp_level 5;gzip_proxied expired no-cache no-store private auth;gzip_types text/plain text/css application/xml text/javascript;# configuration file /etc/nginx/conf.d/myserver-status.cfg:location /nginx-status &#123; stub_status on; access_log off;&#125;#修改配置[root@master1 configmaps_and_secrets]#vim configmaps-volume-demo.yamlgzip_comp_level 6;server_name www.ikubernetes.io;[root@master1 configmaps_and_secrets]#kubectl apply -f configmaps-volume-demo.yaml[root@master1 configmaps_and_secrets]#kubectl exec -it configmaps-volume-demo -- /bin/sh#原本nginx不会自动加载配置，但是新版支持在云原生中自动加载，就不用reload了/ # nginx -T# configuration file /etc/nginx/conf.d/myserver.conf:server &#123; server_name www.ikubernetes.io;&#125;# configuration file /etc/nginx/conf.d/myserver-gzip.cfg:gzip_comp_level 6; 3.3 kube-system中的ConfigMap123456789101112131415[root@master1 ~]#kubectl get cm -n kube-system NAME DATA AGEcoredns 1 10dextension-apiserver-authentication 6 10dkube-apiserver-legacy-service-account-token-tracking 1 10dkube-proxy 2 10dkube-root-ca.crt 1 10dkubeadm-config 1 10dkubelet-config 1 10d#kubelet-config的映射文件，集群安全加固在这个文件里改[root@master1 ~]#cat /var/lib/kubelet/config.yaml#说明kube-root-ca.crt是由 kubeadm init --upload-certs 初始化后生成的，可以将这个证书文件给其他的控制平面节点使用，但是里面的哈希值是有过期时间的，默认2小时，需要额外重新生成哈希值 kube-proxy 配置 123456789101112131415161718192021222324252627282930[root@master1 ~]#kubectl get cm -n kube-system kube-proxy -o yamlmode: &quot;&quot; #表示使用什么流量分发规则，这里不写默认是iptables#修改并不会立即生效[root@master1 ~]#kubectl edit cm -n kube-system kube-proxy -o yamlmode: &quot;ipvs&quot;[root@master1 ~]#ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn #以灰度方式重启生效[root@master1 ~]#kubectl rollout restart daemonsets/kube-proxy -n kube-system[root@master1 ~]#ipvsadm -lnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.0.0.200:30768 rr -&gt; 10.244.1.6:80 Masq 1 0 0 -&gt; 10.244.2.7:80 Masq 1 0 0 TCP 10.96.0.1:443 rr -&gt; 10.0.0.200:6443 Masq 1 0 0 TCP 10.96.0.10:53 rrTCP 10.96.0.10:9153 rrTCP 10.97.65.217:80 rr -&gt; 10.244.1.6:80 Masq 1 0 0 -&gt; 10.244.2.7:80 Masq 1 0 0 UDP 10.96.0.10:53 rr 4 Secret4.1 Secret资源类型Secret主要用于存储密钥、OAuth令牌和 SSH 密钥等敏感信息，这些敏感信息采用base64编码保存，略好于明文存储，但是依然可以被解码，所以要在权限上做安全措施 Secret根据其用途等，还有类型上的区分 内置类型 使用说明 Opaque 任意用户定义的数据 kubernetes.io&#x2F;service-account-token service账户令牌 kubernetes.io&#x2F;dockercfg 序列化的~&#x2F;.dockercfg文件 kubernetes.io&#x2F;dockerconfigjson 序列化的~&#x2F;.docker&#x2F;config.json文件 kubernetes.io&#x2F;basic-auth 用于基本认证的凭据 kubernetes.io&#x2F;ssh-auth 用于SSH认证的凭据 kubernetes.io&#x2F;tls 用于TLS客户端或服务器的数据 bootstrap.kubernetes.io&#x2F;token 引导令牌数据 注意：不同类型的Secret，在定义时支持使用的标准字段也有所不同，例如ssh-auth类型的Secret应该使用ssh-privatekey，而basi-auth类型的Secret则需要使用username和password等。 提示：另外也可能存在一些特殊的用于支撑第三方需求的类型，例如ceph的keyring信息使用的kubernetes.io&#x2F;rbd等 4.2 创建Secret资源支持类似于ConfigMap的创建方式，但Secret有类型子命令，而且不同类型在data或stringData字段中支持嵌套使用的key亦会有所不同 data 字段的数据必须是 base64 编码的任意数据 stringData 字段允许 Secret 使用未编码的字符串。 12345678910[root@master1 configmaps_and_secrets]#kubectl create secret --helpCreate a secret with specified type. A docker-registry type secret is for accessing a container registry. A generic type secret indicate an Opaque secret type. A tls type secret holds TLS certificate and its associated key.Available Commands: docker-registry Create a secret for use with a Docker registry generic Create a secret from a local file, directory, or literal value tls Create a TLS secret 4.2.1 generic 最常用且通用的Secret类型，可以用来存储数据库认证信息、API密钥等一般性的敏感信息，可以通过环境变量或者存储卷的方式在Pod中使用，用于提供各种配置文件或凭据 除了后面docker-registry和tls命令之外的其它类型，都可以使用该命令中的 --type 选项进行定义，但有些类型有key的特定要求 1kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] 12345678910111213141516#命令式命令kubectl create secret generic mysql-root-authn --from-literal=username=root --from-literal=password=MagEdu.c0m#配置文件apiVersion: v1data: password: TWFnRWR1LmMwbQ== username: cm9vdA==kind: Secretmetadata: name: mysql-root-authn namespace: defaulttype: Opaque#解码echo TWFnRWR1LmMwbQ== | base64 -d 4.2.2 tls 专门用于存储TLS证书和密钥，用于加密通讯或验证远程服务的身份，主要用于加密通信或安全传输，比如在Ingress控制器中提供SSL&#x2F;TLS终止功能 通常，其保存cert文件内容的 key 为 tls.crt ，而保存private key的 key 为 tls.key 1kubectl create secret tls NAME --cert=path/to/cert/file --key=path/to/key/file 12345678910111213#命令式命令kubectl create secret tls nginx-tls --cert=./nginx.crt --key ./nginx.key#配置文件apiVersion: v1data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ……SNGQ0PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== tls.key: LS0tLS1CRUdJTiBSU0EgUFJJ……nN0d5U28rCi0tLS0tRU5EIFJTQSBQUklWQVRFIEtFWS0tLS0tCg==kind: Secretmetadata: name: nginx-ssl-secret namespace: defaulttype: kubernetes.io/tls 4.2.3 docker-registry 用于存储与Docker镜像私有仓库进行认证所需的敏感信息。通常作为imagePullSecrets在pod中被引用 通常，从已有的 json 格式的文件加载生成的就是 dockerconfigjson 类型，命令行直接生成的也是该类型 12#--docker-server 指的是仓库名kubectl create secret docker-registry NAME --docker-username=user --docker-password=password --docker-email=email [--docker-server=string] [--from-file=[key=]source] 123456789101112#命令式命令kubectl create secret docker-registry local-harbor --docker-username=admin --docker-password=123456 --docker-email=mage@magedu.com --docker-server=harbor.magedu.com#配置文件apiVersion: v1data: .dockerconfigjson: eyJhdXRocyI6eyJoYXJib3IubWFnZWR1LmNvbSI6eyJ1c2VybmFtZSI6ImFkbWluIiwicGFzc3dvcmQiOiIxMjM0NTYiLCJlbWFpbCI6Im1hZ2VAbWFnZWR1LmNvbSIsImF1dGgiOiJZV1J0YVc0Nk1USXpORFUyIn19fQ==kind: Secretmetadata: creationTimestamp: null name: local-harbortype: kubernetes.io/dockerconfigjson ImagePullSecret是用于Kubernetes集群内部，帮助Pods安全地从私有容器注册表获取容器镜像的认证工具。它的作用包括提升安全性、简化部署流程、支持跨命名空间使用等。不过并不推荐使用，结合ServiceAccount才能更好地去下载私有容器镜像 dockercfg 及 dockerconfigjson 类型的Secret主要用于从私有 Image Registry 中下载容器镜像，其引用定义在 pod.spec.imagePullSecrets 字段上 1234567891011apiVersion: v1kind: Podmetadata: name: demoapp namespace: defaultspec: containers: - name: demoapp image: ikubernetes/demoapp:v1.0 imagePullSecrets: - name: local-harbor 强调：ImagePullSecrets 与 Secrets 不同，因为 Secrets 可以挂载到 Pod 中，但是 ImagePullSecrets 只能由 Kubelet 访问。 除了设置 pod.spec.imagePullSecrets 这种方式来获取私有镜像之外，我们还可以通过在 ServiceAccount 中设置 imagePullSecrets ，然后就会自动为使用该 SA 的 Pod 注入 imagePullSecrets 信息 1234567apiVersion: v1kind: ServiceAccountmetadata: name: default namespace: defaultimagePullSecrets: - name: myregistrykey 4.3 引用Secret对象Secret资源在Pod中引用的方式同样有两种 环境变量 引用Secret对象上特定的key，以valueFrom赋值给Pod上指定的环境变量 在Pod上使用envFrom一次性导入Secret对象上的所有key-value，key（也可以统一附加特定前缀）即为环境变量名，value自动成为相应的变量值 注意：容器很可能会将环境变量打印到日志中，因而不建议以环境变量方式引用Secret中的数据 secret卷（热更新） 在Pod上将Secret对象引用为存储卷，仅引用其中定义的key，而后整体由容器mount至某个目录下 key转为文件名，value即为相应的文件内容 假如修改了Secret对象，会自动将变更的配置推送到pod上 范例：环境变量 1234567891011121314151617181920212223242526272829303132333435363738394041[root@master1 configmaps_and_secrets]#vim secret-mysql.yamlapiVersion: v1data: db.name: d3BkYg== db.pass: bWFnZWR1LmNvbTEyMzQ1Ng== db.user: d3B1c2Vy root.pass: TWFnZUVkdS5DMG0=kind: Secretmetadata: creationTimestamp: null name: mysql-secret[root@master1 configmaps_and_secrets]#kubectl apply -f secret-mysql.yaml[root@master1 configmaps_and_secrets]#vim secrets-env-demo.yamlapiVersion: v1kind: Podmetadata: name: secrets-env-demo namespace: defaultspec: containers: - name: mysql image: mysql:8.0 imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret #指定的 secret 的名 key: root.pass #指定的 secret 的key [root@master1 configmaps_and_secrets]#kubectl apply -f secrets-env-demo.yaml[root@master1 configmaps_and_secrets]#kubectl get podsNAME READY STATUS RESTARTS AGEsecrets-env-demo 1/1 Running 0 17s[root@master1 configmaps_and_secrets]#kubectl exec -it secrets-env-demo -- /bin/sh# printenvMYSQL_ROOT_PASSWORD=MageEdu.C0m 范例：secret卷 文件来源：https://github.com/AquaPluto/kubernetes/tree/main/examples/secret 说明：实现 nginx 的 https 功能，它们的调用关系是 nginx-ssl-conf.d目录下的文件都是nginx的配置内容，使用 configmap 创建出来 secret-nginx-certs.yaml 是 nginx 的证书和私钥信息，使用 secret 创建出来 最后创建 Pod 文件 secrets-volume-demo.yaml ，myserver.conf 中的路径要和其定义的挂载路径一致 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105[root@master1 nginx-ssl-conf.d]#lsmyserver.conf myserver-gzip.cfg myserver-status.cfg[root@master1 nginx-ssl-conf.d]#vim myserver.conf server &#123; listen 443 ssl; server_name www.ik8s.io; ssl_certificate /etc/nginx/certs/tls.crt; #要和secrets-volume-demo.yaml中定义的挂载目录一致，用tls.crt作为文件名，是因为secret的tls的key是tls.crt ssl_certificate_key /etc/nginx/certs/tls.key; # ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; include /etc/nginx/conf.d/myserver-*.cfg; # location / &#123; root /usr/share/nginx/html; &#125;&#125;server &#123; listen 80; server_name www.ilinux.io; return 301 https://$host$request_uri;&#125;[root@master1 configmaps_and_secrets]#kubectl create configmap nginx-sslvhosts-confs --from-file=./nginx-ssl-conf.d/[root@master1 configmaps_and_secrets]#vim secret-nginx-certs.yaml apiVersion: v1data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk... tls.key: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktLS0tLQpNSUlFb...kind: Secretmetadata: creationTimestamp: null name: nginx-ssl-secrettype: kubernetes.io/tls[root@master1 configmaps_and_secrets]#kubectl apply -f secret-nginx-certs.yaml [root@master1 configmaps_and_secrets]#vim secrets-volume-demo.yamlapiVersion: v1kind: Podmetadata: name: secrets-volume-demospec: containers: - image: nginx:alpine name: ngxserver volumeMounts: - name: nginxcerts mountPath: /etc/nginx/certs/ #通过nginx-ssl-secret会在此目录生成tls.crt和tls.key readOnly: true - name: nginxconfs mountPath: /etc/nginx/conf.d/ #通过nginx-sslvhosts-confs会在此目录生成配置文件 readOnly: true volumes: - name: nginxcerts secret: secretName: nginx-ssl-secret - name: nginxconfs configMap: name: nginx-sslvhosts-confs optional: false [root@master1 configmaps_and_secrets]#kubectl apply -f secrets-volume-demo.yaml[root@master1 configmaps_and_secrets]#kubectl exec -it secrets-volume-demo -- /bin/sh/ # netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN / # nginx -T略，跟 nginx-ssl-conf.d 目录下的文件内容一样即可/ # curl -I 127.0.0.1HTTP/1.1 301 Moved PermanentlyServer: nginx/1.21.5Date: Sat, 08 Jun 2024 10:08:51 GMTContent-Type: text/htmlContent-Length: 169Connection: keep-aliveLocation: https://127.0.0.1// # curl -I -k https://127.0.0.1HTTP/1.1 200 OKServer: nginx/1.21.5Date: Sat, 08 Jun 2024 10:09:17 GMTContent-Type: text/htmlContent-Length: 615Last-Modified: Tue, 28 Dec 2021 18:48:00 GMTConnection: keep-aliveETag: &quot;61cb5be0-267&quot;Accept-Ranges: bytes 5 标记不可变一旦Secret或ConfigMap被标记为不可更改，撤销此操作或者更改 data 字段的内容都是不允许的，只能删除并重新创建这个 Secret。现有的 Pod 将维持对已删除 Secret 的挂载点。 1234567apiVersion: v1kind: Secretmetadata: ...data: ...immutable: true # 标记为不可变 6 热更新在Kubernetes中，当你对ConfigMap或Secret进行更新或删除重建时，如果它们被挂载为 Volume 到了 Pod 内，那么这些更新是可以被反映到 Pod 文件系统上的文件内容中的，这也就是我们所说的”热更新”。 但是虽然文件内容确实会被更新，但这并不意味着运行在 Pod 中的应用会自动知道这些更改。事实上，很多的应用在启动时只会读取一次配置文件，而在运行时不会再对配置文件做任何检查。这就意味着，即使配置文件的内容被更新了，这些应用也不会感知到这种变化，通常还需要应用本身支持重新加载配置，或者你需要额外地重启应用来加载新的配置。这时可以增加一些监测配置文件变更的脚本，然后重加载对应服务就可以实现应用的热更新。 范例：增加一个sidecar，让其监控ConfigMap或Secret的变化，来重启pod 123456[root@master01 ~]# wget --no-check-certificate https://raw.githubusercontent.com/stakater/Reloader/master/deployments/kubernetes/reloader.yaml[root@master01 ~]# kubectl apply -f reloader.yaml# deployment的yaml文件中，在metadata.annotations加上reloader.stakater.com/auto: &quot;true&quot;","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"ConfigMap","slug":"ConfigMap","permalink":"https://aquapluto.github.io/tags/ConfigMap/"},{"name":"Secret","slug":"Secret","permalink":"https://aquapluto.github.io/tags/Secret/"}]},{"title":"StorageClass","slug":"Kubernetes/data-storage/storageclass","date":"2025-09-12T05:45:51.000Z","updated":"2025-09-12T06:10:38.864Z","comments":true,"path":"Kubernetes/data-storage/storageclass/","permalink":"https://aquapluto.github.io/Kubernetes/data-storage/storageclass/","excerpt":"","text":"1 StorageClass介绍当PVC没有符合的PC进行绑定时，会处于pending状态，那么整个容器就处于pending状态，所以我们要事先创建好PV，但是要创建多大的呢？怎么预判用户的需求？这是一个很繁琐的问题，所以有了StorageClass StorageClass资源是Kubernetes支持的标准资源类型之一，为管理PV资源之便而按需创建的存储资源类别（逻辑组），为动态创建PV提供“模板”，通过连接Storage的API接口，自动创建PVC所需要的PV大小，并与PVC绑定。 PVC只能在同一个分组中寻找合适的PV，即如果PVC绑定在了一个storageClass模版上，就只能在这个模版寻找PV 动态PV的创建完全依赖 CSI 驱动对接外部存储系统 有些存储类型默认并不支持动态PV机制，如下图所示，多数CSI存储都支持动态PV，且支持卷扩展和卷快照等功能 Volume Plugin Internal Provisioner AWSElasticBlockStore ✓ AzureFile ✓ AzureDisk ✓ CephFS - Cinder ✓ FC - FlexVolume - Flocker ✓ GCEPersistentDisk ✓ Glusterfs ✓ iSCSI - Quobyte ✓ NFS - RBD ✓ VsphereVolume ✓ PortworxVolume ✓ ScaleIO ✓ StorageOS ✓ Local - 2 配置默认StorageClass在yaml文件中的 metadata.annotations 添加 storageclass.kubernetes.io/is-default-class: &quot;true&quot; 若需要在创建完StorageClass后将其设置为默认，可使用类似如下命令进行 1kubectl patch storageclass storageclass_name -p &#x27;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#x27; 取消某StorageClass的默认设定，则将类似上面命令中的annotation的值修改为false即可 3 Local PV因为hostPath不支持使用StorageClass，所以在hostPath的基础上，K8s依靠PV与PVC实现了一个新特性叫 Local PV，是标准的 Kubernetes PV 资源对象，实现的功能类似于hostPath+nodeAffinity，本质也是在目录主机创建目录，所以也会有别人非隔离、混用的问题，那么强烈建议它对应的存储介质应该是一块额外挂载到宿主机上的磁盘或者块设备，可以认为一个PV就是一块盘 3.1 创建Local PV1234567891011121314151617181920212223# pv-local1.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv-local1spec: capacity: storage: 5Gi # PV提供的存储容量 volumeMode: Filesystem # 存储模式为文件系统（默认值，可选 Block 块设备） accessModes: - ReadWriteOnce # 访问模式：仅单节点可读写 persistentVolumeReclaimPolicy: Delete # 回收策略：PVC 删除后自动删除 PV storageClassName: local-storage local: path: /data/k8s/localpv # k8s-node-01节点上的目录，该目录强烈建议挂载一块单独的磁盘，事先创建好 nodeAffinity: # 节点亲和性配置（限制 PV 只能被调度到指定节点） required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname # 匹配节点的主机名标签 operator: In # 匹配方式：在指定值列表中 values: - k8s-node-01 # 仅允许调度到 k8s-node-01 节点 1234567891011121314151617181920212223# pv-local2.yamlapiVersion: v1kind: PersistentVolumemetadata: name: pv-local2spec: capacity: storage: 5Gi # PV提供的存储容量 volumeMode: Filesystem # 存储模式为文件系统（默认值，可选 Block 块设备） accessModes: - ReadWriteOnce # 访问模式：仅单节点可读写 persistentVolumeReclaimPolicy: Delete # 回收策略：PVC 删除后自动删除 PV storageClassName: local-storage local: path: /data/k8s/localpv # k8s-node-02节点上的目录，该目录强烈建议挂载一块单独的磁盘，事先创建好 nodeAffinity: # 节点亲和性配置（限制 PV 只能被调度到指定节点） required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname # 匹配节点的主机名标签 operator: In # 匹配方式：在指定值列表中 values: - k8s-node-02 # 仅允许调度到 k8s-node-02 节点 部署后查看 1234$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASSpv-local1 5Gi RWO Delete Available local-storagepv-local2 5Gi RWO Delete Available local-storage 与普通pv不同的地方 定义了一个 local 字段，表明它是一个 Local PV path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即 /data/k8s/localpv ，必须事先创建好，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 k8s-node-01 节点上 pod使用Local PV时，基于Local PV里设置的选择器，会被调度到固定的节点上 对比普通pod的创建过程，都是先调度pod到某个节点上，然后再持久化&#x2F;创建节点上的Volume目录，但对于使用 local PV 的pod来说，local PV必须事先创建好、存储都需要提前准备好，因为调度器会根据该pv的nodeAffinity把pod调度到固定的节点上，你可以把它叫做“卷亲和” 3.2 创建StorageClass我们不应该像用普通pv那样，直接创建一个pvc去跟pv绑定，因为上述我们创建了多个local pv，当我们创建出pvc时，会与某一个local pv绑定成功，那此时再创建pod时，使用上面的pvc，就限制只能用这个local pv了，而这个节点可能cpu、内存资源已经不够用了，不满足于pod的资源需求，于是会出现新pod一直原地pending的情况。所以就有了延迟绑定，即让pod的调度器综合考虑 资源+pv卷 来进行调度，调度完成后，再绑定pvc与pv，这样才合理 实现延迟绑定，我们可以通过创建 StorageClass 来指定这个动作，在 StorageClass中有一个 volumeBindingMode=WaitForFirstConsumer 的属性，当创建了一个关联该 StorageClass 的 PVC 时，即使集群中存在完全匹配的 PV（容量、访问模式等都符合），Kubernetes 也不会立刻把它们绑定在一起，直到有第一个 Pod 声明要使用这个 PVC 时，调度器会评估所有调度规则（如节点资源、亲和性、污点容忍等），同时考虑所有候选 PV 所在的节点位置，来统一决定这个 Pod 声明的 PVC 到底应该跟哪个 PV 进行绑定。总的来说就是先让 Pod 确定合适的节点，再绑定该节点上的 PV，避免 “绑定了却用不了” 的问题 1234567# local-storageclass.yamlapiVersion: storage.k8s.io/v1 kind: StorageClassmetadata: name: local-storage provisioner: kubernetes.io/no-provisioner # 存储供应器：这里表示我们是手动创建的PV，不需要动态来生成PVvolumeBindingMode: WaitForFirstConsumer # 卷绑定模式：等待第一个使用PVC的Pod调度后再绑定PV 3.3 创建PVC123456789101112# pvc-local.yamlkind: PersistentVolumeClaimapiVersion: v1metadata: name: pvc-localspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local-storage # 关联的存储类名称 3.4 创建Pod12345678910111213141516171819202122232425262728293031# pod-demo.yamlapiVersion: apps/v1kind: Deploymentmetadata: labels: app: web-test name: web-testspec: replicas: 1 selector: matchLabels: app: web-test template: metadata: labels: app: web-test spec: containers: - image: nginx:1.18 name: nginx resources: requests: cpu: 1 memory: 1.5Gi volumeMounts: - name: wwwroot mountPath: /usr/share/nginx/html volumes: - name: wwwroot persistentVolumeClaim: claimName: pvc-local 测试验证 123456[root@k8s-master-01 ~/pv_test]# kubectl get pods -o wide -wweb-test-57b8457598-rbgt9 1/1 Running 10.244.2.157 k8s-node-02[root@k8s-node-02 ~]# echo test &gt; /data/k8s/localpv/index.html[root@k8s-node-02 ~]# curl 10.244.2.157test 3.5 local-path-provisioner由 Rancher 开发的 local-path-provisioner 可以帮助我们动态创建 local pv，它对外提供一个storageclass默认名为 local-path 12345678910# 如果默认镜像地址拉不下了，可以先wget下来修改再apply[root@master01 ~]#kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml[root@master01 /monitor]# kubectl -n local-path-storage get podsNAME READY STATUS RESTARTS AGElocal-path-provisioner-66dc6d9f95-pqwp7 1/1 Running 0 103s[root@master01 /monitor]# kubectl get sc local-pathNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE local-path rancher.io/local-path Delete WaitForFirstConsumer 在PVC的yaml文件中，把 spec.storageClassName 的值改为 local-path，创建PVC后，因为延迟绑定不会立即动态创建PV，待Pod就绪后，在被调度的节点上创建PV，PVC随后绑定该PV。 如果用的是statefulset控制器，在 spec.volumeClaimTemplates.storageClassName 指向 local-path 3.6 Deployment与StatefulSet的差异使用 Local PV 时，Deployment 和 StatefulSet 在使用 StorageClass 动态创建持久卷PV时，Pod调度与存储卷亲和性（volume affinity）的行为是有差异的 Deployment 使用 storageClass 主要看PVC部分 每个Pod共享PVC。当第一批创建的Pod在多个PV所在的物理节点之间做出最优选择后，选定之后那就是绑定了， 后续的扩容都是始终都是“卷亲和”调度到固定的节点了（Pod 被“绑定”到 PV 所在的节点），不会再有变化了，除非你改变使用了其他的PVC Statefulset 使用 storageClass 主要看 volumeClaimTemplates 部分 每个Pod都会有自己的PVC，不会始终都调度到同一个节点中 4 CSI-NFS-DriverCSI-NFS-Driver是kubernetes提供的CSI插件，提供 NFS 存储的挂载 &#x2F; 卸载，配合StorageClass能够使NFS支持动态PV，它的CSI插件名称：nfs.csi.k8s.io，需要事先配置好 NFSv3 或 NFSv4 服务器，它通过在 NFS 服务器下创建新的子目录来支持通过持久卷声明动态配置持久卷。官方地址 1234#官方（DNS解析策略是靠识别主机名来识别每一个节点，需要一个DNS服务器）https://github.com/kubernetes-csi/csi-driver-nfs/blob/master/deploy/example/README.md#马哥教育（支持在本地的hosts文件做DNS解析）注意：马哥部署的CSI-NFS-Deriver不是最新的，需要最新的查看官方文档https://github.com/iKubernetes/learning-k8s/tree/master/csi-driver-nfs 以下步骤说明来自马哥教育文档 需要事先有可用的NFS服务，且能够满足CSI-NFS-Driver的使用要求 在Kubernetes集群上部署NFS CSI Driver 在Kubernetes集群上创建StorageClass资源，其 provisioner 为 nfs.csi.k8s.io，而 parameters.server 要指向准备好的NFS Server的访问入口 4.1 创建 NFS 服务器 运行如下命令，在Kubernetes集群上部署一个测试可用的NFS服务，当然生产环境中需要给该NFS提供外部的独立节点的存储； 1234567[root@master1 ~]#kubectl create namespace nfs[root@master1 ~]#wget https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/deploy/example/nfs-provisioner/nfs-server.yaml[root@master1 ~]#sed -i &#x27;s/namespace: default/namespace: nfs/g&#x27; nfs-server.yaml[root@master1 ~]#kubectl apply -f nfs-server.yaml --namespace nfs 也可以自行部署NFS Server，并按需export相应的目录；以“&#x2F;data”目录为例，其export的示例如下； 1/data *(rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash) 4.2 部署CSI-NFS-Driver支持两种部署方式：远程部署和本地部署。前者是指直接从远程仓库中获取部署配置文件完成的部署，而后者则需要首先克隆csi-driver-nfs项目的仓库至本地，并在克隆而来的本地项目目录中进行部署。 目前NFS CSI Driver项目维护有多个不同的版本，部署前需要首先确定要选择使用的版本。 需要特别说明的是，v4.0、v4.1及v4.2几个版本中，在 csi-nfs-controller 和 csi-nfs-node 相关的Pod的配置中的 dnsPolicy 都使用了“Default”，但又使用了 spec.hostNetwork 配置，这种配置中，此两者相关的Pod将无法使用ClusterDNS解析集群上的服务。因此，为了能够同此前的部署NFS Server协同，我们需要事先修改dnsPolicy的值为“ClusterFirstWithHostNet”。v4.3及其之后的版本中，csi-nfs-controller和csi-nfs-node相关的Pod的配置中的dnsPolicy已经设定使用“ClusterFirstWithHostNet”，因而无须再改。 以v4.6.0为例，相关的文件在 deploy&#x2F;04-csi-driver-nfs-4.6.0 目录下。本示例将直接基于这些文件完成NFS CSI Driver的部署。 local install（基于当前项目的部署，其默认配置已经修改dnsPolicy） 1234567891011[root@master1 04-csi-driver-nfs-4.6.0]#sed -i &#x27;s|registry.k8s.io/sig-storage|m.daocloud.io/registry.k8s.io/sig-storage|g&#x27; *.yaml#直接执行这个目录下的所有yaml文件[root@master1 deploy]#kubectl apply -f 04-csi-driver-nfs-4.6.0/[root@master1 deploy]#kubectl -n kube-system get pod -o wide -l &#x27;app in (csi-nfs-node,csi-nfs-controller)&#x27;NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScsi-nfs-controller-76697d88bc-25rw7 4/4 Running 2 (3m10s ago) 5m59s 10.0.0.184 node1.wu.org &lt;none&gt; &lt;none&gt;csi-nfs-node-4nt8c 3/3 Running 1 (4m45s ago) 5m59s 10.0.0.183 master1.wu.org &lt;none&gt; &lt;none&gt;csi-nfs-node-k4k5w 3/3 Running 1 (3m54s ago) 5m59s 10.0.0.186 node2.wu.org &lt;none&gt; &lt;none&gt;csi-nfs-node-s4q2z 3/3 Running 1 (3m30s ago) 5m59s 10.0.0.184 node1.wu.org &lt;none&gt; &lt;none&gt; 4.3 创建storageClass将 server、share 更改为您现有的 NFS 服务器地址和共享名称 1234567891011121314151617181920212223[root@master1 csi-driver-nfs]#vim nfs-csi-storageclass.yamlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-csi annotations: storageclass.kubernetes.io/is-default-class: &quot;true&quot; # 标记为默认存储类。如果未指定存储类，Kubernetes将使用此默认存储类。provisioner: nfs.csi.k8s.io # 指明存储服务类型，这里是NFS CSI驱动，与后端NFS服务器通信，自动创建PVparameters: server: nfs-server.nfs.svc.cluster.local # NFS服务器的地址 share: / # NFS服务器上的共享路径reclaimPolicy: Delete # PV的回收策略volumeBindingMode: Immediate # 当有PVC请求时，创建完PV立即与PVC进行绑定mountOptions: - hard # NFS挂载为“硬挂载”（若NFS服务器断开，Pod会等待连接恢复，避免数据丢失） - nfsvers=4.1 # 使用NFS版本4.1进行挂载 [root@master1 csi-driver-nfs]#kubectl apply -f nfs-csi-storageclass.yaml# sc是storageClass的缩写，ALLOWVOLUMEEXPANSION表示支不支持扩缩容[root@master1 csi-driver-nfs]#kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEnfs-csi (default) nfs.csi.k8s.io Delete Immediate false 6s 4.4 创建一个PVC进行测试123456789[root@master1 csi-driver-nfs]#kubectl apply -f nfs-pvc-dynamic.yaml [root@master1 csi-driver-nfs]#kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-a9a84173-7308-46ac-99e8-c140cc7f012e 10Gi RWX Delete Bound default/pvc-nfs-dynamic nfs-csi 7s[root@master1 csi-driver-nfs]#kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-nfs-dynamic Bound pvc-a9a84173-7308-46ac-99e8-c140cc7f012e 10Gi RWX nfs-csi 18s 4.5 创建一个Pod进行测试1234567891011121314151617181920[root@master1 csi-driver-nfs]#kubectl apply -f volumes-nfs-demo.yaml[root@master1 csi-driver-nfs]#kubectl exec -it volumes-nfs-csi-demo -- /bin/sh/data # redis-cli127.0.0.1:6379&gt; SET testkey &#x27;hello&#x27;OK127.0.0.1:6379&gt; GET testkey&quot;hello&quot;127.0.0.1:6379&gt; BGSAVEBackground saving started/data # lsdump.rdb/data # exit[root@master1 csi-driver-nfs]#kubectl exec -it -n nfs nfs-server-7cc5bcdcd5-gttk2 -- /bin/sh/ # ls exports/pvc-a9a84173-7308-46ac-99e8-c140cc7f012e/dump.rdb[root@node1 ~]#ls /nfs-vol/pvc-a9a84173-7308-46ac-99e8-c140cc7f012e/dump.rdb 5 NFS-Subdir-External-ProvisionerNFS-Subdir-External-Provisioner 也支持实现NFS的动态创建PV，里面集成了StorageClass的创建。早期版本依赖 Kubernetes 内置的 NFS 插件（in-tree），新版本增加支持基于 CSI-NFS-Driver 完成NFS服务器的挂载&#x2F;卸载 123456789101112131415161718192021222324[root@k8s-node-02 ~]# yum install -y nfs-utils rpcbind[root@k8s-node-02 ~]# mkdir -p /data/nfs[root@k8s-node-02 ~]# chmod 755 /data/nfs[root@k8s-node-02 ~]# cat &gt; /etc/exports &lt;&lt;EOF&gt; /data/nfs *(rw,sync,no_root_squash)&gt; EOF[root@k8s-node-02 ~]# systemctl start rpcbind[root@k8s-node-02 ~]# systemctl enable rpcbind[root@k8s-node-02 ~]# systemctl status rpcbind[root@k8s-node-02 ~]# systemctl start nfs-server[root@k8s-node-02 ~]# systemctl enable nfs-server[root@k8s-node-02 ~]# systemctl status nfs-server[root@k8s-node-02 ~]# rpcinfo -p | grep nfs 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 nfs_acl [root@k8s-node-02 ~]# exportfs -v/data/nfs &lt;world&gt;(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash) 1234567[root@k8s-master-01 ~]#wget https://get.helm.sh/helm-v3.18.5-linux-amd64.tar.gz[root@k8s-master-01 ~]# tar xf helm-v3.18.5-linux-amd64.tar.gz [root@k8s-master-01 ~]# cp linux-amd64/helm /usr/local/bin/helm[root@k8s-master-01 ~]#helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/[root@k8s-master-01 ~]#helm upgrade --install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=10.120.30.240 --set nfs.path=/data/nfs --set storageClass.defaultClass=true -n kube-system 参数说明： --set nfs.server=192.168.71.12：配置 NFS 服务器地址，provisioner 将基于此服务器创建子目录。 --set nfs.path=/data/nfs：配置 NFS 服务器上的根共享目录 --set storageClass.defaultClass=true：将provisioner关联的 StorageClass 设置为集群默认存储类 1234567891011[root@k8s-master-01 ~]#kubectl -n kube-system get pods |grep nfsnfs-subdir-external-provisioner-849d86787-v96bk 1/1 Running 0 2m19s[root@k8s-master-01 ~]# kubectl -n kube-system get sc nfs-clientNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSIONnfs-client (default) cluster.local/nfs-subdir-external-provisioner Delete Immediate false# NFS-Subdir-External-Provisioner提供了数据归档/备份功能[root@k8s-master-01 ~]# kubectl -n kube-system get sc nfs-client -o yaml | grep archiveOnDeleteparameters: archiveOnDelete: &quot;true&quot; # 用于控制当 PVC 被删除时，PV 的处理方式，设置为true代表启用数据备份或归档，即在 PVC 被删除时，存储提供者可能会将数据保存到归档存储中，以便以后可以恢复。需要相应的存储提供者支持才能生效。 创建PVC 1234567891011kind: PersistentVolumeClaimapiVersion: v1metadata: name: test-claim-xxxspec: # storageClassName: nfs-client # 不指定则使用默认的StorageClass（此处默认即为 nfs-client） accessModes: - ReadWriteMany resources: requests: storage: 10Mi","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"StorageClass","slug":"StorageClass","permalink":"https://aquapluto.github.io/tags/StorageClass/"}]},{"title":"PV和PVC","slug":"Kubernetes/data-storage/pv-pvc","date":"2025-09-12T05:45:46.000Z","updated":"2025-09-12T05:53:31.424Z","comments":true,"path":"Kubernetes/data-storage/pv-pvc/","permalink":"https://aquapluto.github.io/Kubernetes/data-storage/pv-pvc/","excerpt":"","text":"1 存储卷类型和相应的插件In-Tree存储卷插件 临时存储卷：emptyDir 节点本地存储卷：hostPath, local 网络存储卷 文件系统：NFS、GlusterFS、CephFS和Cinder（支持多路读写，因为所加持的锁是在该服务器上的，可以被其他服务器看见） 块设备：iSCSI、FC、RBD和vSphereVolume（不支持多路读写） 存储平台：Quobyte、PortworxVolume、StorageOS和ScaleIO 云存储：awsElasticBlockStore、gcePersistentDisk、azureDisk和azureFile 特殊存储卷：Secret、ConfigMap、DownwardAPI和Projected Out-of-Tree存储卷插件：经由CSI或FlexVolume接口扩展出的存储系统称为Out-of-Tree类的存储插件 2 存储卷资源规范存储卷的配置由两部分组成 通过 spec.volumes 字段定义在Pod之上的存储卷列表，它经由特定的存储卷插件并结合特定的存储供给方的访问接口进行定义 嵌套定义在容器的 volumeMounts 字段上的存储卷挂载列表，它只能挂载当前Pod对象中定义的存储卷 1234567891011121314spec: volumes: - name &lt;string&gt; # 卷名称标识，仅可使用DNS标签格式的字符，在当前Pod中必须惟一； VOL_TYPE &lt;Object&gt; # 存储卷插件及具体的目标存储供给方的相关配置； containers: - name: … image: … volumeMounts: - name &lt;string&gt; # 要挂载的存储卷的名称，必须匹配存储卷列表中某项的定义； mountPath &lt;string&gt; # 容器文件系统上的挂载点路径； readOnly &lt;boolean&gt; # 是否挂载为只读模式，默认为否； subPath &lt;string&gt; # 挂载存储卷上的一个子目录至指定的挂载点； subPathExpr &lt;string&gt; # 挂载由指定的模式匹配到的存储卷的文件或目录至挂载点； mountPropagation &lt;string&gt; # 挂载卷的传播模式； 3 PV和PVC介绍如果直接将存储卷定义在Pod中（pod.spec.volumes），那么卷对象的生命周期就无法独立于Pod而存在；而且有那么多的存储类型，每一种存储类型的配置也不一样，如果直接用存储那太费劲了。为了屏蔽底层存储的管理细节，让用户更加方便的使用，k8s提供了两个抽象单位，PV和PVC PV：集群级别的资源，持久化卷，负责将存储空间引入到集群中，通常由管理员定义，对接底层存储，可以是本地磁盘、网络存储或者云存储，屏蔽底层不同存储设备的差异 Volume Mode：当前PV卷提供的存储空间模型，分为块设备和文件系统两种 LabelSelector：在对象元数据上，还能够根据需要定义标签，PVC通过LabelSelector找到该PV StorageClassName：当前PV隶属的存储类 Size：当前PV允许使用的空间上限 PVC：名称空间级别的资源，由用户定义声明的存储卷创建申请，对接PV，屏蔽PV的差异，在空闲的PV中申请使用符合过滤条件的PV之一，与选定的PV是“一对一”的关系 允许用户按需指定期望的存储特性，并以之为条件，向集群请求特定的存储资源，按特定的条件顺序进行PV过滤，只要有一个条件不满足都不符合 VolumeMode → LabelSelector → StorageClassName → AccessMode → Size 支持动态预配的存储类storageClass，根据PVC的条件按需完成PV创建 查看资源使用帮助 12[root@master1 ~]#kubectl explain pv.spec[root@master1 ~]#kubectl explain pvc.spec 3.1 PV的访问模型AccessMode：支持的访问模型，针对节点级别的 ReadWriteOnce（RWO）：读写权限，但是只能被单个节点上的一个或多个pod挂载使用 实际验证中出现与概念不同的行为可能与底层存储的特性或配置有关，即pod即便被调度到了不同的节点，在ReadWriteOnce模式下依然可以在不同节点共用同一个pvc及卷 ReadOnlyMany（ROX）：只读权限，可以同时在多个节点上挂载并被不同的Pod使用 ReadWriteMany（RWX）：读写权限，可以同时在多个节点上挂载并被不同的Pod使用 ReadWriteOncePod：单Pod单路读写，即整个集群内只有一个Pod可以读写该PVC 只有 CSI 类型存储驱动（Out-of-Tree）支持，In-Tree插件不支持 3.2 PV的生命周期PV的生命周期中，处于四种不同的阶段 Available（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PV 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 3.3 PV的回收策略persistentVolumeReclaimPolicy：回收策略，PV是独立于 Pod 和节点存在的，但依赖存储后端可用性，如果存储后端不可用，那么PV也会不可用，至于删不删除就看其回收策略。官网关于回收策略描述 Retain：PVC删除后，PV保留，即数据保留，但是PV会处于不可用状态，这种状态下PV不能与新的PVC绑定，必须手动去将PV里的数据复制后删除，才能将PV转为可用状态，与新的PVC绑定 解绑后不能被随意重新绑定主要是防止数据被其他应用读取造成泄露，或者被写入脏数据 Delete：PVC删除后，PV也随之删除，数据丢失，但k8s出于数据安全考虑，底层存储资源存储的数据并不会删除 Recycle：PVC删除后，自动把PC上的数据清空，将PV转为可用状态，数据丢失（已弃用） 4 使用静态PV和PVC 管理员设置网络存储：集群管理员首先设置某种类型的网络存储，例如NFS导出或类似的存储系统。 创建Persistent Volume：管理员接着创建一个PV，通过向Kubernetes API发布一个PV描述符来完成。 创建Persistent Volume Claim：用户创建一个PVC，对存储进行声明。 Kubernetes绑定PVC到PV：Kubernetes查找一个足够大小且访问模式匹配的PV，并将PVC绑定到该PV上。 创建使用卷的Pod：最后用户创建一个Pod，并在Pod的配置中引用该PVC，以便Pod可以使用绑定的持久卷。 4.1 emptyDir存储卷用于存储临时数据。当Pod被分配到Node上时创建一个临时目录，与Pod生命周期相同，Pod删除时emptyDir中的数据会被永久删除，所以当两个Pod共享一个卷时，当一个Pod删除了，另一个Pod就无法使用卷了。适用于缓存空间、保存日志等场景 配置参数 medium：此目录所在的存储介质的类型，可用值为“default”或“Memory” sizeLimit：当前存储卷的空间限额，默认值为nil，表示不限制 范例：将存储卷直接定义在Pod中 1234567891011121314151617181920212223242526272829303132kind: DeploymentapiVersion: apps/v1metadata: name: emptydirspec: selector: matchLabels: app: emptydir template: metadata: labels: app: emptydir spec: containers: - name: nginx image: nginx:1.18 volumeMounts: - mountPath: /usr/share/nginx/html/ # 容器内挂载路径 name: test-emptydir # 关联的存储卷名称 - name: busybox image: busybox:latest volumeMounts: - mountPath: /data # 容器内的挂载路径 name: test-emptydir # 关联同一个存储卷（实现容器间共享） command: [ &#x27;/bin/sh&#x27;, &#x27;-c&#x27;, &#x27;while true; do echo $(date) &gt;&gt; /data/index.html; sleep 1; done&#x27; ] volumes: - name: test-emptydir emptyDir: &#123;&#125; # medium: Memory # 将emptyDir存储卷的数据存储在节点的内存中（而非磁盘），默认为磁盘 # sizeLimit: 16Mi 在上面，我们定义了2个容器，其中一个容器是输入日期到 index.html 中，然后验证访问 nginx 的 html 是否可以获取日期。以验证两个容器之间挂载的emptyDir实现共享 123456789101112[root@master01 ~]# kubectl apply -f emptydir_demo.yaml[root@master01 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESemptydir-6b6d9b8fbb-pjwpv 2/2 Running 0 54s 10.2.43.177 10.1.1.104 &lt;none&gt; &lt;none&gt;[root@node01 ~]# curl 10.2.43.177Wed Sep 1 05:15:47 UTC 2021Wed Sep 1 05:15:49 UTC 2021Wed Sep 1 05:15:50 UTC 2021Wed Sep 1 05:15:51 UTC 2021Wed Sep 1 05:15:52 UTC 2021Wed Sep 1 05:15:53 UTC 2021 empty本质就是在pod所在的节点的 /var/lib/kubelet/pods 下创建出目录 12345/var/lib/kubelet/pods/pod的metadata.uid号/volumes/kubernetes.io~empty-dir/test-emptydir# 查看pod的uid~# kubectl get pods emptydir-6b6d9b8fbb-pjwpv -o jsonpath=&#x27;&#123;.metadata.uid&#125;&#x27;dc6747b8-77b8-469e-b2cd-b708ff31e1e5 4.2 hostPath存储卷将Pod所在宿主机节点文件系统上的文件或目录挂载到Pod中。由于它直接访问主机文件系统，因此读写速度相对较快，如果用的是SSD，比远程网络存储必然是要快很多。但是也有下列问题 数据就写在当前节点，后面你的pod调度到其他节点了就用不到之前的数据了，所以Pod不能随便漂移，需要固定在某个节点上 hostPath路径所在的节点挂掉，数据就不可用了 用的就是本地目录，这个目录的数据源如果不是一块完整独立的盘，即与别人共享的话，还没等你用呢，就被别人用满了 —&gt; 非隔离、混用导致互相影响 配置参数 path：指定工作节点上的目录路径，必选字段 type：指定节点之上存储类型，kubectl explain pods.spec.volumes.hostPath 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 创建持久化卷 (PV)apiVersion: v1kind: PersistentVolumemetadata: name: my-pv labels: type: local # 自定义标签，可用于匹配PVCspec: capacity: storage: 1Gi # PV提供的存储容量 accessModes: - ReadWriteMany # 访问模式：多节点可读写 hostPath: # 使用节点本地路径作为存储 path: /data/hostpath # 节点上的实际存储路径 type: DirectoryOrCreate # 路径不存在时自动创建目录---# 创建持久化卷声明 (PVC)apiVersion: v1kind: PersistentVolumeClaimmetadata: name: my-pvc spec: accessModes: - ReadWriteMany # 与PV的访问模式匹配 resources: requests: storage: 1Gi # 请求的存储容量（需 &lt;= PV 容量）---# 创建 Deployment 应用 PVCapiVersion: apps/v1kind: Deploymentmetadata: labels: app: web-test name: web-testspec: replicas: 1 selector: matchLabels: app: web-test template: metadata: labels: app: web-test # Pod标签，需与selector匹配 spec: nodeName: k8s-node-01 # 固定调度到指定节点（因使用 hostPath） containers: - image: nginx:1.18 name: nginx volumeMounts: # 容器内挂载配置 - name: wwwroot # 与 volumes 中定义的名称匹配 mountPath: /usr/share/nginx/html # 容器内的挂载路径 volumes: # 定义卷 - name: wwwroot # 卷名称 persistentVolumeClaim: claimName: my-pvc # 关联的 PVC 名称 部署后去目标节点查看并验证 123[root@k8s-node-01 /data/hostpath]# echo ahhahaha &gt; index.html[root@k8s-node-01 /data/hostpath]# curl 10.244.1.190ahhahaha 4.3 NFS存储卷如果对速度没有极致的要求、想实现pod可以随意漂移，那就不能用本地存储了，需要用到网络存储，网络存储里最简单的就属NFS了 NFS存储卷：提供对NFS挂载支持，允许将NFS共享路径挂载到Pod中。适合多节点多Pod共享相同数据的场景，如网站程序文件的持久化存储 将nfs服务器上导出（export）的文件系统用作存储卷 nfs是文件系统级共享服务，它支持多路挂载请求，可由多个Pod对象同时用作存储卷后端 每一个节点都需要安装nfs客户端（nfs-common） 配置参数 kubectl explain pods.spec.volumes.nfs server &lt;string&gt;：NFS服务器的IP地址或主机名，必选字段 path &lt;string&gt;：NFS服务器导出（共享）的文件系统路径，必选字段 readOnly &lt;boolean&gt;：是否以只读方式挂载，默认为false 配置NFS服务器，安装略，请查看：NFS 1234567#定义共享目录[root@NFS ~]#mkdir /data/redis[root@NFS ~]#vim /etc/exports/data/redis 10.0.0.0/24(rw,no_root_squash,no_subtree_check)[root@NFS ~]#exportfs -r 创建PV，PVC 和 Pod 1234567891011121314# pv.yamlapiVersion: v1kind: PersistentVolumemetadata: name: nfs-pvspec: capacity: storage: 1Gi # PV 提供的存储容量 accessModes: - ReadWriteOnce # 访问模式：单节点读写 persistentVolumeReclaimPolicy: Retain # 回收策略：PVC 删除后保留 PV nfs: path: /data/nfs # NFS 服务器上的共享目录路径 server: 192.168.71.12 # NFS 服务器的 IP 地址 1234567891011# pvc.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfs-pvcspec: accessModes: - ReadWriteOnce # 需与匹配的 NFS PV 一致 resources: requests: storage: 1Gi # 需与匹配的 NFS PV 容量一致或更小 1234567891011121314151617181920# nfs-pod.yamlapiVersion: v1kind: Podmetadata: name: test-volumes spec: containers: - name: web image: nginx:1.18 ports: - name: web containerPort: 80 # 容器暴露的端口 volumeMounts: - name: nfs subPath: test-volumes # 挂载 NFS 共享目录下的子目录（/data/nfs/test-volumes） mountPath: &#x27;/usr/share/nginx/html&#x27; # 容器内的挂载路径 volumes: - name: nfs # 需与容器内 volumeMounts.name 对应 persistentVolumeClaim: claimName: nfs-pvc # 关联的PVC名称 部署后验证 12345678910# ===================&gt; 去nfs服务器上$ echo 123 &gt; /data/nfs/test-volumes/index.html# ===================&gt; 找一台k8s机器测试$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE test-volumes 1/1 Running 0 10s 10.244.2.162 k8s-node-02 $ curl 10.244.2.162123","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"PV","slug":"PV","permalink":"https://aquapluto.github.io/tags/PV/"},{"name":"PVC","slug":"PVC","permalink":"https://aquapluto.github.io/tags/PVC/"}]},{"title":"存储接口CSI","slug":"Kubernetes/data-storage/csi","date":"2025-09-12T05:45:39.000Z","updated":"2025-09-12T05:49:28.287Z","comments":true,"path":"Kubernetes/data-storage/csi/","permalink":"https://aquapluto.github.io/Kubernetes/data-storage/csi/","excerpt":"","text":"1 Kubernetes存储架构存储卷的具体管理操作由相关的控制器向卷插件发起调用请求来完成 AD控制器：负责存储设备的Attach&#x2F;Detach操作 Attach：将设备附加到目标节点，即将外部服务器与存储服务的机器对接 Detach：将设备从目标节点上拆除 存储卷管理器：负责完成卷的Mount&#x2F;Umount操作，以及设备的格式化操作等，负责管理卷的生命周期 PV控制器：负责PV&#x2F;PVC的绑定、生命周期管理，以及存储卷的Provision&#x2F;Delete操作 Scheduler：特定调度插件的调度决策会受到目标节点上的存储卷的影响，即节点集对卷的支持能力会影响到调度器的调度决策，例如kubernetes对微软云存储有一个限制，只能关联32个Pod，如果已经到达32个，那么该节点就不能作为目标节点了 2 CSI简介虽然Kubernetes中已经有In-Tree（原生卷插件）提供存储，但并不能满足所有的存储要求，因此出现了CSI存储接口，对接外部存储 Kubernetes的CSI（Container Storage Interface）是一种容器存储接口规范，旨在为容器化工作负载提供统一的存储解决方案。它允许不同的存储后端通过一个统一的API（CSI）与Kubernetes集群交互，从而使得外部存储后端可以为集群提供存储数据的服务 CSI规范定义了一个插件模型，该模型允许第三方提供商实现自己的存储解决方案，并将其集成到Kubernetes中。这些插件需要遵循CSI规范，以便在Kubernetes集群中无缝运行。目前，许多主流的存储提供商已经实现了CSI插件，如亚马逊的EBS卷、GlusterFS卷等。 容器存储接口规范与平台无关 驱动程序组件 CSI Controller：负责与存储服务的API通信从而完成后端存储的管理操作，一般以pod形式运行在Node节点上 Node Plugin：也称为CSI Node，负责在节点级别完成存储卷的管理 Kubernetes CSI 的主要组件包括： CSI 节点：负责处理来自Kubernetes API服务器的请求，将它们转发给相应的CSI插件。 CSI 插件：实现CSI规范的插件，Out-of-Tree，负责处理实际的存储操作。 CSI 驱动程序：作为CSI插件和底层存储后端之间的中介，负责将Kubernetes API中的操作转换为底层存储后端可以理解的命令。 Kubernetes API服务器：作为客户端与CSI系统之间的桥梁，负责接收用户请求并将其转发给CSI节点。 3 CSI存储组件和部署架构CSI Controller：由StatefulSet控制器对象编排运行，副本量需要设置为1，以保证只会在该存储服务运行单个CSI Controller实例，一般部署在工作节点，与控制平面的 PV Controller通信，并转发给相应的CSI插件进行存储操作 Node Plugin：由DaemonSet控制器对象编排运行，以确保每个节点上精确运行一个相关的Pod副本，每一个节点都要部署，当Kubernetes集群中的Pod需要访问存储时，它会通过Node Plugin与底层的存储后端进行通信 CSI Controller Pod中的Sidecar容器 CSI Controller被假定为不受信任而不允许运行于Master节点之上，因此，kube-controller-manager无法借助UnixSock套接字与之通信，而是要经由API Server进行； 该通用需求由Kubernetes提供的external-attacher和external-provisioner程序完成，此两者通常会以一个Sidecar容器运行于CSI Controller Pod中 external-attacher：负责针对CSI上的卷执行attach和detach操作，类似于CSI专用的AD Controller external-provisioner：负责针对CSI上的卷进行Provision和Delete操作等，类似于CSI专用的PV Controller CSI支持卷扩展和快照时，可能还会用到external-resizer和external-snapshotter一类的程序 CSI Node Pod中的Sidecar容器 kubelet（实现卷的Mount&#x2F;Umount操作）将UnixSock套接字与CSI Node Plugin进行通信 将Node Plugin注册到kubelet上，并初始化NodeID（获取从Kubernetes节点名称到CSI驱动程序NodeID的映射）的功能由通用的node-driver-registrar程序提供，它通用运行为CSI Node Pod的Sidecar容器 4 阿里云的CSI插件简介如果我们将来将kubernetes集群部署在公有云上，一般公有云都有自己的存储服务，那么需要将该公有云的存储服务插件部署在集群中，才能使用该公有云的存储服务 阿里云CSI插件实现了在Kubernetes中对阿里云云存储卷的生命周期管理 CSI-Plugin组件遵循标准CSI规范，提供了EBS、NAS、OSS等类型阿里云云存储服务的挂载能力 自ACK 1.16版本的集群开始，部署集群时默认即会自动安装最新版本的CSI组件 CSI Plugin提供了数据卷的全生命周期管理，包括数据卷的：创建、挂载、卸载、删除、扩容等服务","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"CSI","slug":"CSI","permalink":"https://aquapluto.github.io/tags/CSI/"}]},{"title":"LocalDNS","slug":"Kubernetes/service-loadbalancer/localdns","date":"2025-09-12T04:38:12.000Z","updated":"2025-09-12T05:42:38.537Z","comments":true,"path":"Kubernetes/service-loadbalancer/localdns/","permalink":"https://aquapluto.github.io/Kubernetes/service-loadbalancer/localdns/","excerpt":"","text":"1 CoreDNS的问题集群规模大的情况，CoreDNS可能会出现超时5s的情况（dns请求超时，客户端默认策略时等待5s自动重试，如果重试成功，我们看到的现象就是dns请求有5s的延时），这对我们影响很大，因为k8s集群都依赖它来实现域名解析 超时的原因是因为：无论你用的是iptables模式还是ipvs模式，它们的底层都会用到 conntrack内核模块来组织dns的udp查询包，高并发场景，会出现conntrack冲突问题，导致一些dns请求包被丢掉，从而导致超时 超时原因具体分析，可以参考 weave works 总结的文章 Racy conntrack and DNS lookup timeouts 2 CoreDNS的性能测试部署一个被测试的svc 12345678910111213141516171819202122232425262728293031323334# nginx.yamlapiVersion: apps/v1kind: Deploymentmetadata:name: my-nginxspec:selector: matchLabels: app: my-nginxreplicas: 2template: metadata: labels: app: my-nginx spec: containers: - name: my-nginx image: nginx:1.18.0 ports: - containerPort: 80---apiVersion: v1kind: Servicemetadata:name: my-nginxspec:ports: - name: http port: 80 targetPort: 80 protocol: TCPselector: app: my-nginxtype: ClusterIP 123[root@k8s-master-01 ~/coredns_test]# kubectl get svc my-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-nginx ClusterIP 10.102.87.170 &lt;none&gt; 80/TCP 8s go压测工具代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// main.gopackage mainimport ( &quot;context&quot; &quot;flag&quot; &quot;fmt&quot; &quot;net&quot; &quot;sync/atomic&quot; &quot;time&quot;)var host stringvar connections intvar duration int64var limit int64var timeoutCount int64func main() &#123; flag.StringVar(&amp;host, &quot;host&quot;, &quot;&quot;, &quot;Resolve host&quot;) flag.IntVar(&amp;connections, &quot;c&quot;, 100, &quot;Connections&quot;) flag.Int64Var(&amp;duration, &quot;d&quot;, 0, &quot;Duration(s)&quot;) flag.Int64Var(&amp;limit, &quot;l&quot;, 0, &quot;Limit(ms)&quot;) flag.Parse() var count int64 = 0 var errCount int64 = 0 pool := make(chan interface&#123;&#125;, connections) exit := make(chan bool) var ( min int64 = 0 max int64 = 0 sum int64 = 0 ) go func() &#123; time.Sleep(time.Second * time.Duration(duration)) exit &lt;- true &#125;()endD: for &#123; select &#123; case pool &lt;- nil: go func() &#123; defer func() &#123; &lt;-pool &#125;() resolver := &amp;net.Resolver&#123;&#125; now := time.Now() _, err := resolver.LookupIPAddr(context.Background(), host) use := time.Since(now).Nanoseconds() / int64(time.Millisecond) if min == 0 || use &lt; min &#123; min = use &#125; if use &gt; max &#123; max = use &#125; sum += use if limit &gt; 0 &amp;&amp; use &gt;= limit &#123; timeoutCount++ &#125; atomic.AddInt64(&amp;count, 1) if err != nil &#123; fmt.Println(err.Error()) atomic.AddInt64(&amp;errCount, 1) &#125; &#125;() case &lt;-exit: break endD &#125; &#125; fmt.Printf(&quot;request count：%d\\nerror count：%d\\n&quot;, count, errCount) fmt.Printf(&quot;request time：min(%dms) max(%dms) avg(%dms) timeout(%dn)\\n&quot;, min, max, sum/count, timeoutCount)&#125; 首先配置好 golang 环境，然后直接构建上面的测试应用 12yum install golang -ygo build -o testdns ./main.go 构建完成后生成一个 testdns 的二进制文件，然后我们将这个二进制文件拷贝到任意一个 Pod 中去进行测试 12345# 打开一个终端启动podkubectl run test --image=centos:7 sleep 10000 # 打开另外一个终端拷贝go命令进该podkubectl cp testdns test:/root -n default 然后我们执行 testdns 程序来进行压力测试，比如执行 200 个并发，持续 30 秒，-l 设定了一个阈值为5000ms，超过该值的都算超时timeout的请求数 1234567891011121314151617181920212223242526272829303132333435363738394041424344➜ kubectl exec -ti test /bin/shsh-4.4# cat /etc/resolv.confsearch default.svc.cluster.local svc.cluster.local cluster.localnameserver 10.96.0.10options ndots:5sh-4.4# cd /root# 对 my-nginx.default 这个地址进行解析-------------重复测试第1次sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：158401error count：0request time：min(1ms) max(10027ms) avg(36ms) timeout(317n)-------------重复测试第2次sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：152066error count：0request time：min(1ms) max(10022ms) avg(38ms) timeout(266n)-------------重复测试第3次sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：159286error count：0request time：min(1ms) max(5050ms) avg(36ms) timeout(301n)-------------重复测试第4次sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：162082error count：0request time：min(1ms) max(5049ms) avg(35ms) timeout(283n)-------------重复测试第5次sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：160063error count：0request time：min(1ms) max(5059ms) avg(35ms) timeout(304n)-------------重复测试第6次sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：162249error count：0request time：min(1ms) max(5056ms) avg(34ms) timeout(287n) 我们可以看到 timout 的个数都在280个左右，性能很差。我们就来使用 NodeLocal DNSCache 来提升 DNS 的性能和可靠性 3 使用本地DNS缓存3.1 NodeLocal DNSCache介绍NodeLocal DNSCache会在每个物理节点上缓存 DNS 解析，这 可以缩短 DNS 查找的延迟时间、使 DNS 查找时间更加一致，以及减少发送到 kube-dns 的 DNS 查询次数，来提升集群DNS的性能和可靠性 在集群中运行 NodeLocal DNSCache 有如下几个好处 如果本地没有 CoreDNS 实例，则具有最高 DNS QPS 的 Pod 可能必须到另一个节点进行解析，使用 NodeLocal DNSCache 后，拥有本地缓存将有助于改善延迟 跳过 iptables DNAT 和连接跟踪将有助于减少 conntrack 竞争并避免 UDP DNS 条目填满conntrack 表（上面提到的 5s 超时问题就是这个原因造成的） 从本地缓存代理到 kube-dns 服务的连接可以升级到 TCP，TCP conntrack 条目将在连接关闭时被删除，而 UDP 条目必须超时(默认 nfconntrackudp_timeout 是 30 秒) 将 DNS 查询从 UDP 升级到 TCP 将减少归因于丢弃的 UDP 数据包和 DNS 超时的尾部等待时间，通常长达 30 秒（3 次重试+ 10 秒超时） NodeLocal DNSCache采用的是daemonset+hostNetwork，会确保每个节点都有一个NodeLocal DNSCache实例。每个物理节点上启动起来的 pod 都监听在169.254.20.10，流程解析如下 当 Pod 尝试解析一个域名时，它将按照 &#x2F;etc&#x2F;resolv.conf 中的配置发送 DNS 查询到169.254.20.10，由于 NodeLocal DNSCache 已经在该地址上监听，它会接收到查询并相应地处理。 如果 NodeLocal DNSCache 有缓存的响应或者知道答案，它将直接响应 如果没有，它会将请求转发到集群的上游 DNS 服务器，例如 CoreDNS，然后再将响应返回给请求者。 3.2 NodeLocal DNSCache部署12# 下载完毕后，要进行一些编辑配置wget https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml 该资源清单文件中包含几个变量值得注意 __PILLAR__DNS__SERVER__ ：表示 kube-dns 这个 Service 的 ClusterIP，可以通过命令 kubectl get svc -n kube-system | grep kube-dns | awk &#39;&#123; print $3 &#125;&#39; 获取（我们这里就是 10.96.0.10 ） __PILLAR__LOCAL__DNS__ ：表示 DNSCache 本地的 IP，默认为 169.254.20.10 __PILLAR__DNS__DOMAIN__ ：表示集群域，默认就是 cluster.local 另外还有两个参数 __PILLAR__CLUSTER__DNS__ 和 __PILLAR__UPSTREAM__SERVERS__ ，这两个参数会通过镜像 1.15.16 版本去进行自动配置，对应的值来源于 kube-dns 的 ConfigMap 和定制的Upstream Server 配置。直接执行如下所示的命令即可安装 12345678910111213141516# 一、定义变量# 获取coredns的IP：kubectl get svc coredns -n kube-system -o jsonpath=&#123;.spec.clusterIP&#125;kubedns=10.96.0.10# 表示集群域，默认就是 cluster.localdomain=cluster.local# 表示 DNSCache 本地的 IP，默认为169.254.20.10localdns=169.254.20.10# 二、替换# 如果 kube-proxy 运行在 IPTABLES 模式：sed -i &quot;s/__PILLAR__LOCAL__DNS__/$localdns/g;s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g&quot; nodelocaldns.yaml# 如果 kube-proxy 运行在 IPVS 模式：sed -ri &quot;s/__PILLAR__LOCAL__DNS__/$localdns/g;s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,?__PILLAR__DNS__SERVER__//g;s/__PILLAR__CLUSTER__DNS__/$kubedns/g&quot; nodelocaldns.yaml 部署并查看可以通过如下命令来查看对应的 Pod 是否已经启动成功，需要注意的是这里使用 DaemonSet 部署 node-local-dns 使用了 hostNetwork&#x3D;true ，会占用宿主机的 8080 端口，所以需要保证该端口未被占用 123456$ kubectl apply -f nodelocaldns.yaml$ kubectl get pods -n kube-system -l k8s-app=node-local-dnsNAME READY STATUS RESTARTS AGEnode-local-dns-8nmjx 1/1 Running 0 2m35snode-local-dns-c9p7p 1/1 Running 0 2m35snode-local-dns-z4h45 1/1 Running 0 2m35s 如果 kube-proxy 组件使用的是 ipvs 模式的话，我们还需要修改 每个节点上的 kubelet 的 –cluster-dns 参数，将其指向 169.254.20.10 12345$ vim /var/lib/kubelet/config.yamlclusterDNS:- 169.254.20.10 # 原值为10.96.0.10$ systemctl restart kubelet Daemonset 会在每个节点创建一个网卡来绑这个 IP，Pod 向本节点这个 IP 发 DNS 请求，缓存没有命中的时候才会再代理到上游集群 DNS 进行查询 1234567891011[root@k8s-master-01 /test]# ip a33: nodelocaldns: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default link/ether 9a:b5:6a:07:b0:ae brd ff:ff:ff:ff:ff:ff inet 169.254.20.10/32 scope global nodelocaldns valid_lft forever preferred_lft forever [root@k8s-node-01 ~]# ip a |grep 169.254.20.10 inet 169.254.20.10/32 scope global nodelocaldns [root@k8s-node-02 ~]# ip a |grep 169.254.20.10 inet 169.254.20.10/32 scope global nodelocaldns iptables模式下 Pod 还是向原来的集群DNS请求，节点上有这个IP监听，会被本机拦截，再请求集群上游DNS，所以不需要更改 --cluster-dns 参数。如果担心线上环境修改 --cluster-dns 参数会产生影响，我们也可以直接在新部署的 Pod 中通过dnsConfig 配置使用新的 localdns 的地址来进行解析 待 node-local-dns 安装配置完成后，先在宿主机上验证一下，正常应该是用 10.96.0.10 和 169.254.20.10 都可以解析 如果解析不成功，那证明部署有问题，需要重启coredns，删除node local dns后重新部署直到上面两种都可以解析 1234567$ dig @10.96.0.10 my-nginx.default.svc.cluster.local # 用coredns来解析;; ANSWER SECTION:my-nginx.default.svc.cluster.local. 26 IN A 10.105.28.11$ dig @169.254.20.10 my-nginx.default.svc.cluster.local # 用nodelocaldns来解析;; ANSWER SECTION:my-nginx.default.svc.cluster.local. 30 IN A 10.105.28.11 3.3 性能测试1234567891011121314151617$ kubectl run test --image=centos sleep 10000$ kubectl cp testdns test:/root -n default$ kubectl exec -ti test /bin/shsh-4.4# cat /etc/resolv.confsearch default.svc.cluster.local svc.cluster.local cluster.localnameserver 169.254.20.10 # 已经指向了nodelocaldnsoptions ndots:5sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：202378error count：0 request time：min(1ms) max(5057ms) avg(27ms) timeout(316n) # 平均耗时相比之前少了很多sh-4.4# ./testdns -host my-nginx.default -c 200 -d 30 -l 5000request count：201769error count：0request time：min(1ms) max(5052ms) avg(27ms) timeout(346n) # 平均耗时相比之前少了很多 从上面的结果可以看到，无论是最大解析时间还是平均解析时间都比之前默认的 CoreDNS 提示了不少的效率，所以我们还是非常推荐在线上环境部署 NodeLocal DNSCache 来提升 DNS 的性能和可靠性的，唯一的缺点就是由于 LocalDNS 使用的是 DaemonSet 模式部署，所以如果需要更新镜像则可能会中断服务（不过可以使用一些第三方的增强组件来实现原地升级解决这个问题，比如 openkruise","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"LocalDNS","slug":"LocalDNS","permalink":"https://aquapluto.github.io/tags/LocalDNS/"}]},{"title":"CoreDNS","slug":"Kubernetes/service-loadbalancer/coredns","date":"2025-09-12T04:38:06.000Z","updated":"2025-09-12T05:39:05.440Z","comments":true,"path":"Kubernetes/service-loadbalancer/coredns/","permalink":"https://aquapluto.github.io/Kubernetes/service-loadbalancer/coredns/","excerpt":"","text":"1 CoreDNS介绍Kubernetes非常擅长跑微服务，在微服务架构中必须要有的核心组件：服务注册与发现组件，而CoreDNS+Service二者结合就实现服务注册与发现的效果，掌管着被管理服务的可达状态 svc负责与被管理着动态建立关系 + pod里要设置readinessProbe探针 —&gt; 健康检查 coredns负责提供一个可用的固定的域名 —&gt; 提供一种固定的访问方式 微服务架构中，为了应对服务的频繁重启、上下线、ip地址频繁变动等问题而导致的访问配置文件需要频繁变动问题，必须有一个服务发现组件，该组件能够对外提供一个稳定的访问名字，该名字背后对应的服务变动都被该组件屏蔽起来 服务发现组件有很多种，而CoreDNS是其中一种，负责为Kubernetes提供域名解析，对外提供稳定访问的名字即域名。Kubernetes 集群中所有域名的根域名默认为 “cluster.local”。以下资源拥有FQDN域名 Service资源： svc名称.ns.svc.cluster.local. 解析到Service对应的ClusterIP上 名字固定的Pod（Statefulset创建的Pod、裸Pod）+ 无头服务：pod固定的名字.无头svc的名字.ns.svc.cluster.local. 解析为指定Pod的IP列表 说明：Deployment控制器创建pod + 无头服务不能让pod拥有可解析到 pod ip 的 fqdn 域名 deployment控制器创建得pod名字不固定，一重启就变名字了 就算永远不重启，也无法用 pod名.无头服务名.ns.svc.cluster.local 来解析拿到pod的ip地址，去解析会发现没有任何响应 就算能拿到解析结果，也没用，因为deployment控制器无法为每个pod固定名字 CoreDNS在Kubernetes中作为一个Deployment运行，通常会部署两个或多个副本以确保高可用性。它主要通过以下步骤工作： 启动与配置：CoreDNS 读取 ConfigMap 配置文件，根据配置启动相应的插件。 监听 DNS 请求：CoreDNS 监听来自 Kubernetes 集群内部的 DNS 请求。 解析 DNS 请求：根据请求的类型，CoreDNS 调用相应的插件进行解析。 返回解析结果：将解析结果返回给请求方。 12345678[root@master1 ~]#kubectl -n kube-system get podsNAME READY STATUS RESTARTS AGEcoredns-7c445c467-nwhc4 1/1 Running 198 (7h33m ago) 22dcoredns-7c445c467-qvpbp 1/1 Running 25 (7h33m ago) 22d[root@master1 ~]#kubectl -n kube-system get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 31d 每个Service，在CoreDNS上都会有A&#x2F;AAAA、SRV和PTR资源记录 A&#x2F;AAAA资源记录 12&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;cluster-ip&gt;&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;cluster-ip&gt; 为每个定义了名称的端口生成一个SRV记录，以支持服务发现 1_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. 为每个A记录（例如a.b.c.d）或AAAA记录生成对应的PTR记录 123&lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. 范例 12345678910111213141516171819202122232425262728293031323334353637[root@master1 ~]#kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 54mpod-test NodePort 10.99.170.16 &lt;none&gt; 80:31744/TCP 7m47s[root@master1 ~]#kubectl get svc -n registry-proxy NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEregistry-proxy ClusterIP 10.97.119.21 &lt;none&gt; 443/TCP 33m[root@master1 ~]#kubectl exec -it pod-test-b896f97bf-n5z6v -- /bin/sh[root@pod-test-b896f97bf-n5z6v ~]# cat /etc/resolv.conf nameserver 10.96.0.10 #指向coredns的service的IP地址（CLUSTER-IP）search default.svc.cluster.local svc.cluster.local cluster.local baidu.com #自动配置搜索域options ndots:5 #规定了一个域名中必须包含的最少点号，如果少于就会自动追加search配置的搜索域#同一名称空间下可以[root@pod-test-b896f97bf-n5z6v ~]# host -t A pod-testpod-test.default.svc.cluster.local has address 10.99.170.16 #自动补全搜索域[root@pod-test-b896f97bf-n5z6v ~]# host -t A kuberneteskubernetes.default.svc.cluster.local has address 10.96.0.1#不同名称空间下不可以[root@pod-test-b896f97bf-n5z6v ~]# host -t A registry-proxyregistry-proxy.svc.cluster.local has no A record #会发现这一段没有指定名称空间，所以解析不了#指定名称空间就可以[root@pod-test-b896f97bf-n5z6v ~]# host -t A registry-proxy.registry-proxyregistry-proxy.registry-proxy.svc.cluster.local has address 10.97.119.21#查看PTR[root@pod-test-b896f97bf-n5z6v ~]# host -t PTR 10.99.170.1616.170.99.10.in-addr.arpa domain name pointer pod-test.default.svc.cluster.local.#查看SRV#0表示权重，100表示优先级，80表示端口[root@pod-test-b896f97bf-n5z6v ~]# host -t SRV pod-testpod-test.default.svc.cluster.local has SRV record 0 100 80 pod-test.default.svc.cluster.local. 2 Pod的域名解析流程流程如下：Pod应用 → &#x2F;etc&#x2F;resolv.conf → CoreDNS Service → kube-proxy → CoreDNS Pod → API Server&#x2F;外部DNS DNS配置初始化 Pod启动时，kubelet根据集群DNS配置( --cluster-dns 参数)将CoreDNS的Service ClusterIP写入Pod的 /etc/resolv.conf 文件 DNS查询发起：应用程序发起域名查询(如my-service) 若为非FQDN(如my-service)，系统按search域依次补全(如my-service..svc.cluster.local)并尝试解析 若为FQDN(如my-service.ns.svc.cluster.local)，直接发送DNS请求 请求路由至CoreDNS服务 Pod的DNS客户端将请求通过UDP&#x2F;TCP协议发送至nameserver指定的CoreDNS Service ClusterIP(如10.96.0.10)的53端口 然后节点上的kube-proxy实现流量转发，通过 iptables&#x2F;ipvs 规则将目标为CoreDNS ServiceIP的流量负载均衡到后端CoreDNS Pod的Endpoint IP CoreDNS处理请求 CoreDNS Pod接收请求后，根据配置(corefile)执行插件链 kubernetes插件：解析集群内Service、Pod域名，通过API Server查询 Service&#x2F;Pod 的 IP forward插件：非集群域名(如 baidu.com)转发至上游DNS(如节点 /etc/resolv.conf 中的外部DNS服务器 返回解析结果：CoreDNS将解析后的IP地址按原路径返回至Pod，完成域名解析 3 CoreDNS的默认配置CoreDNS的配置都存储在名为coredns的ConfigMap对象中，该对象位于kube-system名称空间中 CoreDns内部采用插件机制，所有功能都是插件形式编写，用户也可以扩展自己的插件 12345678910111213141516171819202122232425262728apiVersion: v1data: Corefile: | .:53 &#123; # 监听所有接口的53端口, .是根区域，表示负责对所有名称都有本地53端口的进行解析 errors # 错误记录插件 health &#123; # 健康检查插件，检查端点8080:health lameduck 5s # 如果连续5秒内没有收到请求，则认为服务不健康 &#125; ready # 就绪检查插件 kubernetes cluster.local in-addr.arpa ip6.arpa &#123; # K8s插件，用于解析K8s集群内部的DNS记录 pods insecure # 表示允许不安全的Pod解析 fallthrough in-addr.arpa ip6.arpa # 表示允许将查询转发到上游DNS服务器 ttl 30 # 缓存时间为30秒 &#125; prometheus :9153 # 用于暴露CoreDNS的metrics指标，9153:metrics forward . /etc/resolv.conf &#123; # 转发插件，根区域解析不了的所有查询都转发到Pod所在节点的/etc/resolv.conf文件中指定的上游DNS服务器，CoreDNS的Pod中的/etc/resolv.conf文件的内容取决于其dnsPolicy设置和集群的DNS配置 max_concurrent 1000 # 最大并发查询数为1000 &#125; cache 30 # 缓存插件，所有内容限制为30s的TTL loop # 循环插件，用于检测DNS循环，检查简单的转发循环并停止服务 reload # 重新加载插件，用于热更新Corefile loadbalance # 负载均衡插件，用于在多个上游DNS服务器之间进行负载均衡，默认round_robin &#125;kind: ConfigMapmetadata: creationTimestamp: &quot;2023-02-27T04:41:23Z&quot; name: coredns namespace: kube-system 服务器配置段（Server Blocks）：用于定义负责解析的权威区域，配置段放置于其后的花括号&#123; &#125;中；而且也可以指定要监听的端口号（端口号之前需要使用一个冒号），默认为53 每个服务器配置段都需要配置一些要使用的插件（这些插件也可称为该服务器的插件链），用来为该服务器提供更加丰富的功能与配置，例如配置中的errors、health、ready、kubernetes、prometheus、forward等 4 CoreDNS的查询路由CoreDNS处理DNS查询的方式 生成DNS服务器：针对每个Socket，CoreDNS会生成一个服务器（dnserver.Server），这意味着不同的端口有不同的配置和行为 配置段组合：当同一端口上配置了多个配置段（Server Block）时，CoreDNS会将这些配置段组合起来，形成一个包含多个独立配置段的单一服务器 路由DNS查询：对于到达特定Socket的DNS查询请求，CoreDNS会根据最长匹配原则（精度匹配）来确定使用哪个目标配置段进行处理，随后请求将由该服务器配置段中的插件链按特定顺序（注意是在构建CoreDNS程序时由 plugin.cfg 文件指定）进行路由 处理匹配失败：若不存在匹配的服务器配置段，则返回SERVERFAIL CoreDNS查询路由示例 123456789101112131415161718192021222324coredns.io:5300 &#123; file /etc/coredns/zones/coredns.io.db # 指定区域数据库&#125;magedu.com:53 &#123; errors log file /etc/coredns/zones/magedu.com.db &#125;magedu.net:53 &#123; file /etc/coredns/zones/magedu.net.db&#125;.:53 &#123; errors log health rewrite name jenkins.magedu.com jenkins.jenkins.svc.cluster.local kubernetes cluster.local 10.244.0.0/16 file /etc/coredns/magedu.db magedu.org forward . /etc/resolv.conf cache 30&#125; 5 CoreDNS的插件CoreDNS的插件分为两大类 1、负责处理请求的常规插件，这些插件才是插件链中的有效组成部分，例如errors、kubernetes和forward等，而他们又分为两类 负责以某种方式处理请求的插件：这类插件不是区域数据源，它们的运行方式是在自身处理完成后，会将查询传递给下一个插件 后端插件：用于配置区域数据的来源，例如etcd、file和kubernetes等 这类插件通常要为其负责的区域生成最终结果，它要么能正常响应查询，要么返回NXDOMAIN 但也可以使用fallthrough改变这种响应行为：未发现查询的目标资源记录时，将DNS查询请求转交给插件链中的下一个插件，随后终结于另一个后端插件，或再次由 fallthrough 改变这种行为 2、不处理请求的特殊插件，它们仅用来修改Server或Server Block中的配置，例如health、tls、startup、shutdown和root等插件，这类插件不会作为服务器配置段的插件链中的有效组成部署 插件的生效顺序是固定的，与它们在配置文件中的顺序无关。这个顺序是在CoreDNS的源码中定义的，在构建CoreDNS程序时由 plugin.cfg 文件指定，修改这个顺序需要重新编译CoreDNS 5.1 常用的插件 5.2 kuberntes插件CoreDNS的Kubernetes插件主要负责处理与Kubernetes相关的DNS解析请求，插件的配置参数简介如下 endpoint：指定kubernetes API Server的URL，默认为CoreDNS Pod所使用的ServiceAccount可接入的自身所在集群的API Server tls：用于建立远程k8s连接时要使用的tls证书、私钥和CA证书 kubeconfig：用于谁到远程k8s时使用的kubeconfig文件 namespaces：仅对外暴露（提供Service名称解析）的k8s名称空间的列表，默认为所有名称空间 labels：namespace资源对象的标签选择器，匹配到的namespace中的Service才会对外暴露 pods：为Pod生成基于IP的资源记录时的处理模式 disabled：不处理Pod级别的名称解析请求，默认配置 insecure：不检查k8s，而直接返回一个带有IP地址的A记录 verified：若同一namespace中存在具有匹配的IP地址的Pod，则返回A记录 noendpoints：禁止endpoint相关的资源记录，所有endpoint查询和headless查询都将生成NXDOMAIN fallthrough：若当前插件解析失败，则仅将针对指定zone列表中的查询请求继续交由插件链中后续的插件进行处理 ignore empty_service：为没有任何就绪的Service返回NXDOMAIN 12345678910111213kubernetes [ZONES...] &#123; endpoint URL tls CERT KEY CACERT kubeconfig KUBECONFIG [CONTEXT] namespaces NAMESPACE... labels EXPRESSION pods POD-MODE endpoint_pod_names ttl TTL noendpoints fallthrough [ZONES...] ignore empty_service&#125; 5.3 rewrite插件5.3.1 对请求进行重写在CoreDNS内部对查询及响应进行重写，重写过程对客户端透明 1rewrite [continue|stop] FIELD [TYPE] [(FROM TO)|TTL] [OPTIONS] continue：继续由后面的rule进行处理 stop：将当前rule视作最后一条，即当前rule处理完成后即返回响应；此为默认行为 FIELD：用于指定请求或响应中的哪部分内容被重写 type：请求的类型字段被重写，此时FROM&#x2F;TO必须是DNS记录类型，如A、MX等 name：重写查询请求中要查询的名称，默认情况下，其将执行名称的完全匹配 class：消息的类别被重写，此时FROM&#x2F;TO必须是DNS class的类型，如IN、CH或HS等 edns0：将EDNS0选项附加到请求中 ttl：重写响应报文中的TTL值 TYPE：FILED字段为name时，用于指定匹配name值的方式 exact（默认）：精确匹配，与查询请求中的名称完全匹配，只要这个方式对客户端透明，其他方式要加answer字段 substring：匹配查询请求中的名称的子串 prefix：匹配查询请求中的名称的前缀 suffix：匹配查询请求中的名称的后缀 regex：使用正则表达式模式匹配查询请求中的名称 FROM：要匹配的名称或类型 TO：要重写成的目标名称或类型 TTL：在响应报文中要设定的ttl值，仅适用于ttl FIELD 123456789101112131415161718192021222324252627282930313233343536.:53 &#123; errors log health # rewrite name regex (jenkins.*)\\.magedu\\.com &#123;1&#125;.jenkins.svc.cluster.local rewrite name jenkins.magedu.com jenkins.jenkins.svc.cluster.local #精确匹配查询的名称jenkins.magedu.com时，并将请求的名称 #重写为jenkins.Jenkins.svc.cluster.local #必须在kubernetes解析之前写 kubernetes cluster.local 10.244.0.0/16 file /etc/coredns/magedu.db magedu.org forward . /etc/resolv.conf cache 30&#125;[root@master1 ~]#kubectl exec -it -n demo demoapp-7c58cd6bb-bldpv -- /bin/sh[root@demoapp-7c58cd6bb-bldpv ~]# host -t A demoapp-svc.demo.svc.cluster.localdemoapp-svc.demo.svc.cluster.local has address 10.105.237.159[root@master1 ~]#kubectl edit configmaps -n kube-system coredns#在解析之前（kubernetes）加上这段rewrite name demoapp-svc.magedu.com demoapp-svc.demo.svc.cluster.local[root@demoapp-7c58cd6bb-bldpv ~]# host -t A demoapp-svc.magedu.comdemoapp-svc.demo.svc.cluster.local has address 10.105.237.159~]# dig -t A jenkins.magedu.com;; QUESTION SECTION:;jenkins.magedu.com. IN A #请求查询域名;; ANSWER SECTION:jenkins.jenkins.svc.cluster.local. 30 IN A 10.108.56.171 #响应回的域名;; Query time: 1 msec;; SERVER: 10.96.0.10#53(10.96.0.10);; WHEN: Mon Feb 27 09:30:40 UTC 2023;; MSG SIZE rcvd: 108 需要注意的是，重写传入DNS请求的名称（字段name）时，CoreDNS会重写QUESTION SECTION请求的部分，但不会自动重写ANSWER SECTION中请求的名称，但这很可能被视为中间人攻击，因此也应该重写响应报文中ANSWER SECTION中请求的名称 5.3.2 对响应进行重写OPTIONS：FILED字段为name时，支持对响应报文进行隐式重写 answer auto：尽力完成响应报文中名称部分的重写 answer name FROM TO：将响应报文中的匹配FROM的查询名称重写为TO answer value FROM TO：将响应报文中的匹配FROM的用于（例如CNAME或SRV资源记录中）响应的名称重写为TO 隐式的响应重写，可以定义在rewrite name规则的OPTIONS中 1rewrite name regex (jenkins.*)\\.magedu\\.com &#123;1&#125;.jenkins.svc.cluster.local answer auto 显式报文重写，要使用明确定义的answer规则进行 1234rewrite stop &#123; name regex (jenkins.*)\\.magedu\\.com &#123;1&#125;.jenkins.svc.cluster.local answer name (jenkins.*)\\.jenkins.\\.svc\\.cluster\\.local &#123;1&#125;.magedu.com&#125; 查询的名称被自动重写为了查询请求中的名称 1234567891011~]# dig -t A jenkins.magedu.com;; QUESTION SECTION:;jenkins.magedu.com. IN A;; ANSWER SECTION:jenkins.magedu.com. 30 IN A 10.108.56.171;; Query time: 1 msec;; SERVER: 10.96.0.10#53(10.96.0.10);; WHEN: Mon Feb 27 09:41:50 UTC 2023;; MSG SIZE rcvd: 93 6 添加CoreDNS自定义解析记录6.1 手动配置CoreDNS的场景第一种场景：配置 CoreDNS 解析集群节点主机名，解决 hostNetwork Pod 无法解析内网节点名称的问题（增强内部解析能力） CoreDNS只会为service解析名称，不会有将节点IP地址解析成节点名称的记录，如果有一个hostNetwork类型的Pod，想要去解析集群内部的节点名称，就会将请求发给CoreDNS，但是CoreDNS没有记录，CoreDNS就会将请求发往根域名服务器，但是集群内部的节点是内网，所以解析不成功，为了让它解析成功，就需要手动配置CoreDNS 第二种场景：转发请求至私有DNS服务器，打通集群内 Pod 对外部私有服务的解析需求（打通外部解析通道） 集群外部有一台私有的DNS服务器，Pod想要去解析集群内部的某一个服务，就会将请求发给CoreDNS，CoreDNS没有记录就会将请求发往根域名服务器，但是根域名服务器肯定没有，因为是在内网，所以需要手动配置CoreDNS将请求转发给我们的私有DNS服务器 第三种场景：通过别名（CNAME）统一内外部访问域名（实现域名访问解耦） 假如集群内部有一个镜像仓库harbor，希望以后推镜像都推到这个harbor上，这个harbor在集群内部中就会有一个service名称，而这个service名称是默认是以.cluster.local 结尾的，在外部不是一个规范的名称，所以我们希望集群内外部都使用 harbor.magedu.com 来解析它，然后集群内部有一个Pod运行着docker，它需要将镜像推到 harbor 上，按道理来说应该是以 harbor.NameSpace.svc.cluster.local 去解析，但是我们需要让这个Pod 用 harbor.magedu.com 来解析，而这个名称 CoreDNS肯定解析不了，只能要么转发给根，要么转发给私有DNS服务器，假如我们也没有私有DNS服务器，想要解析，那么就需要手动配置CoreDNS创建一个别名CNAME记录，将 harbor.magedu.com 解析给 harbor.NameSpace.svc.cluster.local ，最后解析到 harbor 的IP地址。所以在配置文件中，在推镜像的时候实际上还是 harbor.magedu.com ，而没有使用 harbor.NameSpace.svc.cluster.local ，这样子的好处就是以后如果将harbor转换到集群外部，或者是公有云上的 harbor 服务，只需要将 harbor.magedu.com 解析给对应的harbor服务器域名就可以了，就不用改大量的配置文件 6.2 将节点IP地址解析成节点名称的记录123456789101112131415161718192021222324252627282930Corefile: | .:53 &#123; errors health &#123; lameduck 15s &#125; ready hosts &#123; # 自定义hosts风格的解析库，只能正解，不能反解 172.29.9.11 k8s-master01.magedu.com 172.29.9.11 k8s-node01.magedu.com 172.29.9.12 k8s-node02.magedu.com 172.29.9.13 k8s-node03.magedu.com fallthrough &#125; kubernetes cluster.local in-addr.arpa ip6.arpa &#123; # 另外，也可以考虑在CoreDNS上使用外部的kubenodes插件，自动为Kubernetes集群节点创建解析记录 pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 &#125; prometheus :9153 forward . /etc/resolv.conf &#123; prefer_udp &#125; cache 30 loop reload loadbalance &#125; 6.3 将解析转发到私有DNS服务器1234567891011121314151617181920212223Corefile: | .:53 &#123; errors health &#123; lameduck 15s &#125; ready kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 &#125; prometheus :9153 forward . 172.29.9.252 172.29.9.253 &#123; # 将外部域名的解析全部交由指定的外部DNS服务器 prefer_udp #fallthrough # 如果指定个别域名交由外部服务器，就要加这个选项，不然当解析不了时就会返回错误 &#125; cache 30 loop reload loadbalance &#125; 7 Pod的DNS解析策略和配置spec.dnsPolicy：DNS解析策略 Default：Pod直接继承由kubelet指定的DNS解析文件里定义的DNS策略，默认是其所在节点（宿主机的 /etc/resolv.conf）的名称解析配置，就无法解析集群内部的服务，csi-nfs-driver默认使用。通过修改kubelet的参数 –resolv-conf 来指定DNS解析文件 ClusterFirst（默认）：使用集群内部的CoreDNS服务进行域名解析。如果集群内的服务无法解析请求的域名，那么才将查询转发给从所在节点继承的上游名称服务器 ClusterFirstWithHostNet：专用于在设置了hostNetwork的Pod对象上使用的ClusterFirst策略。当Pod与宿主机共用同一个网络命名空间时，这类Pod无法访问集群内的服务，所以该策略可以让这类Pod可以利用集群的DNS服务进行域名解析 None：忽略Kubernetes集群的默认设定，而仅使用由 spec.dnsConfig 自定义的配置 spec.dnsConfig：DNS解析机制配置，仅 spec.dnsPolicy 设置为None时生效 nameservers &lt;[]string&gt;：DNS名称服务器列表，附加于由dnsPolicy生成的DNS名称服务器之后，最多可以指定3个 IP 地址，所列出的服务器将合并到从指定的 DNS 策略生成的基本名称服务器，并删除重复的地址。 searches &lt;[]string&gt;：DNS名称解析时的搜索域，附加由于dnsPolicy生成的搜索域之后，指定此属性时，所提供的列表将合并到根据所选 DNS 策略生成的基本搜索域名中。重复的域名将被删除，Kubernetes 最多允许 6 个搜索域。 options &lt;[]Object&gt;：DNS解析选项列表，同dnsPolicy生成的解析选项合并成最终生效的定义，其中每个对象可能具有 name 属性（必需）和 value 属性（可选）。此属性中的内容将合并到从指定的 DNS 策略生成的选项。重复的条目将被删除。 范例：自定义DNS配置 12345678910111213141516171819202122232425[root@master1 services]#vim pod-with-dnspolicy.yamlapiVersion: v1kind: Podmetadata: name: pod-with-dnspolicy namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent dnsPolicy: None dnsConfig: nameservers: # 指定dns查询时应该使用的DNS服务器ip地址 - 10.96.0.10 - 223.5.5.5 - 223.6.6.6 searches: # 指定搜索域列表，当只提供主机名而不是完整域名时，这些搜索域会被用来补全完整域名 - svc.cluster.local - cluster.local - ilinux.io options: - name: ndots value: &quot;5&quot; # 如果域名中点的数量少于ndots指定的值，DNS客户端将按照search选项中指定的搜索域名后缀顺序，依次尝试将这些后缀加到域名的末尾，并进行DNS查询。如果本身就有五个点，则直接搜索，一个完整的fqdn域名正好5个点：kube-dns.kube-system.svc.cluster.local. 末尾还有一个点 - name: edns0 # 启用选项edns0，这通常用于启用DNS扩展协议 8 CoreDNS问题排查思路如果 nslookup 命令执行失败，按照下面的思路进行排查 检查服务是否在同一名称空间下，不在同一名称空间要指定该服务的名称空间 先检查本地的 DNS 配置：查看 resolv.conf 文件的内容，验证 search 和 nameserver 的配置是否有错 检查 DNS Pod 是否运行：使用 kubectl get pods 命令来验证 DNS Pod 是否运行 检查 DNS Pod 里的错误：使用 kubectl logs 命令来查看 DNS 容器的日志信息 检查是否启用了 DNS 服务：使用 kubectl get service 命令来检查 DNS 服务是否已经启用 验证 DNS 的端点是否公开：使用 kubectl get endpoints 命令来验证 DNS 的端点是否公开 检查 DNS 查询有被接收或者执行：使用 kubectl -n kube-system edit configmap coredns 给 CoreDNS 的配置文件（也叫 Corefile）添加 log 插件来检查查询是否被正确接收 检查 CoreDNS 是否有足够的权限 使用 kubectl describe clusterrole system:coredns -n kube-system 查看CoreDNS 是否有足够的权限 123456789#预期输出PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- endpoints [] [] [list watch] namespaces [] [] [list watch] pods [] [] [list watch] services [] [] [list watch] endpointslices.discovery.k8s.io [] [] [list watch] 示例错误信息：2022-03-18T07:12:15.699431183Z [INFO] 10.96.144.227:52299 - 3686 &quot;A IN serverproxy.contoso.net.cluster.local. udp 52 false 512&quot; SERVFAIL qr,aa,rd 145 0.000091221s 如果缺少任何权限，请编辑 ClusterRole 来添加它们：kubectl edit clusterrole system:coredns -n kube-system","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"CoreDNS","slug":"CoreDNS","permalink":"https://aquapluto.github.io/tags/CoreDNS/"}]},{"title":"Ingress","slug":"Kubernetes/service-loadbalancer/ingress","date":"2025-09-12T04:38:00.000Z","updated":"2025-09-12T05:35:10.821Z","comments":true,"path":"Kubernetes/service-loadbalancer/ingress/","permalink":"https://aquapluto.github.io/Kubernetes/service-loadbalancer/ingress/","excerpt":"","text":"1 在k8s中暴漏微服务微服务：整个软件由很多个应用构成，每个应用都单独部署、单独监听端口，对外提供API接口 微服务场景下，每个微服务都有一个或多个pod副本，意味着每个微服务都需要关联一个svc，每个微服务都负责做不同的事情，而Service作为四层负载均衡，不支持基于URL等机制对HTTP&#x2F;HTTPS协议进行高级路由、超时&#x2F;重试、基于流量的灰度等高级流量治理机制，难以将多个Service流量统一管理，所以需要引入一个七层负载均衡来做一个统一的流量分发，ingress的本质就是在k8s集群内用来做七层分发（不同的URL路径转发给不同的微服务） 流程：ingress的svc —–&gt; ingress的pod（nginx七层转发）—–&gt; 代理到一个微服务的svc—–&gt; 微服务的pod 2 Ingress和Ingress ControllerIngree官方文档 Ingress-Controllers官方文档 Ingress 是Kubernetes上的标准API资源类型之一，由Ingress API和Ingress Controller共同组成 前者负责以k8s标准的资源格式定义流量调度、路由等规则 后者负责监视（watch）Ingress并生成自身的配置，并据此完成流量转发 如果我们需要同时支持Nginx和HAProxy，但是它们的配置格式不一样，怎么办？就是需要Ingress来定义代理和通用格式的配置，注意仅定义了抽象路由配置信息，只是元数据。至于怎么适配到Nginx和HAProxy，需要由相应的Ingress Controller动态加载 Ingress Controller Kubernetes的非内置的控制器，反向代理服务程序，需要额外选择部署（用户空间的代理进程） 实现方案有很多，包括Ingress-Nginx、HAProxy、Envoy、Traefik、Gloo、Contour和Kong等 通常以Pod形式运行于Kubernetes集群之上 一般应该由专用的LB Service负责为其接入集群外部流量 Kubernetes支持同时部署二个或以上的数量的Ingress Controller，所以在创建Ingress资源时，应该指明其所属的Ingress Controller 监视API Server上Ingress资源的变动，并将其反映至自身的配置文件中，而后动态生效 Ingress资源配置可通过特定的annotation或spec中嵌套专有的字段指明期望加载该资源的Ingress Controller 专用的annotation：kubernetes.io&#x2F;ingress.class v1.18版本起，Ingress资源配置中增添了新字段：ingressClassName，引用的IngressClass是一种特定的资源类型 2.1 暴漏Ingress的SVCIngress通常是跑在pod里的，而不会裸部署在集群外，因为要解决两点问题 配置中转发给某个svc最好用其fqdn域名而不是ip —&gt; 默认使用k8s的coredns ingress的软件需要能够被管理起来能够自愈 —&gt; 引入k8s的控制器资源来管理 外部需要访问到，就需要借助于Service资源来发现后端端点 NodePort + 集群外自建的负载均衡 LoadBalaner + 云平台提供的现成的负载均衡 2.2 引入Ingress的完整访问流程 2.3 Ingress的部署模式按照是否需要为ingress的pod创建svc来区分，可以分为两大方案 需要创建，非hostNetwork网络模式（左图） depoyment来部署ingress的pod + svc（type为LoadBalancer） depoyment来部署ingress的pod + svc（type为NodePort） 不需要创建，用hostNetwork网络模式，即共享宿主机的网络名称空间，转发路径更短，效率更高，但这样一个节点就不能部署多个Ingress（右图） Daemonset来部署ingress的pod DaemonSet会让每个节点都部署一个，如果不想，还要使用标签过滤，或者打上污点 2.4 部署Ingress Controller以Kubernetes社区维护的 Ingress-Nginx 为例 1https://kubernetes.github.io/ingress-nginx/deploy/ 范例：使用depoyment部署。根据环境选择，例如，对于kubeadm部署的集群，我们可以选择“Bare-metal”，以v1.12.0版为例 12345678910111213141516171819[root@master1 ~]#kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.12.0/deploy/static/provider/cloud/deploy.yaml# 说明：使用LoadBalancer或是NodePort暴露，在name: ingress-nginx-controller的service中spec.type改[root@master1 ~]#kubectl get pods -n ingress-nginxNAME READY STATUS RESTARTS AGEingress-nginx-admission-create-bxr2m 0/1 Completed 0 63mingress-nginx-admission-patch-nr66d 0/1 Completed 0 63mingress-nginx-controller-7659d6469c-vwkp9 1/1 Running 0 63m[root@master1 ~]#kubectl get svc -n ingress-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx-controller LoadBalancer 10.109.146.217 10.0.0.52 80:30195/TCP,443:32252/TCP 63mingress-nginx-controller-admission ClusterIP 10.98.69.136 &lt;none&gt; 443/TCP 63m#默认会有一个nginx的ingress-controller[root@master1 ~]#kubectl get ingressclassesNAME CONTROLLER PARAMETERS AGEnginx k8s.io/ingress-nginx &lt;none&gt; 129m 范例：使用Daemonset部署 123# 因为我们用的hostNetwork模式暴漏服务，所以根本不需要service资源了# deploy.yaml有两个service，注释掉name: ingress-nginx-controller# ingress-nginx-controller-admission不要注释 12# 将控制器类型改为Daemonset，并将Daemonset不适用的字段注释# 在spec字段加上hostNetwork: true 更多的Ingress Controller Ingress Controller 说明 Kong 基于 NGINX 构建，并增加了扩展其功能的 Lua 模块 Traefik 为微服务请求及其动态环境的路由做代理，支持热加载 HAProxy 提供“软”配置更新（无流量损失），支持完全自定义配置文件模板 Contour 基于 Envoy，可以通过 CRD（IngressRoute）管理 Ingress 资源 Istio 服务网格解决方案，不仅可以管理所有传入的外部流量，还可以控制集群内部的所有流量 Gloo 为后端实现微服务、无服务器功能和遗留应用的混合应用路由流量 如果你的k8s中安装了多个ingress控制器，在创建ingress对象时，你就需要用ingressClassName来选择你想要的ingress控制器 如果一个集群中可能有多个 Ingress 控制器，所以我们还可以将一个特定的 IngressClass 对象标记为集群默认是 Ingress 类。只需要将一个 IngressClass 资源的 ingressclass.kubernetes.io/is-default-class 注解设置为 true 即可，这样未指定 ingressClassName 字段的 Ingress 就会使用这个默认的 IngressClass。 12345apiVersion: networking.k8s.io/v1kind: IngressClassmetadata: annotations: ingressclass.kubernetes.io/is-default-class: &quot;true&quot; 如果集群中有多个 IngressClass 被标记为默认，准入控制器将阻止创建新的未指定 ingressClassName 的 Ingress 对象。最好的方式还是确保集群中最多只能有一个 IngressClass 被标记为默认。 2.5 创建 Ingress API1234567891011kubectl create ingress NAME --rule=host/path=service:port[,tls[=secret]]--annotation=[] # 提供注解，格式为“annotation=value”--rule=[] # 代理规则，这里的service只提供服务发现功能 # host表示要发往的主机 # path表示要发往主机的路径 # service:port表示该主机的服务和端口 # 一般是pod运行的服务，不过我们这里指定service，让service去发现后端pod --class=‘’ # 该Ingress适配的IngressClass，比如nginx，haproxy Ingress的yaml文件中，配置有几个重要的属性 1234567891011121314151617181920# 用来控制启用何种功能，例如nginx.ingress.kubernetes.io/use-regex: &quot;true&quot;，最终是在生成nginx配置中，会采用location ~来表示正则匹配metadata.annotations# 用于定义https密钥、证书tls# 选择你想要的ingress控制器ingressClassName# 用于指定请求路由转发规则rules host # 提供了host域名，则rules则会匹配该域名的相关请求，此外host主机名可以是精确匹配（例如foo.bar.com）或者使用通配符来匹配（例如 *.foo.com） http.paths # 定义访问的路径列表 backend service # 定义后端的Service服务，与路由规则中host和path匹配的流量会将发送到对应的backend后端去 resource # resource是当前Ingress对象命名空间下引用的另外一个Kubernetes资源对象，resource后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端，需要注意的是resource与Service配置是互斥的，只能配置一个 pathType # 定义路径类型 Exact # 路径必须与指定的path完全一致 Prefix # 任何以指定的path为前缀的路径都会匹配 Implementationspecific # 具体行为由Ingress Contro11er决定，比如nginx或traefik，每个控制器可能会有它自己对路径匹配的解释和实现方式 范例：所有的 /icons 请求会被路由到同命名空间下的名为 icon-assets 的 StorageBucket 资源中去进行处理 123456789101112131415apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: ingress-resource-backendspec: rules: - http: paths: - path: /icons pathType: ImplementationSpecific backend: resource: apiGroup: k8s.example.com kind: StorageBucket name: icon-assets 范例：集群外部客户端通过ingress访问单个后端服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798[root@master1 ~]#kubectl create namespace dev[root@master1 ~]#kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 --replicas=2 -n dev[root@master1 ~]#kubectl create service clusterip demoapp --tcp=80:80 -n dev [root@master1 ~]#kubectl get svc -n devNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp ClusterIP 10.104.63.56 &lt;none&gt; 80/TCP 14s[root@master1 ~]#kubectl get ep -n devNAME ENDPOINTS AGEdemoapp 10.244.1.57:80,10.244.2.53:80 18s #后端pod列表# *表示起始于/的路径，不写*表示只能是/自身，或者也可以指定路径，比如/v10[root@master1 ~]#kubectl create ingress demoapp --rule=&quot;demoapp.wu.com/*&quot;=demoapp:80 --class=nginx -n dev --dry-run=client -o yaml &gt; ingress-demoapp.yaml[root@master1 ~]#vim ingress-demoapp.yamlapiVersion: networking.k8s.io/v1 # 可以查看详情kubectl explain ingresskind: Ingressmetadata: creationTimestamp: null name: demoapp namespace: dev annotations: nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; # 开启use-regex，启用path的正则匹配spec: ingressClassName: nginx rules: # 定义基于主机名的路由规则 - host: demoapp.wu.com # 只有请求头中 Host: demoapp.wu.com 的流量才会被匹配 http: paths: # 不同path转发到不同端口 - backend: service: name: demoapp # 起始于/的路径的流量转发到demoapp的clusterIP的80端口 port: number: 80 path: / pathType: Prefix # 起始于/的路径status: loadBalancer: &#123;&#125;[root@master1 ~]#kubectl apply -f ingress-demoapp.yaml#demoapp.wu.com接收外部流量，发往demoapp这个服务[root@master1 ~]#kubectl get ingress -n devNAME CLASS HOSTS ADDRESS PORTS AGEdemoapp nginx demoapp.wu.com 80 7s[root@master1 ~]#kubectl describe ingress demoapp -n devName: demoappLabels: &lt;none&gt;Namespace: devAddress: 10.0.0.52 # 这个地址是Ingress Controller的externalIP，也就是LB Service，demoapp.wu.com要解析为这个地址Ingress Class: nginxDefault backend: &lt;default&gt;Rules: Host Path Backends ---- ---- -------- demoapp.wu.com / demoapp:80 (10.244.1.57:80,10.244.2.53:80) #service发现后端的podAnnotations: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 31s (x2 over 85s) nginx-ingress-controller Scheduled for sync[root@master1 ~]#kubectl exec -it -n ingress-nginx ingress-nginx-controller-7659d6469c-vwkp9 -- /bin/sh/etc/nginx $ bashingress-nginx-controller-7659d6469c-vwkp9:/etc/nginx$ cat nginx.conf#由Ingress-Nginx通过Ingress配置自动转换并生效的server &#123; server_name demoapp.wu.com ; listen 80 ; listen 443 ssl; location / &#123; set $namespace &quot;dev&quot;; set $ingress_name &quot;demoapp&quot;; set $service_name &quot;demoapp&quot;; set $service_port &quot;80&quot;; set $location_path &quot;/&quot;; &#125;&#125;[root@rocky8 ~]#vim /etc/hosts10.0.0.52 demoapp.wu.com#client --&gt; LB Service --&gt; Ingress Controller Pod --&gt; Upstream Service --&gt; Upstream Pod [root@rocky8 ~]#curl demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.2.52, ServerName: demoapp-7c58cd6bb-h6vqs, ServerIP: 10.244.1.57![root@rocky8 ~]#curl demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.2.52, ServerName: demoapp-7c58cd6bb-k7w4j, ServerIP: 10.244.2.53!#10.244.2.52是Ingress Controller Pod的地址[root@master1 ~]#kubectl logs -n dev demoapp-7c58cd6bb-h6vqs * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)10.244.2.52 - - [27/Mar/2025 09:20:16] &quot;GET / HTTP/1.1&quot; 200 - 3 Ingress的类型3.1 Simple fanout在同一个FQDN下通过不同的URI完成不同应用间的流量分发 基于单个虚拟主机接收多个应用的流量 常用于将流量分发至同一个应用下的多个不同子应用，同一个应用内的流量由调度算法分发至该应用的各后端端点 不需要为每个应用配置专用的域名 基于URI方式代理不同应用的请求时，后端应用的URI若与代理时使用的URI不同，则需要启用URL Rewrite完成URI的重写，Ingress-Nginx支持使用 “annotation nginx.ingress.kubernetes.io/rewrite-target” 注解 范例：集群外部客户端通过ingress访问多个后端服务 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108[root@master1 ~]#kubectl create deployment nginx --image=nginx:1.22-alpine --replicas=2 -n dev[root@master1 ~]#kubectl create service clusterip nginx --tcp=80:80 -n dev[root@master1 ~]#kubectl get ep -n devNAME ENDPOINTS AGEdemoapp 10.244.1.57:80,10.244.2.53:80 56mnginx 10.244.1.58:80,10.244.2.54:80 10s#客户端带上/demoapp或/nginx，是为了标识将这个请求应该转发给哪个后端服务#但有个问题：location映射的时候，请求前面带上/demoapp或/nginx，代理的时候也会带上#即client --&gt; web.wu.com/demoapp --&gt; demoapp:80/demoapp#而我们默认代理到后端是demoapp:80/，或者其他已经开发的接口路径，除非你后端接口确实有/demoapp或/nginx#所以Ingress-Nginx支持使用“annotation nginx.ingress.kubernetes.io/rewrite-target”注解进行URI的重定向#下面例子是仅将请求的path简单重定向为另一个path[root@master1 ~]#kubectl create ingress demoapp-nginx --rule=&quot;web.wu.com/demoapp/*&quot;=demoapp:80 --rule=&quot;web.wu.com/nginx/*&quot;=nginx:80 --class=nginx --annotation nginx.ingress.kubernetes.io/rewrite-target=&quot;/&quot; -n dev --dry-run=client -o yaml &gt; ingress-simple-fanout.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: / creationTimestamp: null name: demoapp-nginx namespace: devspec: ingressClassName: nginx rules: - host: web.wu.com http: paths: - backend: service: name: demoapp port: number: 80 path: /demoapp/ pathType: Prefix - backend: service: name: nginx port: number: 80 path: /nginx/ pathType: Prefixstatus: loadBalancer: &#123;&#125; [root@master1 ~]#kubectl apply -f ingress-simple-fanout.yaml[root@master1 ~]#kubectl describe ingress -n dev demoapp-nginx Name: demoapp-nginxLabels: &lt;none&gt;Namespace: devAddress: 10.0.0.52Ingress Class: nginxDefault backend: &lt;default&gt;Rules: Host Path Backends ---- ---- -------- web.wu.com /demoapp/ demoapp:80 (10.244.1.57:80,10.244.2.53:80) /nginx/ nginx:80 (10.244.1.58:80,10.244.2.54:80)Annotations: nginx.ingress.kubernetes.io/rewrite-target: / #URL重定向Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Sync 70s (x2 over 103s) nginx-ingress-controller Scheduled for sync[root@rocky8 ~]#vim /etc/hosts10.0.0.52 demoapp.wu.com web.wu.com[root@rocky8 ~]#curl web.wu.com/demoapp/iKubernetes demoapp v1.0 !! ClientIP: 10.244.2.52, ServerName: demoapp-7c58cd6bb-h6vqs, ServerIP: 10.244.1.57![root@rocky8 ~]#curl web.wu.com/nginx/&lt;title&gt;Welcome to nginx!&lt;/title&gt;[root@rocky8 ~]#curl web.wu.com/demoapp/versioniKubernetes demoapp v1.0 !! ClientIP: 10.244.2.52, ServerName: demoapp-7c58cd6bb-h6vqs, ServerIP: 10.244.1.57!root@rocky8 ~]#curl web.wu.com/nginx/version&lt;title&gt;Welcome to nginx!&lt;/title&gt;#上面有个问题：/*的话，什么路径都能匹配到根上#使用正则表达式，移除客户请求时使用的path前缀[root@master1 ~]#kubectl create ingress demoapp-nginx --rule=&quot;web.wu.com/demoapp(/|$)(.*)&quot;=demoapp:80 --rule=&quot;web.wu.com/nginx(/|$)(.*)&quot;=nginx:80 --class=nginx --annotation nginx.ingress.kubernetes.io/rewrite-target=&#x27;/$2&#x27; -n dev --dry-run=client -o yaml &gt; ingress-simple-fanout.yaml#默认这么写pathType为Exact，Exact不支持正则表达式，会报错#需要手动指定pathType为ImplementationSpecific，表示由具体的Ingress Controller决定如何处理路径（NGINX支持正则表达式）[root@master1 ~]#kubectl apply -f ingress-simple-fanout.yaml# client --&gt; web.wu.com/demoapp --&gt; demoapp:80/[root@rocky8 ~]#curl web.wu.com/demoappiKubernetes demoapp v1.0 !! ClientIP: 10.244.2.52, ServerName: demoapp-7c58cd6bb-k7w4j, ServerIP: 10.244.2.53!# client --&gt; web.wu.com/demoapp/version --&gt; demoapp:80/version[root@rocky8 ~]#curl web.wu.com/demoapp/version&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 3.2 Final//EN&quot;&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;h1&gt;Not Found&lt;/h1&gt;&lt;p&gt;The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.&lt;/p&gt;# client --&gt; web.wu.com/demoapp/hostname --&gt; demoapp:80/hostname[root@rocky8 ~]#curl web.wu.com/demoapp/hostnameServerName: demoapp-7c58cd6bb-k7w4j 补充：Ingress还可以设置白名单 12345apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/whitelist-source-range: &quot;192.168.71.12/32, 192.168.71.13/32&quot; 3.2 Name based virtual hosting为每个应用使用一个专有的主机名，并基于这些名称完成不同应用间的流量转发 每个FQDN对应于Ingress Controller上的一个虚拟主机的定义 同一组内的应用的流量，由Ingress Controller根据调度算法完成请求调度 基于FQDN名称代理不同应用的请求时，需要事先准备好多个域名，且确保对这些域名的解析能够到达Ingress Controller，也支持URL重写 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[root@master1 ~]#kubectl create ingress demoapp-nginx --rule=&quot;demoapp.wu.com/*&quot;=demoapp:80 --rule=&quot;nginx.wu.com/*&quot;=nginx:80 --class=nginx -n dev --dry-run=client -o yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: creationTimestamp: null name: demoapp-nginx namespace: devspec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp port: number: 80 path: / pathType: Prefix - host: nginx.wu.com http: paths: - backend: service: name: nginx port: number: 80 path: / pathType: Prefixstatus: loadBalancer: &#123;&#125; [root@master1 ~]#kubectl create ingress demoapp-nginx --rule=&quot;demoapp.wu.com/(.*)&quot;=demoapp:80 --rule=&quot;nginx.wu.com/(.*)&quot;=nginx:80 --class=nginx --annotation nginx.ingress.kubernetes.io/rewrite-target=&#x27;/$1&#x27; -n dev --dry-run=client -o yaml &gt; ingress-name-based-virtual-hosting.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 creationTimestamp: null name: demoapp-nginx namespace: devspec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp port: number: 80 path: /(.*) pathType: Exact #改成ImplementationSpecific - host: nginx.wu.com http: paths: - backend: service: name: nginx port: number: 80 path: /(.*) pathType: Exactstatus: loadBalancer: &#123;&#125;[root@rocky8 ~]#vim /etc/hosts10.0.0.52 demoapp.wu.com nginx.wu.com[root@rocky8 ~]#curl demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.2.52, ServerName: demoapp-7c58cd6bb-h6vqs, ServerIP: 10.244.1.57![root@rocky8 ~]#curl demoapp.wu.com/hostnameServerName: demoapp-7c58cd6bb-k7w4j[root@rocky8 ~]#curl nginx.wu.com&lt;title&gt;Welcome to nginx!&lt;/title&gt;[root@rocky8 ~]#curl nginx.wu.com/version&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt; 3.3 TLSIngress也可以提供TLS通信机制，但仅限于443&#x2F;TCP端口 若TLS配置部分指定了不同的主机，则它们会根据通过SNI TLS扩展指定的主机名 前提：Ingress控制器支持SNI在同一端口上复用 TLS Secret必须包含名为tls.crt和密钥tls.key，它们分别含有TLS的证书和私钥 基于TLS的Ingress要求事先准备好专用的 “kubernetes.io/tls” 类型的Secret对象，也支持URL重写 当后端pod服务就已经使用了https协议，要开放到互联网上去，使用七层代理会有较高要求，如果不是为了暴漏微服务，可以直接使用四层代理，而不太建议使用七层代理 七层代理：Nginx作为前端的入口点，接收来自客户端的HTTPS请求，解密这些请求（即执行TLS卸载），然后重新加密请求发送给后端服务。这种方法配置较为复杂，因为它涉及到两次加密解密过程，并且需要正确管理证书。 四层代理：TLS穿透，Nginx不会对传输的数据进行任何解密或修改，而是直接将客户端的请求（包括其TLS握手信息）原封不动地转发给后端服务。这种方式允许客户端与后端服务直接建立TLS会话 范例：部署wordpress(使用自带apache的Image)，把wordpress通过Ingress Nginx发布到集群外部 文件来源：https://github.com/AquaPluto/kubernetes/tree/main/wordpress 1234567891011121314151617181920[root@master1 wordpress]#lsmysql mysql-ephemeral nginx README.md wordpress wordpress-apache-ephemeral[root@master1 wordpress]#kubectl create namespace blog[root@master1 wordpress]#kubectl apply -f mysql-ephemeral/ -n blog[root@master1 wordpress]#kubectl apply -f wordpress-apache-ephemeral/ -n blog#创建私钥[root@master1 wordpress]#(umask 077; openssl genrsa -out blog.wu.com.key 2048)#创建自签名证书[root@master1 wordpress]#openssl req -new -x509 -key blog.wu.com.key -out blog.wu.com.crt -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=blog.wu.com[root@master1 wordpress]#kubectl create secret tls blog.wu.com --cert=./blog.wu.com.crt --key=./blog.wu.com.key -n blog#创建常规的虚拟主机代理规则，同时将该主机定义为TLS类型#tls指定的是用来配置https主机的证书和私钥的名字，即secret.tls的名字[root@master1 wordpress]#kubectl create ingress wordpress --rule=&#x27;blog.wu.com/*=wordpress:80,tls=blog.wu.com&#x27; --class=nginx -n blog做好域名解析后，浏览器访问blog.wu.com 注意：启用tls后，该域名下的所有URI默认为强制将http请求跳转至https，若不希望使用该功能，可以使用如下注解选项 1--annotation nginx.ingress.kubernetes.io/ssl-redirect=false 免费证书网站：Let’s Encrypt 4 基于Ingress Nginx进行金丝雀（灰度）发布Ingress-Nginx支持配置Ingress Annotations来实现不同场景下的灰度发布和测试，它能够满足金丝雀发布、蓝绿部署与A&#x2F;B测试等不同的业务场景，而Service作为四层代理，无法精准控制新旧流量分发的比例，而Ingress-Nginx可以精准控制 4.1 基于Ingress Nginx的Canary规则Ingress Nginx Annotations支持的Canary规则 nginx.ingress.kubernetes.io/canary-by-header：基于该Annotation中指定Request Header进行流量切分，适用于灰度发布以及A&#x2F;B测试 在请求报文中，若存在该Header且其值为always时，请求将会被发送到Canary版本 若存在该Header且其值为never时，请求将不会被发送至Canary版本 对于任何其它值，将忽略该Annotation指定的Header，并通过优先级将请求与其他金丝雀规则进行优先级的比较 nginx.ingress.kubernetes.io/canary-by-header-value：基于该Annotation中指定的Request Header的值进行流量切分，标头名称则由前一个Annotation（nginx.ingress.kubernetes.io/canary-by-header）进行指定 请求报文中存在指定的标头，且其值与该Annotation的值匹配时，它将被路由到Canary版本 对于任何其它值，将忽略该Annotation nginx.ingress.kubernetes.io/canary-by-header-pattern 同canary-by-header-value的功能类似，但该Annotation基于正则表达式匹配Request Header的值 若该Annotation与canary-by-header-value同时存在，则该Annotation会被忽略 nginx.ingress.kubernetes.io/canary-weight：基于服务权重进行流量切分，适用于蓝绿部署或灰度发布，权重范围0 - 100按百分比将请求路由到Canary Ingress中指定的服务 权重为 0 意味着该金丝雀规则不会向Canary入口的服务发送任何请求 权重为100意味着所有请求都将被发送到 Canary 入口 nginx.ingress.kubernetes.io/canary-by-cookie：基于 cookie 的流量切分，适用于灰度发布与 A&#x2F;B 测试 cookie的值设置为always时，它将被路由到Canary入口 cookie的值设置为never时，请求不会被发送到Canary入口 对于任何其他值，将忽略 cookie 并将请求与其他金丝雀规则进行优先级的比较 Canary规则会按特定的次序进行评估：canary-by-header ---&gt; canary-by-cookie ---&gt; canary-weight Canary规则策略 基于服务权重的流量切分：假如在生产上已经运行了A应用对外提供服务，此时开发修复了一些Bug，需要发布A1版本将其上线，但是我们又不希望直接的将所有流量接入到新的A1版本，而是希望将10%的流量进入到A1中，待A1稳定后，才会将所有流量接入进来，再下线原来的A版本。 基于用户请求头Header的流量切分：由于基于权重的发布场景比较粗糙，无法限制具体的用户访问行为。我们有时候会有这样的需求，比如我们有北京、上海、深圳这三个地区的用户，已经有A版本的应用为这三个地区提供服务，由于更新了需求，我们需要发布A1应用，但是我们不想所有地区都访问A1应用，而是希望只有深圳的用户可以访问，待深圳地区反馈没问题后，才开放其他地区。 下列示例的文件来源：https://github.com/AquaPluto/kubernetes/tree/main/Ingress-nginx/ingress-canary-demo 4.2 基于标头的金丝雀发布范例：不自定义标头的值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@master1 ingress-canary-demo]#kubectl apply -f deploy-demoap-v1_0.yaml -f deploy-demoap-v1_1.yaml[root@master1 ingress-canary-demo]#kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdemoapp-v10-7f44bbc8c5-zfcd8 1/1 Running 0 5m41s 10.244.2.61 node1.wu.org &lt;none&gt; &lt;none&gt;demoapp-v11-5f64448cd5-htvr6 1/1 Running 0 5m41s 10.244.1.74 node2.wu.org &lt;none&gt; &lt;none&gt;[root@master1 ingress-canary-demo]#vim 01-ingress-demoapp.yaml apiVersion: networking.k8s.io/v1kind: Ingressmetadata: name: demoappspec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp-v10 port: number: 80 path: / pathType: Prefix [root@master1 ingress-canary-demo]#kubectl apply -f 01-ingress-demoapp.yaml[root@rocky8 ~]#curl demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61![root@master1 ingress-canary-demo]#vim 02-canary-by-header.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/canary: &quot;true&quot; #表示启用金丝雀发布，固定的 nginx.ingress.kubernetes.io/canary-by-header: &quot;X-Canary&quot; name: demoapp-canary-by-headerspec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp-v11 port: number: 80 path: / pathType: Prefix [root@master1 ingress-canary-demo]#kubectl apply -f 02-canary-by-header.yaml[root@rocky8 ~]#curl -H &quot;X-Canary: always&quot; demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v11-5f64448cd5-htvr6, ServerIP: 10.244.1.74![root@rocky8 ~]#curl -H &quot;X-Canary: never&quot; demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61! 范例：自定义标头的值 1234567891011121314151617181920212223242526272829303132#不然会有冲突[root@master1 ingress-canary-demo]#kubectl delete -f 02-canary-by-header.yaml[root@master1 ingress-canary-demo]#vim 03-canary-by-header-value.yaml apiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/canary: &quot;true&quot; nginx.ingress.kubernetes.io/canary-by-header: &quot;IsVIP&quot; nginx.ingress.kubernetes.io/canary-by-header-value: &quot;false&quot; name: demoapp-canary-by-header-valuespec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp-v11 port: number: 80 path: / pathType: Prefix [root@master1 ingress-canary-demo]#kubectl apply -f 03-canary-by-header-value.yaml[root@rocky8 ~]#curl -H &quot;IsVIP: false&quot; demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v11-5f64448cd5-htvr6, ServerIP: 10.244.1.74![root@rocky8 ~]#curl -H &quot;IsVIP: true&quot; demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61! 4.3 基于权重的金丝雀发布慢慢的可以把新版本的权重加大，直到完全没问题，就可以把权重改为100了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#要删除，虽然有优先级，但是如果存在这个规则，那么必须带有header，只要header不是IsVIP: false的，才会被weight匹配#不带标头的话，就都会被01-ingress-demoapp.yaml这个规则所匹配[root@master1 ingress-canary-demo]#kubectl delete -f 03-canary-by-header-value.yaml [root@master1 ingress-canary-demo]#vim 05-canary-by-weight.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/canary: &quot;true&quot; nginx.ingress.kubernetes.io/canary-weight: &quot;10&quot; name: demoapp-canary-by-weightspec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp-v11 port: number: 80 path: / pathType: Prefix [root@master1 ingress-canary-demo]#kubectl apply -f 05-canary-by-weight.yaml[root@rocky8 ~]#while true;do curl demoapp.wu.com;sleep 1;doneiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v11-5f64448cd5-htvr6, ServerIP: 10.244.1.74!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v11-5f64448cd5-htvr6, ServerIP: 10.244.1.74!#测试一下不删除03-canary-by-header-value.yaml的结果[root@rocky8 ~]#while true;do curl -H &quot;IsVIP: true&quot; demoapp.wu.com;sleep 1;doneiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v10-7f44bbc8c5-zfcd8, ServerIP: 10.244.2.61!iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v11-5f64448cd5-htvr6, ServerIP: 10.244.1.74! 4.4 基于cookie的金丝雀发布为了避免冲突，把之前的规则删除。如果三个类型的规则都存在，那么请求必须带上header和cookie，才会按照优先级匹配 123456789101112131415161718192021222324[root@master1 ingress-canary-demo]#vim 06-canary-by-cookie.yamlapiVersion: networking.k8s.io/v1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/canary: &quot;true&quot; nginx.ingress.kubernetes.io/canary-by-cookie: &quot;vip_user&quot; name: demoapp-canary-by-cookiespec: ingressClassName: nginx rules: - host: demoapp.wu.com http: paths: - backend: service: name: demoapp-v11 port: number: 80 path: / pathType: Prefix [root@rocky8 ~]#curl -b &quot;vip_user=always&quot; demoapp.wu.comiKubernetes demoapp v1.0 !! ClientIP: 10.244.1.70, ServerName: demoapp-v11-5f64448cd5-htvr6, ServerIP: 10.244.1.74!","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Ingress","slug":"Ingress","permalink":"https://aquapluto.github.io/tags/Ingress/"}]},{"title":"KVM","slug":"Virtualization/KVM","date":"2025-09-12T04:37:55.000Z","updated":"2025-09-13T13:55:56.814Z","comments":true,"path":"Virtualization/KVM/","permalink":"https://aquapluto.github.io/Virtualization/KVM/","excerpt":"","text":"1 虚拟化基础1.1 传统的物理机部署方案 IDC选择,如:联通,电信,世纪互联,鹏博士等 网络和存储规划 服务器选型及采购 服务器系统选择、系统安装、上架、配置网络 应用规划及部署 域名选择及注册,DNS配置名称解析 测试外网访问 传统数据中心面临的问题： 服务器和网络设备资源利用率过低，并且无法共享,导致资源浪费 据统计大部分数据中心中的服务器和网络设备的利用率仅在24%～30%之间，有的CPU利用率、硬盘利用率都在10%以下 资源分配后进行调整困难 资源分配不合理也是传统网络架构存在的问题，因为资源不能动态调配，分配出去的资源是固定的，不能随意添加或删除。 难以实现自动化 初始化成本高,服务器迁移和升级很繁琐，无法实现自动化 成本高昂，集群环境需要大量的服务器主机,硬件投入和后期维护管理成本巨大 1.2 虚拟化1.2.1 什么是虚拟化虚拟化（Virtualization）是一种资源分配和管理技术，是将计算机的各种实体资源,比如CPU、内存、磁盘空间、网络适配器等，进行抽象转换后虚拟的设备,可以实现灵活地分割、组合为一个或多个计算机配置环境，并还支持重新分割、重新组合，以达到最大化合理利用物理资源的目的。 参考资料: https://www.vmware.com/cn/solutions/virtualization.html 1.2.2 虚拟化优势虚拟化可以提高 IT 敏捷性、灵活性和可扩展性，同时大幅节约成本。更高的工作负载移动性、更高的性能和资源可用性、自动化运维 - 这些都是虚拟化的优势，虚拟化技术可以使 IT 部门更轻松地进行管理以及降低拥有成本和运维成本。其优势包括： 资源超分,如实际的物理内存只有128G,可以给虚拟机分配200G内存 降低资金成本和运维成本 最大限度减少或消除停机 提高 IT 部门的工作效率和响应能力 加快应用和资源的调配速度 提高业务连续性和灾难恢复能力 简化数据中心管理 减少资源,比如IP和端口的冲突 支持较旧的硬件和操作系统及应用 1.2.3 虚拟化类型1.2.3.1 服务器虚拟化服务器虚拟化支持在单个物理服务器上运行多个操作系统,每个操作系统作为虚拟机独立运行。 1.2.3.2 网络虚拟化通过软件定义网络(Software Defined Network，SDN)，即网络的创建不再依赖于物理设备，如公有云厂商支持用户自己通过配置界面创建新的网络，在 KVM、docker、kubernetes、openstack等虚拟化技术中都使用到了网络虚拟化 1.2.3.3 桌面虚拟化将桌面部署为托管的服务,使 IT 组织能够更快地响应不断变化的工作场所需求和新出现的机会。还可以将虚拟化桌面和应用快速、轻松地交付给分支机构、外包和远程员工以及使用 iPad 和 Android 平板电脑的移动员工。 Citrix 思杰公司在云计算虚拟化、虚拟桌面和远程接入技术领域的处于优势地位 1.2.3.4 应用虚拟化将软件应用通过网络实现虚拟化，比如 office 365,钉钉,企业微信 1.2.3.5 存储虚拟化将存储用软件虚拟化实现, 如 SAN&#x2F;NAS(NFS&#x2F;Samba)&#x2F;GlusterFS&#x2F;ceph等 1.2.3.6 库虚拟化在Linux上使用 wine来运行Windows 程序，在Mac系统使用CrossOver来运行Windows程序 1.2.3.7 容器虚技术当前比较火的虚拟化技术，典型代表: Docker、Containerd、Podman、Linux Container(LXC)、Pouch 1.3 虚拟机1.3.1 虚拟机虚拟计算机称为“虚拟机”(VM,Virtual Machine)，它是一种严密隔离且内含操作系统和应用的软件容器。每个虚拟机都是完全独立的。通过将多台虚拟机放置在一台物理计算机上，可仅在一台物理服务器或“主机”上运行多个操作系统和应用，名为“hypervisor”的精简软件层可将虚拟机与主机分离开来，并根据需要为每个虚拟机动态分配计算资源。 1.3.2 虚拟机的主要特性虚拟机具有以下特征，这些特征可提供多项优势 分区 可在一台物理机上运行多个操作系统 可在虚拟机之间分配系统资源。 隔离 可在硬件级别进行故障和安全隔离。 可利用高级资源控制功能保持性能 封装 可将虚拟机的完整状态保存到文件中。 移动和复制虚拟机就像移动和复制文件一样轻松 独立于硬件 可将任意虚拟机调配或迁移到任意物理服务器上 安装系统不会受硬件兼容性的影响 1.4 Hypervisor类型1.4.1 Hypervisor 介绍 Hypervisor是一种运行在基础物理服务器和操作系统之间的中间软件层，其可以允许多个操作系统和应用共享底层的内存、CPU、磁盘等物理硬件，也可叫做VMM（ virtual machine monitor），即虚拟机监视器。 Hypervisor是所有虚拟化技术的核心，非中断地支持多工作负载迁移的能力是Hypervisor的基本功能，当服务器启动并执行Hypervisor时，它会给每一台虚拟机分配适量的内存、CPU、网络和磁盘，并加载所有虚拟机的客户操作系统。 Hypervisor 允许多种操作系统在相同的物理系统中运行 Hypervisor 控制硬件并向来宾操作系统提供访问底层硬件的途径 Hypervisor 向来宾操作系统提供虚拟化的硬件 X86 CPU 的保护环 注意：CPU为了保证程序代码执行的安全性、多用户的独立性、保护OS的正常运行，提出了CPU执行状态的概念。这样能够限制不同程序之间的访问能力，避免一个程序获取另一个程序的内存数据造成数据混乱，同时也避免了程序错误的操作物理硬件。一般CPU都会划分为 用户态 和内核态 ，x86的CPU架构更是细分为了Ring3~0四种状态。 Ring3 用户态(User Mode)：运行在用户态的程序代码需要受到CPU的检查，用户态程序代码只能访问内存页表项中规定能被用户态程序代码访问的页面虚拟地址(受限的内存访问)，而且还只能访问任务描述符TSS中的I&#x2F;O Permission Bitmap中规定能被用户态程序代码访问的端口。甚至不能直接访问外围硬件设备、不能抢占CPU。所有的应用程序都运行在用户态上。当运行在用户态的Application需要调用只能被核心态代码直接访问的硬件设备时，CPU会通过特别的接口去调用核心态的代码，以此来实现Application对硬件设备的调用。如果用户态的Application直接调用硬件设备时，就会被Host OS捕捉到并触发异常，弹出警告窗口。 Ring0 核心态(Kernel Mode)：是Host OS Kernel运行的模式，运行在核心态的代码可以无限制的对系统内存、设备驱动程序、网卡接口、显卡接口等外围硬件设备进行访问。只有Host OS能够无限制的访问磁盘、键盘等外围硬件设备的数据，但是首先需要在Host OS上安装驱动程序 1.4.2 Hypervisor 分类1974年Gerald J.Popek和Robert P.Goldberg的文章“Formal Requirements forVirtualizable ThirdGeneration Architectures” 将Hypervisor分为两类: 1.4.2.1 类型 I : 裸金属型直接运行到物理机的Hypervisor上，这种架构搭建的虚拟化环境称为裸机虚拟化环境(Bare-Metal Hardware 12345678KVMXENvmware esxirhev hypervisorHyper-v Server#Redhat将KVM划分到类型I即裸机型：https://www.redhat.com/zh/topics/virtualization/what-is-KVM 1.4.2.2 类型 II : 宿主型即需要运行在具有虚拟化功能的操作系统上的Hypervisor，构建的是主机虚拟化环境(Hosted Virtualization) 1234vmware workstationMicrosoft Hyper-VVirtualBoxparalles desktop #Mac系统最强虚拟机技术 1.5 虚拟化技术分类1.5.1 模拟器&#x2F;软件仿真 通过软件模拟完整的硬件环境来虚拟化来宾平台。 可以模拟X86、ARM 、PowerPC等多种CPU 效率比较低 产品或方案:QEMU、Bochs、PearPC 模拟器：https://yuzu-emu.org/ 1.5.2 全虚拟机化 full virtualization &#x2F; 本地虚拟化 native virtualization不需要对GuestOS操作系统软件的源代码做任何的修改，就可以运行在这样的VMM中 在全虚拟化的虚拟平台中，GuestOS并不知道自己是一台虚拟机，它会认为自己就是运行在计算机物理硬件设备上的HostOS。全虚拟化的GuestOS具有完全的物理机特性。因为全虚拟化的VMM会将一个OS所能够操作的CPU、内存、外设等物理设备逻辑抽象成为虚拟CPU、虚拟内存、虚拟外设等虚拟设备后，再交由GuestOS来操作使用。这样的GuestOS会将底层硬件平台视为自己所有的，但是实际上，这些都是VMM为GuestOS制造了这种假象。 全虚拟化&#x2F;本地虚拟化不做CPU和内存模拟，只对CPU和内存做相应的分配等操作 全虚拟化又分为：软件辅助的全虚拟化 &amp; 硬件辅助的全虚拟化 1.5.2.1 软件辅助的全虚拟化 在Intel等CPU厂商还没有发布x86 CPU虚拟化技术之前，完全虚拟化都是通过软件辅助的方式来实现的。 代表技术: Vmware Workstation,QEMU,Virtual PC 当使用GuestOS的时候，不可避免的会调用GuestOS中的虚拟设备驱动程序 和核心调度程序来操作硬件设备。与HostOS的不同在于，HostOS运行在CPU的核心态中，这就表示HostOS可以直接对硬件设备进行操作。但GuestOS作为一个运行在CPU用户态中应用程序，不能够直接的操作硬件设备。 简而言之，软件辅助虚拟化能够成功的将所有在GuestOS中执行的系统内核特权指令进行捕获、翻译，使之成为只能对GuestOS生效的虚拟特权指令。之所以需要这么做的前提是因为CPU并不能准确的去判断一个特权指令到底是由GuestOS发出的还是由HostOS发出的，这样也就无法针对一个正确的OS去将这一个特权指令执行。 由于全虚拟化VMM会频繁的捕获这些核心态的和敏感的指令，将这些指令进行转换之后，再交给CPU执行。所以经过了转换，导致其效率低，但全虚拟化VMM应用程序的好处在于其不需要对GuestOS的核心源码做修改，所以全虚拟化的VMM可以安装绝大部分的OS(暂时来说只有已Linux、open soralis、BSD等几种OS开源了内核代码)。 直到后来CPU厂商们发布了能够判断特权指令归属的标准x86 CPU之后，迎来了硬件辅助全虚拟化。 1.5.2.2 硬件辅助的全虚拟化 HVM(Hardware Virtual Machine) 2005年Intel提出并开发了由CPU直接支持的虚拟化技术。这种虚拟化技术引入新的CPU运行模式和新的指令集，使得VMM和GuestOS运行于不同的模式下(VMM&#x3D;Root Mode;GuestOS&#x3D;Non-Root Mode)，GuestOS运行于受控模式，原来的一些敏感指令在受控模式下会全部陷入VMM，由VMM来实现模拟，这样就解决了部分非内核态敏感指令的陷入——模拟难题，而且模式切换时上下文的保存恢复由硬件来完成，这样就大大提高了陷入——模拟时上下文切换的效率 。该技术的引入使x86 CPU可以很容易地实现完全虚拟化。故皆被几乎所有之前分歧的各大流派所采用，包括KVM-x86，VMWare ESX Server 3，Xen 3.0 。 虚拟化CPU形成了新的CPU执行状态 Non-Root Mode 和 Root Mode .GuestOS运行在Non-Root Mode的Ring 0核心态中，这意味着GuestOS能够直接执行特权指令,而不再需要特权解除和陷入模拟机制。并且在硬件层上面紧接的就是虚拟化层的VMM，而不需要HostOS。这是因为在硬件辅助全虚拟化的VMM会以一种更具协作性的方式来实现虚拟化 —— 将虚拟化模块加载到HostOS的内核中，例如：KVM，KVM通过在HostOS内核中加载KVM Kernel Module来将HostOS转换成为一个VMM。所以此时VMM可以看作是HostOS，反之亦然。 硬件辅助全虚拟化主要使用了支持虚拟化功能的CPU进行支撑，CPU可以明确的分辨出来自GuestOS的特权指令，并针对GuestOS进行特权操作，而不会影响到HostOS。 硬件辅助全虚拟化需要物理硬件的支持，比如需要CPU必须支持并且打开虚拟化功能，例如 Intel 的 Intel VT-X&#x2F;EPT，AMD的AMD-V&#x2F;RVI，以在CPU 层面支持虚拟化功能和内存虚拟化技术 12345678全虚拟化软件(硬件辅助全虚拟化)：vmware esxiXen3.0KVMMicrosoft Hyper-Vvmware workstation #https://www.vmware.com/cn/products/workstation-pro.htmlVirtualBoxparalles desktop KVM 是硬件辅助的虚拟化技术 主要负责比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I&#x2F;O 虚拟化，两者合作各自发挥自身的功能 1.5.3 半虚拟化 para virtualization 半虚拟化是需要GuestOS协助的虚拟化。因为在半虚拟化VMM中运行的GuestOS，都需要将其内核源码进行都进过了特别的修改。 通过修改客户操作系统代码，将原来在物理机上执行的一些特权指令(主要是修改GuestOS指令集中的敏感指令和核心态指令)，修改成可以和VMM直接交互的方式，实现操作系统的定制化。这样，就不会有捕获异常、翻译和模拟的过程，性能损耗比较少。 半虚拟化VMM在处理敏感指令和内核态指令的流程上相对更简单一些。让HostOS在捕抓到GuestOS内核态指令或敏感指令时，HostOS也能够准确的判断出该指令是否属于GuestOS(GuestOS知道自己是虚拟机)。这样就可以高效的避免了上述问题。典型的半虚拟化软件有——Xen、KVM-PowerPC(简易指令集) 半虚拟化除了修改内核外还有另外一种实现方法——在每一个GuestOS中安装半虚拟化软件，如:VMTools、RHEVTools。 注意：若使用KVM运行Windows时，一定要安装半虚拟化驱动Tools，否则无法工作。现在主流的半虚拟化驱动是由IBM和redhat联合开发一个通用半虚拟机驱动virtio 。 半虚拟化要求guest OS 的内核是知道自己运行在虚拟化环境当中的，因此guest OS的系统架构必须和宿主机的系统架构相同，并且要求对guest OS的内核做相应的修改，因此半虚拟化只支持开源内核的系统，不支持闭源的系统，比较常见的半虚拟化就是早期版本的XEN，但是Xen 从其3.0 版本开始，可以支持利用硬件辅助虚拟化技术 2 KVM架构和部署2.1 KVM 概述2.1.1 KVM 介绍KVM（ Kernel-based Virtual Machine）是一个完整的虚拟化解决方案，适用于包含虚拟化扩展（IntelVT或AMD-V）的x86硬件上的Linux。目前也支持ARM等其它硬件平台. 它由可加载的内核模块kvm.ko组成，它提供核心虚拟化基础架构和处理器特定模块，kvm-intel.ko或kvm-amd.ko，KVM的用户空间组件包含在QEMU1.3后续版本中,KVM目前已成为学术界的主流 VMM (virtual machine monitor，虚拟机监视器，也称为 hypervisor)之一 KVM是开源软件，可运行多个运行未修改的Linux或Windows映像的虚拟机 依赖于HVM；Intel VT-x, AMD AMD-V 官网： https://www.Linux-kvm.org 红帽 kvm 介绍：https://www.redhat.com/zh/topics/virtualization/what-is-KVM RedHat虚拟化指南：https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_Linux/7/html/virtualization_getting_started_guide/index RedHat 创建虚拟机数量限制：https://access.redhat.com/articles/rhel-kvm-limits 2.1.2 KVM架构KVM 是基于虚拟化扩展（Intel VT 或者 AMD-V）的 X86 硬件的开源的 Linux 原生的全虚拟化解决方案。KVM 中，虚拟机被实现为常规的 Linux 进程，由标准 Linux 调度程序进行调度；虚机的每个虚拟CPU 被实现为一个常规的 Linux 进程。这使得 KVM 能够使用 Linux 内核的已有功能。 但是，KVM 本身不执行任何硬件模拟，需要客户空间程序通过 &#x2F;dev&#x2F;kvm 接口设置一个客户机虚拟服务器的地址空间，向它提供模拟的 I&#x2F;O，并将它的视频显示映射回宿主的显示屏。目前这个应用程序使用QEMU。 2.1.2.1 KVM 体系结构 KVM： 初始化CPU硬件,打开虚拟机模式,负责CPU,内存,中断控制器,时钟. 由内核模块kvm_xxx.ko实现，工作于hypervisor，设备&#x2F;dev&#x2F;kvm，是一个字符设备，在用户空间可通过ioctl()系统调用来完成VM创建、启动，为VM分配内存、读写VCPU的寄存器、向VCPU注入中断、时钟等管理功能 QEMU进程：工作于用户空间，主要用于实现模拟IO设备,如显卡，网卡，硬盘等， qemu-kvm进程：工作于用户空间，用于实现一个虚拟机实例 Libvirt：提供统一API，守护进程libvirtd和相关工具，如:virsh，virt-manager等 2.1.2.2 KVM 模块载入后的系统的运行模式内核模式：HostOS执行I&#x2F;O类操作，或其它的特殊指令的操作；称作内核模式 用户模式：GuestOS的I&#x2F;O类操作 来宾模式：GuestOS非I&#x2F;O类操作，称作虚拟机的用户模式更贴切 2.1.3 KVM 集中管理与控制KVM是运行在单机的系统，需要其它软件实现跨主机的统一的管理。常见的虚拟化管理平台如下： http://www.Linux-kvm.org/page/Management_Tools oVirt 功能强大，是Redhat虚拟化管理平台RHEV的开源版本。 http://www.ovirt.org/ WebVirtMgr https://www.webvirtmgr.net virt-manager的Web模式的替代品 OpenStack 最主流的开源虚拟化管理平台 Proxmox virtualization environment 简称PVE，是一个开源免费的基于Linux的企业级虚拟化方案，功能不输专业收费的VMware。是一个完整的企业虚拟化开源平台。借助内置的Web界面，您可以轻松管理VM和容器，软件定义的存储和网络，高可用性集群以及单个解决方案上的多个开箱即用工具。 2.2 宿主机环境准备KVM需要宿主机CPU必须支持虚拟化功能，因此如果是在vmware workstation上使用虚拟机做宿主机，那么必须要在虚拟机配置界面的处理器选项中开启虚拟机化功能。 2.2.1 CPU开启虚拟化注意： 给宿主机的CPU和内存分配足够多的配置 2.2.2 验证开启虚拟化1234grep -Em 1 &quot;vmx|svm&quot; /proc/cpuinfo#Intel CPU 对应 vmx#AMD CPU 对应 svm 范例: 查看AMD主机的内核模块 12345[root@centos8 ~]#lsmod |grep kvmkvm_amd 110592 0ccp 98304 1 kvm_amdkvm 786432 1 kvm_amdirqbypass 16384 1 kvm 2.3 安装KVM工具包2.3.1 KVM 相关工具包介绍 qemu-kvm: 为kvm提供底层仿真支持 libvirt-daemon: libvirtd守护进程，管理虚拟机 libvirt-client: 用户端软件，提供客户端管理命令 libvirt-daemon-driver-qemu: libvirtd连接qemu的驱动 libvirt: 使用最多的KVM虚拟化管理工具和应用程序接口，即通过libvirt调用KVM创建虚拟机 libvirt是KVM通用的访问API，其不但能管理KVM，还能管理VMware、Xen、Hyper-V、 virtualBox等虚拟化方案。 virt-manager: 图形界面管理工具,其底层也是调用libvirt API来完成对虚拟机的操作，包括虚拟机的创建、删除、启动、停止以及一些简单的监控功能等。 virt-install: 虚拟机命令行安装工具 virsh: 命令行工具是基于 libvirt API 创建的命令行工具，它可以作为图形化的 virt-manager 应用的备选工具。virsh 命令可以被用来创建虚拟化任务管理脚本，如安装、启动和停止虚拟机 virt-viewer: 通过 VNC 和 SPICE 协议显示虚拟机器图形控制台的最小工具。该工具在其同名软件包中：virtviewer cockpit: CentOS8 专门提供的基于Web的虚拟机管理界面 2.3.2 libvirt 包功能2.3.2.1 libvirt 介绍libvirt 程序包是一个与虚拟机监控程序相独立的虚拟化应用程序接口，它可以与操作系统的一系列虚拟化性能进行交互 libvirt 程序包提供： 一个稳定的通用层来安全地管理主机上的虚拟机。 一个管理本地系统和连网主机的通用接口。 在虚拟机监控程序支持的情况下，部署、创建、修改、监测、控制、迁移以及停止虚拟机操作都需要这些API。尽管 libvirt 可同时访问多个主机，但 API 只限于单节点操作 libvirt 程序包被设计为用来构建高级管理工具和应用程序，例如 virt-manager 与 virsh 命令行管理工具。libvirt 主要的功能是管理单节点主机，并提供 API 来列举、监测和使用管理节点上的可用资源，其中包括CPU、内存、储存、网络和非一致性内存访问（NUMA）分区。管理工具可以位于独立于主机的物理机上，并通过安全协议和主机进行交流 2.3.2.2 libvirt 结构图 2.3.3 安装KVM相关包2.3.3.1 Ubuntu 安装 KVMUbuntu 18.04： 官方文档: https://ubuntu.com/server/docs/virtualization-libvirt 123456789101112[root@ubuntu2204 ~]#apt -y install cpu-checker[root@ubuntu2004 ~]#apt -y install cpu-checker#如果CPU不支持会如下提示[root@ubuntu2004 ~]#kvm-okINFO: Your CPU does not support KVM extensions KVM acceleration can NOT be used# apt install qemu-kvm virt-manager libvirt-daemon-system# kvm-ok #验证是否支持kvm,只有Ubuntu支持,CentOS 不支持#如果CPU支持，如下提示INFO: /dev/kvm existsKVM acceleration can be used 范例: ubuntu 安装KVM工具 12345678910111213141516171819202122232425262728293031323334353637383940[root@ubuntu2004 ~]#apt update[root@ubuntu2204 ~]#apt -y install qemu-kvm virt-manager libvirt-daemon-system[root@ubuntu2004 ~]#apt -y install qemu-kvm virt-manager libvirt-daemon-system[root@ubuntu1804 ~]#apt -y install qemu-kvm virt-manager libvirt-daemon-system[root@ubuntu1804 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000link/ether 00:0c:29:40:27:06 brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe40:2706/64 scope link valid_lft forever preferred_lft forever3: virbr1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:8d:a6:fe brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr1 valid_lft forever preferred_lft forever4: virbr1-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc fq_codel master virbr1 state DOWN group default qlen 1000 link/ether 52:54:00:8d:a6:fe brd ff:ff:ff:ff:ff:ff [root@ubuntu2204 ~]#apt -y install bridge-utils[root@ubuntu2204 ~]#brctl showbridge name bridge id STP enabled interfacesvirbr1 8000.5254005655f6 yes#如果没有开启CPU虚拟化功能会提示以下信息[root@ubuntu1804 ~]#kvm-okINFO: Your CPU does not support KVM extensionsKVM acceleration can NOT be used#添加CPU的虚拟化支持再执行[root@ubuntu1804 ~]#kvm-okINFO: /dev/kvm existsKVM acceleration can be used 2.3.3.2 CentOS 安装 KVM使用虚拟化，需要至少 qemu-kvm 和 qemu-img(安装qemu-kvm会自动安装) 软件包 建议安装：yum install qemu-kvm libvirt virt-manager virt-install 范例: CentOS 8 安装 KVM相关工具 1234[root@centos8 ~]#yum -y install qemu-kvm libvirt virt-manager virt-installvirt-viewer[root@centos8 ~]#systemctl start --now libvirtd 范例: CentOS 8 还提供基于Web的虚拟机管理方式 123456[root@centos8 ~]#yum -y install cockpit cockpit-machines[root@centos8 ~]#systemctl enable --now cockpit.socketCreated symlink /etc/systemd/system/sockets.target.wants/cockpit.socket → /usr/lib/systemd/system/cockpit.socket.#打开浏览器，访问以下地址：https://centos8主机:9090 2.3.4 图形化工具 virt-manager范例: CentOS 上管理工具 virt-manager 1234[root@centos8 ~]#export DISPLAY=10.0.0.1:0.0[root@centos8 ~]#virt-manager[root@centos8 ~]#libGL error: No matching fbConfigs or visuals foundlibGL error: failed to load driver: swrast 范例: Ubuntu 上管理工具 virt-manager 1234[root@ubuntu1804 ~]#virt-manager#如果出现乱码，设置语言[root@ubuntu2204 ~]#localectl set-locale LANG=en_US.UTF-8;exit 范例：Ubuntu 安装 cockpit 工具 123456[root@ubuntu2204 ~]#apt install cockpit cockpit-machines[root@ubuntu2204 ~]#ss -ntl|grep 9090LISTEN 0 4096 *:9090 *:* #打开浏览器，访问以下地址：https://Ubuntu主机:9090 2.2.5 默认网络配置安装完虚拟工具后,会自动生成一个 virbr1 网卡,类似于Vmware workstation 生成的VMnet8 网卡,充当虚拟机的 NAT 网卡 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@centos8 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:8a:51:21 brd ff:ff:ff:ff:ff:ff inet 10.0.0.8/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe8a:5121/64 scope link valid_lft forever preferred_lft forever3: virbr1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:97:eb:e3 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr1 valid_lft forever preferred_lft forever4: virbr1-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc fq_codel master virbr1 state DOWN group default qlen 1000 link/ether 52:54:00:97:eb:e3 brd ff:ff:ff:ff:ff:ff [root@centos8 ~]#grep -R 192.168.122.1 /etc/libvirt/*/etc/libvirt/qemu/networks/autostart/default.xml: &lt;ip address=&#x27;192.168.122.1&#x27; netmask=&#x27;255.255.255.0&#x27;&gt;/etc/libvirt/qemu/networks/default.xml: &lt;ip address=&#x27;192.168.122.1&#x27; netmask=&#x27;255.255.255.0&#x27;&gt;[root@centos8 ~]#ip a show virbr13: virbr1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000 link/ether 52:54:00:97:eb:e3 brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr1 valid_lft forever preferred_lft forever [root@centos8 ~]#cat /etc/libvirt/qemu/networks/default.xml&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using:virsh net-edit default or other application using the libvirt API.--&gt;&lt;network&gt;&lt;name&gt;default&lt;/name&gt;&lt;uuid&gt;1ebc4504-5da9-4b7e-b367-90b8cb20029b&lt;/uuid&gt;&lt;forward mode=&#x27;nat&#x27;/&gt;&lt;bridge name=&#x27;virbr1&#x27; stp=&#x27;on&#x27; delay=&#x27;0&#x27;/&gt;&lt;mac address=&#x27;52:54:00:97:eb:e3&#x27;/&gt;&lt;ip address=&#x27;192.168.122.1&#x27; netmask=&#x27;255.255.255.0&#x27;&gt; &lt;dhcp&gt; &lt;range start=&#x27;192.168.122.2&#x27; end=&#x27;192.168.122.254&#x27;/&gt; &lt;/dhcp&gt;&lt;/ip&gt;&lt;/network&gt;[root@centos8 ~]#nmcli connection show virbr1 范例: Ubuntu 网络配置 12345678910111213141516171819202122232425262728293031323334353637383940[root@ubuntu1804 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:40:27:06 brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe40:2706/64 scope link valid_lft forever preferred_lft forever3: virbr1: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000link/ether 52:54:00:8d:a6:fe brd ff:ff:ff:ff:ff:ff inet 192.168.122.1/24 brd 192.168.122.255 scope global virbr1 valid_lft forever preferred_lft forever4: virbr1-nic: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc fq_codel master virbr1 state DOWN group default qlen 1000 link/ether 52:54:00:8d:a6:fe brd ff:ff:ff:ff:ff:ff [root@ubuntu1804 ~]#brctl showbridge name bridge id STP enabled interfacesvirbr1 8000.5254008da6fe yes virbr1-nic[root@ubuntu1804 ~]#cat /etc/libvirt/qemu/networks/default.xml&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BEOVERWRITTEN AND LOST. Changes to this xml configuration should be made using:virsh net-edit default or other application using the libvirt API.--&gt;&lt;network&gt;&lt;name&gt;default&lt;/name&gt;&lt;uuid&gt;a3235b68-6dc0-4951-8a35-7a60a567f1a7&lt;/uuid&gt;&lt;forward mode=&#x27;nat&#x27;/&gt;&lt;bridge name=&#x27;virbr1&#x27; stp=&#x27;on&#x27; delay=&#x27;0&#x27;/&gt;&lt;mac address=&#x27;52:54:00:8d:a6:fe&#x27;/&gt;&lt;ip address=&#x27;192.168.122.1&#x27; netmask=&#x27;255.255.255.0&#x27;&gt; &lt;dhcp&gt; &lt;range start=&#x27;192.168.122.2&#x27; end=&#x27;192.168.122.254&#x27;/&gt; &lt;/dhcp&gt;&lt;/ip&gt;&lt;/network&gt; 2.4 准备安装系统的ISO相关文件将需要安装的系统的ISO文件上传到宿主机 12345[root@centos8 ~]#mkdir -pv /data/isos/[root@centos8 ~]#ls /data/isos/CentOS-7-x86_64-Minimal-2009.isoCentOS-8.2.2004-x86_64-minimal.isocn_Windows_server_2008_r2_standard_enterprise_datacenter_and_web_with_sp1_vl_build_x64_dvd_617396.iso 2.5 AMD CPU 创建虚拟机时的故障排错AMD CPU 使用virt-manager 或 virt-install 在创建虚拟机可能会出错,用下面方法解决 2.5.1 AMD CPU 使用virt-manager创建虚拟机出错提示 2.5.2 AMD CPU 使用virt-install创建虚拟机出错提示123456[root@centos8 ~]#virt-install --virt-type kvm --name centos7 --ram 1024 --vcpus 2 --cdrom=/data/isos/CentOS-7-x86_64-Minimal-2009.iso --disk path=/var/lib/libvirt/images/centos7.qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole WARNING No operating system detected, VM performance may suffer. Specify an OS with --os-variant for optimal results.Starting install... ERROR internal error: qemu unexpectedly closed the monitor: 2020-08-09T15:57:08.872365Z qemu-kvm: error: failed to set MSR 0xe1 to 0x0 qemu-kvm: /builddir/build/BUILD/qemu-2.12.0/target/i386/kvm.c:2119: kvm_buf_set_msrs: Assertion `ret == cpu-&gt;kvm_msr_buf-&gt;nmsrs&#x27; failed.Domain installation does not appear to have been successful. If it was, you can restart your domain by running: virsh --connect qemu:///system start centos7 otherwise, please restart your installation. 2.5.3 AMD CPU 创建虚拟机故障修复方法1234567#修复以上故障[root@centos8 ~]# tee /etc/modprobe.d/qemu-system-x86.conf &lt;&lt; EOF&gt; options kvm ignore_msrs=1&gt; EOFoptions kvm ignore_msrs=1[root@centos8 ~]#reboot 3 创建虚拟机3.1 使用 virt-manager 创建虚拟机virt-manager 是一个图形化虚拟机管理工具,方便管理和查看虚拟机 12345#需要提前在Windows上安装X Server的相关软件,比如:Xmanager,xming等才能在Windows显示图形工具[root@centos8 ~]#export DISPLAY=10.0.0.1:0.0[root@centos8 ~]#virt-manager[root@centos8 ~]#libGL error: No matching fbConfigs or visuals foundlibGL error: failed to load driver: swrast 开启virt-manager图形界面的时候，如果当前的语言为zh_CN.UTF-8（中文），会导致乱码，要修改为英文 123456789101112#查看当前使用的语言[root@ubuntu2004 ~]#echo $LANGzh_CN.UTF-8 [root@ubuntu2004 ~]#localectl System Locale: LANG=zh_CN.UTF-8 VC Keymap: n/a X11 Layout: CN X11 Model: pc105 #修改[root@ubuntu2004 ~]#localectl set-locale LANG=en_US.UTF-8 使用virt-manager创建虚拟机 12345678910111213选择安装方法Local install media (lSO image or CDROM) #本地安装Network install (HTTP, HTTPS, Or FTP) #网络 lmport existing disk image #导入现有的虚拟磁盘Manual install #手动安装选择iso光盘Browse：只有一个default空间，里面没有内容，实际上是系统默认的文件夹，所以需要在左下角的+号添加光盘Name：名字，随便起Type：类型，默认dirTarget Path：；路径，写上存有iso光盘的路径Customize confiquration before install #安装是否需要定制，不需要不打勾，直接安装 此处指定的磁盘会生成一个qcow2的稀疏格式文件,此处可以指定文件最大容量,但实际只占用实际的空间大小,使用du -sh 文件可以观察到实际大小,使用ls -l 命令看到是此处指定的大小 注意: 如果安装过程无法用键盘进行输入,修改Display Spice为VNC方式 范例: 生成的虚拟机相关文件 123456#保存虚拟机磁盘[root@centos8 ~]#ll /var/lib/libvirt/images/centos8.qcow2 -h-rw------- 1 qemu qemu 21G Sep 13 20:08 /var/lib/libvirt/images/centos8.qcow2#保存虚拟机配置[root@centos8 ~]#ll /etc/libvirt/qemu/centos8.xml 删除虚拟机 不选中删除磁盘文件，数据还在，到时可以找回来，只是在列表中删除了虚拟机 3.2 使用 virt-install 创建虚拟机虽然使用virt-manager 可以方便的管理虚拟机,但如果需要批量进行虚拟机的创建管理,命令行工具virt-install 更加方便和适合 3.2.1 virt-install 使用说明1234567891011# virt-install --helpusage: virt-install --name NAME --ram RAM STORAGE INSTALL [options]使用 &#x27;--option=?&#x27; 或者 &#x27;--option help&#x27; 查看可用子选项有关示例及完整选项语法，请查看 man page。#使用指定安装介质新建虚拟机optional arguments:-h, --help #show this help message and exit--version #show program&#x27;s version number and exit--connect URI #use libvirt URI 连接到 hypervisor 通用选项: 12345678910111213141516171819202122-n NAME, --name NAME #客户端虚拟机的名称--memory MEMORY #配置虚拟机内存分配例如：--memory 1024 (in MiB)--memory 512,maxmemory=1024--vcpus VCPUS #为虚拟机配置的 vcpus 数。例如：--vcpus 5--vcpus 5,maxcpus=10,cpuset=1-4,6,8--vcpus sockets=2,cores=4,threads=2--cpu CPU #CPU 型号及功能。例如：--cpu coreduo,+x2apic--cpu host--metadata METADATA #配置虚拟机元数据例如：--metadata name=foo,title=&quot;My pretty title&quot;,uuid=...--metadata description=&quot;My nice long description&quot; 安装方法选项: 1234567891011121314151617181920212223--cdrom CDROM #光驱安装介质-l LOCATION, --location LOCATION #安装源例如：nfs:host:/pathhttp://host/pathftp://host/path--pxe #使用 PXE 协议从网络引导--import #在磁盘映像中构建虚拟机--livecd #将光驱介质视为 Live CD-x EXTRA_ARGS, --extra-args EXTRA_ARGS #附加到使用 --location 引导的内核的参数--initrd-inject INITRD_INJECT #使用 --location 为 initrd 的 root 添加给定文件--os-variant DISTRO_VARIANT #在其中安装 OS 变体的虚拟机,比如:&#x27;fedora18&#x27;、&#x27;rhel6&#x27;、&#x27;winxp&#x27; 等等--boot BOOT #配置虚拟机引导设置。例如：--boot hd,cdrom,menu=on--boot init=/sbin/init (for containers)--idmap IDMAP #为 LXC 容器启用用户名称空间。例如：--idmap uid_start=0,uid_target=1000,uid_count=10 设备选项: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950--disk DISK #使用不同选项指定存储。例如：--disk size=10 (new 10GiB image in default location)--disk /my/existing/disk,cache=none--disk device=cdrom,bus=scsi--disk=?-w NETWORK, --network NETWORK #配置虚拟机网络接口。例如：--network bridge=myvirbr1--network network=my_libvirt_virtual_net--network network=mynet,model=virtio,mac=00:11...--network none--network help--graphics GRAPHICS #配置虚拟机显示设置。例如：--graphics vnc--graphics spice,port=5901,tlsport=5902--graphics none--graphics vnc,password=foobar,port=5910,keymap=ja--controller CONTROLLER #配置虚拟机控制程序设备。例如：--controller type=usb,model=ich9-ehci1--input INPUT #配置虚拟机输入设备。例如：--input tablet--input keyboard,bus=usb--serial SERIAL #配置虚拟机串口设备--parallel PARALLEL #配置虚拟机并口设备--channel CHANNEL #配置虚拟机沟通频道--console CONSOLE #配置虚拟机与主机之间的文本控制台连接--hostdev HOSTDEV #将物理 USB/PCI/etc 主机设备配置为与虚拟机共享--filesystem FILESYSTEM #将主机目录传递给虚拟机。例如：--filesystem /my/source/dir,/dir/in/guest--filesystem template_name,/,type=template--sound [SOUND] #配置虚拟机声音设备模拟--watchdog WATCHDOG #配置虚拟机 watchdog 设备--video VIDEO #配置虚拟机视频硬件。--smartcard SMARTCARD #配置虚拟机智能卡设备。例如：--smartcard mode=passthrough--redirdev REDIRDEV #配置虚拟机重定向设备。例如：--redirdevusb,type=tcp,server=192.168.1.1:4000--memballoon MEMBALLOON #配置虚拟机 memballoon 设备。例如：--memballoon model=virtio--tpm TPM #配置虚拟机 TPM 设备。例如：--tpm /dev/tpm--rng RNG #配置虚拟机 RNG 设备。例如：--rng /dev/random--panic PANIC #配置虚拟机 panic 设备。例如：--panic default 虚拟机配置选项: 123456789101112131415--security SECURITY #设定域安全驱动器配置。--numatune NUMATUNE #为域进程调整 NUMA 策略。--memtune MEMTUNE #为域进程调整内粗策略。--blkiotune BLKIOTUNE #为域进程调整 blkio 策略。--memorybacking MEMORYBACKING #为域进程设置内存后备策略。例如：--memorybacking hugepages=on--features FEATURES #设置域 &lt;features&gt; XML。例如：--features acpi=off--features apic=on,eoi=on--clock CLOCK #设置域 &lt;clock&gt; XML。例如：--clockoffset=localtime,rtc_tickpolicy=catchup--pm PM #配置 VM 电源管理功能--events EVENTS #配置 VM 生命周期管理策略--resource RESOURCE #配置 VM 资源分区（cgroups） 虚拟化平台选项: 123456-v, --hvm #客户端应该是一个全虚拟客户端-p, --paravirt #这个客户端一个是一个半虚拟客户端--container #这台虚拟机需要一个容器客户端--virt-type HV_TYPE #要使用的管理程序名称(kvm、qemu、xen等等)--arch ARCH #模拟的 CPU 构架--machine MACHINE #要模拟的机器类型 其它选项: 123456789--autostart #引导主机时自动启动域。--wait WAIT #等待安装完成的分钟数。--noautoconsole #不要自动尝试连接到客户端控制台--noreboot #完成安装后不要引导虚拟机。--print-xml [XMLONLY] #输出所生成域 XML，而不是创建虚拟机。--dry-run #完成安装步骤，但不要创建设备或者定义虚拟机。--check CHECK #启用或禁用验证检查。例如：--check path_in_use=off，--check all=off-q, --quiet #禁止无错误输出-d, --debug #输入故障排除信息 3.2.2 virt-install 命令创建虚拟机3.2.2.1 利用 qemu-img 命令创建虚拟磁盘(可选)注意: qemu-img create 一定要确认对应路径下没有此文件,如果存在将覆盖原文件 1234567[root@centos8 ~]#qemu-img create -f qcow2 /var/lib/libvirt/images/centos7.qcow2 20GFormatting &#x27;/var/lib/libvirt/images/centos7.qcow2&#x27;, fmt=qcow2 size=21474836480cluster_size=65536 lazy_refcounts=off refcount_bits=16#观察文件虚拟磁盘大小,比较用virt-manager创建的虚拟机磁盘文件大小[root@centos8 ~]#ll -h /var/lib/libvirt/images/centos7.qcow2-rw-r--r-- 1 root root 193K Sep 13 21:25 /var/lib/libvirt/images/centos7.qcow2 3.2.2.2 利用 osinfo-query命令查看支持的OS版本12345678910#列出支持OS系统名称#方法1[root@ubuntu2204 ~]#virt-install --osinfo list#方法2[root@ubuntu2004 ~]#apt install libosinfo-bin[root@ubuntu2004 ~]#osinfo-query os |grep -i rhel#查看支持的OS[root@centos8 ~]#osinfo-query os| grep centos 3.2.2.3 创建虚拟机使用光盘启动并手动安装范例：无需事先创建磁盘文件，直接创建虚拟机 1[root@ubuntu2204 ~]#virt-install --virt-type kvm --os-variant=centos7.0 --name centos7 --ram 1024 --vcpus 2 --cdrom=/data/isos/CentOS-7-x86_64-Minimal-2009.iso --disk path=/var/lib/libvirt/images/centos7.qcow2,size=10,format=qcow2,bus=virtio --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole 范例: 事先手动创建磁盘文件，再创建虚拟机 12345678910111213141516171819202122[root@ubuntu2204 ~]#qemu-img create -f qcow2 /var/lib/libvirt/images/rocky8.qcow2 10G[root@ubuntu2204 ~]#virt-install --virt-type kvm --os-variant=rocky8.5 --name rocky8 --memory 1024 --vcpus 2 --cdrom=/data/isos/Rocky-8.5-x86_64-minimal.iso --disk path=/var/lib/libvirt/images/rocky8.qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole#创建默认NAT模式的虚拟机,并不自动打开virt-viewer连接console,需要手动打开virt-manager 连接,并手动安装系统[root@centos8 ~]#virt-install --virt-type kvm --name centos7 --ram 1024 --vcpus 2 --cdrom=/data/isos/CentOS-7-x86_64-Minimal-2009.iso --disk path=/var/lib/libvirt/images/centos7.qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole --os-variant=centos7.0Starting install...Domain installation still in progress. You can reconnect tothe console to complete the installation process[root@centos8 ~]#export DISPLAY=10.0.0.1:0.0[root@centos8 ~]#virt-manager#查看虚拟硬盘大小,注意到只要正在运行的虚拟对应的硬盘文件所有者和组为qemu,而虚拟机关机的为root[root@centos8 ~]#ll /var/lib/libvirt/images/ -htotal 22G-rw-r--r-- 1 qemu qemu 1.6G Sep 13 22:05 centos7.qcow2-rw------- 1 root root 21G Sep 13 22:05 centos8.qcow2#安装过程略#示例：使用桥接网络[root@centos8 ~]#virt-install --virt-type kvm --name centos7 --ram 1024 --vcpus 2 --cdrom=/data/isos/CentOS-7-x86_64-Minimal-2009.iso --disk path=/var/lib/libvirt/images/centos7.qcow2 --network --network=bridge:virbr1,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --os-variant=centos7.0 范例： 123[root@ubuntu2204 ~]#qemu-img create -f qcow2 /var/lib/libvirt/images/rocky8.qcow2 20G[root@ubuntu2204 ~]#virt-install --virt-type kvm --name rocky8 --ram 2048 --vcpus 2 --cdrom=/data/isos/Rocky-8.5-x86_64-minimal.iso --disk path=/var/lib/libvirt/images/rocky8.qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole --os-variant=rocky8.5 3.2.2.4 验证宿主机进程12[root@centos8 ~]#ps aux|grep qemu-kvm[root@centos8 ~]#pstree -p |grep kvm 3.3 基于现有虚拟机磁盘为模版创建新的虚拟机3.3.1 利用virt-manager实现3.3.1.1 导入镜盘文件实现12#基于已经安装好虚拟机磁盘文件,创建新的磁盘文件[root@centos8 ~]#cp -a /var/lib/libvirt/images/centos7.qcow2 /var/lib/libvirt/images/centos7-2.qcow2 3.3.1.2 Clone 3.3.2 利用virt-install实现1234567891011121314151617[root@centos8 images]#pwd/var/lib/libvirt/images[root@centos8 images]#cp centos8.qcow2 centos8-2.qcow2#Ubuntu 必须使用--os-variant 选项指定OS版本[root@ubuntu2204 images]#virt-install --virt-type kvm --os-variant=centos7.0 --name centos7-3 --ram 1024 --vcpus 1 --disk bus=virtio,path=/var/lib/libvirt/images/centos7.3.qcow2 --network network=default,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart --boot hd#CentOS 可以不使用--os-variant 选项指定OS版本[root@centos8 images]#virt-install --virt-type kvm --name centos8-2 --ram 2048 --vcpus 2 --disk bus=virtio,path=/var/lib/libvirt/images/centos8-2.qcow2 --network network=default,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart --boot hdWARNING No operating system detected, VM performance may suffer. Specify an OSwith --os-variant for optimal results.Starting install...Domain creation completed.#运行工具,可以看到下面出现新的虚拟机[root@centos8 images]#virt-manager 3.3.3 利用virt-clone克隆实现123456#基于已有的虚拟机克隆生成新的虚拟机[root@ubuntu2004 ~]#virt-clone -o rocky8 -n rocky8-3 -f /var/lib/libvirt/images/rocky8-3.qcow2-o rocky8 #指已存在的虚拟机的名称-n rocky8-3 #新虚拟机的名称-f /var/lib/libvirt/images/rocky8-3.qcow2 #新虚拟机磁盘文件路径，此文件自动生成，不需要事先创建 3.4 总结自动化构建虚拟机命令1virt-install --virt-type kvm --os-variant=rocky8.5 --name rocky8 --memory 1024 --vcpus 2 --cdrom=/data/isos/Rocky-8.5-x86_64-minimal.iso --disk path=/var/lib/libvirt/images/rocky8.qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole 1virt-install --virt-type kvm --name centos7-2 --ram 1024 --vcpus 2 --cdrom=/data/isos/CentOS-7-x86_64-Minimal-2009.iso --disk path=/var/lib/libvirt/images/centos7-2.qcow2,size=10,format=qcow2,bus=virtio --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole --os-variant=centos7.0 12cp centos7.qcow2 centos7-2.qcow2virt-install --virt-type kvm --os-variant=centos7.0 --name centos7-2 --ram 1024 --vcpus 1 --disk bus=virtio,path=/var/lib/libvirt/images/centos7-2.qcow2 --network network=default,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart --boot hd 1234567891011virt-install \\ --virt-type=kvm \\ --name win2016 \\ --ram 4096 \\ --vcpus=4 \\ --os-variant=win2k16 \\ --cdrom=/data/isos/cn_windows_server_2016_x64_dvd_9718765.iso \\ --network=bridge=virbr0,model=virtio \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --disk path=/var/lib/libvirt/images/win2016.qcow2,size=10,bus=virtio,format=qcow2 \\ --disk path=/data/isos/virtio-win-0.1.189.iso,device=cdrom 1virt-install --virt-type kvm --os-variant=rocky8.6 --name rocky8-2 --ram 1024 --vcpus 1 --disk bus=virtio,path=/var/lib/libvirt/images/rocky8-2.qcow2 --network network=default,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart --boot hd 1virt-clone -o rocky8 -n rocky8-1 -f /var/lib/libvirt/images/rocky8-1.qcow2 4 管理虚拟机4.1 使用半虚拟化驱动 virtio4.1.1 半虚拟化驱动virtio的工作原理为了提高内存、硬盘、网络的性能，需要支持半虚拟化 virtio 是一种 I&#x2F;O 半虚拟化解决方案，是一套通用 I&#x2F;O 设备虚拟化的程序，是对半虚拟化 Hypervisor 中的一组通用 I&#x2F;O 设备的抽象，提供了一套上层应用与各 Hypervisor 虚拟化设备（KVM，Xen，VMware等）之间的通信框架和编程接口，减少跨平台所带来的兼容性问题，大大提高驱动程序开发效率，Windows 系统需要单独安装virtio驱 动，Linux系统自带virtio驱动。 实现IO虚拟化主要有三种方式：全虚拟化、半虚拟化和透传，全虚拟化Guest OS不会感知到自己是虚拟机，也无需修改Guest OS，但是它的效率比较低，半虚拟化Guest OS知道自己是虚拟机，通过Frontend&#x2F;Backend驱动模拟实现IO虚拟化，透传就是直接分配物理设备给VM用，但是需要解决单个硬件在多个虚拟机共享使用的问题。 性能比较： https://www.ilsistemista.net/index.php/virtualization/42-kvm-virtio-paravirtualized-drivers-why-they-matter.html?start=2 4.1.2 半虚拟化设备统一接口virtio 通过统一的接口virtio以支持的多种硬件设备 不同的虚拟设备和不同的虚拟机可以有不同的前端驱动 不同的硬件设备可以有不同的后端驱动 两者之间的交互遵循virtio的标准 4.1.3 驱动获取4.1.3.1 Linux 的 VirtIO 驱动红帽RHEL 4.8之后的虚拟机会自动加载和安装virtio驱动 4.1.3.2 Windows 操作系统需要额外安装 virtio 的驱动方法1∶如果有红帽的RHN订阅，可以从以下位置下载virtio-win包: https://rhn.redhat.com/rhn/software/packages/details/Overview.do?pid=868414 方法2∶从社区获得 1234http://www.Linux-kvm.org/page/Downloadshttps://fedorapeople.org/groups/virt/virtio-win/direct-downloads/https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/https://docs.fedoraproject.org/en-US/quick-docs/creating-Windows-virtual-machines-using-virtio-drivers/index.html 1https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/ 4.1.4 Linux 测试安装virtio驱动前后的虚拟机性能变化默认虚拟机使用的是全虚拟化IDE磁盘,以下测试性能 使用dd 或 hdparm测试 1[root@centos8 ~]# dd | hdparm -t /dev/vda 将硬盘默认的总线IDE修改为VirtIO 重新启动后,执行相同的操作,比较性能 默认网卡为e1000,将之修改网卡驱动为virtIO 4.1.5 安装 Windows Server 虚拟机4.1.5.1 下载并准备相关文件1234567# virtio下载地址：https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/# virtio 历史版本下载地址：https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/#注意:建议下载virtio-win-0.1.141版本，其它版本可能有问题，比如189版本会蓝屏死机,185安装后显示黄色，工作不正常 12345678[root@centos8 ~]#ll /data/isos/total 6031060-rw-r--r-- 1 qemu qemu 1085276160 Aug 9 16:38 CentOS-7-x86_64-Minimal-2009.iso-rw-r--r-- 1 qemu qemu 1718616064 Aug 9 16:31 CentOS-8.2.2004-x86_64-minimal.iso-rw-r--r-- 1 root root 3368962048 Aug 11 12:00cn_Windows_server_2008_r2_standard_enterprise_datacenter_and_web_with_sp1_vl_build_x64_dvd_617396.iso-rw-r--r-- 1 root root 2949120 Aug 11 12:01 virtio-win-0.1.141_amd64.vfd-rw-r--r-- 1 root root 316628992 Apr 3 12:22 virtio-win-0.1.141.iso 4.1.5.2 创建 Windows Server 虚拟机范例: 创建 Window 2016 KVM 虚拟机 1234567891011121314#无需事事创建磁盘文件，注意：Window 2016 磁盘大小不能低于10G[root@ubuntu2204 ~]#virt-install --osinfo list |grep win[root@ubuntu2204 ~]#virt-install \\ --virt-type=kvm \\ --name win2016 \\ --ram 4096 \\ --vcpus=4 \\ --os-variant=win2k16 \\ --cdrom=/data/isos/cn_windows_server_2016_x64_dvd_9718765.iso \\ --network=bridge=virbr0,model=virtio \\ --graphics vnc,listen=0.0.0.0 --noautoconsole \\ --disk path=/var/lib/libvirt/images/win2016.qcow2,size=20,bus=virtio,format=qcow2 \\ --disk path=/data/isos/virtio-win-0.1.189.iso,device=cdrom 范例: 创建 Window 2008-R2 KVM 虚拟机 12345678910111213141516171819#创建磁盘文件（可选）[root@centos8 ~]#qemu-img create -f qcow2 /var/lib/libvirt/images/Windows-2008_r2-x86_64.qcow2 20G#查看支持的Windows版本[root@centos8 ~]#osinfo-query os| grep win#下面版本成功[root@ubuntu2004 ~]#virt-install --virt-type kvm --name Win2008 --memory 3072 --vcpus=2 --os-variant=win2k8r2 --cdrom=/data/isos/cn_windows_server_2008_r2_standard_enterprise_datacenter_and_web_with_sp1.iso --disk path=/var/lib/libvirt/images/Windows-2008_r2-x86_64.qcow2,format=qcow2,bus=virtio --disk path=/data/isos/virtio-win-0.1.141_amd64.vfd,device=floppy --network bridge=virbr0,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart#无需事先创建磁盘文件[root@ubuntu2004 ~]#virt-install --virt-type kvm --name Win2008 --memory 3072 --vcpus=2 --os-variant=win2k8r2 --cdrom=/data/isos/cn_windows_server_2008_r2_standard_enterprise_datacenter_and_web_with_sp1.iso --disk path=/var/lib/libvirt/images/Windows-2008_r2-x86_64.qcow2,size=10,format=qcow2,bus=virtio --disk path=/data/isos/virtio-win-0.1.141_amd64.vfd,device=floppy --network bridge=virbr0,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart#创建 Windows 虚拟机,新版不支持下面格式,可以通过virt-manager进行安装[root@cetos8 ~]#virt-install --virt-type kvm --name Win2008 --memory 3072 --vcpus=2 --os-variant=win2k8r2 --cdrom=/data/isos/cn_Windows_server_2008_r2_standard_enterprise_datacenter_and_web_with_sp1_vl_build_x64_dvd_617396.iso --disk path=/var/lib/libvirt/images/Windows-2008_r2-x86_64.qcow2,format=qcow2,bus=virtio --disk path=/data/isos/virtio-win-0.1.141_amd64.vfd,device=floppy --network bridge=virbr0,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostartStarting install...Domain installation still in progress. You can reconnect tothe console to complete the installation process.[root@centos8 ~]#virt-manager 4.1.5.3 安装 Windows Server 12#可以临时加载光盘ISO镜像virsh attach-disk win2016 /data/isos/virtio-win-0.1.189.iso hda --type cdrom --mode readonly Window2016 安装硬盘驱动 Window2008-R2 安装硬盘驱动 注意: 不要选中 GPU 密码有长度和复杂度要求 4.1.5.4 验证 Windows virtio 驱动 4.1.5.5 安装网卡驱动Windows 2016 网卡默认不支持，需要手动安装网卡驱动 4.1.5.6 安装PCI 内存管理驱动windows 2016 安装驱动 Windows 2008-R2 挂载virtio-win的141的ISO光盘文件virtio-win-0.1.141.iso进行安装驱动 注意:不要安装其它版本,比如189版本会蓝屏死机,185安装后显示黄色，工作不正常 4.1.5.7 生成Windows Server 镜像模版利用sysprep工具,清除个性信息,下次Windows开机时, 会自动生成初始化个性信息 下次开机需要重新初始化 4.1.5.8 基于模版创建新的Windows的虚拟机范例：Windows 2016 12345[root@ubuntu2204 ~]#virt-clone -o win2016 -f /var/lib/libvirt/images/win2016-1.qcow2 -n win2016-1Allocating &#x27;win2016-1.qcow2&#x27; | 9.7 GB 00:00:16 ...Clone &#x27;win2016-1&#x27; created successfully. 范例: Windows 2008-R2 12345678910[root@centos7 images]#pwd/var/lib/libvirt/images[root@centos7 images]#cp windows2008.qcow2 windows2008-2.qcow2[root@centos7 ~]#virt-install --virt-type kvm --name Win2008-2 --memory 3072 --vcpus=2 --os-variant=win2k8r2 --disk path=/var/lib/libvirt/images/windows2008-2.qcow2,format=qcow2,bus=virtio --network bridge=virbr0,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart --boot hd#克隆方法有问题[root@ubuntu2204 ~]#virt-clone -o Win2008-template -f /var/lib/libvirt/images/Win2008-1.qcow2 -n Win2008-1ERROR expected str, bytes or os.PathLike object, not NoneType 4.2 libvirt 架构 注意: 如果libvirtd服务意外关闭,将导致相关工具,如:virt-manager等无法和虚拟机连接,但虚拟机仍会正常运行 范例: libvirtd 服务功能 1234567891011121314151617181920212223242526272829[root@centos8 ~]#virsh listId Name State----------------------------------------------------2 centos8 running3 Win_2008_r2-x86_64 running[root@ubuntu2004 ~]#systemctl stop libvirtd libvirtd.socket libvirtd-admin.socket libvirtd-ro.socket[root@centos8 ~]#systemctl stop libvirtd[root@centos8 ~]#virsh listerror: failed to connect to the hypervisorerror: Failed to connect socket to &#x27;/var/run/libvirt/libvirt-sock&#x27;: No such file or directory#virt-manager工具也无法连接虚拟机#如果再次重新运行virt-manage，会自动激活libvirtd服务#但是systemctl stop libvirtd.socket libvirtd-admin.socket libvirtd-ro.socket libvirtd.service，将无法自动激活libvirtd.service[root@ubuntu2204 ~]#systemctl stop libvirtd.socket libvirtd-admin.socket libvirtd-ro.socket libvirtd.service[root@ubuntu2204 ~]#virsh listerror: failed to connect to the hypervisorerror: Failed to connect socket to &#x27;/var/run/libvirt/libvirt-sock&#x27;: Connection refused[root@ubuntu2204 ~]#systemctl start libvirtd.socket[root@ubuntu2204 ~]#systemctl status libvirtd.socket[root@ubuntu2204 ~]#virsh listId Name State---------------------------20 win2016-1 running 12345678[root@centos8 ~]#systemctl start libvirtd[root@centos8 ~]#virsh listId Name State----------------------------------------------------2 centos8 running3 Win_2008_r2-x86_64 running#virt-manager工具也恢复连接虚拟机 4.3 virt-manager 管理虚拟机virt-manager是一个图形化工具,主要功能: 定义和创建虚拟机 硬件管理 性能监视 控制台 在线和离线迁移 虚拟机的保存和恢复、暂停和继续、关闭和启动 ​ 4.3.1 启动菜单 开机后,按提示按ESC键,会出下面启动菜单 4.3.2 远程管理KVM宿主机Ubuntu20.04和Rocky8相互支持远程连接 可以连接到远程的宿主机进行虚拟机的管理 解决上面问题的方法 123456#方法1:在本机安装openssh-askpass包[root@ubuntu2004 ~]#apt install -y ssh-askpass[root@centos8 ~]#yum -y install openssh-askpass#方法2:实现基于本机到远程主机的key验证[root@centos8 ~]#ssh-copy-id 10.0.0.18 再次连接成功 4.3.3 虚拟机的性能监控先选择需要监控的项目 根据上面的项目,选择对应显示的项目(可选项由前一页的项目决定) 4.3.4 管理快照 4.4 virsh 命令行工具4.4.1 virsh 命令说明virsh是使用libvirt managementAPI构建的管理工具,相比virt-manager可以提高效率 virsh的名称的含义是virtualization shell 4.4.1.1 两种工作模式交互模式 非交互模式 4.4.1.2 virsh 主要功能12345678910111213[root@centos8 ~]#virsh helpDomain Management (help keyword &#x27;domain&#x27;): #域管理（帮助关键字“域”）：Domain Monitoring (help keyword &#x27;monitor&#x27;): #域监控（帮助关键字“monitor”）：Host and Hypervisor (help keyword &#x27;host&#x27;): #主机和虚拟机管理程序（帮助关键字“host”）：Interface (help keyword &#x27;interface&#x27;): #接口（帮助关键字“interface”）：Network Filter (help keyword &#x27;filter&#x27;): #网络过滤器（帮助关键字“过滤器”）：Networking (help keyword &#x27;network&#x27;): #网络（帮助关键字“网络”）：Node Device (help keyword &#x27;nodedev&#x27;): #节点设备（帮助关键字“nodedev”）：Secret (help keyword &#x27;secret&#x27;): Snapshot (help keyword &#x27;snapshot&#x27;): #快照（帮助关键字“快照”）：Storage Pool (help keyword &#x27;pool&#x27;): #存储池（帮助关键字“pool”）：Storage Volume (help keyword &#x27;volume&#x27;): #存储卷（帮助关键字“卷”）：Virsh itself (help keyword &#x27;virsh&#x27;): #Virsh 本身（帮助关键字“virsh”）： 4.4.1.3 virsh 命令格式123456789101112131415161718[root@centos8 ~]#virsh --helpvirsh [options]... [&lt;command_string&gt;]virsh [options]... &lt;command&gt; [args...]options: -c | --connect=URI #虚拟机监控程序连接 URI -d | --debug=NUM #调试级别 [0-4] -e | --escape &lt;char&gt; #设置控制台的转义序列 -h | --help #帮助 -k | --keepalive-interval=NUM #保持活动间隔（以秒为单位），0 表示禁用 -K | --keepalive-count=NUM #可能错过的 keepalive 消息数 -l | --log=FILE #将日志记录输出到文件 -q | --quiet #静音模式 -r | --readonly #连接只读 -t | --timing #打印时序信息 -v #short version -V #long version --version[=TYPE] #version, TYPE is short or long (default short) 4.4.1.4 virsh 子命令说明12345678910111213141516171819202122232425262728293031323334353637help #打印基本帮助信息attach-device #使用XML文件中的设备定义在虚拟机中添加设备attach-disk #在虚拟机中附加新磁盘设备attach-interface #在虚拟机中附加新网络接口create #从 XML 配置文件生成虚拟机并启动新虚拟机define #为虚拟机输出XML配置文件destroy #强制虚拟机停止detach-device #从虚拟机中分离设备，使用同样的XML 描述作为命令attach-devicedetach-disk #从虚拟机中分离磁盘设备detach-interface #从虚拟机中分离网络接口domblkstat #显示正在运行的虚拟机的块设备统计domid #显示虚拟机IDdomifstat #显示正在运行的虚拟机的网络接口统计dominfo #显示虚拟机信息domname #显示虚拟机名称domstate #显示虚以机状态domuuid #显示虚拟机UUIDdumpxml #输出虚拟机 XML配置文件list #列出所有虚拟机migrate #将虚拟机迁移到另一台主机中nodeinfo #有关管理程序的输出信息quit #退出这个互动终端reboot #重新启动虚拟机restore #恢复以前保存在文件中的虚拟机resume #恢复暂停的虚拟机save #将虚拟机当前状态保存到某个文件中setmaxmem #为管理程序设定内存上限setmem #为虚拟机设定分配的内存setvcpus #修改为虚拟机分配的虚拟CPU数目shutdown #关闭某个虚拟机start #启动未激活的虚拟机suspend #暂停虚拟机undefine #删除与虚拟机关联的所有文件vepuinfo #显示虚以机的虚拟CPU信息vcpupin #控制虚拟机的虚拟CPU亲和性version #显示virsh版本 4.4.1.5 查看子命令帮助1virsh help COMMAND 范例: 查看子命令 list 命令用法 123456789101112131415161718192021222324252627[root@centos8 ~]#virsh help listNAME list - list domainsSYNOPSIS list [--inactive] [--all] [--transient] [--persistent] [--with-snapshot] [--without-snapshot] [--state-running] [--state-paused] [--state-shutoff] [--state-other] [--autostart] [--no-autostart] [--with-managed-save] [--without-managed-save] [--uuid] [--name] [--table] [--managed-save] [--title] DESCRIPTION Returns list of domains. OPTIONS --inactive #list inactive domains --all #list inactive &amp; active domains --transient #list transient domains --persistent #list persistent domains --with-snapshot #list domains with existing snapshot --without-snapshot #list domains without a snapshot --state-running #list domains in running state --state-paused #list domains in paused state --state-shutoff #list domains in shutoff state --state-other #list domains in other states --autostart #list domains with autostart enabled --no-autostart #list domains with autostart disabled --with-managed-save #list domains with managed save state --without-managed-save #list domains without managed save --uuid #list uuid&#x27;s only --name #list domain names only --table #list table (default) --managed-save #mark inactive domains with managed save state --title #show domain title 4.4.2 启动和关闭虚拟机123456789101112131415161718192021#查看当前启动的虚拟机[root@centos8 ~]#virsh list#查看所有虚拟机[root@centos8 ~]#virsh list --all#启动[root@centos8 ~]#virsh start centos8#正常关机[root@centos8 ~]#virsh shutdown 1#强制关机,慎重使用[root@centos8 ~]#virsh destroy 1[root@centos8 ~]#virsh shutdown 2#列出开机状态虚拟机的UUID和名称[root@centos8 ~]#virsh list --uuid --name#列出所有虚拟机的UUID和name[root@ubuntu2004 ~]#virsh list --uuid --all --name 在virt-manager 中可以看到关机状态 查看虚拟机UUID,通过UUID启动关闭虚拟机 123456789101112#查看虚拟机的UUID[root@centos8 ~]#virsh domuuid 199765478-cfb1-4164-b038-f62004ccab9e[root@centos8 ~]#virsh destroy 99765478-cfb1-4164-b038-f62004ccab9eDomain 99765478-cfb1-4164-b038-f62004ccab9e destroyed[root@centos8 ~]#virsh domuuid centos899765478-cfb1-4164-b038-f62004ccab9e[root@centos8 ~]#virsh start 99765478-cfb1-4164-b038-f62004ccab9eDomain centos8 started 4.4.3 暂停和恢复虚拟机123456789[root@centos8 ~]#virsh suspend centos8Domain centos8 suspended#虚拟机暂停后,宿主机中还存有相关的进程[root@centos8 ~]#ps aux|grep kvmqemu 1699 36.9 16.0 4439296 1309300 ? Sl 10:10 5:28 /usr/libexec/qemu-kvm -name guest=centos8,debug-threads=on -S -objectsecret,id=masterKey0,format=raw,file=/var/lib/libvirt/qemu/domain[root@centos8 ~]#virsh resume 1Domain 1 resumed 4.4.4 配置虚拟机开机自动启动123456789101112131415[root@centos8 ~]#virsh autostart centos8Domain centos8 marked as autostarted#设置虚拟机随宿主机启动而自动启动,本质就是在下面目录生成软链接[root@centos8 ~]#ll /etc/libvirt/qemu/autostart/total 0lrwxrwxrwx 1 root root 29 Sep 17 18:53 centos8.xml -&gt;/etc/libvirt/qemu/centos8.xmllrwxrwxrwx 1 root root 40 Sep 17 09:18 Win_2008_r2-x86_64.xml -&gt;/etc/libvirt/qemu/Win_2008_r2-x86_64.xml[root@centos8 ~]#virsh autostart 1 --disableDomain 1 unmarked as autostarted[root@centos8 ~]#ll /etc/libvirt/qemu/autostart/total 0lrwxrwxrwx 1 root root 29 Sep 17 18:53 centos8.xml -&gt;/etc/libvirt/qemu/centos8.xml 在virt-manager工具中也可以配置开机自启动 4.4.5 查看虚拟机配置12345#每个虚拟机配置都存放在/etc/libvirt/qemu目录下的xml文件中[root@centos8 ~]#ls /etc/libvirt/qemu/ -l#查看指定虚拟机的配置,以下命令相当于查看 /etc/libvirt/qemu/centos8.xml[root@centos8 ~]#virsh dumpxml centos8 4.4.6 删除虚拟机配置1234567891011121314#删除虚拟机配置,但不删除磁盘文件[root@centos8 ~]#virsh undefine centos8-vm4Domain centos8-vm4 has been undefined#对应虚拟机xml的配置文件被删除[root@centos8 ~]#ll /etc/libvirt/qemu/#对应的磁盘文件并没有删除[root@centos8 ~]#ls /var/lib/libvirt/images/#删除虚拟机包括磁盘文件[root@ubuntu2004 ~]#virsh undefine k8s-node-05 --remove-all-storageDomain k8s-node-05 has been undefinedVolume &#x27;vda&#x27;(/var/lib/libvirt/images/k8s-node-05.qcow2) removed. 4.4.7 冷迁移虚拟机将一个宿主机的处于关机状态的虚拟机迁移到另一台宿主机，注意: 不支持Ubuntu和Rocky8宿主机之间迁移 123456789101112131415161718192021222324252627282930313233#在一台目标宿主机安装相关虚拟化软件[root@ubuntu2004 ~]#apt update[root@ubuntu2004 ~]#apt -y install qemu-kvm virt-manager libvirt-daemon-system#在源宿主机查看虚拟机的相关文件[root@ubuntu2004 ~]#virsh dumpxml --domain rocky8-template...... &lt;disk type=&#x27;file&#x27; device=&#x27;disk&#x27;&gt; &lt;driver name=&#x27;qemu&#x27; type=&#x27;qcow2&#x27;/&gt; &lt;source file=&#x27;/var/lib/libvirt/images/rocky8-template.qcow2&#x27;/&gt;.....#或者#查看块设备[root@ubuntu2004 ~]#virsh domblklist rocky8-templateTarget Source------------------------------------------------vda /var/lib/libvirt/images/rocky8-template.qcow2vdb -#复制源宿主机上的虚拟机的两个文件到目标宿主机[root@ubuntu2004 ~]#scp /etc/libvirt/qemu/rocky8-template.xml 10.0.0.101:/etc/libvirt/qemu/[root@ubuntu2004 ~]#scp /var/lib/libvirt/images/rocky8-template.qcow2 10.0.0.101:/var/lib/libvirt/images#在目标宿主机不重启服务无法看到新的虚拟机[root@ubuntu2004 ~]#virsh list --all[root@ubuntu2004 ~]#systemctl restart libvirtd#在目标宿主机重启服务后看到新的虚拟机[root@ubuntu2004 ~]#virsh list --allId Name State---------------------------------- rocky8-template shut off 4.4.8 热迁移虚拟机将KVM主机上的数据都利用NFS共享存储，不存储在自身主机硬盘上 4.5 virt-whatvirt-what 可以判断当前系统是虚拟机还是物理机 如果系统运行在一个物理机上，virt-what 命令将不会返回任何结果(一般是物理机，也可能是不能识别的虚拟化技术) 相反，如果是运行在虚拟机上，将会输出虚拟机的一些信息，例如kvm、vmware等 范例 123456789101112131415161718#CenOS安装[root@centos7 ~]#yum install -y virt-what#Ubuntu安装[root@ubuntu2004 ~]#apt install -y virt-what#查看系统#vmware虚拟机执行结果[root@ubuntu2004 ~]#virt-whatvmware#阿里云ECS执行结果[root@centos7-liyun-pc ~]# virt-whatkvm#物理机执行结果root@user:~# virt-whatroot@user:~# 5 存储管理5.1 KVM存储模式基于文件系统的存储 dir: Filesystem Directory 需要有挂载点的文件系统 fs: Pre-Formatted Block Device 无需挂载的文件系统,如:位于SAN存储的文件系统,可支持多个主机同时访问,而本地文件系统不支持 netfs: Network Exported Directory 网络文件系统,比如:NFS,SAMBA等 基于设备的存储 无需文件系统,性能更好,但可管理性差,无法实现快照 Disk: Physical Disk Device Iscsi: isCSI Target logical:LVM Volume Group 5.2 虚拟磁盘类型 固定Fixed 在配置时，指定磁盘大小 不管在虚拟磁盘上实际存储多少数据，都将占用相同大小宿主机的磁盘空间动态Dynamic 初始空间占用小 随着空间的使用逐渐增长到最大容量，但是只根据需求使用更多的空间 差异Differencing 因为创建是差异磁盘，所以只保存变更的数据 例如，将操作系统安装在父盘，然后创建差异化磁盘来执行进一步配置 5.3 虚拟镜像文件格式镜像文件储存在主机文件系统中。它可以储存在本地文件系统中，如 ext4 或 xfs；或网络文件系统中，如 NFS 。例如 libguestfs 这样的工具，能管理、备份及监控文件。KVM 上的磁盘镜像格式包括： raw 此为默认磁盘格式,但并是一种真正的磁盘格式，而是代表虚拟机所使用的原始镜像,它并不存储元数据，因此可作为保证虚拟机兼容性的候选方案。然而，也正因为它不存储元数据，因此不支持某些高级特往，比如快照和压缩等格式简单，容易转换为其他的格式。 如果主机文件系统允许，raw 文件可以是预分配（pre-allocated）或 稀疏（sparse）。稀疏文件根据需求分配主机磁盘空间，因此它是一种精简配置形式（thin provisioning）。预分配文件的所有空间需要被预先分配，但它比稀疏文件性能好。当对磁盘 I&#x2F;O 性能要求非常高，而且通常不需要通过网络传输镜像文件时，可以使用 raw文件 优点 : 性能好 缺点 : 空间占用大,功能较少,生产不推荐使用 cow : copy-on-write格式，昙花一现 qcow : QEMU早期的copy-on-write格式，过渡性方案 qcow2 qcow2 镜像文件提供许多高级磁盘镜像特征，如快照、压缩及加密。它们可以用来代表通过模板镜像创建的虚拟机。因为只有虚拟机写入的扇区部分才会分配在镜像中，所以 qcow2 文件的网络传输效率较高。RHEL 7.0 及更新版本支持 qcow2 v3 镜像文件格式 按需进行分配磁盘空间，不管文件系统是否支持 支持快照 支持zlib的磁盘压缩 支持AES的加密 优点 : 空间节约,功能丰富 缺点 : 性能较差,生产推荐使用 vmdk( Virtual Machine Disk ): VMware环境当中默认使用的磁盘格式 vhd \\ vhdx ( Virtual Hard Disk ):微软默认采用的文件格式 vdi : VirtualBox 采用的文件格式 查看支持格式 1qemu-img --help 范例: 查看KVM支持的磁盘格式 12[root@centos8 ~]#qemu-img --help|grep SupportSupported formats: blkdebug blkreplay blkverify copy-on-read file ftp ftps gluster host_cdrom host_device http https iscsi iser luks nbd null-aio null-convme qcow2 quorum raw rbd ssh throttle vhdx vmdk vpc 5.4 使用 qemu-img 管理虚拟磁盘文件5.4.1 qemu-img 概述qemu-img 是一个功能强大的磁盘镜像管理工具 查看帮助: qemu-img –help qemu-img 包括以下子命令 1234567891011check #检查完整性create #创建镜像commit #提交更改compare #比较convert #转换info #获得信息map #映射snapshot #快照管理rebase #在已有的镜像的基础上创建新的镜像resize #调整大小amend #修订镜像格式选项 5.4.2 创建虚拟磁盘文件5.4.2.1 创建默认稀疏格式的磁盘123456789101112131415161718192021#默认为raw格式稀疏文件[root@centos8 ~]#qemu-img create vm1.img 1gFormatting &#x27;vm1.img&#x27;, fmt=raw size=1073741824[root@centos8 ~]#file vm1.imgvm1.img: data#显示文件大小[root@centos8 ~]#ll -h vm1.img-rw-r--r-- 1 root root 1.0G Sep 20 12:17 vm1.img#实际文件只占4k空间[root@centos8 ~]#du -h vm1.img4.0K vm1.img#显示文件信息[root@centos8 ~]#qemu-img info vm1.imgimage: vm1.imgfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 4.0K 5.4.2.2 查看不同磁盘文件格式支持选项1234567891011121314151617181920212223[root@centos8 ~]#qemu-img create -f raw -o ?Supported options:size Virtual disk size[root@centos8 ~]#qemu-img create -f qcow2 -o ?Supported options:size Virtual disk sizecompat Compatibility level (0.10 or 1.1)backing_file File name of a base imagebacking_fmt Image format of the base imageencryption Encrypt the image with format &#x27;aes&#x27;. (Deprecated in favor of encrypt.format=aes)encrypt.format Encrypt the image, format choices: &#x27;aes&#x27;, &#x27;luks&#x27;encrypt.key-secret ID of secret providing qcow AES key or LUKS passphraseencrypt.cipher-alg Name of encryption cipher algorithmencrypt.cipher-mode Name of encryption cipher modeencrypt.ivgen-alg Name of IV generator algorithmencrypt.ivgen-hash-alg Name of IV generator hash algorithmencrypt.hash-alg Name of encryption hash algorithmencrypt.iter-time Time to spend in PBKDF in millisecondscluster_size qcow2 cluster sizepreallocation Preallocation mode (allowed values: off, metadata, falloc,full)lazy_refcounts Postpone refcount updatesrefcount_bits Width of a reference count entry in bits 5.4.2.3 qcow2 格式选项12345backing_file #指定后端镜像文件backing_fmt #设置后端镜像的镜像格式cluster_size #设置镜像中的簇大小，取值在512到2M之间，默认值为64Kpreallocation #设置镜像文件空间的预分配模式encryption #用于设置加密 5.4.2.4 创建raw格式非稀疏文件12345678910111213141516[root@centos8 ~]#dd if=/dev/zero of=vm2.img bs=1M count=10241024+0 records in1024+0 records out1073741824 bytes (1.1 GB, 1.0 GiB) copied, 3.20332 s, 335 MB/s[root@centos8 ~]#qemu-img info vm2.imgimage: vm2.imgfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 1.0G[root@centos8 ~]#ls -hl vm2.img-rw-r--r-- 1 root root 1.0G Sep 20 12:31 vm2.img[root@centos8 ~]#du -h vm2.img1.0G vm2.img 5.4.2.5 创建raw格式稀疏文件12345678910111213141516[root@centos8 ~]#dd if=/dev/zero of=vm3.img bs=1M count=0 seek=10240+0 records in0+0 records out0 bytes copied, 9.2173e-05 s, 0.0 kB/s[root@centos8 ~]#qemu-img info vm3.imgimage: vm3.imgfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 0[root@centos8 ~]#ll -h vm3.img-rw-r--r-- 1 root root 1.0G Sep 20 12:34 vm3.img[root@centos8 ~]#du -h vm3.img0 vm3.img 5.4.2.6 raw文件复制的格式控制12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#复制非稀疏文件,默认也为非稀疏文件[root@centos8 ~]#cp vm2.img vm2.img.bak[root@centos8 ~]#du -h vm2.img.bak1.0G vm2.img.bak[root@centos8 ~]#ll -h vm2.img.bak-rw-r--r-- 1 root root 1.0G Sep 20 12:38 vm2.img.bak[root@centos8 ~]#qemu-img info vm2.img.bakimage: vm2.img.bakfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 1.0G#复制稀疏文件,默认仍为稀疏文件[root@centos8 ~]#cp vm3.img vm3.img.bak[root@centos8 ~]#ll -h vm3.img.bak-rw-r--r-- 1 root root 1.0G Sep 20 12:36 vm3.img.bak[root@centos8 ~]#du -h vm3.img.bak0 vm3.img.bak[root@centos8 ~]#qemu-img info vm3.img.bakimage: vm3.img.bakfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 0#指定将非稀疏文件复制为稀疏格式格式[root@centos8 ~]#cp --sparse=always vm2.img vm2.img.bak2[root@centos8 ~]#qemu-img info vm2.imgimage: vm2.imgfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 1.0G[root@centos8 ~]#qemu-img info vm2.img.bak2image: vm2.img.bak2file format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 0#指定将稀疏文件复制为非稀疏格式格式[root@centos8 ~]#cp --sparse=never vm3.img vm3.img.bak2[root@centos8 ~]#qemu-img info vm3.imgimage: vm3.imgfile format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 0[root@centos8 ~]#qemu-img info vm3.img.bak2image: vm3.img.bak2file format: rawvirtual size: 1.0G (1073741824 bytes)disk size: 1.0G 5.4.3 检查虚拟磁盘对于关机状态的虚拟机磁盘,可以检查文件错误 12345678910111213141516171819202122232425[root@centos8 ~]#virsh listId Name State----------------------------------------------------2 centos8 running[root@centos8 ~]#qemu-img check /var/lib/libvirt/images/centos8.qcow2qemu-img: Could not open &#x27;/var/lib/libvirt/images/centos8.qcow2&#x27;: Failed to get shared &quot;write&quot; lockIs another process using the image [/var/lib/libvirt/images/centos8.qcow2]?[root@centos8 ~]#virsh suspend centos8Domain centos8 suspended[root@centos8 ~]#virsh listId Name State----------------------------------------------------2 centos8 paused[root@centos8 ~]#qemu-img check /var/lib/libvirt/images/centos8.qcow2qemu-img: Could not open &#x27;/var/lib/libvirt/images/centos8.qcow2&#x27;: Failed to get shared &quot;write&quot; lockIs another process using the image [/var/lib/libvirt/images/centos8.qcow2]?[root@centos8 ~]#qemu-img check /var/lib/libvirt/images/centos7.qcow2No errors were found on the image.25572/327680 = 7.80% allocated, 1.44% fragmented, 0.00% compressed clustersImage end offset: 1676869632 5.4.4 磁盘预分配策略raw 文件的预分配策略和文件系统是否支持有关,而qcow2则无关 预分配策略 off 此为缺省策略，即不使用预分配策略,预分配后的虚拟磁盘占用空间很小,不属于稀疏映像类型 生成的磁盘文件占用空间很小 相当于房地产开发商拆迁，只圈地但不禁止旧房的使用 metadata 只预分配元数据(metadata)，预分配后的磁盘文件属于稀疏映像类型,相当于vmware中的磁盘置备选项: Thin Provision(精简配置) 生成的磁盘文件为稀疏格式,实际占用的空间比off策略稍大一些 相当于房地产开发商拆迁，只圈地但不禁止旧房的使用，占一小块地建了售楼处 falloc 分配文件的块并标识它们的状态为未初始化，即只分配空间,但不置零. 预分配后的虚拟磁盘属于非稀疏映像类型,相对full模式来说，创建虚拟磁盘的速度要快很多,相当于vmware中的磁盘置备选项:厚置备延迟置零 生成的磁盘文件实际占用的空间和分配的空间相同大小 相当于房地产开发商拆迁，只圈地且禁止旧房的使用，但旧房还存在 full 分配所有磁盘空间并置零，预分配后的虚拟磁盘属于非稀疏映像类型,创建最慢,相当于vmware中的磁盘置备选项: 厚置备置零 生成的磁盘文件实际占用的空间和分配的空间相同大小 相当于房地产开发商拆迁，只圈地且完全清除旧房 总结： 追求创建速度，节约磁盘空间 off metadata 追求使用速度，立即分配空间，虽然存在一点磁盘空间浪费，即磁盘空间没那么多，但是还是分那么多，这样的话磁盘空间占用比较大，但是他的磁盘空间是连续的，不是碎片化的，性能好 falloc full 范例: 创建预分配置策略 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394[root@centos8 ~]#qemu-img create -f qcow2 test1.qcow2 1gFormatting &#x27;test1.qcow2&#x27;, fmt=qcow2 size=1073741824 cluster_size=65536 lazy_refcounts=off refcount_bits=16[root@centos8 ~]#qemu-img info test1.qcow2image: test1.qcow2file format: qcow2virtual size: 1.0G (1073741824 bytes)disk size: 196Kcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false #指定关闭预分配[root@centos8 ~]#qemu-img create -f qcow2 test2.qcow2 1g -o preallocation=offFormatting &#x27;test2.qcow2&#x27;, fmt=qcow2 size=1073741824 cluster_size=65536 preallocation=off lazy_refcounts=off refcount_bits=16[root@centos8 ~]#qemu-img info test2.qcow2image: test2.qcow2file format: qcow2virtual size: 1.0G (1073741824 bytes)disk size: 196Kcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false #指定预分配metadata[root@centos8 ~]#qemu-img create -f qcow2 test3.qcow2 1g -o preallocation=metadataFormatting &#x27;test3.qcow2&#x27;, fmt=qcow2 size=1073741824 cluster_size=65536 preallocation=metadata lazy_refcounts=off refcount_bits=16[root@centos8 ~]#qemu-img info test3.qcow2image: test3.qcow2file format: qcow2virtual size: 1.0G (1073741824 bytes)disk size: 836Kcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false #指定预分配falloc[root@centos8 ~]#qemu-img create -f qcow2 test4.qcow2 1g -o preallocation=fallocFormatting &#x27;test4.qcow2&#x27;, fmt=qcow2 size=1073741824 cluster_size=65536 preallocation=falloc lazy_refcounts=off refcount_bits=16[root@centos8 ~]#qemu-img info test4.qcow2image: test4.qcow2file format: qcow2virtual size: 1.0G (1073741824 bytes)disk size: 1.0Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false #指定预分配full[root@centos8 ~]#qemu-img create -f qcow2 test5.qcow2 1g -o preallocation=fullFormatting &#x27;test5.qcow2&#x27;, fmt=qcow2 size=1073741824 cluster_size=65536 preallocation=full lazy_refcounts=off refcount_bits=16[root@centos8 ~]#qemu-img info test5.qcow2image: test5.qcow2file format: qcow2virtual size: 1.0G (1073741824 bytes)disk size: 1.0Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false #文件系统显示大小[root@centos8 ~]#ll -h test*.qcow2-rw-r--r-- 1 root root 193K Sep 20 13:31 test1.qcow2-rw-r--r-- 1 root root 193K Sep 20 13:32 test2.qcow2-rw-r--r-- 1 root root 1.1G Sep 20 13:35 test3.qcow2-rw-r--r-- 1 root root 1.1G Sep 20 13:36 test4.qcow2-rw-r--r-- 1 root root 1.1G Sep 20 13:37 test5.qcow2#查看真实大小[root@centos8 ~]#du -h test*.qcow2196K test1.qcow2196K test2.qcow2836K test3.qcow21.1G test4.qcow21.1G test5.qcow2 5.4.5 虚拟磁盘格式转换qemu-img 可以将不同格式的虚拟磁盘文件进行格式转化 语法格式 1qemu-img convert [--object objectdef] [--image-opts] [--target-image-opts] [-U] [-C] [-c] [-p] [-q] [-n] [-f fmt] [-t cache] [-T src_cache] [-O output_fmt] [-B backing_file] [-o options] [-s snapshot_id_or_name] [-l snapshot_param] [-S sparse_size] [-m num_coroutines] [-W] filename [filename2 [...]] output_filename 范例: 将vmdk转化为raw 和qcow2格式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@centos8 ~]#qemu-img info CentOS8.2.vmdkimage: CentOS8.2.vmdkfile format: vmdkvirtual size: 200G (214748364800 bytes)disk size: 1.6Gcluster_size: 65536Format specific information: cid: 2898578192 parent cid: 4294967295 create type: monolithicSparse extents: [0]: virtual size: 214748364800 filename: CentOS8.2.vmdk cluster size: 65536 format: #默认转化为raw格式[root@centos8 ~]#qemu-img convert CentOS8.2.vmdk CentOS8.2.img#比较大小[root@centos8 ~]#ll -h CentOS8.2.vmdk CentOS8.2.img-rw-r--r-- 1 root root 1.6G Sep 20 16:00 CentOS8.2.vmdk-rw-r--r-- 1 root root 200G Sep 20 16:10 CentOS8.2.img[root@centos8 ~]#du -h CentOS8.2.vmdk CentOS8.2.img1.6G CentOS8.2.vmdk1.5G CentOS8.2.img[root@centos8 ~]#qemu-img info CentOS8.2.imgimage: CentOS8.2.imgfile format: rawvirtual size: 200G (214748364800 bytes)disk size: 1.4G[root@centos8 ~]#mv CentOS8.2.img /var/lib/libvirt/images/[root@centos8 ~]#virt-install --import --name=centos8-test2 --vcpus=1 --ram=2048 --disk bus=scsi,path=/var/lib/libvirt/images/CentOS8.2.img --network network=default --graphics vnc,listen=0.0.0.0 --os-type=Linux --os-variant=centos8 --noautoconsole --boot hd#转化为qcow2格式[root@centos8 ~]#qemu-img convert -f vmdk -O qcow2 CentOS8.2.vmdk CentOS8.2.qcow2#比较大小[root@centos8 ~]#ll -h CentOS8.2.vmdk CentOS8.2.qcow2-rw-r--r-- 1 root root 1.6G Sep 20 16:00 CentOS8.2.vmdk-rw-r--r-- 1 root root 1.6G Sep 20 16:12 CentOS8.2.qcow2[root@centos8 ~]#du -h CentOS8.2.vmdk CentOS8.2.qcow21.6G CentOS8.2.vmdk1.6G CentOS8.2.qcow2[root@centos8 ~]#qemu-img info CentOS8.2.qcow2image: CentOS8.2.qcow2file format: qcow2virtual size: 200G (214748364800 bytes)disk size: 1.5Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false [root@centos8 ~]#mv CentOS8.2.qcow2 /var/lib/libvirt/images/[root@centos8 ~]#virt-install --import --name=centos8-test3 --vcpus=1 --ram=2048 --disk bus=scsi,path=/var/lib/libvirt/images/CentOS8.2.qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --os-type=Linux --os-variant=centos8 --noautoconsole --boot hd 5.4.6 调整虚拟磁盘大小虚拟磁盘文件创建后,还可以调整虚拟磁盘大小 语法格式 1qemu-img resize [--shrink] filename [+l -]size 操作之前，一定要做好数据备份 增加文件大小后，需要在客户机中使用fdisk、parted等分区工具进行相应的操作才能真正让客户机使用到增加后的镜像空间。 缩小镜像之前，要在客户机中保证里面的文件系统有空余空间，否则会数据丢失。另外xfs文件系统不支持缩减 qcow2不支持缩小镜像的操作 范例: 扩展虚拟磁盘 1234567891011121314151617181920212223242526272829[root@centos8 ~]#qemu-img info /var/lib/libvirt/images/centos8.qcow2image: /var/lib/libvirt/images/centos8.qcow2file format: qcow2virtual size: 20G (21474836480 bytes)disk size: 20Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: true refcount bits: 16 corrupt: false #增加10G空间[root@centos8 ~]#qemu-img resize /var/lib/libvirt/images/centos8.qcow2 +10GImage resized.[root@centos8 ~]#qemu-img info /var/lib/libvirt/images/centos8.qcow2image: /var/lib/libvirt/images/centos8.qcow2file format: qcow2virtual size: 30G (32212254720 bytes)disk size: 20Gcluster_size: 65536Format specific information: compat: 1.1 lazy refcounts: true refcount bits: 16 corrupt: false #启动虚拟机后,还需要使用fdisk等工具进行空间的继续管理才能使用 范例: 缩减虚拟磁盘 12[root@centos8 images]#qemu-img resize --shrink /var/lib/libvirt/images/centos7.qcow2 -2GImage resized. 范例：qemu-img resize 扩展空间后，在虚拟机内还需执行如下操作才能最终扩容 1234567891011121314151617181920#利用前面qemu-img resize扩容的空间来扩容分区fdisk /dev/vdad 删除旧的20G的/dev/vda2n 重新创建/dev/vda2,容量使用所有空间，/dev/vda2扩容到30Gt 指定8e 类型w 保存退出#如果是逻辑卷，执行下面操作#当前物理卷空间并没有扩容，需要执行下面操作扩容PVpvresize --setphysicalvolumesize 29G /dev/vda2#扩容逻辑卷和文件系统lvextend -r -l +100%free /dev/rl/root#如果是分区不是逻辑卷，则执行如下操作#如果是xfs文件系统xfs_growfs /#如果是extresize2fs /dev/vda2 5.5 磁盘快照管理5.5.1 快照Snapshot介绍磁盘快照,对磁盘数据进行快照,主要用于虚拟机备份等场合 磁盘快照分类 按快照信息保存分为: 内置快照∶快照数据和base磁盘数据放在同一个qcow2文件中 外置快照︰快照数据单独的另一个qcow2文件存放 按虚拟机状态可以分为: 关机态快照︰数据可以保证一致性 运行态快照∶数据无法保证一致性，类似与系统crash后的磁盘数据。使用是可能需要fsck等操作。 按磁盘数量可以分为: 单盘:单盘快照不涉及原子性 多盘:涉及原子性。主要分两个方面:1.是所有盘快照点相同2.所有盘要么都快照成功，要么都快照失败。主要依赖于qemu的transaction实现 5.5.2 qemu-img 管理磁盘快照命令格式 1234567qemu-img snapshot [--object objectdef] [--image-opts] [-U] [-q] [-l | -a snapshot | -c snapshot | -d snapshot] filename snapshot is the name of the snapshot to create, apply or delete -a applies a snapshot (revert disk to saved state) -c creates a snapshot -d deletes a snapshot -l lists all snapshots in the given image 范例: 12345678910111213141516#查看块设备[root@centos8 ~]#virsh domblklist centos7#查看快照,如果没有快照,则无显示信息[root@centos8 ~]#qemu-img snapshot -l /var/lib/libvirt/images/centos7.qcow2#关机才能创建快照[root@centos8 ~]#qemu-img snapshot -c centos7-s1 /var/lib/libvirt/images/centos7.qcow2#查看快照[root@centos8 ~]#qemu-img snapshot -l /var/lib/libvirt/images/centos7.qcow2#查看快照信息[root@centos8 ~]#qemu-img info /var/lib/libvirt/images/centos7.qcow2#删除文件,模拟破坏 12#关机后才能还原快照修复故障[root@centos8 ~]#qemu-img snapshot -a centos7-s1 /var/lib/libvirt/images/centos7.qcow2 查看文件恢复 123#关机后才能删除快照[root@centos8 ~]#qemu-img snapshot -d centos7-s1 /var/lib/libvirt/images/centos7.qcow2[root@centos8 ~]#qemu-img snapshot -l /var/lib/libvirt/images/centos7.qcow2 5.5.3 virsh 管理虚拟机快照123456789101112[root@centos8 ~]#virsh snapshot-list centos8Name Creation Time State------------------------------------------------------------#开机状态可以创建虚拟机快照[root@centos8 ~]#virsh snapshot-create centos8Domain snapshot 1600593611 created[root@centos8 ~]#virsh snapshot-list centos8Name Creation Time State------------------------------------------------------------1600593611 2020-09-20 17:20:11 +0800 shutoff 使用 virsh 命令还原快照 1234567891011[root@centos8 ~]#virsh listId Name State----------------------------------------------------#无需关机，即可还原快照[root@centos8 ~]#virsh snapshot-revert centos8 --snapshotname 1600593611 --running[root@centos8 ~]#virsh listId Name State----------------------------------------------------23 centos7 running 1234567#删除快照[root@centos8 ~]#virsh snapshot-delete centos8 --snapshotname 1600593611Domain snapshot 1600593611 deleted[root@centos8 ~]#virsh snapshot-list centos8Name Creation Time State------------------------------------------------------------ 6 网络管理官方文档:https://wiki.libvirt.org/page/VirtualNetworking 6.1 Linux 网桥实现范例: Ubuntu 配置网桥 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970root@ubuntu1804:~# apt install -y bridge-utilsroot@ubuntu1804:~# dpkg -L bridge-utils/sbin/brctl......#三个网卡配置使用一个配置文件root@ubuntu1804:~# cat /etc/netplan/01-netcfg.yaml# This file describes the network interfaces available on your system# For more information, see netplan(5).network: version: 2 renderer: networkd ethernets: eth0: dhcp4: yes eth1: dhcp4: no dhcp6: no eth2: dhcp4: no bridges: virbr1: dhcp4: no dhcp6: no addresses: [10.0.0.18/16] gateway4: 10.0.0.2 nameservers: addresses: [223.6.6.6] interfaces: - eth1 - eth2 #桥接配置单独一个文件[root@ubuntu1804 netplan]#cat virbr1.yamlnetwork: version: 2 renderer: networkd ethernets: eth1: dhcp4: no dhcp6: no eth2: dhcp4: no bridges: virbr1: dhcp4: no dhcp6: no addresses: [10.0.0.10/16] gateway4: 10.0.0.2 nameservers: addresses: [223.6.6.6] interfaces: - eth1 - eth2 root@ubuntu1804:~# netplan applyroot@ubuntu1804:~# ifconfig virbr1virbr1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.0.18 netmask 255.255.0.0 broadcast 10.0.255.255 inet6 fe80::9cbe:1dff:fe85:6601 prefixlen 64 scopeid 0x20&lt;link&gt; ether 9e:be:1d:85:66:01 txqueuelen 1000 (Ethernet) RX packets 158 bytes 19534 (19.5 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 10 bytes 796 (796.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@ubuntu1804:~# brctl showbridge name bridge id STP enabled interfacesvirbr1 8000.9ebe1d856601 no eth1 eth2 范例: CentOS 配置网桥 1234567891011121314151617181920212223242526272829303132333435363738394041#创建网桥nmcli con add type bridge con-name virbr1 ifname virbr1nmcli connection modify virbr1 ipv4.addresses 10.0.0.100/24 ipv4.method manualnmcli con up virbr1#加入物理网卡nmcli con add type bridge-slave con-name virbr1-port0 ifname eth0 master virbr1nmcli con add type bridge-slave con-name virbr1-port1 ifname eth1 master virbr1nmcli con up virbr1-port0nmcli con up virbr1-port1#查看网桥配置文件cat /etc/sysconfig/network-scripts/ifcfg-virbr1DEVICE=virbr1NAME=virbr1STP=yesTYPE=BridgeBOOTPROTO=staticIPADDR=10.0.0.100PREFIX=24cat /etc/sysconfig/network-scripts/ifcfg-virbr1-port0TYPE=EthernetNAME=virbr1-port0DEVICE=eth0ONBOOT=yesBRIDGE=virbr1UUID=23f41d3b-b57c-4e26-9b17-d5f02dafd12d#安装管理软件包,注意:CentOS8取消了此包yum install bridge-utils#查看网桥brctl showip link show master virbr1bridge link show#删除virbr1nmcli con down virbr1rm /etc/sysconfig/network-scripts/ifcfg-virbr1*nmcli con reload 6.2 qemu-kvm支持的网络虚拟机的网络模式: 基于NAT ( Network Addresss Translation)的虚拟网络,此为virt-install的默认模式，相当于Vmware中的NAT模式 基于自定义网桥（Bridge ）的虚拟网络，支持虚拟机和宿主机的网卡桥接在一个网桥，从而实现外部网络访问虚拟机 用户自定义的隔离的虚拟网络，相当于Vmware中的仅主机模式 直接分配物理网络设备（包括VT-d和SR-IOV)，即桥接到宿主机的网卡，类似于Vmware中的桥接模式,性能最好 虚拟机的网卡设备: RTL8139、e1000、…. virtio 生产建议使用 注意: 桥接物理网卡,会自动生成macvtap网卡,并且会导致本宿主机无法访问虚拟机,但其它物理机可以访问当前宿主机上面的虚拟机 范例: 查看qemu-kvm支持的网卡型号 123#Ubuntu20.04没有此命令[root@centos8 ~]#/usr/libexec/qemu-kvm -net nic,model=?qemu: Supported NIC models: e1000,e1000-82540em,e1000e,rtl8139,virtio-net-pci 6.3 默认的网络配置NAT模式6.3.1 默认网络连接的架构图默认虚拟机网络配置为NAT模式,相当于vmware的NAT模式的Vmnet8 6.3.2 宿主机默认网络相关服务和信息范例：修改默认的网段信息 12345678910111213141516171819202122232425262728293031323334#默认网面是192.168.122.0/24，可以修改为其它网段[root@ubuntu2204 ~]#vim /etc/libvirt/qemu/networks/default.xml&lt;!--WARNING: THIS IS AN AUTO-GENERATED FILE. CHANGES TO IT ARE LIKELY TO BE OVERWRITTEN AND LOST. Changes to this xml configuration should be made using:virsh net-edit default or other application using the libvirt API.--&gt;&lt;network&gt;&lt;name&gt;default&lt;/name&gt;&lt;uuid&gt;b19f0f76-a1b7-4b6a-b940-b62cd8c59bf0&lt;/uuid&gt;&lt;forward mode=&#x27;nat&#x27;/&gt;&lt;bridge name=&#x27;virbr0&#x27; stp=&#x27;on&#x27; delay=&#x27;0&#x27;/&gt;&lt;mac address=&#x27;52:54:00:1c:f2:eb&#x27;/&gt;&lt;ip address=&#x27;192.168.12.1&#x27; netmask=&#x27;255.255.255.0&#x27;&gt; #修改网段信息 &lt;dhcp&gt; &lt;range start=&#x27;192.168.12.2&#x27; end=&#x27;192.168.12.254&#x27;/&gt; &lt;/dhcp&gt;&lt;/ip&gt;&lt;/network&gt;[root@ubuntu2204 ~]#reboot#自动更新DNSMASQ的相关文件[root@ubuntu2204 ~]#cat /var/lib/libvirt/dnsmasq/default.confstrict-orderuser=libvirt-dnsmasqpid-file=/run/libvirt/network/default.pidexcept-interface=lobind-dynamicinterface=virbr0dhcp-range=192.168.12.2,192.168.12.254,255.255.255.0dhcp-no-overridedhcp-authoritativedhcp-lease-max=253dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfileaddn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts 默认宿主机安装dnsmasq包指供DHCP服务 1234567891011121314151617#Ubuntu20.04[root@ubuntu2004 ~]#dpkg -l dnsmasq-baseDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name Version Architecture Description+++-==============-=================-============-============================================ii dnsmasq-base 2.80-1.1ubuntu1.5 amd64 Small caching DNS proxy and DHCP/TFTP server[root@ubuntu2004 ~]#cat /var/lib/libvirt/dnsmasq/default.conf#CentOS[root@centos8 ~]#rpm -q dnsmasqdnsmasq-2.79-11.el8.x86_64[root@centos8 ~]#cat /var/lib/libvirt/dnsmasq/default.conf 范例：查看宿主机的网桥信息 123456789101112[root@ubuntu2004 ~]#virsh list[root@ubuntu2004 ~]#ip a[root@ubuntu2004 ~]#apt -y install bridge-utils#查看桥接信息[root@ubuntu2004 ~]#brctl show#查看virbr1网络的情况[root@ubuntu2004 ~]#ip link show master virbr1#查看所有桥接网卡信息及对应网桥[root@ubuntu2004 ~]#bridge link show 范例: 查看宿主机的网桥信息 12345678910111213141516171819[root@centos8 ~]#virsh list --all[root@centos8 ~]#ip a[root@centos8 ~]#virsh list[root@centos8 ~]#virsh start centos7Domain centos7 started[root@centos8 ~]#virsh start centos8Domain centos8 started[root@centos8 ~]#ip a#查看virbr1网络的情况[root@centos8 ~]#ip link show master virbr1#查看所有桥接网卡信息及对应网桥[root@centos8 ~]#bridge link show[root@centos8 ~]#nmtui 123456[root@centos8 ~]#virsh net-list[root@centos8 ~]#cat /etc/libvirt/qemu/networks/default.xml[root@centos8 ~]#virsh net-dumpxml default#查看NAT策略实现从虚拟机可以访问外部网络,反之不通,此策略由libvirtd服务启动时自动加载到NAT表[root@centos8 ~]#iptables -vnL -t nat 6.3.3 虚拟机网卡默认设置Ubutun20.04的虚拟机默认网卡设置 Rocky8的虚拟机默认网卡设置 12345678910[root@centos8 ~]#virsh dumpxml centos7 |sed -n &#x27;/interface/,/interface/p&#x27; &lt;interface type=&#x27;network&#x27;&gt; &lt;mac address=&#x27;52:54:00:95:25:fb&#x27;/&gt; &lt;source network=&#x27;default&#x27; bridge=&#x27;virbr1&#x27;/&gt; &lt;target dev=&#x27;vnet0&#x27;/&gt; &lt;model type=&#x27;virtio&#x27;/&gt; &lt;alias name=&#x27;net0&#x27;/&gt; &lt;address type=&#x27;pci&#x27; domain=&#x27;0x0000&#x27; bus=&#x27;0x00&#x27; slot=&#x27;0x03&#x27;function=&#x27;0x0&#x27;/&gt; &lt;/interface&gt; 6.3.4 virsh 查看虚拟机网络配置12345678#查看虚拟机的网卡配置[root@centos8 ~]#virsh domiflist centos8#查看虚拟机的网卡地址信息[root@centos8 ~]#virsh domifaddr centos8#查看虚拟机的指定网卡的状态[root@centos8 ~]#virsh domifstat centos8 vnet0 6.4 配置虚拟机网卡桥接到宿主机的物理网卡相当于vmware的桥接模式的Vmnet0 此方式让宿主机网络的主机可以直接访问虚拟机,性能更优 Ubuntu22.04 宿主机的虚拟机界面如下 Ubuntu20.04 宿主机的虚拟机界面如下 选择Network source 为 Host deivce eth0: macvtap Rocky8 宿主机的虚拟机界面如下 需要手动输入device name 为 eth0 CentOS 7 界面如下 12345678[root@centos8 ~]#virsh domiflist centos7Interface Type Source Model MAC-------------------------------------------------------macvtap0 direct eth0 virtio 52:54:00:95:25:fb[root@centos8 ~]#virsh domifaddr centos7Name MAC address Protocol Address------------------------------------------------------------------------------- 在宿主机上自动生成虚拟网卡macvtap0@eth0 123456[root@centos8 ~]#ip a6: macvtap0@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codelstate UP group default qlen 500 link/ether 52:54:00:0d:8a:0e brd ff:ff:ff:ff:ff:ff inet6 fe80::5054:ff:fe0d:8a0e/64 scope link valid_lft forever preferred_lft forever 6.5 基于自定义网桥的虚拟网络6.5.1 自定义网桥架构桥接网络可以让运行在宿主机上的虚拟机使用和宿主机相同网段的IP，并且可以从外部直接访问到虚拟机，目前企业中大部分场景都使用桥接网络。 6.5.2 在宿主机创建网桥6.5.2.1 CentOS 创建桥接网卡12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@centos8 network-scripts]#pwd/etc/sysconfig/network-scripts#创建网桥配置文件[root@centos8 network-scripts]#cat ifcfg-virbr1TYPE=BridgeNAME=virbr1DEVICE=virbr1ONBOOT=yesBOOTPROTO=staticIPADDR=10.0.0.8NETMASK=255.255.255.0GATEWAY=10.0.0.2DNS1=180.76.76.76DNS2=223.6.6.6#将物理网卡eth0加入网桥[root@centos8 network-scripts]#cat ifcfg-eth0TYPE=EthernetNAME=eth0DEVICE=eth0ONBOOT=yesBRIDGE=virbr1[root@centos8 network-scripts]#nmcli connection reload[root@centos8 network-scripts]#nmcli connection up eth0 virbr1 [root@centos8 ~]#ip a[root@centos8 network-scripts]#pwd/etc/sysconfig/network-scripts#创建网桥配置文件[root@centos8 network-scripts]#cat ifcfg-virbr1TYPE=BridgeNAME=virbr1DEVICE=virbr1ONBOOT=yesBOOTPROTO=staticIPADDR=10.0.0.8NETMASK=255.255.255.0GATEWAY=10.0.0.2DNS1=180.76.76.76DNS2=223.6.6.6#将物理网卡eth0加入网桥[root@centos8 network-scripts]#cat ifcfg-eth0TYPE=EthernetNAME=eth0DEVICE=eth0ONBOOT=yesBRIDGE=virbr1[root@centos8 network-scripts]#nmcli connection reload[root@centos8 network-scripts]#nmcli connection up eth0 virbr1 [root@centos8 ~]#ip a[root@centos8 ~]#bridge link show[root@centos8 ~]#ip link show master virbr1 6.5.2.2 Ubuntu 创建桥接网卡12345678910111213141516171819202122232425[root@ubuntu2004 ~]# cat /etc/netplan/01-netcfg.yaml# This file describes the network interfaces available on your system# For more information, see netplan(5).network: version: 2 renderer: networkd ethernets: eth0: addresses: [10.0.0.100/16] gateway4: 10.0.0.2 nameservers: addresses: [223.6.6.6] eth1: dhcp4: no dhcp6: no bridges: virbr1: dhcp4: no dhcp6: no #addresses: [10.0.0.100/16] #gateway4: 10.0.0.2 #nameservers: # addresses: [223.6.6.6] interfaces: - eth1 6.5.3 基于现有虚拟机镜像批量创建新虚拟机将前面生成的虚拟机做为模版,生成新的虚拟机 注意:先在前面的模版虚拟机修改配置,如:关闭firewalld和SELinux,禁用NetworkManager(CentOS 7)等,安装常用包等 1234567891011121314151617181920[root@centos8 images]#pwd/var/lib/libvirt/images[root@centos8 images]#cp centos8.qcow2 centos8-2.qcow2[root@centos8 images]#virt-install --virt-type kvm --os-variant centos8 --name centos8-2 --ram 2048 --vcpus 2 --disk bus=virtio,path=/var/lib/libvirt/images/centos8-2.qcow2 --network bridge=virbr1,model=virtio --graphics vnc,listen=0.0.0.0 --noautoconsole --autostart --boot hdWARNING No operating system detected, VM performance may suffer. Specify an OSwith --os-variant for optimal results.Starting install...Domain creation completed.#运行工具,可以看到下面出现新的虚拟机[root@centos8 images]#virt-manager#开启的虚拟机磁盘文件所有者为qemu,关闭后为root[root@centos8 ~]#ll /var/lib/libvirt/imagestotal 9977352-rw-r--r-- 1 qemu qemu 2029518848 Sep 21 23:37 centos8-2.qcow2-rw------- 1 root root 1929183232 Sep 21 23:33 centos8-clone.qcow2-rw-r--r-- 1 root root 2004221952 Sep 21 23:32 centos8.qcow2 6.5.4 查看新虚拟机设置 6.6 内外网络隔离综合案例注意：vm2和vm3的虚拟机的IP需要静态配置，用 ip a a 添加的IP不稳定可能会丢失 实现一个外网的web服务和内网的数据库相互隔离的环境 两台宿主机host1和host2 每个宿主机上面各有两个虚拟机,分别连接外网和内网交换机 范例: 多网卡绑定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778root@ubuntu1804:~# vim /etc/netplan/01-netcfg.yaml# This file describes the network interfaces available on your system# For more information, see netplan(5).network: version: 2 renderer: networkd ethernets: eth0: dhcp4: no dhcp6: no eth1: dhcp4: no dhcp6: no bonds: bond0: interfaces: - eth0 - eth1 addresses: [10.0.0.18/16] gateway4: 10.0.0.1 nameservers: addresses: [223.6.6.6,223.5.5.5] parameters: mode: active-backup mii-monitor-interval: 100 fail-over-mac-policy: active root@ubuntu1804:~# netplan applyroot@ubuntu1804:~# ifconfig bond0bond0: flags=5187&lt;UP,BROADCAST,RUNNING,MASTER,MULTICAST&gt; mtu 1500 inet 10.0.0.18 netmask 255.255.0.0 broadcast 10.0.255.255 inet6 fe80::2820:b7ff:fea8:5837 prefixlen 64 scopeid 0x20&lt;link&gt; ether 2a:20:b7:a8:58:37 txqueuelen 1000 (Ethernet) RX packets 296 bytes 25327 (25.3 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 295 bytes 34876 (34.8 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@ubuntu1804:~# ifconfig eth0eth0: flags=6211&lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST&gt; mtu 1500 ether 2a:20:b7:a8:58:37 txqueuelen 1000 (Ethernet) RX packets 3 bytes 180 (180.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@ubuntu1804:~# ifconfig eth1eth1: flags=6211&lt;UP,BROADCAST,RUNNING,SLAVE,MULTICAST&gt; mtu 1500 ether 2a:20:b7:a8:58:37 txqueuelen 1000 (Ethernet) RX packets 340 bytes 28893 (28.8 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 336 bytes 39940 (39.9 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 root@ubuntu1804:~# cat /proc/net/bonding/bond0Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: fault-tolerance (active-backup)Primary Slave: NoneCurrently Active Slave: eth1MII Status: upMII Polling Interval (ms): 100Up Delay (ms): 0Down Delay (ms): 0Slave Interface: eth1MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:34:df:9bSlave queue ID: 0Slave Interface: eth0MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: 00:0c:29:34:df:91Slave queue ID: 0","categories":[],"tags":[]},{"title":"Service","slug":"Kubernetes/service-loadbalancer/service","date":"2025-09-12T04:37:55.000Z","updated":"2025-09-12T05:33:35.821Z","comments":true,"path":"Kubernetes/service-loadbalancer/service/","permalink":"https://aquapluto.github.io/Kubernetes/service-loadbalancer/service/","excerpt":"","text":"1 Service资源概念Service是Kubernetes标准的API资源类型之一，用户声明一个service的终态，提交给API Server，接下来就是交给Service Controller去落实和实现，一个Service对象存在于集群中的各节点之上，不会因个别节点故障而丢失，可为Pod提供固定的流量入口，因为它一旦被创建，Kubernetes就会自动为它分配一个可用的CLUSTER-IP，在整个声明周期内，CLUSTER-IP不会发生改变 Kubernetes设计Service资源主要是为了解决Pod在服务提供上的不足，Pod具有动态性，其IP地址也会在基于配置清单重构后重新进行分配，因而需要服务发现机制的支撑 Pod在集群中可能会因为故障或重启而更换，导致IP地址发生变化，这对于客户端来说会造成访问上的困难。 Pod的IP地址只能在Kubernetes集群内部访问，外部客户端无法直接访问这些IP地址。 Kubernetes使用Service资源和DNS服务（CoreDNS）进行服务发现和负载均衡 服务发现（发现一组提供了相同服务的Pod）：通过标签选择器，在同一namespace中筛选符合条件的Pod，完成Pod筛选。实际上并非由Service资源自己完成，是由与Service同名的Endpoint或EndpointSlice资源及控制器完成的 四层负载均衡（Service作为流量入口和负载均衡器，其流量入口为Cluster IP）：这组筛选出的Pod的IP地址，将作为Service的后端服务器。其流量调度规则是由运行各工作节点的kube-proxy根据配置的模式生成，可以是iptables或ipvs。需要注意的是，Cluster IP仅作为东西向流量的入口，不能作为南北向流量的入口 为该组Pod所代表的服务提供一个名称：依赖于CoreDNS，对于每个Service，自动生成一个或多个A、PTR和SRV（端口名称解析）记录，将Service的name与Service的Cluster IP地址做一个DNS域名映射，基于名称的方式去访问该组Pod上的服务 持续监视着相关Pod资源的变动，并实时反映至相应的流量调度规则之上 从Service的视角来看，Kubernetes集群的每个工作节点都是动态可配置的负载均衡器 对于隶属某Service的一组Pod资源，该Service资源能够将集群中的每个工作节点配置为该组Pod的负载均衡器 客户端可以是来自集群之上的Pod，也可以是集群外部的其它端点 对于一个特定的工作节点上的某Service来说，其客户端通常有两类 该节点之上的进程，可通过该Service的Cluster IP进入（集群内部，东西向流量） Service_IP:Service_Port 该节点之外的端点，可经由该Service的NodePort进入（集群外部，南北向流量，两级调度） Node_IP:Node_Port 显然，若客户端进入的节点，并非目标Pod所在的节点时，报文转发的路径中必然存在跃点 Service负载均衡有两种模式：基于iptables或者基于ipvs（效率更高） 在所有安装有kube-proxy组件的k8s节点上都会有一模一样的转发规则，并且是动态感知 在任何一个安装有kube-proxy组件的k8s节点上都可以发起对Service的请求（Service必须有cluster ip），请求的链路如下 请求 —&gt; svc —&gt; 基于转发模式计算出应该访问的pod ip –&gt; 用网络插件例如vxlan模式来封包发送 iptables效率低，随着iptables规则越来越多，整个内核的转发效率会变得很慢，而ipvs专为负载均衡而生 当kube-proxy以ipvs代理模式启动时，kube-proxy将验证节点上是否安装了IPVS模块，如果未安装，则kube-proxy将回退到iptables代理模式 2 Endpoints和EndpointSliceEndpoints和Service一起工作，将流量从Service代理到后端的Pod。当创建一个Service时，Kubernetes会为该Service创建一个同名的Endpoints资源。这个Endpoints资源包含了一个或多个后端Pod的IP地址和端口号，是否有该pod取决于这个pod有没有通过readiness探针测试。当Service收到请求时，它会将请求转发给Endpoints中的后端Pod EndpointSlice可以支持更大规模的集群。当一个服务有大量端点时，Endpoints资源可能会变得非常大，更新和传播这些信息会消耗大量的 API 服务器资源和网络带宽，而EndpointSlice将端点信息拆分成多个较小的切片，而不是将所有端点放在一个资源中，这样可以减少单个资源的负载，减少每次更新的数据量 他们也可以将集群外部的服务引入到集群中，如果该服务没有域名的情况下。还是WordPress访问MySQL的例子，可以先在endpoints.subsets.addresses 字段中写入MySQL机器的IP地址，然后再创建与其同名的service资源，就可以让WordPress访问MySQL。但是这种手动创建endpoints的坏处就是没有探针检测机制了，因为它压根不是pod，所以这种方法只是对于将传统架构迁移到k8s集群时，有状态应用的一种折中方案 1234567891011121314151617181920212223242526272829303132[root@master1 ~]#kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 10dpod-test NodePort 10.97.65.217 &lt;none&gt; 80:30768/TCP 10d[root@master1 ~]#kubectl get endpointsNAME ENDPOINTS AGEkubernetes 10.0.0.200:6443 10dpod-test 10.244.1.6:80,10.244.2.7:80 10d[root@master1 ~]#kubectl get endpointslicesNAME ADDRESSTYPE PORTS ENDPOINTS AGEkubernetes IPv4 6443 10.0.0.183 8dpod-test IPv4 80 10.244.1.6,10.244.2.7 8d#查看endpoints资源规范[root@master1 ~]#kubectl explain endpoints.subsetsFIELDS: addresses &lt;[]EndpointAddress&gt; IP addresses which offer the related ports that are marked as ready. These endpoints should be considered safe for load balancers and clients to utilize. （提供标记为就绪的相关端口的 IP 地址。这些端点应被视为安全，以便负载均衡器和客户端利用。） notReadyAddresses &lt;[]EndpointAddress&gt; IP addresses which offer the related ports but are not currently marked as ready because they have not yet finished starting, have recently failed a readiness check, or have recently failed a liveness check. （提供相关端口但当前未标记为准备好了，因为他们还没有完成启动，最近失败了就绪情况检查，或最近未通过活体检查。） ports &lt;[]EndpointPort&gt; Port numbers available on the related IP addresses.（相关 IP 地址上可用的端口号） 3 Service资源规范123456789101112131415161718apiVersion: v1kind: Servicemetadata: name: … namespace: …spec: type &lt;string&gt; # Service类型，默认为ClusterIP selector &lt;map[string]string&gt; # 等值类型的标签选择器，内含“与”逻辑 ports： # Service的端口对象列表 - name &lt;string&gt; # 端口名称 protocol &lt;string&gt; # 协议，目前仅支持TCP、UDP和SCTP，默认为TCP port &lt;integer&gt; # Service的端口号 targetPort &lt;string&gt; # 后端目标进程的端口号或名称，名称需由Pod规范定义 nodePort &lt;integer&gt; # 节点端口号，仅适用于NodePort和LoadBalancer类型 clusterIP &lt;string&gt; # Service的集群IP，建议由系统自动分配 externalTrafficPolicy &lt;string&gt; # 外部流量策略处理方式，Local表示由当前节点处理，Cluster表示向集群范围调度 loadBalancerIP &lt;string&gt; # 外部负载均衡器使用的IP地址，仅适用于LoadBlancer externalName &lt;string&gt; # 外部服务名称，该名称将作为Service的DNS CNAME值 由Service表示的负载均衡器，主要定义如下内容 负载均衡器入口：ClusterIP及相关的Service Port、NodePort（每个节点的Node IP都可用） 根据通信需求，确定选择的类型 标签选择器：用于筛选Pod，并基于筛选出的Pod的IP生成后端端点列表（被调度的上游端点） Service类型的专有配置 4 Service类型 4.1 ClusterIP支持Service_IP:Service_Port接入，用于集群内部的通信 Client –&gt; Service_IP:Service_Port –&gt; Pod_IP:Pod_Port 1234567891011121314kind: ServiceapiVersion: v1metadata: name: demoappspec: type: ClusterIP # 类型标识，默认即为ClusterIP； clusterIP: 10.97.72.1 # 建议不要指定，自动分配 selector: app: demoapp # 指定pod的标签，pod上必须有app=demoapp的标签 ports: - name: http # 端口名称标识 protocol: TCP # 协议，支持TCP、UDP和SCTP port: 80 # Service的端口号 targetPort: 80 # 目标端口号，即后端端点提供服务的监听端口号 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@master1 ~]#cd learning-k8s/examples/services/[root@master1 services]#kubectl create namespace demo[root@master1 services]#kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 --replicas=2 -n demo[root@master1 services]#kubectl get pods -n demo --show-labels NAME READY STATUS RESTARTS AGE LABELSdemoapp-7c58cd6bb-2zbb8 1/1 Running 0 20s app=demoapp,pod-template-hash=7c58cd6bbdemoapp-7c58cd6bb-t8dwm 1/1 Running 0 20s app=demoapp,pod-template-hash=7c58cd6bb[root@master1 services]# kubectl get pods -o wide -n demoNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdemoapp-7c58cd6bb-2zbb8 1/1 Running 0 3h19m 10.244.2.10 node1.wang.org &lt;none&gt; &lt;none&gt;demoapp-7c58cd6bb-t8dwm 1/1 Running 0 3h19m 10.244.1.8 node2.wang.org &lt;none&gt; &lt;none&gt;[root@master1 services]#vim services-clusterip-demo.yaml kind: ServiceapiVersion: v1metadata: name: demoapp-svc namespace: demospec: selector: app: demoapp ports: - name: http protocol: TCP port: 80 targetPort: 80 [root@master1 services]#kubectl apply -f services-clusterip-demo.yaml[root@master1 services]#kubectl get services -n demoNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp-svc ClusterIP 10.101.226.122 &lt;none&gt; 80/TCP 14s[root@master1 services]#kubectl get services -n demo demoapp-svc -o yamlspec: internalTrafficPolicy: Cluster #内部流量策略 type: ClusterIP #创建自主式Pod测试，可以调度到两个节点上的pod[root@master1 services]#kubectl run client-$RANDOM --image=ikubernetes/admin-box:v1.2 -it --restart=Never --rm --command -- /bin/shroot@client-5400 ~# curl 10.101.226.122ikubernetes admin-box:v1.2 !! ClientIP: 10.244.1.9,Servername: demoapp-7c58cd6bb-2zbb8,ServerIP: 10.244.2.10root@client-5400 ~# curl 10.101.226.122ikubernetes admin-box:v1.2 !! ClientIP: 10.244.1.9,Servername: demoapp-7c58cd6bb-t8dwm,ServerIP: 10.244.1.8#也能利用名字，注意要指定名称空间root@client-5400 ~# curl demoapp-svc.demo 4.2 NodePort支持Node_IP:Node_Port接入，同时支持ClusterIP，用于集群和外部的通信，以下类型都是 Client –&gt; Node_IP:NodePort –&gt; Pod_IP:Pod_Port，这里的Node_IP可以集群中任一节点 CIP –&gt; PIP(DNAT) PIP –&gt; CIP(SNAT) 节点会监听一个端口与service的端口映射，这就是NodePort，一般会分配一个在30000-32767之间的端口 12345678910111213141516kind: ServiceapiVersion: v1metadata: name: demoapp namespace: demospec: type: NodePort # 必须明确给出Service类型 selector: app: demoapp ports: - name: http protocol: TCP port: 80 targetPort: 80 nodePort: 30080 # 可选，为避免冲突，建议由系统动态分配 # externalTrafficPolicy: Local 12345678910111213141516171819[root@master1 services]#kubectl apply -f services-nodeport-demo.yaml[root@master1 services]#kubectl get svc -n demoNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp-clusterip-svc ClusterIP 10.97.72.1 &lt;none&gt; 80/TCP 26mdemoapp-nodeport-svc NodePort 10.96.45.106 &lt;none&gt; 80:32697/TCP 2s#集群内部[root@master1 services]#kubectl run client-$RANDOM --image=ikubernetes/admin-box:v1.2 -it --restart=Never --rm --command -- /bin/sh[root@client-3546 /]# curl 10.96.45.106iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.22, ServerName: demoapp-7c58cd6bb-9g92g, ServerIP: 10.244.1.20![root@client-3546 /]# curl 10.96.45.106iKubernetes demoapp v1.0 !! ClientIP: 10.244.1.22, ServerName: demoapp-7c58cd6bb-mzgqk, ServerIP: 10.244.2.13!#集群外部，集群内部任一节点[root@rocky8 ~]#curl 10.0.0.183:32697iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: demoapp-7c58cd6bb-mzgqk, ServerIP: 10.244.2.13![root@rocky8 ~]#curl 10.0.0.183:32697iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: demoapp-7c58cd6bb-9g92g, ServerIP: 10.244.1.20! 4.3 LoadBalancer4.3.1 LoadBalancer概念在接入外部流量方面，NodePort存在着几个方面的问题：非知名端口、私网IP地址、节点故障转移、节点间负载均衡、识别能适配到某Service的Local流量策略的节点等 比如用户访问nginx，只知道是80端口，但是我们是30080端口，用户就不知道了，所以直接让用户访问Node不现实，所以需要在集群外部多加一级代理LoadBalancer，将接入的流量转发至工作节点上的NodePort LoadBalancer通过外部的LB_IP:LB_Port接入，同时支持NodePort和ClusterIP Client –&gt; LB_IP:LB_PORT –&gt; Node_IP:NodePort –&gt; Pod_IP:Pod_Port 当创建LoadBalancer类型的Service时，需要与云提供商进行集成，以便向云提供商申请一个独立于k8s的负载均衡器（例如AWS的Elastic Load Balancer或GCP的Cloud Load Balancer），并将该负载均衡器的IP地址分配给Service，该负载均衡器会将流量转发到每个物理节点 比如我们把K8S集群部署在公有云上，云服务商一般会提供一个负载均衡器，当集群创建一个LoadBalancer类型的service资源时，会自动帮我们关联一个负载均衡器，也就是与其LoadBalancer所属的管理API联动，生成 EXTERNAL-IP(LB_IP) 供外部客户端使用 在VM配置的虚拟机，如果想使用LoadBalancer类型，就要部署OpenELB或MetalLB，才能有EXTERNAL-IP，它们是模拟负载均衡器的组件。如果部署不了，我们还有另一种方式让 EXTERNAL-IP 生效，前提是宿主机中接入外部流量的网卡有多余的 IP 地址，即外部流量可以到这个节点上 123456789101112131415161718192021222324252627#假如有个多余的10.0.0.100[root@node1 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:fe:8e:b4 brd ff:ff:ff:ff:ff:ff inet 10.0.0.184/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fefe:8eb4/64 scope link valid_lft forever preferred_lft forever#clusterip或者NodePort类型都行[root@master1 ~]#kubectl create service clusterip demoapp --tcp=80:80[root@master1 ~]#kubectl edit service demoappspec: externalIPs: - 10.0.0.100 [root@master1 ~]#kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp LoadBalancer 10.98.160.147 10.0.0.100 80/TCP 18d 4.3.2 定义LoadBalancer资源123456789101112131415161718192021222324252627kind: ServiceapiVersion: v1metadata: name: demoapp-loadbalancer-svcspec: type: LoadBalancer selector: app: demoapp ports: - name: http protocol: TCP port: 80 targetPort: 80 loadBalancerIP: 1.2.3.4 #建议不指定 [root@master1 services]#kubectl apply -f services-loadbalancer-demo.yaml[root@master1 MetalLB]#kubectl get svc -n demoNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp-clusterip-svc ClusterIP 10.97.72.1 &lt;none&gt; 80/TCP 48mdemoapp-loadbalancer-svc LoadBalancer 10.98.160.147 10.0.0.51 80:31179/TCP 15mdemoapp-nodeport-svc NodePort 10.96.45.106 &lt;none&gt; 80:32697/TCP 21m[root@rocky8 ~]#curl 10.0.0.51iKubernetes demoapp v1.0 !! ClientIP: 10.244.2.0, ServerName: demoapp-7c58cd6bb-9g92g, ServerIP: 10.244.1.20![root@rocky8 ~]#curl 10.0.0.51iKubernetes demoapp v1.0 !! ClientIP: 10.244.2.1, ServerName: demoapp-7c58cd6bb-mzgqk, ServerIP: 10.244.2.13! 4.3.3 流量策略使用LoadBalancer，流量可能有两级跳转，所以有一个外部流量策略 externalTrafficPolicy 流量策略一：Cluster，表示在整个Kubernetes集群范围内调度 该流量策略下，请求报文从某个节点上的NodePort进入，该节点上的Service会将其调度至任何一个可用后端Pod之上，而不关心Pod运行于哪个节点； 全集群调度。外部LB把流量转发给一个Node上，但是与该Service关联的Pod可能在别的Node上，那么这个Node就要把流量转发给拥有这个pod的Node上，这里就是两级跳转，第二级涉及到了内网转发 流量策略二：Local，表示仅将请求调度至当前节点上运行的可用后端Pod，可以获取客户端真实ip地址 该流量策略下，仅应该从运行有目标Service对象后端Pod对象的节点的NodePort发起访问，请求报文从该节点NodePort进入后，该节点上的Service会将请求调度至当前节点上适配到该Service的后端Pod 如果本地无匹配 Pod，请求被丢弃或返回连接失败（不会转发到其他节点），意味着要一直用某个物理节点的ip来访问，就没有负载均衡的效果，该参数的本意是尽量减少网络跳数，相比Cluster只有一级跳转 4.3.4 部署OpenELB1kubectl apply -f https://raw.githubusercontent.com/openelb/openelb/master/deploy/openelb.yaml 确认openelb-manager Pod已经处于Running或Completed状态，且容器已经Ready。 12345678[root@master1 ~]#kubectl get pods -n openelb-systemNAME READY STATUS RESTARTS AGEopenelb-admission-create-lcbp5 0/1 Completed 0 85sopenelb-admission-patch-7xv48 0/1 Completed 2 85sopenelb-controller-64f7fb77f8-jbb4p 1/1 Running 0 85sopenelb-speaker-89hmg 1/1 Running 0 85sopenelb-speaker-rl76j 1/1 Running 0 85sopenelb-speaker-snp2s 1/1 Running 0 85s 配置layer2模式 下面的示例创建了一个Eip资源对象，它提供了一个地址池给LoadBalancer Service使用 123456789101112131415161718[root@master1 ~]#vim eip-pool.yamlapiVersion: network.kubesphere.io/v1alpha2kind: Eipmetadata: name: eip-pool annotations: eip.openelb.kubesphere.io/is-default-eip: &quot;true&quot; # 指定当前Eip作为向LoadBalancer Server分配地址时使用默认的eip对象；spec: address: 10.0.0.51-10.0.0.60 # 集群所在ip地址空闲(没机器用)范围，也可以使用单个IP，或者带有掩码长度的网络地址； protocol: layer2 # 要使用的OpenELB模式，支持bgp、layer2和vip三种，默认为bgp； interface: eth0 # OpenELB侦听ARP或NDP请求时使用的网络接口名称，仅layer2模式下有效；这里的要和本机的网卡名一致 disable: false [root@master1 ~]#kubectl apply -f eip-pool.yaml 创建完成后，可使用如命令验证 12345678910111213141516171819202122232425262728[root@master1 ~]#kubectl get eipNAME CIDR USAGE TOTALeip-pool 10.0.0.50-10.0.0.60 11[root@master1 ~]#kubectl get eip eip-pool -o yamlapiVersion: network.kubesphere.io/v1alpha2kind: Eipmetadata: annotations: eip.openelb.kubesphere.io/is-default-eip: &quot;true&quot; kubectl.kubernetes.io/last-applied-configuration: | &#123;&quot;apiVersion&quot;:&quot;network.kubesphere.io/v1alpha2&quot;,&quot;kind&quot;:&quot;Eip&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;eip.openelb.kubesphere.io/is-default-eip&quot;:&quot;true&quot;&#125;,&quot;name&quot;:&quot;eip-pool&quot;&#125;,&quot;spec&quot;:&#123;&quot;address&quot;:&quot;10.0.0.50-10.0.0.60&quot;,&quot;disable&quot;:false,&quot;interface&quot;:&quot;eth0&quot;,&quot;protocol&quot;:&quot;layer2&quot;&#125;&#125; creationTimestamp: &quot;2024-06-01T10:28:40Z&quot; finalizers: - finalizer.ipam.kubesphere.io/v1alpha1 generation: 2 name: eip-pool resourceVersion: &quot;3902&quot; uid: 791498be-c69a-492e-8db2-7387df54f777spec: address: 10.0.0.50-10.0.0.60 interface: eth0 protocol: layer2status: firstIP: 10.0.0.50 #显示这三个说明成功了 lastIP: 10.0.0.60 # poolSize: 11 # v4: true 4.3.5 部署MetalLBMetalLB核心功能的实现依赖于两种机制： 地址分配：基于指定的地址池进行分配； 对外公告：让集群外部的网络了解新分配的IP地址，MetalLB使用ARP、NDP或BGP实现 kube-proxy工作于ipvs模式时，必须要使用严格ARP（StrictARP）模式，因此，若有必要，先运行如下命令，配置kube-proxy。 123kubectl get configmap kube-proxy -n kube-system -o yaml | \\sed -e &quot;s/strictARP: false/strictARP: true/&quot; | \\kubectl apply -f - -n kube-system 随后，运行如下命令，即可部署MetalLB至Kubernetes集群。 12METALLB_VERSION=&#x27;v0.13.12&#x27;kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/$&#123;METALLB_VERSION&#125;/config/manifests/metallb-native.yaml 创建Layer2模式地址池，下面是一个IPAddressPool资源示例 12345678910111213[root@master1 ~]#vim localip-pool.yamlapiVersion: metallb.io/v1beta1kind: IPAddressPoolmetadata: name: localip-pool namespace: metallb-systemspec: addresses: - 10.0.0.51-10.0.0.80 autoAssign: true avoidBuggyIPs: true [root@master1 ~]#kubectl apply -f localip-pool.yaml 创建二层公告机制 1234567891011121314151617181920212223242526[root@master1 ~]#vim localip-pool-l2a.yamlapiVersion: metallb.io/v1beta1kind: L2Advertisementmetadata: name: localip-pool-l2a namespace: metallb-systemspec: ipAddressPools: - localip-pool interfaces: - eth0 [root@master1 ~]#kubectl apply -f localip-pool-l2a.yaml[root@master1 ~]#kubectl get pods -n metallb-system NAME READY STATUS RESTARTS AGEcontroller-786f9df989-qd2w2 1/1 Running 0 8m40sspeaker-lmw9x 1/1 Running 4 (6m33s ago) 8m40sspeaker-r48xn 1/1 Running 4 (6m22s ago) 8m40sspeaker-zjw8f 1/1 Running 1 (6m17s ago) 8m40s[root@master1 ~]#kubectl get service demoappNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp LoadBalancer 10.103.38.104 10.0.0.51 80:32765/TCP 18m[root@master1 ~]#curl 10.0.0.51iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: demoapp-7c58cd6bb-r6q6c, ServerIP: 10.244.2.5! 注意事项：OpenStack 上的 MetalLB 您可以在 OpenStack VM 上运行 Kubernetes 集群，并使用 MetalLB 作为负载均衡器。但是，如果您想使用 L2 模式，则必须禁用 OpenStack 的 ARP 欺骗保护。您必须在所有运行 Kubernetes 的 VM 上禁用它。 根据设计，MetalLB 的 L2 模式看起来像是针对 OpenStack 的 ARP 欺骗尝试，因为我们正在宣布 OpenStack 不知道的 IP 地址。目前没有办法让 OpenStack 与 MetalLB 合作，所以我们必须完全关闭欺骗保护。 在执行 kubectl apply -f metallb-native.yaml 后看到的输出显示 MetalLB 的各种资源已经被成功创建了。然而，当你检查 metallb-system 命名空间中的 pod 时却没有看到任何资源。这种情况可能有几种原因，以下是一些排查步骤： 1234#检查事件和日志kubectl get events -n metallb-systemkubectl logs deployment/controller -n metallb-systemkubectl logs daemonset/speaker -n metallb-system 部署地址池或者二层公告机制报错 1234567891011121314[root@master1 ~]#kubectl apply -f localip-pool.yaml Error from server (InternalError): error when creating &quot;localip-pool.yaml&quot;: Internal error occurred: failed calling webhook &quot;ipaddresspoolvalidationwebhook.metallb.io&quot;: failed to call webhook: Post &quot;https://webhook-service.metallb-system.svc:443/validate-metallb-io-v1beta1-ipaddresspool?timeout=10s&quot;: context deadline exceeded#解決[root@master1 ~]#kubectl get validatingwebhookconfigurationsNAME WEBHOOKS AGEmetallb-webhook-configuration 7 17m[root@master1 ~]#kubectl get mutatingwebhookconfigurationsNAME WEBHOOKS AGEregistry-proxy-webhook 1 105s[root@master1 ~]#kubectl delete validatingwebhookconfigurations metallb-webhook-configuration[root@master1 ~]#kubectl delete mutatingwebhookconfigurations registry-proxy-webhook 4.4 HeadlessService的各类型中，ClusterIP、NodePort和LoadBalancer都为其Service配置一个ClusterIP，CoreDNS上，这些Service对象的A记录也解析为它的ClusterIP。广义上，那些没有ClusterIP的Service则称为Headless Service 应用于有标签选择器的有状态应用StatefulSet；或者没有标签选择器但有着与Service对象同名的Endpoint资源。Service的DNS名称直接解析为后端各就绪状态的Pod的IP地址，调度功能也将由DNS完成。各Pod IP相关PTR记录将解析至Pod自身的名称，不会解析至Service的DNS名称 假设Pod IP为 a.b.c.d，则其名称为 a-b-c-d.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt; 范例：有标签选择器 1234567891011121314151617181920212223242526272829303132333435[root@master1 services]#vim demoapp-headless-svc.yamlkind: ServiceapiVersion: v1metadata: name: demoapp-headless-svcspec: clusterIP: None selector: app: demoapp #解析到这个标签下的后端各就绪状态的Pod的IP地址 ports: - port: 80 targetPort: 80 name: http [root@master1 services]#kubectl apply -f demoapp-headless-svc.yaml -n demo[root@master1 services]#kubectl get svc -n demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdemoapp-headless-svc ClusterIP None &lt;none&gt; 80/TCP 8sexternalname-redis-svc ExternalName &lt;none&gt; redis.ik8s.io 6379/TCP 12m[root@master1 services]#kubectl exec -it -n demo demoapp-7c58cd6bb-bldpv -- /bin/sh#解析至Pod自身的IP[root@demoapp-7c58cd6bb-bldpv ~]# host -t A demoapp-headless-svcdemoapp-headless-svc.demo.svc.cluster.local has address 10.244.2.4demoapp-headless-svc.demo.svc.cluster.local has address 10.244.1.4#反向解析至Pod自身的名称[root@demoapp-7c58cd6bb-bldpv ~]# host -t PTR 10.244.1.44.1.244.10.in-addr.arpa domain name pointer 10-244-1-4.demoapp-headless-svc.demo.svc.cluster.local.[root@master1 ~]#kubectl get pods -n demo -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdemoapp-7c58cd6bb-bldpv 1/1 Running 0 10m 10.244.1.4 node1.wang.org &lt;none&gt; &lt;none&gt;demoapp-7c58cd6bb-bls8m 1/1 Running 0 10m 10.244.2.4 node2.wang.org &lt;none&gt; &lt;none&gt; 范例：没有标签选择器，但有着与Service对象同名的Endpoint资源 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@master1 services]#vim mysql-endpoints-demo.yamlapiVersion: v1kind: Endpointsmetadata: name: mysql-external namespace: defaultsubsets:- addresses: #指定集群外部的后端服务器的IP地址 - ip: 172.29.9.51 - ip: 172.29.9.52 ports: #定义Endpoints的端口映射 - name: mysql port: 3306 protocol: TCP---apiVersion: v1kind: Servicemetadata: name: mysql-external #必须与Endpoints一致 namespace: default #必须与Endpoints一致spec: type: ClusterIP ports: - name: mysql port: 3306 targetPort: 3306 protocol: TCP [root@master1 services]#kubectl apply -f mysql-endpoints-demo.yaml[root@master1 services]#kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmysql-external ClusterIP 10.103.90.57 &lt;none&gt; 3306/TCP 17s[root@master1 services]#kubectl describe svc mysql-external Name: mysql-externalNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: &lt;none&gt;Type: ClusterIPIP Family Policy: SingleStackIP Families: IPv4IP: 10.103.90.57IPs: 10.103.90.57Port: mysql 3306/TCPTargetPort: 3306/TCPEndpoints: 172.29.9.51:3306,172.29.9.52:3306 #这两个集群外部的服务器，与节点在同一网段，Pod就可以正常访问Session Affinity: NoneEvents: &lt;none&gt; 4.5 ExternalNameDNS 重定向机制，用于将集群外部的服务引入到集群中，将集群内的服务名称映射到集群外的域名，即Service的DNS名称将会生成一条CNAME记录，通过externalName字段进行设置。当集群内的 Pod 访问这个服务时，DNS 解析会将服务名称解析为外部域名。 ServiceName –&gt; external Service DNS Name 适用于还没有引入K8s集群，但准备引入K8s集群中的外部服务 需要借助于ClusterDNS上的CNAME资源记录完成，即外部服务必须有可以解析的域名，如果外部服务没有域名，而只有ip+port，那我们无法指定ExternalName，此时只能通过自建endpoint来实现 特殊类型Headless Service，无需ClusterIP和NodePort，也无须定义标签选择器发现Pod对象，即无标签选择器且也没有与Service对象同名的Endpoint资源 例如nginx和WordPress在集群内，MySQL在集群外，想要让WordPress访问MySQL，就在集群中定义一个service name，通过CoreDNS解析到MySQL机器的域名，最后再解析成MySQL机器的IP地址，即 Service_name –&gt; Mysql_name –&gt; Mysql_IP 12345678910111213141516171819202122232425262728293031323334[root@master1 services]#kubectl create namespace demo[root@master1 services]#kubectl create deployment demoapp --image=ikubernetes/demoapp:v1.0 --replicas=2 -n demo[root@master1 services]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-bldpv 1/1 Running 0 48sdemoapp-7c58cd6bb-bls8m 1/1 Running 0 48s[root@master1 services]#vim externalname-redis-svc.yamlkind: ServiceapiVersion: v1metadata: name: externalname-redis-svc #Service名称 namespace: demospec: type: ExternalName externalName: redis.ik8s.io #DNS解析的外部目标域名 ports: - protocol: TCP port: 6379 targetPort: 6379 nodePort: 0 selector: &#123;&#125; #选择器部分为空，表示这个Service不与任何特定的Pod关联 [root@master1 services]#kubectl apply -f externalname-redis-svc.yaml[root@master1 services]#kubectl get svc -n demo NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEexternalname-redis-svc ExternalName &lt;none&gt; redis.ik8s.io 6379/TCP 15s[root@master1 services]#kubectl exec -it -n demo demoapp-7c58cd6bb-bldpv -- /bin/sh[root@demoapp-7c58cd6bb-bldpv ~]# host -t A externalname-redis-svcexternalname-redis-svc.demo.svc.cluster.local is an alias for redis.ik8s.io.redis.ik8s.io has address 1.2.3.4 5 Service流量转发机制ClusterIP类型：默认不 SNAT，可保留源 IP 没启动 masquerade-all ：请求报文的目标IP和目标端口由Service转为挑选出的后端Pod的IP和端口，并由后端Pod直接响应给客户端Pod 发夹问题：客户端 Pod 和后端 Pod 是同一个 Pod，Pod访问自身所属的Service后，由Service又调度回该Pod，需要启用源IP地址转换来解决 启用 masquerade-all：请求报文的源IP和目标IP都将由Service进行转换，源IP将转为客户端Pod所在节点在Pod网络中的IP地址 NodePort和LoadBalancer类型 流量来自集群外部，节点扮演网关的角色，请求报文的源IP和目标IP都将由Service进行转换，源IP将转为客户端Pod所在节点的IP地址，以确保响应报文能正确送达，但会掩盖客户端 IP，通过local策略可获取 流量转发示意图：请求报文，目标地址转换都会进行，未启用 masquerade-all 时，源地址转换将视情况进行 Pod网络是虚拟网络，最终还是要通过节点网络完成报文传送，因此，SNAT通常会使用节点网络中的地址 6 在k8s中暴漏单体服务单体服务（整个软件就只有一个应用构成）跑在一个或多个pod中（为了实现pod的故障自愈会控制器来管理） 给这些pod创建一个svc来负责代理，svc可以是两种type都可以 NodePort + 在k8s集群外自建负载均衡 应用场景：集群规模不大，自己管理负载均衡能应付过来 LoadBalaner+ 在k8s集群外云平台会提供现成的负载均衡 应用场景：整个k8s环境是跑在云平台上，可以利用云平台的特性，集群规模大也能应付过来，因为是自动维护的 流程：用户访问集群外的负载均衡的 ip:port —&gt; 集群外的负载均衡 —&gt; 计算出一个k8s的物理节点的 ip:Port —&gt; 匹配ipvs规则 —&gt; ipvs规则会计算出一个pod的 ip:port —&gt; 基于网络插件完成封包然后发送","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Service","slug":"Service","permalink":"https://aquapluto.github.io/tags/Service/"}]},{"title":"Operator","slug":"Kubernetes/controller/operator","date":"2025-09-12T03:34:12.000Z","updated":"2025-09-12T04:00:07.864Z","comments":true,"path":"Kubernetes/controller/operator/","permalink":"https://aquapluto.github.io/Kubernetes/controller/operator/","excerpt":"","text":"1 Operator 简介比如我们创建Redis集群或者MySQL集群，StatefulSet只能帮我们创建好Pod，给他们提供各自存储数据的卷，但是从Pod怎么去关联主Pod，StatefulSet做不到，需要我们手动执行命令，想要它自动关联，就需要自己写代码来完成，所以就有了Operator Operator 是增强型的控制器（Controller），它扩展了Kubernetes API的功能，并基于该扩展管理复杂应用程序 Operator 是 Kubernetes 的扩展软件， 它利用定制的资源类型来增强自动化管理应用及其组件的能力，从而扩展了集群的行为模式 使用**自定义资源（例如CRD）**来管理应用程序及其组件，支持新的资源类型 提供 RESTful 接口，方便用户通过 kubectl 或 API 客户端操作自定义资源 支持声明式 API，用户可以通过 YAML 文件定义资源的状态，由控制器处理具体实现 将应用程序视为单个对象，并提供面向该应用程序的自动化管控操作，例如部署、配置、升级、备份、故障转移和灾难恢复等 使用 Operator 部署 kafka 1https://strimzi.io/quickstarts/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Operator","slug":"Operator","permalink":"https://aquapluto.github.io/tags/Operator/"}]},{"title":"HPA","slug":"Kubernetes/controller/HPA","date":"2025-09-12T03:34:06.000Z","updated":"2025-09-12T09:04:46.988Z","comments":true,"path":"Kubernetes/controller/HPA/","permalink":"https://aquapluto.github.io/Kubernetes/controller/HPA/","excerpt":"","text":"1 HPA概念HPA（Horizontal Pod Autoscaler）控制器：主要用于通过增加或减少Pod副本数来水平扩展应用，适用于无状态应用，根据指定的指标自动调整。HPA不适用于无法缩放的对象，例如Daemonset v1 版本：仅支持 CPU 和内存指标，若实际使用率超过设定的目标值，HPA 会增加 Pod 的数量，反之亦然 v2 版本：在 v1 版本的基础上进行了扩展，支持自定义指标，需要借助Metrics Server获取需要的指标 VPA（Vertical Pod Autoscaler）控制器：通过调整单个Pod的资源请求和限制来垂直扩展应用，适用于有状态应用和那些不能简单通过增加副本数来扩展的场景。 2 HPA的工作原理HPA控制器资源周期性从一系列的API中获取pod的运行指标 常规指标：一般由 metrics-server 提供例如cpu、内存指标，metrics-server 提供 metrics.k8s.io API 为pod资源的使用提供支持 自定义指标Custom metrics：从 Kubernetes 集群内部的应用程序中收集的。这些指标可以通过Prometheus或其他监控工具采集，通常与特定应用程序或 Pod 的性能相关 外部指标：允许 HPA 根据 Kubernetes 集群外部的服务指标来进行扩展，来自于 Kubernetes 集群外部的资源或服务，通常是由外部的监控系统或服务提供的。这些指标不依赖于 Kubernetes 内部的资源，可能包括外部服务的响应时间、消息队列的长度、数据库的请求数等 然后HPA基于用户定义的扩缩容规则，通过自己内部计算算法来计算出应该扩容&#x2F;缩容的副本数，根据计算出的desiredReplicas数量，HPA控制器就像Pod的副本控制器(Deployment、Replicaset或Statefulset)发起Scale操作来自动实现Pod副本的扩缩 如果 desiredReplicas 大于当前的 Pod 数量，HPA 会触发扩容 如果 desiredReplicas 小于当前的 Pod 数量，HPA 会触发缩容 如果同一个 Deployment（或其他可伸缩资源）被多个 HPA 同时关联，最终的 Pod 副本数会以这些 HPA 计算出的「最大期望副本数」为准 综上，kubernetes的指标体系分为两种指标 核心指标：metrics.k8s.io, Metrics Server 自定义指标：custom.metrics.k8s.io, Prometheus Adpater + Prometheus Server 2.1 最小和最大副本数限制HPA 还考虑了 minReplicas 和 maxReplicas 的设置,确保最终的 Pod 副本数在定义的最小和最大范围内。 即使计算出的 desiredReplicas 超过了 maxReplicas 或低于 minReplicas，HPA 也会将其限制在定义的范围内。 2.2 防抖机制为了避免频繁地扩缩容(可能因为短暂的指标波动)，HPA引入了防抖机制。 扩容：如果计算出的扩缩比（度量指标 &#x2F; 期望指标）例接近 1.0，将会放弃本次扩缩 缩容：缩容前有个冷却&#x2F;延迟时间，如果延迟(冷却)时间设置的太短，那么副本数量有可能跟以前一样出现抖动。 默认值是5分钟 – horizontal-pod-autoscaler-downscale-stabilization 此外需要注意：指定了多个指标， 那么会按照每个指标分别计算扩缩副本数，取最大值进行扩缩。 2.3 容忍度在 Kubernetes 中，Horizontal Pod Autoscaler（HPA）通过监控资源指标（如 CPU、内存或自定义指标），自动调整工作负载（Deployment &#x2F; StatefulSet 等）的副本数。为了避免因为轻微抖动导致频繁扩缩容（flapping），Kubernetes 1.33 起，HPA 引入了“容忍度”机制：当前指标与目标值的偏差在一定范围内，不触发伸缩。 Kubernetes 1.33：HPA 可配置容忍度（tolerance）特性深度解读与实践指南 2.4 防抖机制和容忍度的区别它们都是用于避免 HPA 频繁扩缩容（“抖动”）的策略，但它们的设计目标和实现方式不同 维度 防抖机制（Stabilization Window） 容忍度（Tolerance） 核心目标 过滤短期波动（指标在时间维度上的不稳定）。 过滤微小偏差（指标与目标值在数值上的接近程度）。 判断依据 基于时间窗口内的指标趋势（是否持续满足条件）。 基于实际值与目标值的差异比例（是否超过阈值）。 适用场景 解决指标因突发流量（如瞬间峰值）导致的频繁扩缩容。 解决指标因正常波动（如微小负载变化）导致的频繁调整。 参数性质 时间参数（秒），控制等待时长。 比例参数（0~1），控制偏差容忍范围。 3 Metrics ServerMetrics Server的作用是收集和存储Kubernetes集群中各种资源的性能指标数据，并提供查询和分析这些数据的功能。Metrics Server可以收集以下类型的指标数据 Pod指标：包括CPU使用率、内存使用情况、磁盘IO等。 Node指标：包括CPU使用率、内存使用情况、磁盘IO等。 Service指标：包括请求次数、响应时间等。 Endpoints指标：包括请求成功率、错误率等。 Custom Metrics：用户自定义的指标，例如应用程序的性能指标等。 Metrics Server可以将收集到的指标数据存储在Prometheus格式的时间序列数据库中，并提供查询API供其他组件（如Grafana、Prometheus等）进行数据分析和可视化展示。通过Metrics Server，用户可以更好地了解Kubernetes集群的资源使用情况，优化应用程序的性能，并及时发现潜在的问题。 Metrics API URl为 /apis/metrics.k8s.io/，在 k8s.io/metrics 下维护。例如：用如下API可以获取到该 Pod 的资源数据 1https://10.96.0.1/apis/metrics.k8s.io/v1beta1/namespaces/&lt;namespace-name&gt;/pods/&lt;pod-name&gt; 具体来说 metrics-server 通过调用 kubelet 的 Summary APl 来获取上述数据的。此外需要注意：Metrics API只可以查询当前的度量数据，并不保存历史数据。 1HPA --(metrics.k8s.io/vlbetal)--&gt; Metrics APi --&gt; Metrics Server --(kubernetes.summary api)--&gt; Kubelet 3.1 API Aggregator在Kubernetes 中，API Aggregator(API聚合器)是一种机制，用于扩展 Kubernetes APl Server 的功能。它允许 Kubernetes 集群支持额外的 API 组，使得自定义的 API 资源可以被注册并通过Kubernetes的API Server进行访问 Metrics Server 是一个扩展的 APServer，是依赖于API Aggregator(API聚合器)。所以，在安装Metrics Server 之前需要先在 kube-apiserver 中开启 API Aggregator。 3.2 部署Metrics Server开启 API Aggregator 123# 修改每个 API Server 的 kube-apiserver.yaml 配置开启 Aggregator Routing：修改 manifests 配置后 API Server 会自动重启生效vim /etc/kubernetes/manifests/kube-apiserver.yaml在spec.containers.command中添加 --enable-aggregator-routing=true 部署安装 [GitHub地址](Releases · kubernetes-sigs&#x2F;metrics-server · GitHub) 1~# wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.8.0/components.yaml 在部署之前，修改 components.yaml 之前部署集群用的自签名证书，metrics-server直接请求kubelet接口会证书校验失败，因此deployment中 spec.template.spec.containers.args 增加 - --kubelet-insecure-tls 参数。 另外镜像原先在registry.k8s.io，国内下载不方便，配置中可以修改成国内镜像仓库地址。内网环境中可以先下载，然后再推到内网镜像仓库，镜像也改成内网镜像地址。 12k8s.mirror.nju.edu.cn/metrics-server/metrics-server:v0.8.0registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.8.0 默认部署的Metrics Server依赖于Cluster DNS响应关于各节点DNS名称的解析，如果我们自定义K8s集群，而且通过节点上的hosts文件解析时，可以使用以下的 yaml 文件 1wget https://raw.githubusercontent.com/iKubernetes/learning-k8s/master/metrics-server/components.yaml 如果想在Cluster DNS做解析， 在 CoreDNS 的配置中添加静态的 A 记录 12345678910111213141516apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; ... hosts &#123; 10.0.0.184 node1 10.0.0.186 node2 fallthrough &#125; ... &#125; 安装成功后就可以使用 kubectl top 命令查看 123456789101112131415kubectl top nodes # 查看的是使用情况# 如下：（cpu的使用了多少、使用的百分比是多少），（内存使用了多少、使用的百分比是多少）NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master-01 97m 2% 1133Mi 61% k8s-node-01 37m 0% 653Mi 35% k8s-node-02 28m 2% 764Mi 41% # 查看 top 命令的帮助kubectl top --help# 查看node节点的资源使用情况kubectl top node# 查看pod的资源使用情况kubectl top pod# 查看所有命名空间的pod资源使用情况kubectl top pod -A 3.3 基于CPU的HPA对象创建一个deployment，然后利用HPA来控制它自动扩缩 注意：HPA 主要使用 Pod 的 resources.requests 来计算 Pod 的资源使用率。而 resources.limits 字段并不直接参与 HPA 的计算 如果创建的 Pod 对象没有添加 request 资源声明，会导致 HPA 读取不到 CPU 或内存的指标信息，虽然HPA的Pod成功Running，但是使用 describe 命令查看的时候，会有警告信息：failed to get cpu utilization: missing request for cpu 12345678910111213141516171819202122apiVersion: apps/v1kind: Deploymentmetadata: name: hpa-demospec: selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 resources: requests: memory: 50Mi cpu: 50m # 添加cpu的requests 创建一个HPA对象来控制上面的deployment 123kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10创建了一个关联资源 hpa-demo 的HPA，最小的Pod副本数为1，最大为10。示例中，--cpu-percent=10表示我们希望Pod的平均CPU使用率保持在10%左右。如果实际CPU使用率高于这个值，HPA会增加副本数;如果低于这个值，HPA会减少副本数。 123456789101112131415161718apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata: name: hpa-nginxspec: scaleTargetRef: # 目标作用对象 apiVersion: apps/v1 kind: Deployment name: hpa-nginx # 关联的 Deployment 名称 minReplicas: 1 # 最小扩容1个节点（pod） maxReplicas: 10 # 最大扩容到10个节点（pod） metrics: # 目标指标值。在metrics中通过参数type定义指标的类型;通过参数target定义相应的指标目标值系统将在指标数据达到目标值时(考虑容忍度的区间，见前面算法部分的说明)触发扩缩容操作。 - type: Resource # 基于资源的指标类型 resource: name: cpu # 监控 CPU 资源 target: type: Utilization # 基于使用率的目标值 averageUtilization: 40 # CPU 平均使用率达到 40% 触发扩缩容，低于40%就是缩容 3.4 基于内存的HPA对象要使用基于内存或者自定义指标进行扩缩容（现在的版本都必须依赖 metrics-server 这个项目） 12345678910111213141516171819apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata: name: hpa-mem-demo namespace: defaultspec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: hpa-mem-demo # 关联的 Deployment 名称 minReplicas: 1 # 最小副本数 maxReplicas: 5 # 最大副本数 metrics: - type: Resource resource: name: memory # 监控内存资源 target: type: Utilization # 基于使用率的阈值类型 averageUtilization: 30 # 内存平均使用率达到 30% 触发扩缩容 4 Prometheus和Prometheus AdapterPrometheus Server：扩展的指标服务器，比如应用层的指标，扩展提供更多的指标给Kubernetes上的组件使用 Prometheus的原生指标不被Kubernetes兼容，自身也不支持作为指标服务器使用（没有API群组作为调用入口） 适配器组件：Prometheus Adapter 提供一个API入口，注册了一个特定的API群组，custom.metrics.k8s.io（默认）或 external.metrics.k8s.io，或许后面会变为 metrics.k8s.io 转换指标格式为Kubernetes兼容的格式，从而允许像HPA这样的Kubernetes组件利用这些扩展指标进行决策 https://github.com/kubernetes-sigs/prometheus-adapter Prometheus Adapter：将 Prometheus 中的监控指标转换为 Kubernetes 可识别的 Custom Metrics API 格式 指标抓取：定期从 Prometheus 查询预设的指标（如 http_requests_total）。 格式转换：将 Prometheus 的时序数据转换为 Kubernetes 的 metrics.k8s.io/v1beta1 API 结构。 API 注册：作为 Custom Metrics API Server 注册到 Kubernetes API Server聚合层。 4.1 kube-aggregator我们知道kubernetes的资源类型是由 API Resources + Controller 控制的，那么想要扩展Kubernetes，支持更多的资源类型，有以下三种方式 API Server和Controller Manager的二次开发，但是这种方式需要每次都要去和官方的新版本进行适配； 扩展的API Server，Prometheus Adapter就是这种方式，这种方式需通过 API 聚合层 (kube-aggregator) 去扩展； kube-aggregator：将多个 API Server（如 metrics-server、Prometheus Adapter）聚合到主 API Server Aggregator 允许开发人员编写一个自己的服务，把这个服务注册到 Kubernetes 的 APIServer 里面去，这样我们就可以像原生的 APIServer 提供的 API 使用自己的 API 了，我们把自己的服务运行在 Kubernetes 集群里面，然后 Kubernetes 的 Aggregator 通过 Service 名称就可以转发到我们自己写的 Service 里面去了 https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/ 用户请求 → Kubernetes API Server → kube-aggregator → 路由到对应的扩展 API Server（如 Prometheus Adapter） CRD (Custom Resource Defination) + Operator 对于扩展 API Server，kubernetes必须让Kubernetes API 聚合层（kube-aggregator）强制要求所有扩展 API Server 必须通过 HTTPS 提供服务，以确保通信的安全性。所以Prometheus Adapter 必须提供 TLS 证书和私钥，证书需由Kubernetes 集群的根证书颁发机构（CA）签发或者该证书的CA添加到 Kubernetes API Server 的信任链中。 4.2 部署Prometheus和Prometheus Adapter在k8s部署Prometheus 部署Prometheus Adapater 1、创建名为custom-metrics的名称空间 1kubectl create namespace custom-metrics 2、创建一个名为 cm-adapter-serving-certs 的secret对象，它需要具备 serving.crt 和 serving.key 两个键。These are the serving certificates used by the adapter for serving HTTPS traffic，更多的信息请参考 auth concepts documentation。我们也可通过如下命令，运行目录中预置的 gencerts.sh 脚本进行创建。该脚本会在manifests目录下创建一个 cm-adapter-serving-certs.yaml 的文件，它提供了相关secret对象的配置。另外，该脚本依赖于golang的cfssl模块 12apt-get install -y golang-cfsslbash gencerts.sh 3、运行如下命令，在custom-metrics名称空间中部署prometheus-adapter 1kubectl apply -f manifests/ 提示： 在部署prometheus-adpater之前，你可能需要事先修改ConfigMap资源，以添加需要暴露的自定义指标 该部署示例中的prometheus-adapter会向prom名称空间中prometheus service（prometheus.prom.svc.cluster.local）发起查询请求，请确保prometheus service的访问路径指向了正确的位置。 4、部署示例应用。该示例应用提供了一个Counter类型的指标http_requests_total 1kubectl apply -f example-metrics/metrics-example-app.yaml 5、将metrics-app示例应用中暴露的指标http_requests_total转为速率指标，并提供给Kubernetes的自定义指标管道使用 1kubectl apply -f example-metrics/custom-metrics-config-map.yaml 6、确认配置被重新加载后，即可使用相关的指标。例如下面的命令，通过Kubernetes的 custom.metrics API 获取到了metrics-app的两个Pod上的指标数据 1kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second | jq . 4.3 基于Pod QPS的HPA对象HPA资源参数 kubectl explain hpa.spec --api-version=&quot;autoscaling/v2&quot; 12345678910111213141516171819202122kind: HorizontalPodAutoscalerapiVersion: autoscaling/v2metadata: name: metrics-app-hpaspec: scaleTargetRef: # 指定要扩缩的目标资源 apiVersion: apps/v1 kind: Deployment name: metrics-app minReplicas: 2 # Pod 副本数的下限 maxReplicas: 10 # Pod 副本数的上限 metrics: - type: Pods # 指标类型，这里表示基于 Pod 级别的自定义指标 pods: metric: name: http_requests_per_second # 指标名称 target: type: AverageValue # 表示每个 Pod 的平均值 averageValue: 5 # 表示每个Pod平均每秒处理5个请求，如果超过或低于这个平均值，HPA会扩缩容 behavior: # 定义了 HPA 扩缩容的行为策略 scaleDown: # 定义了缩减副本时的行为 stabilizationWindowSeconds: 120 # 指定缩容稳定窗口时间，避免因指标波动导致频繁缩容 指标查询过程 HPA 控制器定期向 Kubernetes API Server 发起指标查询请求。 API Server 通过 kube-aggregator 将请求路由到 Prometheus Adapter。 Adapter 将请求查询转换为 PromQL，从Prometheus获取数据，然后将结果转换为Kubernetes自定义指标格式，返回给HPA HPA 收到的数据示例 metrics-app-abc123 Pod：8 QPS metrics-app-def456 Pod：12 QPS 扩缩决策 计算当前总负载：假设当前有 2 个 Pod，总 QPS 为 8 + 12 = 20 计算所需副本数：根据 HPA 配置的目标值 averageValue: 5（每个 Pod 平均处理 5 QPS），所需副本数 &#x3D; ceil(总负载 / 目标平均值) &#x3D; ceil(20 / 5) &#x3D; 4 应用边界约束：当前副本数 2、计算结果 4、约束条件：minReplicas=2，maxReplicas=10、最终决策：将副本数从 2 扩容到 4 稳定窗口（缩容保护）：如果后续流量下降，触发缩容。如果总 QPS 降至 6，计算所需副本数为 ceil(6/5) = 2（两个 Pod 各 3 QPS），HPA 不会立即缩容到 2，而是等待 120 秒 若在此期间指标反弹（如 QPS 又升到 10），则取消缩容。 若窗口期内指标持续低于目标值，最终缩容到 2。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"HPA","slug":"HPA","permalink":"https://aquapluto.github.io/tags/HPA/"}]},{"title":"Job-CronJob","slug":"Kubernetes/controller/job-cronjob","date":"2025-09-12T03:33:59.000Z","updated":"2025-09-12T03:57:57.663Z","comments":true,"path":"Kubernetes/controller/job-cronjob/","permalink":"https://aquapluto.github.io/Kubernetes/controller/job-cronjob/","excerpt":"","text":"1 Job控制器的应用编排机制Job负责编排运行有结束时间的“一次性”任务，而前面的Deployment和DaemonSet主要负责编排始终运行的守护进程类应用 控制器要确保Pod内的进程“正常（成功完成任务）”地退出 非正常退出的Pod可以根据需要重启，并在重试一次的次数后终止 有些 Job 是单次任务，也有些 Job 需要运行多次（次数通常固定） 有些任务支持同时创建及并行运行多个Pod以加快任务处理速度，Job控制器还允许用户自定义其并行度 Job中也是一个Pod模板，不一样的是 job的pod中的容器要求是一个可以执行完毕、可以退出的任务，而不是一个常驻前台的进程 Job的RestartPolicy仅支持设置为Never或OnFailure，不能是Always 2 Job资源规范Job资源同样需要标签选择器和Pod模板，但它不需要指定replicas，而是应该给定completions，需要完成的作业次数，默认为1次 Job资源会为其Pod对象自动添加 “job-name=JOB_NAME”和“controller-uid=UID” 标签，并使用标签选择器完成对controller-uid标签的关联，因此，selector必选但可以不定义 Pod的命名格式：$(job-name)-$(index)-$(random-string) ，其中的$(index)字段取值与completions和completionMode有关 Job资源所在群组为 “batch&#x2F;v1” 重启策略以及backoffLimit restartPolicy: OnFailure： 当pod运行失败后，会重启该pod本身（不会创建新pod），最多重启backoffLimit次 restartPolicy: Never：当pod运行失败后，不会重启pod本身了，但是job控制器会创建新的pod，最多新建backoffLimit个新pod 123456789101112131415apiVersion: batch/v1 # API群组及版本；kind: Job # 资源类型特有标识；metadata: name &lt;string&gt; # 资源名称，在作用域中要惟一； namespace &lt;string&gt; # 名称空间；Job资源隶属名称空间级别；spec: selector &lt;object&gt; # 标签选择器，必须匹配template字段中Pod模板中的标签；必须要有但可以不定义 suspend &lt;boolean&gt; # 是否挂起当前Job的执行，挂起作业会重置StartTime字段的值； template &lt;object&gt; # Pod模板对象； completions &lt;integer&gt; # 期望的成功完成的作业次数，成功运行结束的Pod数量； completionMode &lt;string&gt; # 追踪Pod完成的模式，支持Indexed和NonIndexed（默认）两种； ttlSecondsAfterFinished &lt;integer&gt; # 终止状态作业的生存时长，超期将被删除，即job完成后多少秒后自动做清理操作，不然会一直存在 parallelism &lt;integer&gt; # 作业的最大并行度，默认为1； backoffLimit &lt;integer&gt; # 将作业标记为“Failed”之前的重试次数，默认为6； activeDeadlineSeconds &lt;integer&gt; # 作业启动后可处于活动状态的时长； 范例 1234567891011121314151617apiVersion: batch/v1kind: Jobmetadata: name: job-demospec: template: spec: containers: - name: myjob image: ikubernetes/admin-box:v1.0 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 30&quot;] restartPolicy: Never completions: 3 ttlSecondsAfterFinished: 3600 backoffLimit: 3 activeDeadlineSeconds: 300 3 Job的状态123[root@master1 jobs_and_cronjobs]#kubectl get jobsNAME COMPLETIONS DURATION AGEjob-demo 0/2 21s 21s 状态描述（示例集群中共有两个工作节点）： COMPLETIONS：已经正常完成任务并退出的Pod数量 DURATION：Job业务的实际运行时长 AGE：Job资源创建后的时长 使用 kubectl describe jobs 命令 4 并行式JobJob对象能够支持多个Pod的可靠、并发执行 编排彼此间相互通信的并行进程并非Job的设计目标，它仅用于支撑一组相互独立而又有所关联的工作任务的并行处理 常见的场景，有如发送电子邮件、渲染视频帧、编解码文件、NoSQL数据库中扫描主键范围等 并行式Job的关键配置参数 parallelism：任务并行度，即最大可同行运行的Pod数量，可以将其理解为工作队列的数量 completions：总共需要完成的任务数量，即总共需要多少个相关的Pod成功完成执行，通常要大于parallelism的值 12345678910111213141516171819apiVersion: batch/v1kind: Jobmetadata: name: para-job-demospec: template: spec: containers: - name: myjob image: ikubernetes/admin-box:v1.0 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 30&quot;] restartPolicy: OnFailure completions: 7 parallelism: 2 completionsMode: Indexed ttlSecondsAfterFinished: 3600 backoffLimit: 3 activeDeadlineSeconds: 1200 5 CronJob需要周期性运行的 Job，则由CronJob控制器负责编排，CronJob资源也是标准的API资源类型 CronJob 建立在 Job 的功能之上，是更高层级的控制器 它以 Job 控制器完成单批次的任务编排，而后为这种 Job 作业提供需要运行的周期定义 用于管理Job资源的运行时间，它允许用户在特定的时间或以指定的间隔运行Job 注意：在CronJob中，通配符 “?” 和 “*” 的意义相同，它们都表示任何可用的有效值 concurrencyPolicy 参数说明 Allow（默认）：CronJob 允许并发 Job 执行。，如果一个任务还没有结束，而下一次调度时间到了，新的任务会直接启动 Forbid：CronJob 不允许并发执行；如果新 Job 的执行时间到了而老 Job 没有执行完，CronJob 会忽略新 Job 的执行即新job不会启动。适用于只允许一个任务实例运行的场景。 Replace：如果新 Job 的执行时间到了而老 Job 没有执行完，那么老的会被终止Terminating掉，CronJob 会用新 Job 替换当前正在运行的 Job。 123456789101112131415apiVersion: batch/v1 # API群组及版本；kind: CronJob # 资源类型特有标识；metadata: name &lt;string&gt; # 资源名称，在作用域中要惟一； namespace &lt;string&gt; # 名称空间；CronJob资源隶属名称空间级别；spec: jobTemplate &lt;Object&gt; # job作业模板，必选字段； metadata &lt;object&gt; # 模板元数据； spec &lt;object&gt; # 作业的期望状态； schedule &lt;string&gt; # 调度时间设定，必选字段； concurrencyPolicy &lt;string&gt; # 并发策略，可用值有Allow、Forbid和Replace； failedJobsHistoryLimit &lt;integer&gt; # 失败作业的历史记录数，默认为1； successfulJobsHistoryLimit &lt;integer&gt; # 成功作业的历史记录数，默认为3； startingDeadlineSeconds &lt;integer&gt; # 因错过时间点而未执行的作业的可超期时长； suspend &lt;boolean&gt; # 是否挂起后续的作业，不影响当前作业，默认为false； 范例 12345678910111213141516171819202122232425262728apiVersion: batch/v1kind: CronJobmetadata: name: cronjob-demo namespace: defaultspec: schedule: &quot;*/2 * * * *&quot; jobTemplate: metadata: labels: controller: cronjob-demo spec: parallelism: 1 completions: 1 ttlSecondsAfterFinished: 600 backoffLimit: 3 activeDeadlineSeconds: 60 template: spec: containers: - name: myjob image: ikubernetes/admin-box:v1.2 command: - /bin/sh - -c - date; echo Hello from CronJob, sleep a while...; sleep 10 restartPolicy: OnFailure startingDeadlineSeconds: 300","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"job-cronjob","slug":"job-cronjob","permalink":"https://aquapluto.github.io/tags/job-cronjob/"}]},{"title":"Daemonset","slug":"Kubernetes/controller/daemonset","date":"2025-09-12T03:33:47.000Z","updated":"2025-09-12T03:50:52.292Z","comments":true,"path":"Kubernetes/controller/daemonset/","permalink":"https://aquapluto.github.io/Kubernetes/controller/daemonset/","excerpt":"","text":"1 DaemonSet简介使用DaemonSet编排应用 同Deployment相似，DaemonSet基于标签选择器管控一组Pod副本； DaemonSet用于确保所有或选定的工作节点上都运行有一个Pod副本 有符合条件的新节点进入时，DaemonSet会将Pod自动添加至相应节点 而节点的移出，相应的Pod副本也将被回收 工作原理 节点监视：DaemonSet通过监听节点的变化来工作。当新节点加入集群时，DaemonSet会自动在该节点上创建Pod副本。如果节点被删除，DaemonSet也会删除该节点上的Pod副本。 Pod管理：DaemonSet确保每个节点上只运行一个Pod实例，如果配置更新或Pod故障，DaemonSet会自动更新或重新创建Pod。 常用场景 特定类型的系统化应用，例如kube-proxy，以及Calico网络插件的节点代理calico-node等 集群存储守护进程、集群日志收集守护进程以及节点监控守护进程等 2 DaemonSet的资源规范与Deployment相似，DaemonSet对象也使用标签选择器和Pod模板，区别之处在于，DaemonSet不需要定义replicas（Pod副本数量），其Pod数量随节点数量而定；不存在先增加后删除Pod，所以没有maxSurge 1234567891011121314apiVersion: apps/v1 # API群组及版本；kind: DaemonSet # 资源类型特有标识；metadata: name &lt;string&gt; # 资源名称，在作用域中要惟一； namespace &lt;string&gt; # 名称空间；DaemonSet资源隶属名称空间级别；spec: minReadySeconds &lt;integer&gt; # Pod就绪后多少秒内任一容器无crash方可视为“就绪”； selector &lt;object&gt; # 标签选择器，必须匹配template字段中Pod模板中的标签； template &lt;object&gt; # Pod模板对象； revisionHistoryLimit &lt;integer&gt; # 滚动更新历史记录数量，默认为10； updateStrategy &lt;Object&gt; # 滚动更新策略 type &lt;string&gt; # 滚动更新类型，可用值有OnDelete和RollingUpdate； rollingUpdate &lt;Object&gt; # 滚动更新参数，专用于RollingUpdate类型； maxUnavailable &lt;string&gt; # 更新期间可比期望的Pod数量缺少的数量或比例； 范例 1234567891011121314151617181920212223242526apiVersion: apps/v1kind: DaemonSetmetadata: name: daemonset-demo namespace: defaultspec: selector: matchLabels: app: prometheus component: node-exporter template: metadata: name: prometheus-node-exporter labels: app: prometheus component: node-exporter spec: containers: - image: prom/node-exporter:v1.2.0 name: prometheus-node-exporter ports: - name: prom-node-exp containerPort: 9100 hostPort: 9100 hostNetwork: true #表示容器使用主机网络命名空间，因为node-exporter需要获取主机的信息 hostPID: true #表示容器使用主机进程ID命名空间 3 查看DaemonSet状态使用kubectl get daemonset命令 使用kubectl describe daemonsets命令 123456[root@master1 ~]#kubectl get daemonsets -ANAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEkube-flannel kube-flannel-ds 3 3 3 3 3 &lt;none&gt; 13dkube-system csi-nfs-node 3 3 3 3 3 kubernetes.io/os=linux 6d5hkube-system kube-proxy 3 3 3 3 3 kubernetes.io/os=linux 13dmetallb-system speaker 3 3 3 3 3 kubernetes.io/os=linux 5d 状态描述（示例集群中共有三个工作节点）： DESIRED：期望存在的Pod副本数 CURRENT：当前已存在的Pod副本数 READY：当前已经转为就绪状态的Pod副本数 UP-TO-DATE ：已经更新到期望版本的Pod副本数 AVAILABLE：可用的Pod副本数 NODE SELECTOR：节点选择器，&lt;none&gt;表示未使用选择器，因而将适配到所有节点 AGE：资源已经创建的时长 4 更新策略rollingUpdate：滚动更新，自动触发 onDelete：删除时更新，手动触发 5 触发更新配置策略：rollingUpdate更新策略支持使用maxUnavailable参数来定义单批次允许更新的最大副本数量 更新方式 kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... kubectl patch (-f FILENAME | TYPE NAME) [-p PATCH|--patch-file FILE] [options] 直接更新原配置文件，而后使用“kubectl apply”命令触发更新 其他操作跟 Deployment 一样","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Daemonset","slug":"Daemonset","permalink":"https://aquapluto.github.io/tags/Daemonset/"}]},{"title":"Statefulset","slug":"Kubernetes/controller/statefulset","date":"2025-09-12T03:33:41.000Z","updated":"2025-09-12T03:57:02.178Z","comments":true,"path":"Kubernetes/controller/statefulset/","permalink":"https://aquapluto.github.io/Kubernetes/controller/statefulset/","excerpt":"","text":"1 StatefulSet简介有状态指的是：应用的运行要依赖之前的数据 –&gt; 限制调度，pod只能调度到之前状态存在的节点上才可以 有状态应用会在其会话中保存客户端的数据，并且有可能会在客户端下一次的请求中使用这些数据 应用上常见的状态类型：会话状态、连接状态、配置状态、集群状态、持久性状态等 大型应用通常具有众多功能模块，这些模块通常会被设计为有状态模块和无状态模块两部分 业务逻辑模块一般会被设计为无状态，这些模块需要将其状态数据保存在有状态的中间件服务上，如消息队列、数据库或缓存系统等 无状态的业务逻辑模块易于横向扩展，有状态的后端则存在不同的难题 功能：负责编排有状态（Stateful Application）应用，为实现有状态应用编排，它依赖于几个特殊设计 稳定的网络标识（网络标识即dns域名）：每个pod都有一个固定的域名（FQDN），有状态应用的多个实例之间有依赖，不能相互替换，无论怎么调度，pod的ip地址随意变化，但这个域名不变，这依赖于一个专用的 Headless Service 实现 StatefulSet给每个Pod有一个固定的名字 &#96;&#96;$(statefulset name)-$(ordinal)&#96;，使得每个pod各自有一个固定FQDN名字，上游的访问可以直接访问该域名来访问pod FQDN：$(statefulset name)-$(ordinal).$(headless server name).namespace.svc.cluster.local，由CoreDNS直接解析为Pod IP 不需要SVC做流量转发、没有负载均衡的效果 稳定的次序：对于N个副本的StatefulSet，每个Pod名字后缀从0-N，按次序启停 OrderedReady：创建或扩容时，顺次完成各Pod副本的创建，且要求只有前一个Pod转为Ready状态后，才能进行后一个Pod副本的创建；删除或缩容时，逆序、依次完成相关Pod副本的终止 Parallel：各Pod副本的创建或删除操作不存在顺序方面的要求，可同时进行 稳定的存储：各Pod副本存储的状态数据并不相同，通过VolumeClaimTemplate为每个Pod创建一个PV。删除、减少副本，不会删除相关的卷，提供专用且稳定的 Volume 基于podTemplate定义Pod的PVC模板，每个pod副本都会创建出一个自己独有的pvc，不与其他pod共享 在podTemplate上使用volumeTemplate为各Pod副本动态或静态置备PersistentVolume中完成绑定 删除Pod（例如缩容），并不会一并删除相关的PVC，只要PVC还在，数据就还在 StatefulSet存在的问题 各有状态、分布式应用在启动、扩容、缩容等运维操作上的步骤存在差异，甚至完全不同，因而StatefulSet只能提供一个基础的编排框架 有状态应用所需要的管理操作，需要由用户自行编写代码完成 例如使用StatefulSet创建的MySQL主从复制pod，其中我们需要主pod是读写操作的，从pod是读操作的，因为StatefulSet会为每个pod都有一个唯一的名称标识，所以我们只需要通过这个名称标识去访问主pod和从pod就行，不需要额外的操作。但其他高级操作（建立主从关系等）需要编写代码来完成 2 StatefulSet资源规范除了标签选择器和Pod模板，StatefulSet必须要配置一个专用的Headless Service，而且还可能要根据需要，编写代码完成扩容、缩容等功能所依赖的必要操作步骤；其次它也不能先增加后删除Pod，所以没有maxSurge，因为有状态先创建就会有数据，再缩回去是不行 12345678910111213141516171819apiVersion: apps/v1 # API群组及版本；kind: StatefulSet # 资源类型特有标识；metadata: name &lt;string&gt; # 资源名称，在作用域中要惟一； namespace &lt;string&gt; # 名称空间；DaemonSet资源隶属名称空间级别；spec: minReadySeconds &lt;integer&gt; # Pod就绪后多少秒内任一容器无crash方可视为“就绪”； serviceName &lt;string&gt; # 关联的Headless Service的名称，需要事先存在； selector &lt;object&gt; # 标签选择器，必须匹配template字段中Pod模板中的标签； replicas &lt;integer&gt; # Pod的副本数量； template &lt;object&gt; # Pod模板对象； volumeClaimTemplates &lt;[]Object&gt; # 卷请求模板； podManagementPolicy &lt;string&gt; # Pod管理策略，支持OrderedReady和Parallel两种，默认为前者； revisionHistoryLimit &lt;integer&gt; # 滚动更新历史记录数量，默认为10； updateStrategy &lt;Object&gt; # 滚动更新策略 type &lt;string&gt; # 滚动更新类型，可用值有OnDelete和RollingUpdate； rollingUpdate &lt;Object&gt; # 滚动更新参数，专用于RollingUpdate类型； maxUnavailable &lt;string&gt; # 更新期间可比期望的Pod数量缺少的数量或比例； partition &lt;integer&gt; # 更新策略中的partition号码，默认为0；假如有5个Pod，编号为0~4，当设置为3的时候，大于等于3的编号的Pod都会更新，即3,4更新，0，1和2不更新 范例 12345678910111213141516171819202122232425262728293031323334353637apiVersion: apps/v1kind: StatefulSetmetadata: name: demodb namespace: defaultspec: selector: matchLabels: app: demodb serviceName: &quot;demodb&quot; #指定StatefulSet关联的服务名称 replicas: 2 template: metadata: labels: app: demodb spec: containers: - name: demodb-shard image: ikubernetes/demodb:v0.1 ports: - containerPort: 9907 name: db env: - name: DEMODB_DATADIR value: &quot;/demodb/data&quot; volumeMounts: - name: data mountPath: /demodb/data volumeClaimTemplates: #启动的每个Pod的PVC模版 - metadata: name: data #定义PVC名字的前缀为data，会为每个pod创建一个独立的PVC spec: accessModes: [ &quot;ReadWriteOnce&quot; ] #定义了卷的访问模式为ReadWriteOnce，即只能被单个节点以读写方式挂载 storageClassName: &quot;nfs-csi&quot; #根据PVC的规格自动创建PV，然后每个Pod的PVC将自动绑定其名称空间下的PV resources: requests: storage: 2Gi 创建NFS-CSI 存储类：管理员在 Kubernetes 集群中已经配置好了 NFS-CSI 存储storageClass类 创建Pod和PVC：StatefulSet 控制器开始创建第一个 Pod (demodb-0)。同时基于 volumeClaimTemplates，Kubernetes 会为 demodb-0 创建一个名为 data-demodb-0 的 PVC。 PV 动态供应：由于指定了 storageClassName: &quot;nfs-csi&quot;，在 NFS-CSI 存储系统中为每个 PVC 请求分配一块 2GiB 的存储 PVC 绑定 PV：一旦 PV 准备好，Kubernetes 会将这个新创建的 PV 与 data-demodb-0 的 PVC 进行绑定 挂载：最后Pod 启动时，会将对应的 PVC 挂载到容器内的/demodb/data路径，即使 Pod 被删除后再次重建，只要 PVC 还存在，就能重新挂载之前的 PV，从而保留数据 3 更新策略rollingUpdate：滚动更新，自动触发 onDelete：删除时更新，手动触发 4 滚动更新配置策略 maxUnavailable：定义单批次允许更新的最大副本数量 partition &lt;integer&gt;：用于定义更新分区的编号，其序号大于等于该编号的Pod都将被更新，小于该分区号的都不予更新；默认编号为0，即更新所有副本； 更新方式 kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... kubectl patch (-f FILENAME | TYPE NAME) [-p PATCH|--patch-file FILE] [options] 直接更新原配置文件，而后使用“kubectl apply”命令触发更新 5 操作示例部署 nfs-csi 创建 StatefulSet 资源 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122[root@master1 statefulsets]#vim demodb.yaml #这个Service定义了一个无头服务（Headless Service），用于StatefulSet。无头服务不会分配ClusterIP，而是通过DNS直接解析到StatefulSet的每个Pod。apiVersion: v1kind: Servicemetadata: name: demodb namespace: demo labels: app: demodbspec: clusterIP: None #该Service是一个有标签选择器的Headless Service，不会分配ClusterIP ports: - port: 9907 selector: app: demodb---apiVersion: apps/v1kind: StatefulSetmetadata: name: demodb namespace: demospec: selector: matchLabels: app: demodb serviceName: &quot;demodb&quot; #与前面定义的Service名称匹配 replicas: 2 template: metadata: labels: app: demodb spec: containers: - name: demodb-shard image: ikubernetes/demodb:v0.1 ports: - containerPort: 9907 #容器暴露的端口 name: db env: - name: DEMODB_DATADIR value: &quot;/demodb/data&quot; livenessProbe: #存活探针，用于检查容器是否存活 initialDelaySeconds: 2 #初始延迟时间 periodSeconds: 10 #检查周期 httpGet: path: /status port: db readinessProbe: #就绪探针，用于检查容器是否准备好接受流量 initialDelaySeconds: 15 periodSeconds: 30 httpGet: path: /status?level=full port: db volumeMounts: - name: data #引用了在volumeClaimTemplates中定义的卷 mountPath: /demodb/data #容器内部的路径 volumeClaimTemplates: - metadata: name: data #PVC的名称 spec: accessModes: [ &quot;ReadWriteOnce&quot; ] storageClassName: &quot;nfs-csi&quot; #对应kubectl get sc的name resources: requests: storage: 2Gi#确认已经创建好了storageClass[root@master1 statefulsets]#kubectl get scNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGEnfs-csi (default) nfs.csi.k8s.io Delete Immediate false 13d[root@master1 statefulsets]#kubectl apply -f demodb.yaml#可以看见先创建完第一个pod等待其running才会创建第二个（OrderedReady）[root@master1 statefulsets]#kubectl get pod -n demo -wNAME READY STATUS RESTARTS AGEdemodb-0 0/1 ContainerCreating 0 3sdemodb-0 0/1 Running 0 4sdemodb-0 1/1 Running 0 32sdemodb-1 0/1 ContainerCreating 0 2sdemodb-1 0/1 Running 0 4sdemodb-1 1/1 Running 0 33s#每个Pod都有各自的PVC，各自使用，不互相影响[root@master1 statefulsets]#kubectl get pvc -n demoNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdata-demodb-0 Bound pvc-372404ed-8c31-4ffc-bbf1-5844e3bbf923 2Gi RWO nfs-csi 2m52sdata-demodb-1 Bound pvc-2cf16f3f-9fba-45cc-b0e3-9579add5196d 2Gi RWO nfs-csi 2m20s#扩容[root@master1 statefulsets]#vim demodb.yaml replicas: 3[root@master1 statefulsets]#kubectl apply -f demodb.yaml[root@master1 statefulsets]#kubectl get pvc -n demoNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdata-demodb-0 Bound pvc-372404ed-8c31-4ffc-bbf1-5844e3bbf923 2Gi RWO nfs-csi 8m23sdata-demodb-1 Bound pvc-2cf16f3f-9fba-45cc-b0e3-9579add5196d 2Gi RWO nfs-csi 7m51sdata-demodb-2 Bound pvc-e2030cb4-39f7-4ea3-840c-7069b66cb949 2Gi RWO nfs-csi 8s#缩容[root@master1 statefulsets]#vim demodb.yaml replicas: 2[root@master1 statefulsets]#kubectl apply -f demodb.yaml#原来的PVC不会删除，等扩容后又可以复用[root@master1 statefulsets]#kubectl get pvc -n demoNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdata-demodb-0 Bound pvc-372404ed-8c31-4ffc-bbf1-5844e3bbf923 2Gi RWO nfs-csi 8m59sdata-demodb-1 Bound pvc-2cf16f3f-9fba-45cc-b0e3-9579add5196d 2Gi RWO nfs-csi 8m27sdata-demodb-2 Bound pvc-e2030cb4-39f7-4ea3-840c-7069b66cb949 2Gi RWO nfs-csi 44s#其他应用通过名称解析访问也没有问题[root@master1 statefulsets]#kubectl exec -it -n demo demoapp-7c58cd6bb-9g92g -- /bin/sh[root@demoapp-7c58cd6bb-9g92g /]# host -t A demodb-0.demodb.demo.svc.cluster.localdemodb-0.demodb.demo.svc.cluster.local has address 10.244.2.24[root@demoapp-7c58cd6bb-9g92g /]# host -t A demodb-1.demodb.demo.svc.cluster.localdemodb-1.demodb.demo.svc.cluster.local has address 10.244.1.38[root@demoapp-7c58cd6bb-9g92g /]# host -t PTR 10.244.2.2424.2.244.10.in-addr.arpa domain name pointer demodb-0.demodb.demo.svc.cluster.local.","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Statefulset","slug":"Statefulset","permalink":"https://aquapluto.github.io/tags/Statefulset/"}]},{"title":"Deployment","slug":"Kubernetes/controller/deployment","date":"2025-09-12T03:25:32.000Z","updated":"2025-09-12T03:50:24.029Z","comments":true,"path":"Kubernetes/controller/deployment/","permalink":"https://aquapluto.github.io/Kubernetes/controller/deployment/","excerpt":"","text":"1 Deployment简介无状态指的是：应用的运行不需要依赖之前的数据 –&gt; 对调度没有限制，pod可以随意漂移到最合适的节点 负责编排无状态应用的基础控制器是ReplicaSet，相应的资源类型通过三个关键组件定义如何编排一个无状态应用 replicas：期望运行的Pod副本数 selector：标签选择器 注意不能让两个应用分别属于不同的工作负载型控制器且使用相同的标签选择器，不然它们都会试图管理具有相同标签的Pods，将对方的Pods干掉 podTemplate：Pod模板 Replicatset控制器是维护pod副本为固定数量的，它的功能很有限，只能保证pod挂掉后重启，如果你更新镜像它不会自动更新，所以Deployment是建立在ReplicaSet控制器上层的更高级的控制器 借助于ReplicaSet完成无状态应用的基本编排任务 它位于ReplicaSet更上面一层，基于ReplicaSet提供了滚动更新、回滚等更为强大的应用编排功能 是ReplicaSet资源的编排工具 Deployment编排ReplicaSet ReplicaSet编排Pod 在定义资源时，直接定义Deployment资源来编排Pod应用，ReplicaSet无须显式给出 为什么不直接使用ReplicaSet来编排Pod呢？如下图，在ReplicaSet控制器有一个应用 V1 版本，现在要升级到 V2 版本，传统的方法就是基于删除的更新操作，在这个V1版本，我们如果说把 ReplicaSet 当中的一个模板当中的镜像直接改成 V2 版本，要注意的是控制器关心的是 Pod 副本数量够不够，它所管控的 Pod 数量是否与我们所期望的数量相吻合，V1 版的时候Pod 数量是相同的，当改到 V2 版的时候，Pod 数量还是两个，只是改了版本，那他这个时候不会更新的，所以要想更新我们就要手动删一个 Pod，基于V2版创建新的Pod 所以我们就有Deployment借助于ReplicaSet去完成部署和卸载，当我们去定义好一个ReplicaSet以后，这个ReplicaSet是唯一的，Deployment 把这个对应的 Pod 的模板的哈希值当做这个唯一的标识，一旦我们更新了这个Pod的模板，他的哈希值就会变，Deployment 就会自动根据更新策略，自动干掉 V1 版本中一个Pod，然后基于 V2 版本创建新的 Pod，以此类推，而且 V1 版本不会消除，当 V2 更新完以后发现有问题，我们还能回滚，回滚的逻辑跟刚才更新逻辑一样。总的来说，Deployment能够在整个的更新过程当中保存多个历史版本。 2 Deployment资源规范Deployment资源示例 ReplicaSet对象名称以Deployment名称为前缀，后缀是Pod模板的hash码 各Pod的名字以Deployment名称为前缀，中间是Pod模板的hash码，后缀随机生成 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: deploy-examplespec: replicas: 2 selector: # 注意和service的selector不一样，这里多加一个matchLabels字段 matchLabels: # 表示Deployment只管理含有下列标签的pod app: demoapp release: stable template: # pod定义，跟之前定义pod资源规范一样，不需要定义apiVersion，kind，metadata.name和metadata.namespace metadata: labels: # 为pod打上标签 app: demoapp release: stable spec: containers: - name: demoapp image: ikubernetes/demoapp:v1.0 3 获取资源状态获取ReplicaSet资源对象的状态 1234#可简写为rs[root@master1 ~]#kubectl get replicasetsNAME DESIRED CURRENT READY AGEdemoapp-5b79574789 2 2 2 12d DESIRED：期望的Pod数量 CURRENT：当前的Pod数量 READY：就绪状态的Pod数量 AGE：当前资源的创建后生存时长 获取Deployment资源对象的状态 1234#可简写为deploy[root@master1 ~]#kubectl get deploymentNAME READY UP-TO-DATE AVAILABLE AGEdemoapp 2/2 2 2 12d READY：就绪的Pod数量&#x2F;期望的Pod数量 UP-TO-DATE：到达期望的版本的Pod数量 AVAILABE：运行中并可用的Pod数量 AGE：当前资源的创建后生存时长 Deployment 创建的 ReplicaSet 不仅会带有与它将要创建的 Pod 相同的标签，还会带上唯一的 pod-template-hash 标签，这样它就能精确地管理这一组 Pod 123456789101112[root@master1 ~]#kubectl get rs -l app=demoappNAME DESIRED CURRENT READY AGEdemoapp-5b79574789 2 2 2 12d[root@master1 ~]#kubectl get rs -l pod-template-hash=5b79574789NAME DESIRED CURRENT READY AGEdemoapp-5b79574789 2 2 2 12d[root@master1 ~]#kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELSdemoapp-5b79574789-bk9n4 1/1 Running 4 (5m34s ago) 12d app=demoapp,pod-template-hash=5b79574789demoapp-5b79574789-n8k98 1/1 Running 4 (5m23s ago) 12d app=demoapp,pod-template-hash=5b79574789 4 更新策略Deployment控制器支持两种更新策略 滚动式更新（rolling updates） 逐批次更新Pod的方式，支持按百分比或具体的数量定义批次规模，默认策略（灰度发布） 触发条件：podTemplate的hash码变动 仅podTemplate的配置变动才会导致hash码改变 replicas和selector的变更不会导致podTemplate的hash变动 重建式更新（recreate） 在Pod资源被删除时，使用新的模板定义被足缺失的Pod数量，完成更新 触发条件：现有Pod被删除 Deployment的滚动更新支持使用如下两个字段来配置相关的策略 maxSurge：指定升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是相对于期望值的一个百分比。允许集群在滚动更新期间拥有更多的 Pod 数量，以确保在更新过程中有足够的资源处理请求，减少服务中断的可能性。 在滚动更新的过程中，在 spec.replicas 的基础上可以额外新建最大pod数，控制的是创建新的 maxUnavailable：升级期间不可用的旧Pod副本数，最多不能高于期望值的个数，其值可以是0或正整数，也可以是相对于期望值的一个百分比，默认值为1。允许在滚动更新期间有一定数量的 Pod 不可用，以便逐步替换旧版本的 Pod，并且避免更新过程中服务完全中断。 在滚动更新的过程中，旧pod最大同时干掉几个，控制的是干掉旧的 存在的问题：必须以Pod为原子单位切割规模比例，且无法控制流量路由比例 强调：maxSurge和maxUnavailable的属性值不可同时为0，否则Pod对象的副本数量在符合用户期组的数量后无法做出合理变动以进行更新操作。 思考：如果设置maxUnavaible为0，是否代表升级过程中，不允许杀死任何旧日的pod，必须等待maxSurge指定的新pod全部启动之后，才会删掉旧的pod 设置 maxUnavaible 为0可能会影响更新的速度，因为系统需要等待新的 Pod 启动和准备就绪后，才能开始册除旧的 Pod。这可能导致更新过程变得更长，尤其是在Pod 启动或准备时间较长的情况下 范例 12345678910111213spec: minReadySeconds: 5 #指定Pod在被视为Ready状态前需要等待的时间（秒），即在真正“就绪”之前，必须持续健康运行一段时间。在滚动更新过程中，当一个新的 Pod启动并通过健康检查后，Kubernetes会等待指定的秒数(这里是5秒)再将该Pod视为Ready状态接收流量。这个设置可以确保新版本的Pod稳定运行一段时间，避免因瞬时状态的误判而影响服务的稳定性。 progressDeadlineSeconds: 600 #如果某个操作（如创建、更新等）超过600秒仍未完成，那么该操作将被标记为失败 replicas: 2 revisionHistoryLimit: 10 #表示保留最近10次的部署历史记录 selector: matchLabels: app: demoapp strategy: type: RollingUpdate #指定更新策略类型 rollingUpdate: maxSurge: 25% #表示在更新过程中允许超出期望副本数的最大百分比 maxUnavailable: 25% #表示在更新过程中允许不可用的Pod的最大百分比 5 触发更新我们最为常见的更新需求是应用升级，即镜像文件更新 触发滚动更新方法 kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... kubectl patch (-f FILENAME | TYPE NAME) [-p PATCH|--patch-file FILE] [options] 直接更新原配置文件，而后使用“kubectl apply”命令触发更新 第二和第三种方法可用于模板中任何必要配置的更新 123456789101112131415161718192021222324[root@master1 ~]#kubectl get deploy,rs -n demo -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.apps/demoapp 2/2 2 2 4d20h demoapp ikubernetes/demoapp:v1.0 app=demoappNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORreplicaset.apps/demoapp-7c58cd6bb 2 2 2 4d20h demoapp ikubernetes/demoapp:v1.0 app=demoapp,pod-template-hash=7c58cd6bb# demoapp=ikubernetes/demoapp:v1.1中的格式是上述的CONTAINERS=IMAGES[root@master1 ~]#kubectl set image -n demo deployment/demoapp demoapp=ikubernetes/demoapp:v1.1deployment.apps/demoapp image updated[root@master1 ~]#kubectl get deployments,replicasets -n demo -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.apps/demoapp 2/2 2 2 3h20m demoapp ikubernetes/demoapp:v1.1 app=demoappNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORreplicaset.apps/demoapp-5894dd4b47 2 2 2 67s demoapp ikubernetes/demoapp:v1.1 app=demoapp,pod-template-hash=5894dd4b47replicaset.apps/demoapp-7c58cd6bb 0 0 0 3h20m demoapp ikubernetes/demoapp:v1.0 app=demoapp,pod-template-hash=7c58cd6bb#哈希值变了，版本更新成功[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-5894dd4b47-d995p 1/1 Running 0 2m58sdemoapp-5894dd4b47-mglr9 1/1 Running 0 3m20s 6 查看更新 更新状态：kubectl rollout status (TYPE NAME | TYPE/NAME) [flags] [options] 更新历史：kubectl rollout history (TYPE NAME | TYPE/NAME) [flags] [options] 12345[root@master1 ~]#kubectl rollout history deployment demoapp -n demodeployment.apps/demoapp REVISION CHANGE-CAUSE #CHANGE-CAUSE表示更新的原因1 &lt;none&gt;2 &lt;none&gt; #这里代表了有两个版本 这里并不能看见CHANGE-CAUSE的内容，可以触发更新时加上 --record 选项，不管哪种方法，都可以方便的查看每次 revision 的变化。 12345678[root@master1 ~]# kubectl apply -f test1.yaml --record[root@master1 ~]# kubect1 rollout history deployment deployment -n demodeployment.apps/deploymentREVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt;3 kubectl apply --filename=test1.yaml --record=true --revision 选项可以查看单个 revision 的详细信息 1kubect1 rollout history deployment deployment -n demo --revision=3 7 回滚 回滚到前一版本：kubectl rollout undo (TYPE NAME | TYPE/NAME) 回滚到指定版本：kubectl rollout undo (TYPE NAME | TYPE/NAME) --to-revision=X 123456789101112[root@master1 ~]#kubectl rollout undo deployment demoapp --to-revision=1 -n demo[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 48sdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 40s[root@master1 ~]#kubectl rollout history deployment demoapp -n demodeployment.apps/demoapp REVISION CHANGE-CAUSE2 &lt;none&gt;3 &lt;none&gt; 暂停滚动更新进程：&#96;&#96;kubectl rollout pause (TYPE NAME | TYPE&#x2F;NAME)&#96; 1[root@master1 ~]#kubectl rollout pause deployment/nginx -n demo 继续滚动更新进程：kubectl rollout resume (TYPE NAME | TYPE/NAME) 1[root@master1 ~]#kubectl rollout resume deployment/nginx -n demo 8 扩容和缩容容量管理类型 横向伸缩：增加或减少Pod数量 纵向（垂直）伸缩：调整Pod上的资源需求和资源限制 变动Deployment资源对象中的replicas字段的值即会触发应用规模的扩容或缩容 扩容和缩容是ReplicaSet的功能，具体操作由ReplicaSet完成 根据应用规模的需要进行手动配置 操作方法 patch命令 12kubectl patch (-f FILENAME | TYPE NAME) [-p PATCH|--patch-file FILE] [options]kubectl patch deployment deploy-example -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:3&#125;&#125;&#x27; 直接更新原配置文件，而后使用“kubectl apply”命令触发更新 9 操作示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293[root@master1 ~]#kubectl create deployment nginx --image=nginx:1.24-alpine --replicas=2 --dry-run=client -o yaml -n demo &gt; deployment-nginx.yaml[root@master1 ~]#vim deployment-nginx.yaml apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx name: nginx namespace: demospec: replicas: 2 selector: matchLabels: app: nginx version: v1.20 template: metadata: labels: app: nginx version: v1.20 spec: containers: - image: nginx:1.20-alpine name: nginx[root@master1 ~]#kubectl apply -f deployment-nginx.yaml[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 40mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 40mnginx-5cf865bdd7-ckz9c 1/1 Running 0 3m24snginx-5cf865bdd7-vxktv 1/1 Running 0 3m19s#只是扩容，原来旧的Pod不会删除，因为template字段的东西没变，那么哈希值也不会变[root@master1 ~]#vim deployment-nginx.yamlreplicas: 4[root@master1 ~]#kubectl apply -f deployment-nginx.yaml[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 43mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 43mnginx-5cf865bdd7-bpgk6 1/1 Running 0 10snginx-5cf865bdd7-ckz9c 1/1 Running 0 6mnginx-5cf865bdd7-dvkvw 1/1 Running 0 5snginx-5cf865bdd7-vxktv 1/1 Running 0 5m55s[root@master1 ~]#vim deployment-nginx.yamlspec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 [root@master1 ~]#kubectl apply -f deployment-nginx.yaml#验证更新策略[root@master1 ~]#kubectl rollout status deployment nginx -n demodeployment &quot;nginx&quot; successfully rolled out[root@master1 ~]#vim deployment-nginx.yaml #在更新的时候标签和标签选择器上的字段不能改，所以version不用改- image: nginx:1.22-alpine[root@master1 ~]#kubectl apply -f deployment-nginx.yaml[root@master1 ~]#kubectl rollout status deployment nginx -n demoWaiting for deployment &quot;nginx&quot; rollout to finish: 0 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 2 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 2 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 2 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 3 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 3 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 3 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx&quot; successfully rolled out[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 75mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 75mnginx-748c476b8f-5nj8t 1/1 Running 0 2m8snginx-748c476b8f-6xwfr 1/1 Running 0 2m15snginx-748c476b8f-nmn98 1/1 Running 0 2m56snginx-748c476b8f-zcgpx 1/1 Running 0 3m49s 创建service去调度流量来去验证下面的版本更新发布 1[root@master1 ~]#kubectl create service loadbalancer demoapp --tcp=80:80 --dry-run=client -o yaml -n demo &gt;&gt; deployment-nginx.yaml 金丝雀（灰度）发布：逐步推出新版本应用程序到生产环境，先在小部分用户或流量中测试新版本，然后根据反馈逐渐扩大范围，直至全部替换旧版本 123456789101112131415161718192021222324252627282930313233[root@master1 ~]#vim deployment-nginx.yaml- image: nginx:1.24-alpine#更新完第一批次后暂停更新[root@master1 ~]#kubectl apply -f deployment-nginx.yaml &amp;&amp; kubectl rollout pause deployment/nginx -n demo#更新完第一个暂停root@master1 ~]#kubectl rollout status deployment nginx -n demoWaiting for deployment &quot;nginx&quot; rollout to finish: 0 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 out of 4 new replicas have been updated...Waiting for deployment &quot;nginx&quot; rollout to finish: 1 out of 4 new replicas have been updated...[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 83mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 83mnginx-748c476b8f-5nj8t 1/1 Running 0 10mnginx-748c476b8f-6xwfr 1/1 Running 0 10mnginx-748c476b8f-nmn98 1/1 Running 0 11mnginx-748c476b8f-zcgpx 1/1 Running 0 12mnginx-74f977bcff-zpcpl 1/1 Running 0 2m12s #只更新了一个#当service分发流量到新版本上，效果还不错，就可以继续更新[root@master1 ~]#kubectl rollout resume deployment/nginx -n demo[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 89mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 89mnginx-74f977bcff-2krdz 1/1 Running 0 4m1snginx-74f977bcff-cdfsr 1/1 Running 0 4m8snginx-74f977bcff-q9m2x 1/1 Running 0 4m15snginx-74f977bcff-zpcpl 1/1 Running 0 8m20s 蓝绿发布：在生产环境中准备两套完全相同的环境——蓝色环境和绿色环境。一开始所有用户都在使用蓝色环境，当新版本准备就绪后，直接切换所有用户到绿色环境。如果新版本有问题，可以快速切换回蓝色环境 1234567891011121314151617181920212223242526272829303132333435#先更新策略[root@master1 ~]#vim deployment-nginx.yamlmaxSurge: 100%maxUnavailable: 0[root@master1 ~]#kubectl apply -f deployment-nginx.yaml#再更新版本[root@master1 ~]#vim deployment-nginx.yaml- image: nginx:1.26-alpine[root@master1 ~]#kubectl apply -f deployment-nginx.yaml#一次性创建完新的Pod[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 103mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 103mnginx-74f977bcff-2krdz 1/1 Running 0 17mnginx-74f977bcff-cdfsr 1/1 Running 0 17mnginx-74f977bcff-q9m2x 1/1 Running 0 17mnginx-74f977bcff-zpcpl 1/1 Running 0 21mnginx-f5c57f94f-49w5w 0/1 ContainerCreating 0 6snginx-f5c57f94f-4sz2c 0/1 ContainerCreating 0 6snginx-f5c57f94f-57zts 0/1 ContainerCreating 0 11snginx-f5c57f94f-sz9jh 0/1 ContainerCreating 0 1s[root@master1 ~]#kubectl get pods -n demoNAME READY STATUS RESTARTS AGEdemoapp-7c58cd6bb-h5vpf 1/1 Running 0 107mdemoapp-7c58cd6bb-q8qzl 1/1 Running 0 107mnginx-f5c57f94f-49w5w 1/1 Running 0 4m12snginx-f5c57f94f-4sz2c 1/1 Running 0 4m12snginx-f5c57f94f-57zts 1/1 Running 0 4m17snginx-f5c57f94f-sz9jh 1/1 Running 0 4m7s","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"Deployment","slug":"Deployment","permalink":"https://aquapluto.github.io/tags/Deployment/"}]},{"title":"控制器介绍","slug":"Kubernetes/controller/introduce","date":"2025-09-12T02:38:40.000Z","updated":"2025-09-13T14:11:36.435Z","comments":true,"path":"Kubernetes/controller/introduce/","permalink":"https://aquapluto.github.io/Kubernetes/controller/introduce/","excerpt":"","text":"1 声明式APIAPI设计方法 命令式API 也称为指令式API，用户需要一步步地告诉机器该如何做（How），机器自身不具有任何“智能”，只被动接受指令 高度依赖用户自身理解和达成目标的能力和处理各类异常问题的经验，实现的是“命令式编程（Imperative Programming）” 声明式API 也称为申明式API，用户只需要告诉机器想要的结果（What），机器自身需要确定如何达成该目标 机器需要一定的“智能”，但通常只能支持事先预设和可被其理解的特定任务 实现的是“声明式编程（Declarative Programming） 相较于命令式编程，声明式编程是一个更高的层次上的编程 声明式API允许用户以给出最终期望目标的方式编写代码，但具体的执行过程（即机器智能的那部分代码），最终仍然需要以命令式编程实现，只不过，它们可由不同的人群完成 类比来说，声明式编程的用户类似于企业的高管，只用关心和交待最终目标；而命令式编程的用户类似于企业部门经理，他需要理解目标的达成路径，并组织人力完成目标 Kubernetes的声明式API 用户能够以声明式定义资源对象的目标状态（spec） 由控制器代码（机器智能组件）负责确保实际状态（status）与期望状态一致 控制器即“部门经理”，负责确保部门内的各项具体任务得以落地 控制器通常由API的提供者负责编写 用户需要做的是根据资源类型及其控制器提供的DSL进行声明式编程 2 控制器模式控制器模型（控制回路） 初始，Controller负责根据Input（目标状态）控制System，并生成Output（结果状态） Feedback根据Output生成Feedback Signal，而后由Error Detector基于Feedback Signal和Input来判定是否存在错误，并在有错误时生成Error Signal Error Signal将驱动Controller生成Actuating Signal，并控制System的行为与Input要求相同 Kubernetes Controller的控制回路 Controller根据spec，控制System生成Status Controller借助于Sensor持续监视System的Spec和Status，在每一次控制回路中都会对二者进行比较，并确保System的Status不断逼近或完全等同Status 3 控制器类型Pod的部署、更新、扩缩容和删除等应用编排功能需要由专用的控制器实现，这类控制器即工作负载型控制器，主要落实声明式API定义的副本数量（API Resource –&gt; Object ） ReplicaSet和Deployment：用于无状态应用的部署，典型的像web服务、分布式服务等 DaemonSet：编排系统级应用，如运行日志收集器、监控工具或其他需要在所有节点上运行的服务。 StatefulSet和第三方专用的Operator：用于有状态应用的部署，如数据库或消息队列 Job和CronJob：周期性地在给定的调度时间执行Job，用于需要定时运行的任务，如周期性的数据备份、定期报告生成等。 Kubernetes的控制器类型 打包于Controller Manager中内置提供的控制器，例如Service Controller、Deployment Controller等 基础型、核心型控制器 打包运行于kube-controller-manager中 插件或第三方应用的专用控制器，例如Ingress插件ingress-nginx的Controller，网络插件Project Calico的Controller等 高级控制器，通常需要借助于基础型控制器完成其功能 以Pod形式托管运行于Kubernetes之上，而且这些Pod很可能会由内置的控制器所控制 工作负载型控制器的工作重心 确保选定的Pod精确符合期望的数量，数量不足时依据Pod模板创建，超出时销毁多余的对象 也是通过标签选择器筛选Pod标签从而完成关联 按配置定义进行扩容和缩容 依照策略和配置进行应用更新 4 控制器编排定义工作负载型控制器资源对象以编排应用 内置的各工作负载型控制器都有对应的资源类型，控制器专用于管控其对应类型的资源对象 例如，Deployment控制器对应有Deployment资源类型 这些资源类型都是Kubernetes上标准的API资源，并遵循资源规范的通用格式 用户需要编排某应用时，需要事先确认好应用的运行目标模式，例如实例数据及结束条件等，并根据模式选出对应的工作负载控制器类型 而后根据相应控制器的资源类型的规范，编写资源配置，并提交给API Server即可 相应控制器监视到API Server上新建的资源后，即会运行代码确保对象实例的实际状态（Status）与期望状态（Spec）一致 注意：控制器对象仅负责确保API Server上有相应副本数量的符合标签选择器的Pod对象的定义，至于Pod对象的Status如何与Spec保持一致，则要由相应节点上的kubelet负责保证 5 ownerReferencer字段ownerReferences 字段用于设置对象的所有者关系。这一字段允许你将一个 Kubernetes 对象标记为另一个对象的所有者，这样在所有者对象被删除时，Kubernetes 控制器会自动删除被拥有的对象。这种机制被称为级联删除，和数据库的外键相似 12345678# 以Pod的yaml文件为例ownerReferencer: apiVersion: apps/vl blockOwnerDeletion:true controller: true kind: Replicaset name: myapp uid:666b4e33-6acd-4005-bc14-3a75ce350f0d #对应的就是你定义的Replicast控制器的uuid metadata.ownerReferences 指针指向当前的Replicaset，表示当前Pod的“所有者”，如果要彻底删除 Pod，我们就只能删除RS对象","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[]},{"title":"集群管理指南","slug":"Kubernetes/deploy/cluster","date":"2025-09-12T02:38:40.000Z","updated":"2025-09-13T15:45:30.711Z","comments":true,"path":"Kubernetes/deploy/cluster/","permalink":"https://aquapluto.github.io/Kubernetes/deploy/cluster/","excerpt":"","text":"1 基于现有集群加入节点如果是控制节点，操作如下 1234567# 获取certificate-keykubectl get secret kubeadm-certs -n kube-system -o jsonpath=&#x27;&#123;.data.certificate-key&#125;&#x27; | base64 -d# 获取当初init结束生成的join命令kubeadm token create --print-join-command --control-plane --certificate-key=YOUR_CERT_KEY# 执行join命令即可 说明：如果获取不了certificate-key，无法直接恢复，此时需重新生成证书并上传。需要说明的是，这个重新上传并不是覆盖原有的证书，只有新加入的控制平面节点在执行 kubeadm join 时会用到该证书（因为你无法找到原先的证书信息），已运行的控制平面组件还是使用原先的证书信息 12345# 重新生成证书密钥NEW_CERT_KEY=$(kubeadm certs generate-key --certificate-key)# 重新上传证书kubeadm init phase upload-certs --upload-certs --certificate-key=$NEW_CERT_KEY 如果是工作节点，操作如下 1234# 获取当初init结束生成的join命令kubeadm token create --print-join-command [-ttl 0]# 执行join命令即可 2 基于现有集群执行删除操作2.1 重置集群后重建提示：危险操作，请务必再三确认是否必须要执行该操作，尤其是在管理生产环境时要更加注意 kubeadm reset 负责尽最大努力还原通过 ‘kubeadm init’ 或者 ‘kubeadm join’ 命令对主机所作的更改 一般要先reset各工作节点，而后再reset控制平面各节点，这与集群初始化的次序相反 最后还需一些清理操作，包括清理iptables规则或ipvs规则、删除相关的各文件等 1、集群重置（所有节点执行） 1234567891011121314151617181920212223242526272829303132# 重置 kubeadm 配置，指定 Docker CRI 套接字# 1、基于dockerkubeadm reset --cri-socket unix:///run/cri-dockerd.sock# 2、基于containerdkubeadm reset -f# 清理残留文件（K8s 配置、网络规则、数据目录等）rm -rf /etc/kubernetes/ \\ /var/lib/kubelet/ \\ /var/lib/dockershim/ \\ /var/run/kubernetes/ \\ /var/lib/cni/ \\ /etc/cni/net.d/ \\ /var/lib/etcd/ \\ /run/flannel/ \\ ~/.kube/# 停止 kubelet 并卸载相关包systemctl stop kubeletyum remove -y kubelet kubeadm kubectl# 卸载 kubelet 挂载的目录（避免文件占用）for i in $(df | grep kubelet | awk &#x27;&#123;print $NF&#125;&#x27;); do umount -l $idone# 清理 iptables 规则iptables -Fiptables -t nat -F# 重启节点确保清理生效reboot 2、初始化第一个控制节点 12345678910111213141516171819202122232425# 安装指定版本的 kubeadm、kubelet、kubectlyum install -y kubelet-1.30.* kubeadm-1.30.* kubectl-1.30.*# 启动并设置 kubelet 自启（此时 kubelet 会处于 CrashLoop 状态，初始化后恢复正常）systemctl enable --now kubeletsystemctl status kubelet# 初始化控制平面，指定关键参数，注意pod-network-cidr要和CALICO_IPV4POOL_CIDR指定的一样# 基于docker才需要 --cri-socket选项，基于containerd不需要kubeadm init \\ --control-plane-endpoint=&quot;kubeapi.wu.org&quot; \\ --kubernetes-version=v1.28.2 \\ --pod-network-cidr=192.168.0.0/16 \\ --service-cidr=10.96.0.0/12 \\ --token-ttl=0 \\ --upload-certs \\ --image-repository=registry.aliyuncs.com/google_containers \\ --cri-socket=unix:///run/cri-dockerd.sock# 配置kubectl客户端（master节点）mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 加入其他master节点和node节点，基于docker记得加上--cri-socket=unix:///run/cri-dockerd.sock 2.2 删除节点2.2.1 仅拆除单个已然处于正常工作状态的节点12345678#1、先禁止该节点作为调度目标： kubectl cordon NODE#2、然后排空该节点： kubectl drain NODE#3、待排空操作完成后，从控制平面上删除该节点： kubectl delete nodes NODE#4、最后，即可重置该节点： 参考上述操作 2.2.2 拆除单个未能添加成功添加到集群中的节点12345#1、重置该节点： 参考上述操作#2、在控制平面上删除该节点： kubctl delete node NODE 3 证书管理kubeadm部署集群时设定的证书通常在一年后到期 检查证书是否过期 命令：kubeadm certs check-expiration 手动更新证书 命令：kubeadm certs renew 提示：kubeadm会在控制平面升级时自动更新所有的证书 4 集群升级注意事项 升级前，务必要备份所有的重要组件，例如存储在数据库中应用层面的状态等；但kubeadm upgrade并不会影响工作负载，它只会涉及Kubernetes集群的内部组件 必须禁用Swap 整体步骤概览 先升级控制平面节点 而后再升级工作节点 各节点升级的步骤简介 升级kubeadm，但升级控制平面的第一个节点，同升级控制平面的其它节点以及各工作节点的命令有所不同 排空节点，而后升级kubelet和kubectl 具体的升级步骤：官方文档","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"k8s部署","slug":"k8s部署","permalink":"https://aquapluto.github.io/tags/k8s%E9%83%A8%E7%BD%B2/"}]},{"title":"基于docker部署k8s-v1.30","slug":"Kubernetes/deploy/docker","date":"2025-09-12T02:38:40.000Z","updated":"2025-09-13T14:12:41.885Z","comments":true,"path":"Kubernetes/deploy/docker/","permalink":"https://aquapluto.github.io/Kubernetes/deploy/docker/","excerpt":"","text":"1 集群环境 IP地址 主机名 角色 OS 192.168.15.31 k8s-master-01 Master Ubuntu2204 192.168.15.32 k8s-node-01 Worker Ubuntu2204 192.168.15.33 k8s-node-02 Worker Ubuntu2204 说明：下列部署示例中，Kube Proxy我们使用的是 iptables 模式 2 初始工作2.1 修改主机名123hostnamectl set-hostname k8s-master-01hostnamectl set-hostname k8s-node-01hostnamectl set-hostname k8s-node-02 2.2 添加hosts解析和DNS解析123456789cat /etc/hosts192.168.15.31 k8s-master-01 m1192.168.15.32 k8s-node-01 n1192.168.15.33 k8s-node-02 n2[root@k8s-master-01 ~]# vim /etc/resolv.conf# Generated by NetworkManagernameserver 223.5.5.5nameserver 114.114.114.114 2.3 关闭Swap分区1234567# 先临时关闭swap分区swapoff -a # 或者kubelet忽略swapecho &#x27;KUBELET_EXTRA_ARGS=&quot;--fail-swap-on=false&quot;&#x27; &gt; /etc/sysconfig/kubelet# 然后注释swap分区vim /etc/fstabsed -i &#x27;/swap/s/^/#/&#x27; /etc/fstab 2.4 sshd服务优化123456789101112# 1、加速访问sed -ri &#x27;s@^#UseDNS yes@UseDNS no@g&#x27; /etc/ssh/sshd_config sed -ri &#x27;s#^GSSAPIAuthentication yes#GSSAPIAuthentication no#g&#x27; /etc/ssh/sshd_config grep UseDNS /etc/ssh/sshd_config grep GSSAPIAuthentication /etc/ssh/sshd_configsystemctl restart sshd # 2、密钥登录（主机点做）:为了让后续一些远程拷贝操作更方便ssh-keygenssh-copy-id -i root@k8s-master-01ssh-copy-id -i root@k8s-node-01ssh-copy-id -i root@k8s-node-02 2.5 增大文件打开数量（退出当前会话立即生效）1234567cat &gt; /etc/security/limits.d/k8s.conf &lt;&lt;EOF* soft nofile 65535 * hard nofile 131070 EOF ulimit -Sn ulimit -Hn 2.6 配置模块自动加载123456789101112131415modprobe br_netfiltermodprobe ip_conntrackcat &gt;&gt;/etc/rc.sysinit&lt;&lt;EOF#!/bin/bashfor file in /etc/sysconfig/modules/*.modules ; do[ -x $file ] &amp;&amp; $filedoneEOFecho &quot;modprobe br_netfilter&quot; &gt;/etc/sysconfig/modules/br_netfilter.modulesecho &quot;modprobe ip_conntrack&quot; &gt;/etc/sysconfig/modules/ip_conntrack.moduleschmod 755 /etc/sysconfig/modules/br_netfilter.moduleschmod 755 /etc/sysconfig/modules/ip_conntrack.moduleslsmod | grep br_netfilter 2.7 同步集群时间123456789101112timedatectl set-timezone Asia/Shanghaiapt updateapt install chrony -yvim /etc/chrony/chrony.conf#加下面一行pool ntp.aliyun.com iburst maxsources 2sed -i &#x27;/pool 2\\.centos\\.pool\\.ntp\\.org iburst/a pool ntp.aliyun.com iburst maxsources 2&#x27; /etc/chrony.conf[root@master1 ~]#systemctl enable chrony[root@master1 ~]#systemctl restart chrony 2.8 内核参数优化123456789101112131415161718192021222324252627cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp.keepaliv.probes = 3net.ipv4.tcp_keepalive_intvl = 15net.ipv4.tcp.max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp.max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.top_timestamps = 0net.core.somaxconn = 16384EOF # 立即生效sysctl --system 3 安装cri_dockerdcri_dockerd github网站 123456789# 说明：用 cat /etc/os-release 查看你的Ubuntu是jammy还是focal，再选择下载wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.20/cri-dockerd_0.3.20.3-0.ubuntu-jammy_amd64.debdpkg -i cri-dockerd_0.3.20.3-0.ubuntu-jammy_amd64.debsed -i &#x27;/^ExecStart/s#$# --network-plugin=cni --cni-bin-dir=/opt/cni/bin --cni-cache-dir=/var/lib/cni/cache --cni-conf-dir=/etc/cni/net.d --pod-infra-container-image registry.aliyuncs.com/google_containers/pause:3.9#&#x27; /lib/systemd/system/cri-docker.servicesystemctl daemon-reload systemctl restart cri-docker.service 1234--network-plugin：指定网络插件规范的类型，这里要使用CNI；--cni-bin-dir：指定CNI插件二进制程序文件的搜索目录；--cni-cache-dir：CNI插件使用的缓存目录；--cni-conf-dir：CNI插件加载配置文件的目录； 4 安装docker1234567891011121314apt update &amp;&amp; apt -y install docker.iocat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123;&quot;registry-mirrors&quot;: [&quot;https://docker.1ms.run/&quot;,&quot;https://docker.m.daocloud.io/&quot;,&quot;https://docker.1panel.top/&quot;,&quot;https://doublezonline.cloud/&quot;]&#125;EOFsystemctl restart docker.service 5 安装K8S5.1 安装kubeadm等相关包参考文档 12345678910111213apt-get update &amp;&amp; apt-get install -y apt-transport-httpscurl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpgecho &quot;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /&quot; | tee /etc/apt/sources.list.d/kubernetes.list#查看版本信息apt-get update &amp;&amp; apt-cache madison kubeadmapt -y install kubeadm=1.30.14-1.1 kubelet=1.30.14-1.1 kubectl=1.30.14-1.1#实现kubectl命令自动补全功能 kubectl completion bash &gt; /etc/profile.d/kubectl_completion.sh 5.2 master节点操作123456789# 可以kubeadm config images list查看[root@k8s-master-01 ~]# kubeadm config images listregistry.k8s.io/kube-apiserver:v1.30.0registry.k8s.io/kube-controller-manager:v1.30.0registry.k8s.io/kube-scheduler:v1.30.0registry.k8s.io/kube-proxy:v1.30.0registry.k8s.io/coredns/coredns:v1.11.1registry.k8s.io/pause:3.9registry.k8s.io/etcd:3.5.12-0 部署方法一：先生成配置文件，编辑修改后，再部署（推荐，因为高级配置只能通过配置文件指定，方法二直接用kubeadm init则无法指定，例如配置使用 ipvs 模式） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# kubeadm config print init-defaults &gt; kubeadm.yaml先生成配置文件，内容修改如下apiVersion: kubeadm.k8s.io/v1beta3bootstraptokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.71.12 #控制节点的ip bindPort: 6443nodeRegistration: criSocket: unix:///run/cri-dockerd.sock #指定cri-dockerd容器运行时 imagePullPolicy: IfNotPresent name: k8s-master-01 # 控制节点的主机名 taints: null---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta3certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: &#123;&#125;etcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers #换成阿里云镜像仓库地址kind: ClusterConfigurationkubernetesVersion: 1.30.0 #指定k8s版本networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 #指定Service网段 podSubnet: 10.244.0.0/16 #增加一行，指定pod网段scheduler: &#123;&#125;#在文件最后，插入以下内容，（复制时，要带着---）：---apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemd # Kubelet 将通过 systemd 来管理 cgroups#最终形式apiVersion: kubeadm.k8s.io/v1beta3bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 10.120.30.91 bindPort: 6443nodeRegistration: criSocket: unix:///run/cri-dockerd.sock imagePullPolicy: IfNotPresent name: node taints: null---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta3certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: &#123;&#125;etcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: 1.30.0networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16scheduler: &#123;&#125;---apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemd# kubeadm init --config=kubeadm.yaml --ignore-preflight-errors=SystemVerification --ignore-preflight-errors=Swap cgroupDriver: systemd 说明：containerd 将使用 systemd cgroup 驱动来创建和管理容器的 cgroups来实现资源限制和隔离。这意味着容器的资源限制和隔离将由 systemd 控制，而不是其他cgroup 驱动程序（如 cgroupfs 或 systemd-nspawn），这通常用于确保容器的行为与系统上其他服务和进程的行为保持一致，以及利用 systemd 提供的更丰富的功能和工具来管理容器 部署方案二：直接命令行敲命令（命令行不能指定用什么模式，只能用默认为iptables模式） 123456789101112# 初始化kubeadm init \\ --control-plane-endpoint=&quot;k8s-master-01&quot; \\ --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version=v1.30.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --token-ttl=0 \\ --upload-certs \\ --cri-socket=unix:///run/cri-dockerd.sock# 可选项: --apiserver-advertise-address=192.168.71.12 # 如果是高可用部署，那该地址指向vip地址即可 5.3 提供认证配置文件12~# mkdir $HOME/.kube~# cp /etc/kubernetes/admin.conf $HOME/.kube 5.4 将节点加入集群1234567891011121314# 依据初始化时最后面信息，添加node节点Then you can join any number of worker nodes by running the following on each as root:kubeadm join k8s-master-01:6443 --token re9e0s.t9fuaqipopck9hzs \\ --discovery-token-ca-cert-hashsha256:cf6d6b66275bd9917563004bc6f71eb3391f083dcd434d109361ea0a7e5db2a8# 注意：在上述命令基础上必须加上 --cri-socket=unix:///run/cri-dockerd.sock# 即最后执行的命令为kubeadm join k8s-master-01:6443 --token re9e0s.t9fuaqipopck9hzs \\ --discovery-token-ca-cert-hashsha256:cf6d6b66275bd9917563004bc6f71eb3391f083dcd434d109361ea0a7e5db2a8 \\--cri-socket=unix:///run/cri-dockerd.sock 5.5 部署网络插件12wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.ymlsed -ri &#x27;s|ghcr.io/flannel-io|m.daocloud.io/docker.io/flannel|&#x27; kube-flannel.yml","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"k8s部署","slug":"k8s部署","permalink":"https://aquapluto.github.io/tags/k8s%E9%83%A8%E7%BD%B2/"}]},{"title":"基于containerd部署k8s-v1.30","slug":"Kubernetes/deploy/containerd","date":"2025-09-12T02:38:40.000Z","updated":"2025-09-13T14:12:05.197Z","comments":true,"path":"Kubernetes/deploy/containerd/","permalink":"https://aquapluto.github.io/Kubernetes/deploy/containerd/","excerpt":"","text":"1 集群环境 IP地址 主机名 角色 OS 192.168.15.31 k8s-master-01 Master Centos 7.9 192.168.15.32 k8s-node-01 Worker Centos 7.9 192.168.15.33 k8s-node-02 Worker Centos 7.9 说明：下列部署示例中，Kube Proxy我们使用的是 IPVS 模式 2 初始工作2.1 修改主机名123hostnamectl set-hostname k8s-master-01hostnamectl set-hostname k8s-node-01hostnamectl set-hostname k8s-node-02 2.2 添加hosts解析和DNS解析123456789cat /etc/hosts192.168.15.31 k8s-master-01 m1192.168.15.32 k8s-node-01 n1192.168.15.33 k8s-node-02 n2[root@k8s-master-01 ~]# vim /etc/resolv.conf# Generated by NetworkManagernameserver 223.5.5.5nameserver 114.114.114.114 2.3 关闭防火墙，SELinux和Swap分区12345678910111213141516# 永久关闭sed -i &#x27;s#enforcing#disabled#g&#x27; /etc/selinux/config#临时关闭setenforce 0systemctl disable --now firewalld# -----------------上述操作Ubuntu不用---------------------# 先临时关闭swap分区swapoff -a # 或者kubelet忽略swapecho &#x27;KUBELET_EXTRA_ARGS=&quot;--fail-swap-on=false&quot;&#x27; &gt; /etc/sysconfig/kubelet# 然后注释swap分区vim /etc/fstabsed -i &#x27;/swap/s/^/#/&#x27; /etc/fstab 2.4 sshd服务优化123456789101112# 1、加速访问sed -ri &#x27;s@^#UseDNS yes@UseDNS no@g&#x27; /etc/ssh/sshd_config sed -ri &#x27;s#^GSSAPIAuthentication yes#GSSAPIAuthentication no#g&#x27; /etc/ssh/sshd_config grep UseDNS /etc/ssh/sshd_config grep GSSAPIAuthentication /etc/ssh/sshd_configsystemctl restart sshd # 2、密钥登录（主机点做）:为了让后续一些远程拷贝操作更方便ssh-keygenssh-copy-id -i root@k8s-master-01ssh-copy-id -i root@k8s-node-01ssh-copy-id -i root@k8s-node-02 2.5 增大文件打开数量（退出当前会话立即生效）1234567cat &gt; /etc/security/limits.d/k8s.conf &lt;&lt;EOF* soft nofile 65535 * hard nofile 131070 EOF ulimit -Sn ulimit -Hn 2.6 配置模块自动加载123456789101112131415modprobe br_netfiltermodprobe ip_conntrackcat &gt;&gt;/etc/rc.sysinit&lt;&lt;EOF#!/bin/bashfor file in /etc/sysconfig/modules/*.modules ; do[ -x $file ] &amp;&amp; $filedoneEOFecho &quot;modprobe br_netfilter&quot; &gt;/etc/sysconfig/modules/br_netfilter.modulesecho &quot;modprobe ip_conntrack&quot; &gt;/etc/sysconfig/modules/ip_conntrack.moduleschmod 755 /etc/sysconfig/modules/br_netfilter.moduleschmod 755 /etc/sysconfig/modules/ip_conntrack.moduleslsmod | grep br_netfilter 2.7 同步集群时间1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 设置时区timedatectl set-timezone Asia/Shanghai# =====================》chrony服务端：服务端我们可以自己搭建，也可以直接用公网上的时间服务器，所以是否部署服务端看你自己# 1、安装yum -y install chrony# 2、修改配置文件mv /etc/chrony.conf /etc/chrony.conf.bakcat &gt; /etc/chrony.conf &lt;&lt; EOFserver ntp1.aliyun.com iburst minpoll 4 maxpoll 10server ntp2.aliyun.com iburst minpoll 4 maxpoll 10server ntp3.aliyun.com iburst minpoll 4 maxpoll 10server ntp4.aliyun.com iburst minpoll 4 maxpoll 10server ntp5.aliyun.com iburst minpoll 4 maxpoll 10server ntp6.aliyun.com iburst minpoll 4 maxpoll 10server ntp7.aliyun.com iburst minpoll 4 maxpoll 10driftfile /var/lib/chrony/driftmakestep 10 3rtcsyncallow 0.0.0.0/0local stratum 10keyfile /etc/chrony.keyslogdir /var/log/chronystratumweight 0.05noclientloglogchange 0.5EOF# 4、启动chronyd服务systemctl restart chronyd.service # 最好重启，这样无论原来是否启动都可以重新加载配置systemctl enable chronyd.servicesystemctl status chronyd.service # =====================》chrony客户端：在需要与外部同步时间的机器上安装，启动后会自动与你指定的服务端同步时间# 下述步骤一次性粘贴到每个客户端执行即可# 1、安装chronyyum -y install chrony# 2、需改客户端配置文件mv /etc/chrony.conf /etc/chrony.conf.bakcat &gt; /etc/chrony.conf &lt;&lt; EOFserver 服务端的ip地址或可解析的主机名 iburstdriftfile /var/lib/chrony/driftmakestep 10 3rtcsynclocal stratum 10keyfile /etc/chrony.keylogdir /var/log/chronystratumweight 0.05noclientloglogchange 0.5 EOF# 3、启动chronydsystemctl restart chronyd.servicesystemctl enable chronyd.servicesystemctl status chronyd.service # 4、验证chronyc sources -v 2.8 更新yum源12345678910111213# 1、清理rm -rf /etc/yum.repos.d/*yum remove epel-release -yrm -rf /var/cache/yum/x86_64/6/epel/ # 2、安装阿里的base与epel源curl -s -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo curl -s -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repoyum clean all yum makecache # 或者用华为的也行# curl -o /etc/yum.repos.d/CentOS-Base.repo https://repo.huaweicloud.com/repository/conf/CentOS-7-reg.repo # yum install -y https://repo.huaweicloud.com/epel/epel-release-latest-7.noarch.rpm 2.9 更新系统软件，排除内核1yum update -y --exclud=kernel* 2.10 安装常用软件1yum -y install expect wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvm2 git ntpdate chrony bind-utils rsync unzip git 2.11 更新系统内核（docker要求内核4.4+）master01操作 12345678wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-5.4.274-1.el7.elrepo.x86_64.rpmwget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-devel-5.4.274-1.el7.elrepo.x86_64.rpm for i in n1 n2 m1 ; do scp kernel-lt-* $i:/opt; done 补充：如果下载的慢就从网盘里拿吧链接：https://pan.baidu.com/s/1gVyeBQsJPZjc336E8zGjyQ 提取码：Egon 所有节点操作 1234567891011 #安装yum localinstall -y /opt/kernel-lt* #调到默认启动grub2-set-default 0 &amp;&amp; grub2-mkconfig -o /etc/grub2.cfg #查看当前默认启动的内核grubby --default-kernel #重启系统reboot 2.12 安装并加载IPVS模块12345678910111213141516171819# 1、安装ipvsadm等相关工具yum -y install ipvsadm ipset sysstat conntrack libseccomp # 2、配置加载# 说明：centos stream是/etc/modules-load.d/cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;&quot;EOF&quot; #!/bin/bash ipvs_modules=&quot;ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack&quot; for kernel_module in $&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename $&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe $&#123;kernel_module&#125; fi done EOF chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs modinfo ：检查该模块是否存在于当前内核中 modprobe ：加载模块 lsmod ：查看当前已加载的内核模块 2.13 内核参数优化123456789101112131415161718192021222324252627cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1fs.may_detach_mounts = 1vm.overcommit_memory=1vm.panic_on_oom=0fs.inotify.max_user_watches=89100fs.file-max=52706963fs.nr_open=52706963net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp.keepaliv.probes = 3net.ipv4.tcp_keepalive_intvl = 15net.ipv4.tcp.max_tw_buckets = 36000net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp.max_orphans = 327680net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.ip_conntrack_max = 65536net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.top_timestamps = 0net.core.somaxconn = 16384EOF # 立即生效sysctl --system 3 安装containerd3.1 升级libseccompCentOS 7默认的libseccomp版本为2.3.1，不满足containerd的需求，需要下载 2.4 以上的版本即可。这里部署2.5.1版本。如果你不升级libseccomp的话，启动容器会报错：**Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create containerd task: 123456789101112131415# 查看版本rpm -qi libseccomp# 卸载旧版本rpm -e libseccomp-2.3.1-4.el7.x86_64 --nodeps# 下载新版本（注意：官网已经gg了，不更新了，请用阿里云）wget http://rpmfind.net/linux/centos/8-stream/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpmwget https://mirrors.aliyun.com/centos/8/BaseOS/x86_64/os/Packages/libseccomp-2.5.1-1.el8.x86_64.rpm# 安装新版本rpm -ivh libseccomp-2.5.1-1.el8.x86_64.rpm# 验证安装rpm -qa | grep libseccomp 3.2 安装containerd123456yum remove docker docker-ce containerd docker-common docker-selinux docker-engine -ycd /etc/yum.repos.d/wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install containerd* -y 3.3 配置containerd的镜像加速123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263641、配置mkdir -pv /etc/containerdcontainerd config default &gt; /etc/containerd/config.toml # 为containerd生成配置文件2、替换默认pause镜像地址# 这一步非常非常重要，国内的镜像地址可能导致下载失败，最红kubeadm安装失败！！！！！！！！！！！！！！！grep sandbox_image /etc/containerd/config.tomlsed -i &#x27;s|registry.k8s.io|registry.cn-hangzhou.aliyuncs.com\\/google_containers|&#x27; /etc/containerd/config.tomlgrep sandbox_image /etc/containerd/config.toml# 请务必确认新地址是可用的: sandbox_image = &quot;registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6&quot;3、配置systemd作为容器的cgroup drivergrep SystemdCgroup /etc/containerd/config.tomlsed -i &#x27;s/SystemdCgroup \\= false/SystemdCgroup \\= true/&#x27; /etc/containerd/config.tomlgrep SystemdCgroup /etc/containerd/config.toml4、配置加速器# 参考: https://github.com/containerd/containerd/blob/main/docs/cri/config.md#registry-configuration# 添加 config_path = &quot;/etc/containerd/certs.d&quot;sed -i &#x27;s/config_path \\= .*/config_path \\= &quot;\\/etc\\/containerd\\/certs.d&quot;/g&#x27; /etc/containerd/config.tomlmkdir -p /etc/containerd/certs.d/docker.iocat &gt; /etc/containerd/certs.d/docker.io/hosts.toml &lt;&lt; EOFserver = &quot;https://docker.io&quot;[host.&quot;https://dockerproxy.com&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.agsv.top&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://registry.docker-cn.com&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOFmkdir /etc/containerd/certs.d/ghcr.iocat &gt; /etc/containerd/certs.d/ghcr.io/hosts.toml &lt;&lt; EOFserver = &quot;https://ghcr.io&quot;[host.&quot;https://dockerproxy.com&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.agsv.top&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://registry.docker-cn.com&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOF5、配置containerd开机自启动# 5.1 启动containerd服务并配置开机自启动systemctl daemon-reload &amp;&amp; systemctl restart containerdsystemctl enable --now containerd# 5.2 查看containerd状态systemctl status containerd# 5.3 查看containerd的版本ctr version 4 安装K8S4.1 安装kubeadm等相关包参考文档 1234567891011121314cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; &quot;EOF&quot; [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/rpm/ enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/rpm/repodata/repomd.xml.key EOFyum install -y kubelet-1.30* kubeadm-1.30* kubectl-1.30*systemctl enable kubelet &amp;&amp; systemctl start kubelet &amp;&amp; systemctl status kubelet#实现kubectl命令自动补全功能 kubectl completion bash &gt; /etc/profile.d/kubectl_completion.sh 4.2 master节点操作123456789# 可以kubeadm config images list查看[root@k8s-master-01 ~]# kubeadm config images listregistry.k8s.io/kube-apiserver:v1.30.0registry.k8s.io/kube-controller-manager:v1.30.0registry.k8s.io/kube-scheduler:v1.30.0registry.k8s.io/kube-proxy:v1.30.0registry.k8s.io/coredns/coredns:v1.11.1registry.k8s.io/pause:3.9registry.k8s.io/etcd:3.5.12-0 部署方法一：先生成配置文件，编辑修改后，再部署（推荐，因为高级配置只能通过配置文件指定，方法二直接用kubeadm init则无法指定，例如配置使用 ipvs 模式） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# kubeadm config print init-defaults &gt; kubeadm.yaml先生成配置文件，内容修改如下apiVersion: kubeadm.k8s.io/v1beta3bootstraptokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 192.168.71.12 #控制节点的ip bindPort: 6443nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock #指定containerd容器运行时 imagePullPolicy: IfNotPresent name: k8s-master-01 # 控制节点的主机名 taints: null---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta3certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: &#123;&#125;etcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers #换成阿里云镜像仓库地址kind: ClusterConfigurationkubernetesVersion: 1.30.0 #指定k8s版本networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 #指定Service网段 podSubnet: 10.244.0.0/16 #增加一行，指定pod网段scheduler: &#123;&#125;#在文件最后，插入以下内容，（复制时，要带着---）：---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs #表示kube-proxy代理模式是ipvs，如果不指定ipvs，会默认使用iptables，但是iptables效率低，所以我们生产环境建议开启ipvs，阿里云和华为云托管的k8s，也提供ipvs模式---apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemd # Kubelet 将通过 systemd 来管理 cgroups#最终形式apiVersion: kubeadm.k8s.io/v1beta3bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 10.120.30.91 bindPort: 6443nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock imagePullPolicy: IfNotPresent name: node taints: null---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta3certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: &#123;&#125;etcd: local: dataDir: /var/lib/etcdimageRepository: registry.cn-hangzhou.aliyuncs.com/google_containerskind: ClusterConfigurationkubernetesVersion: 1.30.0networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16scheduler: &#123;&#125;---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationmode: ipvs---apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: systemd# kubeadm init --config=kubeadm.yaml --ignore-preflight-errors=SystemVerification --ignore-preflight-errors=Swap cgroupDriver: systemd 说明：containerd 将使用 systemd cgroup 驱动来创建和管理容器的 cgroups来实现资源限制和隔离。这意味着容器的资源限制和隔离将由 systemd 控制，而不是其他cgroup 驱动程序（如 cgroupfs 或 systemd-nspawn），这通常用于确保容器的行为与系统上其他服务和进程的行为保持一致，以及利用 systemd 提供的更丰富的功能和工具来管理容器 部署方案二：直接命令行敲命令（命令行不能指定用什么模式，只能用默认为iptables模式） 1234567891011# 初始化kubeadm init \\ --control-plane-endpoint=&quot;k8s-master-01&quot; \\ --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version=v1.30.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --token-ttl=0 \\ --upload-certs# 可选项: --apiserver-advertise-address=192.168.71.12 # 如果是高可用部署，那该地址指向vip地址即可 4.3 提供认证配置文件12~# mkdir $HOME/.kube~# cp /etc/kubernetes/admin.conf $HOME/.kube 4.4 将节点加入集群123456# 依据初始化时最后面信息，添加node节点Then you can join any number of worker nodes by running the following on each as root:kubeadm join k8s-master-01:6443 --token re9e0s.t9fuaqipopck9hzs \\ --discovery-token-ca-cert-hashsha256:cf6d6b66275bd9917563004bc6f71eb3391f083dcd434d109361ea0a7e5db2a8 4.5 部署网络插件12wget https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.ymlsed -ri &#x27;s|ghcr.io/flannel-io|m.daocloud.io/docker.io/flannel|&#x27; kube-flannel.yml","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"k8s部署","slug":"k8s部署","permalink":"https://aquapluto.github.io/tags/k8s%E9%83%A8%E7%BD%B2/"}]},{"title":"概论","slug":"Kubernetes/deploy/explanation","date":"2025-09-12T02:38:40.000Z","updated":"2025-09-13T14:12:29.699Z","comments":true,"path":"Kubernetes/deploy/explanation/","permalink":"https://aquapluto.github.io/Kubernetes/deploy/explanation/","excerpt":"","text":"1 集群部署方式Kubernetes 部署方式有很多，建议使用CNCF认证的安装方法进行安装，当前有23种安装方式通过了CNCF认证 https://www.cncf.io/certification/software-conformance/ 部署方式 说明 kubeadm 执行必要的操作以启动并运行最小的可行集群，只关心引导，而不关心配置机器 minikube 快速在单主机上部署kubernetes集群，主要用于测试目的 二进制部署 企业生产级别的部署方式，部署时间长，需要配置内容有很多：证书、服务配置文件、使用systemd管理服务的管理文件、kubeconfig 公有云平台 阿里云：ACK；华为云：CCE；腾讯云：EKS；微软云：AKS 2 集群组件运行模式2.1 独立组件模式此模式应用于二进制安装方式，各附件Add-ons以Pod形式运行，各关键组件都以二进制方式部署于主机节点上，并以守护进程形式运行 需要实现各种证书的申请颁发 部署过程繁琐复杂 2.2 静态Pod模式此模式应用于使用kubeadm部署方式。kubelet和容器运行时以二进制部署，运行为守护进程，除此之外所有组件为Pod 方式运行 静态Pod由kubelet所控制实现创建管理，而无需依赖kube-apiserver等控制平台组件 相关pod早期是从仓库k8s.gcr.io下载镜像，新版改为仓库registry.k8s.io 使用kubernetes官方提供的kubeadm工具实现kubernetes集群方便快速的部署 3 kubeadm部署工具3.1 kubeadm介绍kubeadm：Kubernetes社区提供的集群构建工具 负责执行构建一个最小化可用集群并将其启动等必要的基本步骤 Kubernetes集群全生命周期管理工具，可用于实现集群的部署、升级&#x2F;降级及卸载等 kubeadm仅关心如何初始化并拉起一个集群，其职责仅限于下图中背景着色的部分 蓝色的部分以外的其它组件还需要自行部署 参考文档 1234https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.10.md 3.2 Kubeadm命令1https://kubernetes.io/docs/reference/setup-tools/kubeadm Kubeadm 是一个工具，它提供了 kubeadm init 以及 kubeadm join 这两个命令作为快速创建kubernetes集群的最佳实践。 12345678kubeadm init #启动引导一个 Kubernetes 主节点kubeadm join #启动引导一个 Kubernetes 工作节点并且将其加入到集群kubeadm upgrade #更新 Kubernetes 集群到新版本kubeadm config #如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadm upgrade命令kubeadm token #使用 kubeadm join 来管理令牌kubeadm reset #还原之前使用 kubeadm init 或者 kubeadm join 对节点所作改变(非常危险)kubeadm version #打印出 kubeadm 版本kubeadm alpha #预览一组可用的新功能以便从社区搜集反馈 3.2.1 kubeadm init“init” 命令执行以下阶段： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748preflight 预检certs 生成证书 /ca 生成自签名根 CA 用于配置其他 kubernetes 组件 /apiserver 生成 apiserver 的证书 /apiserver-kubelet-client 生成 apiserver 连接到 kubelet 的证书 /front-proxy-ca 生成前端代理自签名CA(扩展apiserver) /front-proxy-client 生成前端代理客户端的证书（扩展 apiserver） /etcd-ca 生成 etcd 自签名 CA /etcd-server 生成 etcd 服务器证书 /etcd-peer 生成 etcd 节点相互通信的证书 /etcd-healthcheck-client 生成 etcd 健康检查的证书 /apiserver-etcd-client 生成 apiserver 访问 etcd 的证书 /sa 生成用于签署服务帐户令牌的私钥和公钥 kubeconfig 生成建立控制平面和管理所需的所有 kubeconfig 文件，有些放入/etc/kubernetes/manifests中 /admin 生成一个 kubeconfig 文件供管理员使用以及供 kubeadm 本身使用 /super-admin 为超级管理员生成 kubeconfig 文件 /kubelet 为 kubelet 生成一个 kubeconfig 文件，仅用于集群引导，以便 kubelet 连接到 API server /controller-manager 生成 kubeconfig 文件供控制器管理器使用 /scheduler 生成 kubeconfig 文件供调度程序使用 etcd 为本地 etcd 生成静态 Pod 清单文件 /local 为本地单节点本地 etcd 实例生成静态 Pod 清单文件 control-plane 生成建立控制平面所需的所有静态 Pod 清单文件 /apiserver 生成 kube-apiserver 静态 Pod 清单 /controller-manager 生成 kube-controller-manager 静态 Pod 清单 /scheduler 生成 kube-scheduler 静态 Pod 清单 kubelet-start 写入 kubelet 设置并启动（或重启） kubeletupload-config 将 kubeadm 和 kubelet 配置上传到 ConfigMap /kubeadm 将 kubeadm 集群配置上传到 ConfigMap /kubelet 将 kubelet 组件配置上传到 ConfigMapupload-certs 将证书上传到 kubeadm-certsmark-control-plane 将节点标记为控制面bootstrap-token 生成用于将节点加入集群的引导令牌kubelet-finalize 在 TLS 引导后更新与 kubelet 相关的设置 /experimental-cert-rotation 启用 kubelet 客户端证书轮换 addon 安装用于通过一致性测试所需的插件 /coredns 将 CoreDNS 插件安装到 Kubernetes 集群 /kube-proxy 将 kube-proxy 插件安装到 Kubernetes 集群show-join-command 显示控制平面和工作节点的加入命令 参数说明 1234567891011121314151617181920212223242526272829303132--kubernetes-version # kubernetes程序组件的版本号，它必须要与安装的kubelet程序包的版本号相同--control-plane-endpoint # 多主节点必选项,用于指定控制平面的固定访问地址，可以是IP地址或DNS名称，会被用于集群管理员及集群组件的kubeconfig配置文件的API Server的访问地址,如果是单主节点的控制平面部署时不使用该选项,注意:kubeadm 不支持将没有 --control-plane-endpoint 参数的单个控制平面集群转换为高可用性集群。--pod-network-cidr # Pod网络的地址范围，其值为CIDR格式的网络地址，Flannel网络插件的默认值为10.244.0.0/16，Calico网络插件的默认值为192.168.0.0/16--service-cidr # Service的网络地址范围，其值为CIDR格式的网络地址，默认为10.96.0.0/12；通常，仅Flannel一类的网络插件需要手动指定该地址--token-ttl # 共享令牌（token）的过期时长，默认为24小时，0表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在token过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建token，并生成节点加入命令。kubeadm token create --print-join-command--upload-certs # 将控制平面证书上传到 kubeadm-certs Secret，即三个master节点共享一份证书，默认有效期只有24小时--image-repository string # 设置镜像仓库地址，默认为 k8s.gcr.io,此地址从国内可能无法访问,可以指向国内的镜像地址--cri-socket # v1.24版之后指定连接cri的socket文件路径,注意;不同的CRI连接文件是不同的# 如果是CRI是containerd，则使用--cri-socket unix:///run/containerd/containerd.sock# 如果是CRI是docker，则使用--cri-socket unix:///var/run/cri-dockerd.sock# 如果是CRI是CRI-o，则使用--cri-socket unix:///var/run/crio/crio.sock# 注意:CRI-O与containerd的容器管理机制不一样，所以镜像文件不能通用。--apiserver-advertise-address # API 服务器所公布的其正在监听的 IP 地址。如果未设置，则使用默认网络接口。apiserver通告给其他组件的IP地址，一般应该为Master节点的用于集群内部通信的IP地址，0.0.0.0表示此节点上所有可用地址,非必选项--service-dns-domain string # 指定k8s集群域名，默认为cluster.local，会自动通过相应的DNS服务实现解析--control-plane-endpoint string # 为控制平面指定一个稳定的 IP 地址或 DNS 名称。--ignore-preflight-errors=Swap # 若各节点未禁用Swap设备，默认会安装失败，可以附加此选项让kubeadm忽略该错误--config string # kubeadm配置文件的路径。警告：配置文件的功能是实验性的。--node-name string # 指定节点的名称--cert-dir string # 证书的存储路径。缺省值: &quot;/etc/kubernetes/pki&quot;--certificate-key string # 用于加密 kubeadm-certs Secret 中的控制平面证书的密钥。证书密钥为32字节的AES密钥--skip-certificate-key-print # 不要打印用于加密控制平面证书的密钥。--token string # 这个令牌用于建立控制平面节点与工作节点间的双向通信--skip-token-print # 跳过打印 &#x27;kubeadm init&#x27; 生成的默认引导令牌。 在使用 kubeadm init 初始化 Kubernetes 控制平面时，会生成一系列敏感的 TLS 证书（如 apiserver、etcd 等的证书）。当需要添加新的控制平面节点（即高可用集群中新增 master 节点）时，这些新节点需要获取这些证书才能加入集群。但由于证书非常敏感，不能直接暴露，kubeadm 提供了安全的机制（加密）来共享这些证书。 所以当我们不指定 --certificate-key 参数时，kubeadm会自动生成一个密钥。我们也可以通过 kubeadm alpha certs certificate-key 命令生成新密钥，通过 --certificate-key 参数指定 了解：kubeadm init 用于执行完整的初始化流程；kubeadm init phase 则用于分步执行初始化阶段，参考文档 3.2.2 kubeadm joinkubeadm join 用于初始化控制平面节点或工作节点并将其添加到集群中。 对于工作节点，该操作包括以下步骤 获取集群信息：新节点先通过引导令牌和 CA 密钥哈希（或直接提供的根 CA）验证并从集群的 API 服务器下载必要信息，确保连接的是正确的集群。 TLS 引导：节点上的 kubelet（节点代理）通过共享令牌临时认证，向 API 服务器提交证书签名请求（CSR），控制平面会自动批准这个请求，为节点生成永久证书。 完成配置：kubeadm 最终会配置 kubelet，使用新生成的身份证书连接到 API 服务器，正式成为集群的工作节点。 对于控制平面节点，除了类似工作节点的基础连接步骤外，还需要额外的步骤： 同步证书：如果用户指定，会从集群下载控制平面节点间共享的证书（确保所有控制节点使用相同的安全凭证）。 生成控制平面组件配置：创建控制平面组件（如 API Server、Controller Manager 等）的配置文件、证书和 kubeconfig（用于组件间通信的配置）。 加入 etcd 集群：作为控制平面的一部分，新节点会加入集群的 etcd 数据库集群（etcd 是存储集群数据的关键组件，控制节点需共同维护）。 参数说明 12345678910111213141516--apiserver-advertise-address string #设置新建控制平面中API Server的IP地址并通告其正在侦听的地址。如果未设置，将使用默认网络接口。--apiserver-bind-port int32 Default: 6443 #设置新建控制平面中API Server监听的端口号，默认是6443--certificate-key string #使用此密钥可以解密由init上传的证书secret--config string #kubeadm配置文件的路径。--control-plane #在此节点上创建一个新的控制平面实例--cri-socket string #要连接的CRI套接字的路径。如果为空，则kubeadm将尝试自动检测此值；仅当您安装了多个CRI或具有非标准CRI插槽时，才使用此选项。--discovery-file string #从中加载集群信息的文件或URL--discovery-token string #发现token,从中加载集群信息的文件或URL--discovery-token-ca-cert-hash stringSlice #对于基于令牌的发现，验证根CA公共密钥是否与此哈希匹配--discovery-token-unsafe-skip-ca-verification #对于基于令牌的发现，允许在未指定 --discovery-token-ca-cert-hash 参数的情况下添加节点-k, --experimental-kustomize string #kustomize静态pod清单的补丁的存储路径。--ignore-preflight-errors stringSlice #检查清单，其错误将显示为警告。例如：“ IsPrivilegedUser，Swap”。值“ all”忽略所有检查的错误。--node-name string #指定节点名称。--skip-phases stringSlice #要跳过的阶段列表--tls-bootstrap-token string #指定用于在加入节点时临时通过Kubernetes控制平面进行身份验证的令牌。--token string #如果未提供这些值，则将它们用于发现令牌和tls-bootstrap令牌。 init 阶段结束后会自动生成 join 命令，示例如下 1kubeadm join kubeapi.wang.org:6443 --token 2jaxx6.pd46opw36105vjvh --discovery-token-ca-cert-hash sha256:78a5dcfb5767c49f7ad45bd7f6ecf21e05158460a3327f013c43bd93551c1a02 --token 是一串临时的身份验证字符串，用于工作节点（Node）首次连接到控制平面（Master）时的身份校验，相当于 “临时密码”。 --discovery-token-ca-cert-hash 是集群根 CA 证书的哈希值，用于验证控制平面的真实性，防止连接到伪造的集群。 总结：节点加入集群时，先通过 Token 证明自己有权限加入，再通过 CA 证书 Hash 验证控制平面的真实性，最终通过证书签名流程获取永久证书，完成加入并建立长期通信 3.2.3 kubeadm token上述 join 命令中指定的 token，在 K8s ‘1.8+’ 之后,，默认生成的 token 有效期只有24小时，过期后 token 将不可用，需重新生成。生产环境中使用默认有效期是出于安全考虑 —— 短期有效可降低 Token 泄露带来的风险。 1234567891011121314151617#生成默认24小时的Tokenkubeadm token create#生成永久有效的Tokenkubeadm token create --ttl 0#列出当前所有有效的Tokenkubeadm token list#删除Tokenkubeadm token delete &lt;Token值&gt;#生成Node节点加入K8s集群的完整命令kubeadm token create --print-join-command [-ttl 0]#生成master节点加入集群的命令，以获取--certificate-keykubeadm token create --print-join-command --control-plane --certificate-key=YOUR_CERT_KEY 4 使用kubeadm部署K8S集群说明12https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/https://mp.weixin.qq.com/s/7i68jmvi2eo_6wlqYEOupQ 4.1 部署前提使用kubeadm部署Kubernetes集群的前提条件 支持Kubernetes运行的Linux主机，例如Debian、RedHat及其变体等 每主机2GB以上的内存，以及2颗以上的CPU 各主机间能够通过网络正常通信，支持各节点位于不同的网络中 独占的hostname、MAC地址以及product_uuid，主机名能够正常解析 放行由Kubernetes使用到的各端口，或直接禁用iptables 禁用各主机的上的Swap设备 各主机时间同步 准备代理服务，以便接入registry.k8s.io，或根据部署过程提示的方法获取相应的 Image 重要提示 kubeadm不仅支持集群部署，还支持集群升级、卸载、更新数字证书等功能 目前，kubeadm为各节点默认生成的SSL证书的有效期限为1年，在到期之前需要renew这些证书 4.2 需要开放的端口","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"}],"tags":[{"name":"k8s部署","slug":"k8s部署","permalink":"https://aquapluto.github.io/tags/k8s%E9%83%A8%E7%BD%B2/"}]},{"title":"","slug":"Kubernetes/deploy/HighAvailability","date":"2025-09-11T08:23:21.452Z","updated":"2025-09-11T08:23:21.452Z","comments":true,"path":"Kubernetes/deploy/HighAvailability/","permalink":"https://aquapluto.github.io/Kubernetes/deploy/HighAvailability/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"containerd","slug":"Container/containerd","date":"2025-09-10T04:00:48.000Z","updated":"2025-09-11T03:48:30.056Z","comments":true,"path":"Container/containerd/","permalink":"https://aquapluto.github.io/Container/containerd/","excerpt":"","text":"1 命令介绍1.1 与docker不同之处1.1.1 标准输出日志路径的区别针对容器内输出到&#x2F;dev&#x2F;stdout与&#x2F;dev&#x2F;stderr的日志（即容器应用直接打印到终端的内容，包含正确的与错误的），默认情况下会将其输出到宿主机上的一个json文件中（如果是docker引擎用docker logs可以直接查看该json日志里的内容，如果是containerd引擎，那可以用 crictl查看） 对比项 docker containerd 存储路径 /var/lib/docker/containers/&lt;container-id&gt;/&lt;container-id&gt;-json.log /var/log/pods/&lt;namespace&gt;_&lt;pod-name&gt;_&lt;pod-uid&gt;/&lt;container-name&gt;/&lt;container-id&gt;.log，并在/var/log/containers下有软链接 配置参数 在 docker 配置文件中指定：&quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123;&quot;max-size&quot;: &quot;100m&quot;,&quot;max-file&quot;: &quot;5&quot;&#125; 方法一：kubelet 参数--container-log-max-files=5 --container-log-max-size=100Mi；方法二：KubeletConfiguration 中设置&quot;containerLogMaxSize&quot;: &quot;100Mi&quot;, &quot;containerLogMaxFiles&quot;: 5 把容器日志保存到数据盘 挂载数据盘到data-root（默认/var/lib/docker） 创建软链接/var/log/pods指向数据盘挂载点目录 日志切割： 查看 kubelet 系统服务配置路径：systemctl status kubelet（通常为/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf） 编辑/var/lib/kubelet/kubeadm-flags.env，新增参数：--container-log-max-files=5 --container-log-max-size=&#39;5Ki&#39; 1KUBELET_KUBEADM_ARGS=&quot;--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9 --container-log-max-files=5 --container-log-max-size=&#x27;5Ki&#x27;&quot; 创建测试 Pod，观察日志文件超过限制后自动切割 123456789101112131415161718192021222324apiVersion: apps/v1kind: Deploymentmetadata: creationTimestamp: null labels: app: web name: webspec: replicas: 1 selector: matchLabels: app: web strategy: &#123;&#125; template: metadata: creationTimestamp: null labels: app: web spec: containers: - image: registry.cn-shanghai.aliyuncs.com/egon-k8s-test/busybox:v1.0 name: nginx command: [&quot;sh&quot;, &quot;-c&quot;, &quot;while true; do echo 111; sleep 1; done&quot;]status: &#123;&#125; 1.1.2 配置文件路径 containerd 配置文件：/etc/containerd/config.toml 关键存储路径： root = &quot;/var/lib/containerd&quot;：保存持久化数据（快照、元数据等） state = &quot;/run/containerd&quot;：保存运行时临时数据（sockets、pid 等） 1.1.3 stream serverkubectl exec&#x2F;logs 等命令需在 apiserver 与容器运行时之间建立流转发通道。 Docker API 自带 stream 服务；kubelet内部的docker-shim会通过docker API做流转发 icontainerd 需单独配置 在k8s 1.11之前，kubelet并不会做stream proxy，只会做redirect。也就是把containerd暴露的stream server地址告诉apiserver，让apiserver直接来访问containerd的stream server。这种情况下，需要给stream server做tle认证来做安全防护。 从k8s1.11引入了kubelet stream proxy，从而使得containerd stream server只 需要监听本地地址即可。 1.1.4 CNI网络 对比项 docker containerd 谁负责调用 CNI kubelet 内部的 docker-shim containerd 内置的 cri-plugin（containerd 1.1+） 如何配置 CNI kubelet 参数--cni-bin-dir和--cni-conf-dir containerd配置文件(toml)： plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni bin_dir = &quot;/opt/cni/bin&quot; conf_dir = &quot;/etc/cni/net.d&quot; 1.2 containerd客户端命令介绍1.2.1 crictl、ctr命令更换 Containerd 后，以往我们常用的 docker 命令也不再使用，取而代之的分别是 crictl 和 ctr 两个命令客户端。 1、crictl 命令（k8s提供） crictl 是遵循 CRI 接口规范的一个命令行工具，通常用它来检查和管理kubelet节点上的容器运行时和镜像。 crictl -v 输出的是当前 k8s 的版本 2、ctr 命令（containerd提供） ctr 是 containerd 的一个客户端工具。 ctr -v 输出的是 containerd 的版本 使用crictl命令之前，需要先配置&#x2F;etc&#x2F;crictl.yaml如下（不配置的话，你用该命令会一直报warn警告） 123456cat &gt; /etc/crictl.yaml &lt;&lt; &#x27;EOF&#x27;runtime-endpoint: unix:///run/containerd/containerd.sockimage-endpoint: unix:///run/containerd/containerd.socktimeout: 10debug: falseEOF 也可以通过命令进行设置(会自动生成&#x2F;etc&#x2F;crictl.yaml) 12crictl config runtime-endpoint unix:///run/containerd/containerd.sockcrictl config image-endpoint unix:///run/containerd/containerd.sock 1.2.2 nerdctl命令前面我们介绍了可以使用 ctr 操作管理 containerd 镜像容器,但是大家都习惯了使用 docker cli,ctr 使用起来可能还是不太顺手,为了 能够让大家更好的转到 containerd 上面来,社区提供了一个新的命令行工具:nerdctl｡nerdctl 是一个与 docker cli 风格兼容的 containerd 客户端工具,而且直接兼容 docker compose 的语法的,这就大大提高了直接将 containerd 作为本地开发､测试或者单机容器 部署使用的效率｡ 123456789101112131415# 如果没有安装 containerd,则可以下载 nerdctl-full-&lt;VERSION&gt;-linux-amd64.tar.gz 包进行安装wget https://github.com/containerd/nerdctl/releases/download/v1.7.6/nerdctl-1.7.6-linux-amd64.tar.gzmkdir -p /usr/local/containerd/bin/ &amp;&amp; tar -zxvf nerdctl-1.7.6-linux-amd64.tar.gz nerdctl &amp;&amp; mv nerdctl /usr/local/containerd/bin/ln -s /usr/local/containerd/bin/nerdctl /usr/local/bin/nerdctl[root@k8s-node-01 ~]# nerdctl version # 报错warn,提示buildkit未安装WARN[0000] unable to determine buildctl version: exec: &quot;buildctl&quot;: executable file not found in $PATH 需安装 buildkit 以支持镜像构建，安装buildkit(参考:https://zhuanlan.zhihu.com/p/366671300) wget https://github.com/moby/buildkit/releases/download/v0.13.2/buildkit-v0.13.2.linux-amd64.tar.gztar -zxvf buildkit-v0.13.2.linux-amd64.tar.gz -C /usr/local/containerd/ln -s /usr/local/containerd/bin/buildkitd /usr/local/bin/buildkitdln -s /usr/local/containerd/bin/buildctl /usr/local/bin/buildctl 1.2.3 在名称空间支持方面的不同之处 工具 是否支持命名空间 示例 docker 不支持 - ctr 支持 ctr ns ls、ctr -n k8s.io image ls、ctr -n k8s.io container ls crictl 仅支持 k8s.io 默认操作k8s.io命名空间，无-n参数 nerdctl 支持 nerdctl -n k8s.io image ls、nerdctl -n k8s.io container ls 1.3 镜像相关命令 命令 docker ctr crictl nerdctl 查看本地镜像 docker images ctr image ls crictl images nerdctl images 拉取镜像 docker pull ctr image pull crictl pull nerdctl pull 镜像打标签 docker tag ctr image tag 无 nerdctl tag 推送镜像 docker push ctr image push 无 nerdctl push 删除镜像 docker rmi ctr image rm crictl rmi nerdctl rmi 查看镜像详情 docker inspect 无 crictl inspecti nerdctl inspect 导出镜像 docker save ctr image export 无 nerdctl save 导入镜像 docker load ctr image import 无 nerdctl load 登录镜像仓库 docker login 不支持，理由：ctr 工具主要是用于底层的容器操作和管 理，而不是直接与镜像仓库进行交互如推拉镜像等 不支持，理由：主要用于管理 Pod 和容器，而非直接与镜像仓库交互 nerdctl login 1.3.1 ctr(containerd)镜像命令详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 1､查看 ctr image ls # 分字段展示 ctr image ls -q # 展示一个地址,如 docker.io/library/nginx:alpine ctr image check # 检测default名称空间下所有镜像的可用性,主要看STATUS为complete代表可用# 2､拉取 Docker Hub 官方镜像 nginx:alpine,需要注意的是镜像地址需要加上 docker.io Host 地址 ctr images pull docker.io/library/nginx:alpine # 3､重新打标签 ctr image tag docker.io/library/nginx:alpine harbor.k8s.local/course/nginx:alpine# 4､上传镜像 ctr images push docker.io/library/nginx:alpine --user xxx# 5､删除镜像 ctr image rm harbor.k8s.local/course/nginx:alpine# 6､查看镜像详情 无# 7､将镜像导出为压缩包 ctr image export nginx.tar.gz docker.io/library/nginx:alpinectr -n k8s.io image export a.tar.gz registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.11.1 # 8､从压缩包导入镜像 ctr image import nginx.tar.gz ctr -n default image import a.tar.gz直接导入可能会出现类似于 ctr: content digest sha256:xxxxxx not found 的错误,要解决这个办法需要 pull 可以通用/适用所有平台的镜像: ➜ ctr i pull --all-platforms docker.io/library/nginx:alpine ➜ ctr i export --all-platforms nginx.tar.gz docker.io/library/nginx:alpine ➜ ctr i rm docker.io/library/nginx:alpine ➜ ctr i import nginx.tar.gz# 9､将镜像挂载到主机目录 ctr image mount docker.io/library/nginx:alpine /mnt ctr image unmount /mnt# 10､命名空间:Containerd 也有 namespaces 的概念,镜像或容器可以分散到不同的空间,对于多租户的场景十分有用ctr ns lsctr ns create testctr ns rm testctr -n test image ls说明:1､Docker 其实也是默认调用的 containerd,事实上 Docker 使用的 containerd 下面的命名空间默认是 moby,而不是 default2､Kubernetes 下使用的 containerd 默认命名空间是 k8s.io 1.3.2 crictl(kubernetes)镜像命令详解12345678910111213crictl操作的均在k8s.io命名空间,且crictl 是只有一个k8s.io命名空间,但是没有-n 参数｡# 1､crictl 没有-n参数,操作都在`k8s.io`命名空间下｡# crictl image list 等同于 ctr -n k8s.io image list# crictl image ls 等同于 ctr -n k8s.io image ls# crictl images 等同于 ctr -n k8s.io image list# crictl images 等同于 ctr -n k8s.io image ls【温馨提示】1､ctr images pull 拉取的镜像默认放在default2､而 crictl pull 和 kubelet 默认拉取的镜像都在 k8s.io 命名空间下｡所以通过ctr导入镜像的时候特别注意一点,最好指定命名空间｡ 1.3.3 nerdctl镜像命令详解12345678910# 镜像管理nerdctl imagesnerdctl pull docker.io/library/busybox:latestnerdctl login --username=egonlinhaifeng --password xxx registry.cn-shanghai.aliyuncs.com nerdctl logoutnerdctl tag centos:7 registry.cn-shanghai.aliyuncs.com/egon/test:v1.0nerdctl push registry.cn-shanghai.aliyuncs.com/egon/test:v1.0nerdctl save -o busybox.tar.gz busybox:latestnerdctl load -i busybox.tar.gznerdctl rmi busybox ctr不能build镜像，我们可以用nerdctl 12345678910111213141516171819202122232425262728293031323334353637383940414243# 1､Dockerfile文件如下 FROM centos:7 CMD sleep 1000# 2､构建镜像 nerdctl build -t test:v1.0 -f Dockerfile .可以看到有一个错误提示，需要我们安装 `buildctl` 并运行 `buildkitd`，这是因为 `nerdctl build` 需要依赖 `buildkit` 工具。buildkit项目也是 Docker 公司开源的一个构建工具包，支持 OCI 标准的镜像构建。它主要包含以下部分:1、服务端 `buildkitd`：当前支持 runc 和 containerd 作为 worker，默认是 runc，我们这里使用 containerd2、客户端 `buildctl`：负责解析 Dockerfile，并向服务端 buildkitd 发出构建请求buildkit 是典型的 C/S 架构，客户端和服务端是可以不在一台服务器上，而 `nerdctl` 在构建镜像的时候也作为 `buildkitd` 的客户端，所以需要我们安装并运行 `buildkitd`。wget https://github.com/moby/buildkit/releases/download/v0.13.2/buildkit-v0.13.2.linux-amd64.tar.gztar -zxvf buildkit-v0.13.2.linux-amd64.tar.gz -C /usr/local/containerd/ln -s /usr/local/containerd/bin/buildkitd /usr/local/bin/buildkitdln -s /usr/local/containerd/bin/buildctl /usr/local/bin/buildctlcat &gt; /etc/systemd/system/buildkit.service &lt;&lt; &quot;EOF&quot;[Unit]Description=BuildKitDocumentation=https://github.com/moby/buildkit[Service]ExecStart=/usr/local/bin/buildkitd --oci-worker=false --containerd-worker=true[Install]WantedBy=multi-user.targetEOFsystemctl daemon-reloadsystemctl enable buildkit --nowsystemctl status buildkit.service[root@k8s-node-01 test]# nerdctl build -t test:v1.0 -f Dockerfile .FATA[0000] lstat /test/Containerfile: no such file or directory[root@k8s-node-01 test]# lsdockerfile[root@k8s-node-01 test]# mv dockerfile Containerfile[root@k8s-node-01 test]# nerdctl build -t test:v1.0 -f Dockerfile .[root@k8s-node-01 test]# nerdctl imagesREPOSITORY TAG IMAGE ID CREATED PLATFORM SIZE VIRTUAL SIZEtest v1.0 b1dd8c2e8f41 33 seconds ago linux/amd64 79.7 MiB 239.6 MiBnerdctl run -d --name=test test:v1.0nerdctl container lsnerdctl exec -ti test sh 1.4 容器相关命令 命令 docker ctr ( containerd ) crictl ( kubernetes ) nerdctl 显示本地运行的容器列表 docker ps ctr task ls&#x2F;ctr container ls crictl ps 同 docker 命令 创建一个新的容器 docker create ctr container create crictl create 同 docker 命令 运行一个新的容器 docker run ctr run 无(最小单元为 pod ) 同 docker 命令 启动容器 docker start ctr task start crictl start 同 docker 命令 关闭容器 docker stop ctr task kill crictl stop 同 docker 命令 删除容器 docker rm ctr container rm crictl rm 同 docker 命令 查看容器详情 docker inspect ctr container info crictl inspect 同 docker 命令 查看容器日志 docker logs 无 crictl logs 同 docker 命令 查看容器资源 docker stats 无 crictl stats 同 docker 命令 登录或在容器内部执行命令 docker exec 无 crictl exec 同 docker 命令 清空不用的容器 docker image prune 无 crictl rmi –prune 同 docker 命令 1.4.1 ctr(containerd)容器相关命令详解12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 1 创建容器ctr image pull docker.io/library/nginx:alpine --hosts-dir=/etc/containerd/certs.dctr container create docker.io/library/nginx:alpine nginx# 2 查看ctr container ls -q ctr container lsctr container info nginx# 3 删除ctr container rm nginx# 4 任务## （1）任务介绍上面用container create只是创建了一个静态的容器对象，把运行容器需要的资源及配置数据例如namespaces、rootfs和容器的配置都初始化完毕了，但是容器内的进程还没有启动，即容器尚未处于运行状态## （2）启动任务（激活容器）ctr task start -d nginx # nginx是你创建的容器名字## （3）查看[root@k8s-node-01 ~]# ctr task lsTASK PID STATUSnginx 113846 RUNNING## （4）进入容器# 注意必须要指定 --exec-id 参数，这个 id 可以随便写，只要唯一就行。ctr task exec --exec-id 0 -t nginx sh ## （5）暂停容器ctr task pause nginx # 可以查看容器ctr task ls发现状态变成PAUSED## （6）恢复容器ctr task resume nginx## （7）ctr无法stop容器，只能暂停或杀死ctr task kill nginx # 可以查看容器ctr task ls发现状态变成STOPPED## （8）删掉ctr task rm nginx## （9）获取容器cgroup相关信息（内存、CPU 和 PID 的限额与使用量。）ctr task metrics nginx # 记得要先启动才能看：ctr task start -d nginx## （10）看容器中所有进程在宿主机中的 PIDctr task ps nginx # 第一行的就对应容器内的1号进程 1.4.2 nerdctl容器相关命令详解123456789101112nerdctl run -d -p 80:80 --name=nginx --restart=always nginx:alpinenerdctl exec -it nginx /bin/shnerdctl psnerdctl ps -anerdctl inspect nginx nerdctl logs nerdctl logs -fnerdctl stop nginxnerdctl rm nginxnerdctl rm -f nginx 也可以通过命令进行设置(会自动生成&#x2F;etc&#x2F;crictl.yaml) 2 配置镜像加速2.1 配置文件说明新版本 containerd 推荐将镜像仓库配置放在单独目录（如/etc/containerd/certs.d），并在/etc/containerd/config.toml中通过config_path指向该目录，后续修改无需重启 containerd。 目录结构示例 12345/etc/containerd/certs.d/├── docker.io│ └── hosts.toml└── registry.k8s.io └── hosts.toml hosts.toml格式：支持server、capabilities、ca、skip_verify等配置，示例 12345678910111213141516171819202122232425262728293031[host.&quot;https://mirror.registry&quot;]capabilities = [&quot;pull&quot;]ca = &quot;/etc/certs/mirror.pem&quot;skip_verify = false[host.&quot;https://mirror.registry&quot;.header]x-custom-2 = [&quot;value1&quot;, &quot;value2&quot;][host.&quot;https://mirror-bak.registry/us&quot;]capabilities = [&quot;pull&quot;]skip_verify = true[host.&quot;http://mirror.registry&quot;]capabilities = [&quot;pull&quot;][host.&quot;https://test-1.registry&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;]ca = [&quot;/etc/certs/test-1-ca.pem&quot;, &quot;/etc/certs/special.pem&quot;]client = [ [&quot;/etc/certs/client.cert&quot;, &quot;/etc/certs/client.key&quot;], [&quot;/etc/certs/client.pem&quot;, &quot;&quot;]][host.&quot;https://test-2.registry&quot;]client = &quot;/etc/certs/client.pem&quot;[host.&quot;https://test-3.registry&quot;]client = [&quot;/etc/certs/client-1.pem&quot;, &quot;/etc/certs/client-2.pem&quot;][host.&quot;https://non-compliant-mirror.registry/v2/upstream&quot;]capabilities = [&quot;pull&quot;]override_path = true 2.2 配置私有镜像harbor新建目录，以实际配置的私有域名为准，如这里私有镜像地址为 harbor.node.com ，则需要新建这个域名的目录 1mkdir -p /etc/containerd/certs.d/harbor.node.com 在 /etc/containerd/certs.d/harbor.node.com 创建 hosts.toml 1234server = &quot;https://harbor.node.com&quot;[host.&quot;https://harbor.node.com&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;]skip_verify = true # 跳过证书验证 2.3 为 docker.io 配置镜像加速配置config_path 12345vim /etc/containerd/config.toml# 设置：[plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry] config_path = &quot;/etc/containerd/certs.d&quot;systemctl daemon-reload &amp;&amp; systemctl restart containerd 创建加速配置 123456789101112131415161718192021222324252627282930313233# docker hub镜像加速mkdir -p /etc/containerd/certs.d/docker.iocat &gt; /etc/containerd/certs.d/docker.io/hosts.toml &lt;&lt; &#x27;EOF&#x27;server = &quot;https://docker.io&quot;[host.&quot;https://registry-1.docker.io&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.211678.top&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.1panel.live&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://hub.rat.dev&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.m.daocloud.io&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://do.nark.eu.org&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://dockerpull.com&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://dockerproxy.cn&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;][host.&quot;https://docker.awsl9527.cn&quot;]capabilities = [&quot;pull&quot;, &quot;resolve&quot;]EOF 2.4 为其他库配置镜像加速以registry.k8s.io为例 123456mkdir -p /etc/containerd/certs.d/registry.k8s.iotee /etc/containerd/certs.d/registry.k8s.io/hosts.toml &lt;&lt; &#x27;EOF&#x27;server = &quot;https://registry.k8s.io&quot;[host.&quot;https://k8s.m.daocloud.io&quot;] capabilities = [&quot;pull&quot;, &quot;resolve&quot;, &quot;push&quot;]EOF 对于nerdctl命令来说，会自动使用 &#x2F;etc&#x2F;containerd&#x2F;certs.d 目录下的配置镜像加速，但是对于ctr命令，需要指 定 –hosts-dir&#x3D;&#x2F;etc&#x2F;containerd&#x2F;certs.d 123456# 1､registry.k8s.io镜像仓库验证nerdctl --debug=true image pull registry.k8s.io/sig-storage/csi-provisioner:v3.5.0# 2､k8s.gcr.io镜像仓库验证nerdctl --debug=true image pull k8s.gcr.io/kube-apiserver:v1.17.3# 3､docker.io镜像仓库验证nerdctl --debug=true image pull docker.io/library/ubuntu:20.04 3 配置Nvidia container runtime如果你使用的是 containerd 作为容器引擎，并且希望为其安装 NVIDIA Container Runtime，同时配置 runc 为默认的运行时，你可以按照以下步骤进行配置 安装 NVIDIA Container Runtime 首先，你需要在每个 GPU 节点上安装 NVIDIA Container Toolkit，以便容器能够使用 GPU。以下是安装步骤： 12345678910# 设置容器运行时的安装源distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\ &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-container-toolkit/gpgkey | sudo apt-key add - \\ &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-container-toolkit/$distribution/nvidia-container-toolkit.list | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list# 更新包列表并安装 NVIDIA Container Toolkitsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit# 重启 containerd 服务以应用更改sudo systemctl restart containerd 配置 containerd 以使用 NVIDIA Container Runtime 你需要在 containerd 的配置文件中指定 NVIDIA Container Runtime，并将 runc 设置为默认运行时。 编辑 containerd 的配置文件，通常位于 /etc/containerd/config.toml。如果这个文件不存在，你可以通过运行以下命令生成默认配置文件： 1sudo containerd config default | sudo tee /etc/containerd/config.toml 接着，编辑 config.toml 文件，添加 NVIDIA 运行时配置。 修改 config.toml： 1sudo nano /etc/containerd/config.toml 1234567891011[plugins] [plugins.&quot;io.containerd.grpc.v1.cri&quot;] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd] default_runtime_name = &quot;runc&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes] [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc] runtime_type = &quot;io.containerd.runc.v2&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia] runtime_type = &quot;io.containerd.runc.v2&quot; [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.nvidia.options] BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot; 在这个配置中： default_runtime_name = &quot;runc&quot;: 设置 runc 为默认的运行时。 runtime_type = &quot;io.containerd.runc.v2&quot;: 指定使用 runc 作为容器的运行时。 BinaryName = &quot;/usr/bin/nvidia-container-runtime&quot;: 指定 NVIDIA Container Runtime 的路径。 重启 containerd 在完成配置修改后，重启 containerd 使其生效： 1sudo systemctl restart containerd 验证配置 你可以通过以下命令验证配置是否正确： 1sudo ctr run --rm --gpus 1 docker.io/nvidia/cuda:11.0-base nvidia-smi 这个命令应该输出 GPU 的信息，表明容器可以正确访问 GPU。 通过以上步骤，将成功配置 containerd 使用 NVIDIA Container Runtime，并将 runc 设置为默认运行时。这样做之后，默认情况下，Pod 会使用 runc，但如果你在 Pod 中请求 GPU 资源，containerd 会自动使用 NVIDIA Container Runtime 来运行这些 Pod。","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"容器基础","slug":"Container/basics","date":"2025-09-10T04:00:48.000Z","updated":"2025-09-10T04:13:17.048Z","comments":true,"path":"Container/basics/","permalink":"https://aquapluto.github.io/Container/basics/","excerpt":"","text":"1 容器技术介绍1.1 容器的由来&#x3D;&#x3D;&#x3D;&#x3D;》单机时代 特点：一个计算机硬件之上只能启动&#x2F;运行一个操作系统 问题：如何提升单机的资源使用 解决：计算机硬件—》操作系统—-》运行多个应用程序 新问题：一旦操作系统挂掉，则其上运行的所有应用程序都会一起挂掉，可用差 &#x3D;&#x3D;&#x3D;&#x3D;》集群时代 特点：一个计算机硬件之上只能启动&#x2F;运行一个操作系统，但是多台计算机硬件构成一个集群，不用的应用程序部署在自己的独立操作系统里（隔离） 解决： 计算机硬件1—》操作系统—-》应用1，应用2 计算机硬件2—》操作系统—-》应用3，应用4 计算机硬件3—》操作系统—-》应用5 新问题：虽然提高了可用性，但是又导致单机资源利用率不足，陷入了一个两难的境地 &#x3D;&#x3D;&#x3D;&#x3D;》虚拟化时代 特点：一个计算机硬件之上可以运行多个虚拟机，每个虚拟机里运行隔离独立且完整的操作系统，多台虚拟机构成一个集群 解决：提高了可用性和单机资源利用率 虚拟机实现的效果： 隔离 限定虚拟机对资源的使用，不要干扰其他虚拟机 虚拟化技术实现资源限制的思路是通过在创建虚拟机时，提前设定好虚拟机硬件的资源指标 新问题：虚拟机太臃肿了，不够轻量级，对于一台虚拟机来说，除了应用程序耗费的资源之外，虚拟机本身也会占用很多资源 &#x3D;&#x3D;&#x3D;&#x3D;》容器时代 特点：一个计算机硬件上可以运行多个容器，每个容器里运行隔离独立的应用程序（进程），统一宿主机上的多个容器共享该宿主机的内核 解决：容器比虚拟机更加轻量级，自身不会占有很多资源 容器技术实现的效果 隔离：namespace 限定容器对资源的使用，不要干扰其他容器：cgroup 容器里只有文件系统（rootfs），没有内核（bootfs）：UnionFS联合文件系统 总结：虚拟机实现的是操作系统层面的隔离，容器则是实现了进程中所有资源的隔离，因为进程只是可以隔离内存空间而已，其他的资源是共享的，而容器可以实现属于进程自己的其他资源。进程和操作系统谁更轻量？当然是进程了 所以启动一个容器其实就是启动一个进程，但不准确，因为容器里，每个名称空间内可以启动多个进程 1.2 容器的作用在传统的技术中，假如要跑两个go服务，一个服务5M，另一个服务10M，为了跑这两个服务，需要开两台虚拟机，而一台虚拟机最少都需要5G的内存，相当于开一辆大卡车去运一部手机，而容器技术解决了这个问题，相当于提供了一个极其精简的虚拟机 容器技术提高了硬件资源利用率、 方便了企业的业务快速横向扩容（可以达到秒级快速扩容）、 实现了业务宕机自愈功能（配合K8S可以实现，但OpenStack无此功能） 容器是应用层的抽象，它将代码和依赖项打包在一起。多个容器可以在同一台机器上运行，并与其他容器共享操作系统内核，每个容器在用户空间中作为独立进程运行。与 VM 相比，容器占用的空间更少（容器映像的大小通常为数十MB），可以处理更多应用程序并且需要更少的 VM 和操作系统。 由于所有的容器共享同一个 Host OS，这使得容器在体积上要比虚拟机小很多。另外，启动容器不需要启动整个操作系统，所以容器部署和启动速度更快，开销更小，也更容易迁移。 如今的应用系统需要使用多种服务（MQ，缓存，微服务等）来进行构建和组装应用，而且这些应用会部署到不同的环境中（虚拟主机，私有云，公有云，混合云等），在以往如果要迁移这些应用是一件非常麻烦的事情。因为主流的软件部署过程是由管理员编译或下载好二进制安装包，根据软件的部署说明文档准备好正确的操作系统、第三方库、配置文件、资源权限等各种前置依赖以后，才能将程序正确地运行起来。 但容器的出现，改变了现状，容器让软件分发部署过程从传统的发布安装包或人工部署转变为直接发布已经部署好的、包含整套运行环境的容器镜像。只要我们直接运行容器镜像，就可以运行对应的环境。 容器的灵活性和可移植性优势使得它非常完美地契合了混合云部署的需要，使得我们产品团队的成员（开发，测试，运维）掌握更多的灵活性来处理和面对复杂且多环境部署的需要。 1.3 容器，虚拟机和物理机的比较 &#x3D;&#x3D;&#x3D;&#x3D;》传统部署时代（物理机）：资源分配冲突、资源利用效率低下 从物理服务器自身管理角度 物理服务器环境部署人力成本大，特别是在自动化手段不足的情况下，依靠人肉运维的方式解决。 当物理服务器出现宕机后，服务器重启时间过长，短则1-2分钟，长则3-5分钟，有背于服务器在线时长达到99.999999999%标准的要求 物理服务器在应用程序运行期间硬件出现故障，解决较麻烦 物理服务器计算资源不能有效调度使用，无法发挥其充足资源的优势 从物理服务器部署应用程序角度 物理服务器环境部署浪费时间，没有自动化运维手段，时间是成倍增加的 在物理服务器上进行应用程序配置变更，需要重新实施前述步骤 &#x3D;&#x3D;&#x3D;&#x3D;》虚拟化部署时代：提高资源的高效利用、灵活的应用访问 优点 虚拟机较物理服务器轻量，可借助虚拟机模板实现虚拟机快捷生成及应用 虚拟机中部署应用与物理服务器一样可控性强，且当虚拟机出现故障时，可直接使用新的虚拟机代替 在物理服务器中使用虚拟机可高效使用物理服务器的资源 虚拟机与物理服务器一样可达到良好的应用程序运行环境的隔离 在虚拟机中部署应用，容易扩容及缩容实现 与物理服务器相比较，当部署应用程序的虚拟机出现宕机时，可以快速启动，时间通常可达秒级，10秒或20秒即可启动，应用程序可以继续提供服务 应用程序迁移方便 缺点 虚拟机管理软件本身占用物理服务器计算资源较多，例如:VMware Workstation Pro就会占用物理服务器大量资源，所以一般在企业应用中使用KVM虚拟机较多。 虚拟机底层硬件消耗物理服务器资源较大，例如：虚拟机操作系统硬盘，会直接占用大量物理服务器硬盘空间 相较于容器技术，虚拟机启动时间过长，容器启动可按毫秒级计算 虚拟机对物理服务器硬件资源调用添加了调链条，存在浪费时间的现象，所以虚拟机性能弱于物理服务器 由于应用程序是直接部署在虚拟机硬盘上，应用程序迁移时，需要连同虚拟机硬盘中的操作系统一同迁移，会导致迁移文件过大，浪费更多的存储空间及时间消耗过长 &#x3D;&#x3D;&#x3D;&#x3D;》容器部署时代：应用与底层基础架构分离，实现更灵活的业务管理。 优点 快速部署: 短时间内可以部署成百上千个应用，更快速交付到线上 构建：将需要的软件安装配置好，打包到一个容器中 运输：上传到指定的服务器，需要时，下载下来 运行：下载好镜像文件，创建容器运行就可以了 高效虚拟化: 不需要额外hypervisor支持，也不需要为容器安装操作系统，基于linux内核实现应用虚拟化，相比虚拟机大幅提高性能和效率 节省开支: 毫秒级启动，可直接使用物理服务器硬件资源，提高服务器利用率，降低IT支出 简化配置: 不需要管理容器网络，以自动调用的方式访问容器中应用提供的服务，将运行环境打包保存至容器，使用时直接启动即可 环境统一: 将开发，测试，生产的应用运行环境进行标准化和统一，减少环境不一样带来的各种问题，一次构建，到处运行 快速迁移和扩展: 可实现跨平台运行在物理机、虚拟机、公有云等环境，良好的兼容性可以方便将应用从A宿主机迁移到B宿主机，甚至是A平台迁移到B平台 更好的实现面向服务的架构,推荐一个容器只运行一个应用,实现分布的应用模型,可以方便的进行横向扩展,符合开发中高内聚，低耦合的要求，减少不同服务之间的相互影响 缺点 隔离性没有虚拟机好，因多个容器共用宿主机的内核，各应用之间的隔离不如虚拟机彻底，如果存在内核漏洞或者不当配置，可能会导致容器逃逸攻击 由于和宿主机之间的进程也是隔离的，需要进入容器查看和调试容器内进程等资源,变得比较困难和繁琐 如果容器内进程需要查看和调试，需要在每个容器内都需要安装相应的工具，这也造成存储空间的重复浪费 容器和虚拟机技术比较 使用虚拟机是为了更好的实现服务运行环境隔离，每个虚拟机都有独立的内核，虚拟化可以实现不同操作系统的虚拟机，但是通常一个虚拟机只运行一个服务，很明显资源利用率比较低且造成不必要的性能损耗，我们创建虚拟机的目的是为了运行应用程序，比如Nginx、PHP、Tomcat等web程序，使用虚拟机无疑带来了一些不必要的资源开销，而容器技术则基于减少中间运行环节带来较大的性能提升。 根据实验，一个运行着CentOS的KVM虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用100~200 MB内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘I&#x2F;O的损耗非常大。 比如: 一台96G内存的物理服务器，为了运行java程序的虚拟机一般需要分配8G内存&#x2F;4核的资源，只能运行13台左右虚拟机，但是改为在docker容器上运行Java程序,每个容器只需要分配4G内存即可，同样的物理服务器就可以运行25个左右容器，运行数量相当于提高一倍，可以大幅节省IT支出，通常情况下至少可节约一半以上的物理设备 对比属性 容器（Container） 虚拟机（VM） 隔离性 基于进程隔离 提供资源的完全隔离 启动时间 毫秒级或秒级 秒级或分钟级 内核 共用宿主机内核 使用独立内核 占用资源 MB级 GB级 系统支持容量（同级别） 支持上千个容器 几十台虚拟机 2 NameSpacehttps://man7.org/linux/man-pages/man7/namespaces.7.html https://en.wikipedia.org/wiki/Linux_namespaces 2.1 引入问题一个宿主机运行了N个容器，多个容器共用一个 OS，必然带来的以下问题: 怎么样保证每个容器都有不同的文件系统并且能互不影响？ 一个docker主进程内的各个容器都是其子进程，那么如果实现同一个主进程下不同类型的子进程？ 各个容器子进程间能相互通信(内存数据)吗？ 每个容器怎么解决IP及端口分配的问题？ 多个容器的主机名能一样吗？ 每个容器都要不要有root用户？怎么解决账户重名问题？ 2.2 概念namespace是Linux系统的底层概念，在LInux内核层实现，即有一些不同类型的命名空间被部署在内核，各个docker容器运行在同一个docker主进程并且共用同一个宿主机系统内核，各docker容器运行在宿主机的用户空间，每个容器都要有类似于虚拟机一样的相互隔离的运行空间，但是容器技术是在一个进程内实现运行指定服务的运行环境，并且还可以保护宿主机内核不受其他进程的干扰和影响，如文件系统空间、网络空间、进程空间等 容器引擎创建并启动一个容器，本质就是创建了一个名称空间，核心目的就是为了隔离关键资源，保证不与其他空间冲突。namespace到底隔离了哪些关键资源呢？主要分为6种： pid：保证不同名称空间里拥有自己完全独立的一套pid号码，不同的名称空间里即便pid号一样，也不会冲突 ps，top等命令的输出，容器内看到的进程还是会和宿主机的一样，因为他们的输出信息是从 /proc 来的，所以需要对文件系统隔离，才有了mount的隔离 uts：主机名与网络信息服务 ipc：ipc的全称是进程间通信，是unix&#x2F;linux下进程直接通信的一种方式，只有同一个namespace下的进程才可以相互通信 mount：容器内挂载点与系统分离开 network：用于隔离网络资源（&#x2F;proc&#x2F;net，IP地址，网卡、路由），一个名称空间里可以有自己独立的网卡，监听的端口不会与其他名称空冲突 user：用户的id、组id与其他名称空间与冲突 隔离类型 功能 系统调用参数 内核版本 MNT Namespace(mount) 提供磁盘挂载点和文件系统的隔离能力 CLONE_NEWNS 2.4.19 IPC Namespace(Inter-Process Communication) 提供进程间通信的隔离能力,包括信号量,消息队列和共享内存 CLONE_NEWIPC 2.6.19 UTS Namespace(UNIX Timesharing System) 提供内核,主机名和域名隔离能力 CLONE_NEWUTS 2.6.19 PID Namespace(Process Identification) 提供进程隔离能力 CLONE_NEWPID 2.6.24 Net Namespace(network) 提供网络隔离能力,包括网络设备,网络栈,端口等 CLONE_NEWNET 2.6.29 User Namespace(user) 提供用户隔离能力,包括用户和组 CLONE_NEWUSER 3.8 2.3 lsns命令用于列出当前系统上的所有命名空间 12345678910111213141516171819202122232425262728293031323334[root@master1 ~]#lsns --help用法：lsns [选项] [&lt;名字空间&gt;]选项：-J, --json #使用 JSON 输出格式-l, --list #使用列表格式的输出-n, --noheadings #不打印标题-o, --output &lt;list&gt; #定义使用哪个输出列 --output-all #output all columns-p, --task &lt;pid&gt; #打印进程名字空间-r, --raw #使用原生输出格式-u, --notruncate #不截断列中的文本-W, --nowrap #don&#x27;t use multi-line representation-t, --type &lt;name&gt; #namespace type (mnt, net, ipc, user, pid, uts, cgroup,time)-h, --help #display this help-V, --version #display versionAvailable output columns: NS #名字空间标识符 (inode 号) TYPE #名字空间类型 PATH #名字空间路径 NPROCS #名字空间中的进程数 PID #名字空间中的最低 PID PPID #PID 的 PPID COMMAND #PID 的命令行 UID #PID 的 UID USER #PID 的用户名 NETNSID #namespace ID as used by network subsystem NSFS #nsfs mountpoint (usually used network subsystem) PNS #parent namespace identifier (inode number) ONS #owner namespace identifier (inode number)更多信息请参阅 lsns(8)。 12345678910111213141516[root@ubuntu2004 ~]# lsns -t net NS TYPE NPROCS PID USER NETNSID NSFS COMMAND4026531840 net 163 1 root unassigned /usr/lib/systemd/systemd --switched-root --system --deserialize=464026532237 net 1 833 root unassigned ├─/usr/sbin/irqbalance4026532356 net 1 931 polkitd unassigned └─/usr/lib/polkit-1/polkitd --no-debug --log-level=err[root@ubuntu2004 ~]#ls -l /proc/4140/nstotal 0lrwxrwxrwx 1 root root 0 Apr 6 07:22 cgroup -&gt; &#x27;cgroup:[4026531835]&#x27;lrwxrwxrwx 1 root root 0 Apr 6 07:22 ipc -&gt; &#x27;ipc:[4026531839]&#x27;lrwxrwxrwx 1 root root 0 Nov 19 04:11 mnt -&gt; &#x27;mnt:[4026531840]&#x27;lrwxrwxrwx 1 root root 0 Apr 6 07:20 net -&gt; &#x27;net:[4026531992]&#x27;lrwxrwxrwx 1 root root 0 Apr 6 07:22 pid -&gt; &#x27;pid:[4026531836]&#x27;lrwxrwxrwx 1 root root 0 Apr 6 07:22 pid_for_children -&gt; &#x27;pid:[4026531836]&#x27;lrwxrwxrwx 1 root root 0 Apr 6 07:22 user -&gt; &#x27;user:[4026531837]&#x27;lrwxrwxrwx 1 root root 0 Apr 6 07:22 uts -&gt; &#x27;uts:[4026531838]&#x27; 2.4 nsenter命令以其他程序的名字空间运行某个程序。 1234567891011121314151617181920212223242526[root@ubuntu2204 ~]#nsenter --help用法：nsenter [选项] [&lt;程序&gt; [&lt;参数&gt;...]]选项：-a, --all #enter all namespaces-t, --target &lt;pid&gt; #要获取名字空间的目标进程-m, --mount[=&lt;文件&gt;] #进入 mount 名字空间-u, --uts[=&lt;文件&gt;] #进入 UTS 名字空间(主机名等)-i, --ipc[=&lt;文件&gt;] #进入 System V IPC 名字空间-n, --net[=&lt;文件&gt;] #进入网络名字空间-p, --pid[=&lt;文件&gt;] #进入 pid 名字空间-C, --cgroup[=&lt;文件&gt;] #进入 cgroup 名字空间-U, --user[=&lt;文件&gt;] #进入用户名字空间-T, --time[=&lt;file&gt;] #enter time namespace-S, --setuid &lt;uid&gt; #设置进入空间中的 uid-G, --setgid &lt;gid&gt; #设置进入名字空间中的 gid --preserve-credentials #不干涉 uid 或 gid-r, --root[=&lt;目录&gt;] #设置根目录-w, --wd[=&lt;dir&gt;] #设置工作目录-F, --no-fork #执行 &lt;程序&gt; 前不 fork-Z, --follow-context #根据 --target PID 设置 SELinux 环境-h, --help #display this help-V, --version #display version更多信息请参阅 nsenter(1)。 123#说明:4136为容器在宿主机的Pid,下面表示进入4136容器的对应网络名称空间执行命令[root@ubuntu2204 ~]#nsenter -t 4136 -n ip a[root@ubuntu2204 ~]#nsenter -t 4140 -n ip a 3 UnionFS联合文件系统3.1 概念对于容器来说，我们不是把操作系统镜像直接拷贝给每个容器的名称空间，而是通过UnionFS技术，将容器的rootfs以只读方式挂载。所以这就是同一个镜像启动多个容器的本质，多个容器是共享镜像的，而不是每个容器有自己的镜像，这样就不是轻量级了！ UnionFS联合文件系统是一种分层、轻量级的文件系统，它允许将多个文件系统层叠在一起，形成一个统一的视图。容器镜像使用联合文件系统来存储和管理文件，实现了分层存储和版本控制。当容器启动时，它会创建一个新的可写层，用于存储运行时的数据。这样，容器之间可以共享相同的基础镜像，减少了存储空间的占用。overlay是UnionFS的一种，分为三层： LowerDir：镜像层，只读 UpperDir：容器层，修改&#x2F;删除的内容都会放到这一层指定的目录里 Merged：展现层，内容来自于lowerdir与upperdir，在容器内看到就是这一层 如果是读，upperdir中没有则直接读lowerdir 如果是写，则将文件从lowerdir中拷贝到upperdir中才能写，写完后，下次再写就直接从upperdir中取 如果是删除，则在upperdir中将文件设置为隐藏，并不会真的删除掉lowerdir中的文件 如果是重命名目录 只有在源文件和目的路径都在顶层容器层upperdir时，才允许执行rename操作 说明：实际应用中，还有work目录，它是一个存放临时文件的目录 强调：容器内挂载的文件系统类型不是之前熟悉的ext4、xfs，而是overlay文件系统 3.2 联合挂载演示1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 1、联合挂载mkdir -p /test/lower1mkdir -p /test/lower2mkdir -p /test/lower3mkdir -p /test/upperdirmkdir -p /test/workmkdir /test/mergedecho 111 &gt; /test/lower1/1.txtecho 222 &gt; /test/lower2/2.txtecho 333 &gt; /test/lower3/3.txtmount -t overlay overlay -o lowerdir=/test/lower1:/test/lower2:/test/lower3,upperdir=/test/upperdir,workdir=/test/work /test/merged# 2、查看挂载[root@yq01-aip-aikefu19 ~]# dfoverlay 94465716 81721684 12727648 87% /test/merged# 3、查看merge目录[root@yq01-aip-aikefu19 ~]# ls /test/merged/1.txt 2.txt 3.txt# 4、修改[root@yq01-aip-aikefu19 ~]# cd /test/merged/[root@yq01-aip-aikefu19 merged]# echo 666 &gt; 1.txt[root@yq01-aip-aikefu19 merged]# cat 1.txt666# 然后我们去查看lowerdir，猜一下，1.txt改了没有，肯定没改啊[root@yq01-aip-aikefu19 merged]# cat /test/lower1/1.txt111# 修改的内容都在upperdir里[root@yq01-aip-aikefu19 merged]# cat /test/upperdir/1.txt666# 我们在merged目录下新增文件，本质也都是建立到了upperdir里[root@yq01-aip-aikefu19 merged]# cd /test/merged/[root@yq01-aip-aikefu19 merged]# echo 444 &gt; 4.txt[root@yq01-aip-aikefu19 merged]# ls1.txt 2.txt 3.txt 4.txt[root@yq01-aip-aikefu19 merged]# ls /test/upperdir/1.txt 4.txt[root@yq01-aip-aikefu19 merged]# ls /test/lower1/1.txt[root@yq01-aip-aikefu19 merged]# ls /test/lower2/2.txt[root@yq01-aip-aikefu19 merged]# ls /test/lower3/3.txt# 在merge目录下删除文件[root@yq01-aip-aikefu19 merged]# rm -rf 2.txt[root@yq01-aip-aikefu19 merged]# ls -l /test/upperdir/total 8-rw-r--r-- 1 root root 4 Jun 28 15:47 1.txtc--------- 1 root root 0, 0 Jun 28 15:50 2.txt # 在upperdir里新建一个特殊文件来告诉OverlayFS这个文件不能出现在merged里，即代表它已经被删除了-rw-r--r-- 1 root root 4 Jun 28 15:50 4.txt 3.3 容器文件系统的总结 启动任何一个容器其实都是两层，一层是lowerdir镜像层，另外一层就是容器自己的可写层upperdir，这个lowerdir可能由很多目录组成，也可能只由一个目录组成 当对容器进行增删改操作时，这些更改会记录到 upperdir，如果容器销毁，upperdir 中的内容会丢失，容器退出不会 可以通过docker commit将正在运行的容器保存为新的镜像，这样 upperdir 中的更改就会保留下来，并成为镜像的一部分。新镜像包含两个目录：原始镜像的lowerdir和upperdir中的更改 构建镜像时，从一个基础操作系统镜像开始，每执行一个操作，相当于在镜像上增加了一层文件系统。这些层是累积的，修改会覆盖底层的文件，用户只看到合并后的结果，而不需要了解各层的具体内容 传统的 Linux 加载 bootfs 时会先将 rootfs 设为 read-only，然后在系统自检之后将 rootfs 从read-only 改为 read-write，然后我们就可以在 rootfs 上进行读写操作了。 但 Docker 在 bootfs 自检完毕之后并不会把 rootfs 的 read-only 改为 read-write，而是利用 union mount(UnionFS 的一种挂载机制)将image 中的其他的 layer 加载到之前的 read-only的 rootfs 层之上，每一层layer 都是 rootfs 的结构，并且是read-only 的。所以，我们是无法修改一个已有镜像里面的 layer的 只有当我们创建一个容器，也就是将 Docker 镜像进行实例化，系统会分配一层空的 read-write 的 rootfs，用于保存我们做的修改。一层 layer 所保存的修改是增量式的，就像 git 一样。 4 Cgroup4.1 引入问题如果不对一个容器做任何资源限制，则宿主机会允许其占用无限大的内存空间，有时候会因为代码bug程序会一直申请内存，直到把宿主机内存占完，为了避免此类的问题出现，宿主机有必要对容器进行资源分配限制，比如CPU、内存等 4.2 概念Cgroups是一种用来限制容器对宿主机资源使用量的一种机制，k8s里pod的request与limit底层其实就是在配置cpu cgroup 使用cgroup来限制容器对宿主机资源的使用上限，保证其不要因为无限占用&#x2F;消耗 宿主机资源而影响到其他容器的情况，从而保证了宿主机上容器运行的稳定性 每个Cgroups的子系统都是通过一个虚拟文件系统挂载点的方式，挂到一个缺省的目录下。在linux发行版里，cpu cgroup一般是挂载到 &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu 目录下，在该目录下，每个控制组（Control Group)都是一个子目录，各个控制组之间的关系是一个树状的层级关系（hiearchy） 例如，我们在子系统的最顶层目录开始创建两个控制组，其实就是创建两个目录group1与group2，然后再在group2下面创建两个控制组group3与group4，如此，我们便建立了一个树状的控制组层级，如下图所示 创建控制组后，会在目录下自动生成一系列文件 12345678910[root@test04 cgroup]# df -h |grep systmpfs 981M 0 981M 0% /sys/fs/cgroup[root@test04 cgroup]# cd /sys/fs/cgroup/cpu[root@test04 cpu]# mkdir group1[root@test04 cpu]# mkdir group2[root@test04 cpu]# cd group2[root@test04 group2]# mkdir group3 group4[root@test04 group2]# ls cpu.*cpu.cfs_period_us cpu.cfs_quota_us cpu.rt_period_us cpu.rt_runtime_us cpu.shares cpu.stat 删除控制组 1234567891011121314151617181920rm -rf 控制组目录，会报错不允许删除 # 解决方法借助libcgroup工具删除目录 # 安装LIBCGROUP工具:使用 libcgroup 工具前，请先安装 libcgroup 和 libcgroup-tools 数据包# REDHAT系统安装yum install libcgroup libcgroup-tools -y # UBUNTU系统安装:apt-get install cgroup-bin # 验证是否安装成功cgdelete -h cgdelete cpu:/group1cgdelete cpu:/group2 #有子group需要-r删除cgdelete -r cpu:/group1 4.3 资源限制默认情况下，容器没有资源的使用限制，可以使用主机内核调度程序允许的尽可能多的资源Docker 提供了控制容器使用资源的方法,可以限制容器使用多少内存或 CPU等， 在docker run 命令的运行时配置标志实现资源限制功能。官方文档 memory cgroup cpu cgroup disk cgroup","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[]},{"title":"容器内存","slug":"Container/memory","date":"2025-09-10T04:00:48.000Z","updated":"2025-09-10T04:06:23.010Z","comments":true,"path":"Container/memory/","permalink":"https://aquapluto.github.io/Container/memory/","excerpt":"","text":"1 容器级别的OOM案例演示OOM概念 mem_alloc.c 1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;malloc.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt; #define BLOCK_SIZE (1024*1024) int main(int argc, char **argv)&#123; int thr, i; char *p1; if (argc != 2) &#123; printf(&quot;Usage: mem_alloc &lt;num (MB)&gt;\\n&quot;); exit(0); &#125; thr = atoi(argv[1]); /*此处指定我们for循环的次数，后面我们启一个for循环每次申请1M*/ printf(&quot;Allocating,&quot; &quot;set to %d MBytes\\n&quot;, thr); sleep(30); for (i = 0; i &lt; thr; i++) &#123; p1 = malloc(BLOCK_SIZE); /*单位为Bytes，此处代表申请1M的内存，得到的就是一个虚拟地址*/ memset(p1, 0x00, BLOCK_SIZE); /*单位为Bytes，此处是根据虚拟地址真正应用物理内存*/ &#125; sleep(6000); return 0;&#125; 编译生成可执行文件 1gcc -o mem_alloc mem_alloc.c 把该程序打包到容器里，是一样的效果，因为容器本身就是进程 1234567891011121314# 1、DockerfileFROM centos:7 ADD mem_alloc /opt ENV memSize 1000 CMD /opt/mem_alloc $memSize # 2、构建镜像docker build -t mem_alloc:v1.0 ./ # 3、测试docker run -e memSize=1000 --name mem_test mem_alloc:v1.0 紧接着我们为该该容器设置mem cgroup上限为515MB 123456789sleep 2CONTAINER_ID=$(sudo docker ps --format &quot;&#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;&quot; | grep -i mem_test | awk &#x27;&#123;print $1&#125;&#x27;)echo $CONTAINER_ID CGROUP_CONTAINER_PATH=$(find /sys/fs/cgroup/memory/ -name &quot;*$CONTAINER_ID*&quot;)echo $CGROUP_CONTAINER_PATH echo 536870912 &gt; $CGROUP_CONTAINER_PATH/memory.limit_in_bytescat $CGROUP_CONTAINER_PATH/memory.limit_in_bytes 容器启动后运行mem_alloc，不断申请内存，直到512MB，会触发OOM被干掉 1234567[root@test04 test]# docker inspect mem_test |grep -i status -A 5 &quot;Status&quot;: &quot;exited&quot;, &quot;Running&quot;: false, &quot;Paused&quot;: false, &quot;Restarting&quot;: false, &quot;OOMKilled&quot;: true, # 该值为true &quot;Dead&quot;: false, 2 memory cgroup2.1 参数介绍每个Cgroups的子系统都是通过一个虚拟文件系统挂载点的方式，挂到一个缺省的目录下。在linux发行版里，memory cgroup一般是挂载到 /sys/fs/cgroup/memory 目录下，我们创建一个子目录作为控制组。控制组参数有很多，我们主要关注3个 参数1：memory.limit_in_bytes 最关键的一个参数，用于控制该控制组内所有进程可用内存的最大值，一旦超过该值默认就会触发OOM，但是该OOM是控制组级的而不是系统级的，你看系统的内存完全有可能是充足的，这只是在限制控制组级发生了超出限制的事情，所以只会杀掉该控制组内的某些进程。而且是否一定会发生OOM，得看下面参数 memory.oom_control 对应 docker 命令选项：-m, --memory= 补充：在k8s中关于内存的request与limit设置，yaml文件里指定的limit就是在改上面参数，至于request那只是创建容器时的一个调度指标，并不修改任何参数，也就是说k8s request不会修改Memory Cgroup里的参数，它只是在kube scheduler里调度的时候，看做个计算，看节点上是否还有内存给这个新的container。 参数2：memory.oom_control 控制当控制组内所有进程占用内存达到上限时，是否会触发OOM，默认为触发，设置为1代表不触发，一旦容器内存达到最大限制值，并不会被oom，但是malloc申请内存的行为会中止，并且会因为长期等待而进入可中断睡眠状态 对应 docker 命令选项：--oom-kill-disable 123456789101112cd /sys/fs/cgroup/memory/system.slice/docker-xxx[root@test04 xxx]# cat memory.oom_control oom_kill_disable 0under_oom 0oom_kill 0[root@test04 xxx]# echo 1 &gt; memory.oom_control [root@test04 xxx]# cat memory.oom_control oom_kill_disable 1under_oom 0oom_kill 0 参数3：memory.usage_in_bytes 该参数只是只读的，代表的就是控制组内所有进程使用的内存量，通过判断该值得大小是否接近 memory.limit_in_bytes 可以断定发生OOM的风险 注意对于memory cgroup的控制组目录树来说，如果父级group1的 memory.limit_in_bytes 设置为500M，那么子级group3设置的1000M是无效的，最大也只能到500M 2.2 Swap对容器memory cgroup的影响我们知道，Swap存在的意义就在于，突然仅仅只在一段时间内内存暴涨的情况下，进程不会因OOM被干掉，而是会将一部分内存交换到磁盘上即Swap分区里。 如果开启swap，那么对容器的内存限制memory group就会失去意义，因为一旦容器它没有内存了，总会交换到swap里，此时对其的内存限制是一定要考虑swap分区大小的，实际上启动docker容器时，可以用 -m 指定容器可用的物理内存大小，用 –memory-swap 指定可用 物理内存+swap分区 的总大小 示例 1、启用swap 12345678910111213141516171819# 方式一：基于现有的swap分区[root@test04 page_test]# cat /etc/fstab .......#UUID=94de1024-5700-491b-861d-4bc343fa79dc swap swap defaults 0 0[root@test04 page_test]# swapon UUID=94de1024-5700-491b-861d-4bc343fa79dc[root@test04 page_test]# free -m total used free shared buff/cache availableMem: 1960 1003 386 10 570 810Swap: 2047 0 2047 # 方式二：fallocate -l 20G ./swapfiledd if=/dev/zero of=./swapfile bs=1024 count=20971520chmod 600 ./swapfile mkswap ./swapfileswapon swapfile 2、制作镜像并且启动容器，占用100M内存 1234# 我们这里直接使用第1节OOM案例演示制作的镜像就可以 # 启动，占用100M内存docker run -d -e memSize=100 --name mem_test3 mem_alloc:v1.0 3、为该容器设置mem cgroup上限为150MB 12345678CONTAINER_ID=$(sudo docker ps --format &quot;&#123;&#123;.ID&#125;&#125;\\t&#123;&#123;.Names&#125;&#125;&quot; | grep -i mem_test3 | awk &#x27;&#123;print $1&#125;&#x27;)echo $CONTAINER_ID CGROUP_CONTAINER_PATH=$(find /sys/fs/cgroup/memory/ -name &quot;*$CONTAINER_ID*&quot;)echo $CGROUP_CONTAINER_PATH echo 157286400 &gt; $CGROUP_CONTAINER_PATH/memory.limit_in_bytescat $CGROUP_CONTAINER_PATH/memory.limit_in_bytes 容器启动后运行mem_alloc，大概30s后会占用100M内存，此时我们再进入容器内，继续执行mem_alloc申请100M内存，此时总共就是200M内存超过了150M这个最大限制，我们看容器是否会被OOM 123[root@test04 ~]# docker exec -ti mem_test3 shsh-4.2# /opt/mem_alloc 100 # 会发现是可以申请成功的，容器不会被OOMAllocating,set to 100 Mbytes 查看，会发现交换分区已经使用 1234[root@test04 ~]# free -m total used free shared buff/cache availableMem: 1960 1161 70 10 728 638Swap: 2047 63 1984 总结：启用了swap分区，对容器带来的影响是进行最大内存限制时，可用内存需要考虑物理内存以及swap分区的大小，而不能单纯地考虑物理内存的使用量。如果只以物理内存为准来进行限制，那么一旦容器的进程发生内存泄漏（Memory leak），本来memory croup是可以及时杀死该进程的，但因为swap分区的存在，并不会立刻杀死进程。宿主机操作系统会不断把进程的内存数据交换到磁盘swap分区上，产生大量的磁盘IO，这会带来宿主机整体性能的下降，进而影响宿主机上其他进程或容器的影响。 但是有时候我们的程序是需要使用swap分区来防止内存突然暴涨，而被OOM killer干掉的，比如有些程序的启动需要进行大量的初始操作，需要占用很多内存，但是启动之后的内存没有那么大，所以我们如果给此类容器分配很大内存是不合理的，分配小了又会导致在启动阶段被OOM干掉，此时swap分区就显得很有用了。 如果有些应用场景，恰好就遇到了需要上述需要swap分区的场景，一些应用需要用swap，另外一些应用又不想用swap分区。怎么办呢？此时我们需要在宿主机上进行全局配置swap分区的使用频率，这会影响到所有容器，然后我们需要针对个别需要关闭swap分区使用的容器进行局部设置，让这些个别容器彻底不用swap分区 2.3 控制局部对swap的使用控制全局就是使用 vm.swappiness，详情看 swap分区 配置容器控制组中的 memory.swappiness 参数来控制局部，对应的命令选项为 --memory-swappiness ，该参数会覆盖全局的swappiness，即容器自己的优先级更高 123# cd /sys/fs/cgroup/memory/system.slice/docker-xxx# cat memory.swappiness 60 要注意的是，memory.swappiness的值为0，是表示禁止容器对swap分区的使用，匿名内存是永远无法写入swap分区的，如果内存使用量超出最大限制就会被OOM干掉，这一点与操作系统的swappiness是不同的，操作系统的swappiness配置为0代表的是尽可能不用swap分区，并非彻底不用。 补充：在k8s里默认kubelet不能在开启swap分区的节点上运行，但是可以配置 –fail-swap-on 参数为false来让kubelet正常启动。 3 如何定位OOM问题与解决系统级与容器级的OOM演示一个就行，下列直接演示系统级OOM，启动容器把剩余内存占完，令主机内存剩余不多 123456789# 启动容器，占用1000M内存docker contaienr rm -f mem_testdocker run -e memSize=1000 --name mem_test mem_alloc:v1.0# 我们的物理内存此时只剩774M了[root@test04 ~]# free -m total used free shared buff/cache availableMem: 1960 927 292 108 741 774Swap: 0 0 0 一旦发生OOM之后，我们查看内核日志，可以执行 journal -k 或者查看 /var/log/messages 日志，以图为例主要有三部分内容 第一部分内容：我们找到我们容器的进程mem_alloc，发现占用的 rss（resident set size）物理机内存页面数量为211270个页面，每个页面大小通常为4KB 123456789Python 2.7.5 (default, Jun 28 2022, 15:30:04) [GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; 211270 * 4845080&gt;&gt;&gt; 211270 * 4/1024 825# 计算出结果是825M，而物理机内存只有774M剩余，所以发生了系统级别的OOM 第二部分：列出了发生OOM的memory控制组，可以看到具体是哪个容器发生了OOM 第三部分：显示了最终被OOM killer杀死的进程为68219 有上述三部分信息作为依据之后，我们接下来的处理方案有两种 被OOM干掉的进程可能本身就是需要很大的内存，我们需要调大 memory.limit_in_bytes 被OOM干掉的额进程存在BUG，导致内存泄漏，从而达到了 memory.limit_in_bytes 的限制，此时就需要解决bug了 4 容器内存占满了就一定会OOM吗先说答案：容器级别的OOM，容器内存占满了，不一定会OOM。因为memory_usage_in_bytes 并不代表真正的内存使用量，memory_uages_in_bytes &#x3D; rss真正占用的物理内存 + page cache的大小。page cache是被linux系统借走用来提升内存利用的。在内存不充足时 linux 系统会释放page cache来给 rss 用的 1234# 如果你想手动释放，命令为echo 3 &gt; /proc/sys/vm/drop_caches补充：free里的buffer写缓冲与cache读缓存统称为page cache 所以在一些场景，容器的内存使用 memory_usage_in_bytes 虽然达到了 memory.limit_in_bytes 也不会oom，反而还能继续申请到内存。什么场景呢？就是容器内在进行大量的读写，这些数据会大量缓存到page cache里，memory_usage_in_bytes 随即升高，因此整体内存用量 memory_uages_in_bytes 很快就达到了 memory.limit_in_bytes 的最大限制，但此时的容器内存其实并没有真的用满，因为 memory_uages_in_bytes 使用掉的内存里多半是给了cache，而cache占用掉的内存，是会被linux系统释放掉的。所以你再申请内存并不会OOM，而会触发linux的内存回收机制、采用LRU算法回收内存，cache里因为是一些缓存内存无关紧要，比起关键数据，cache里的符合LRU的最近肯定最少使用，自然是被优先释放的，这样就可以继续分配给rss使用了， 综上，判断内存的真实使用率，不能看控制组下的 memory_usage_in_bytes，而是应该看控制组下的 memory.stat 里面的 rss 值，至于里面的page cache则不是，即控制组下的 memory.stat 记录的才是当前控制组内存的实际开销 12345678910[root@test04 xxx]# cd /sys/fs/cgroup/memory/system.slice/docker-xxx[root@test04 xxx]# cat memory.stat |grep rssrss 45056 # ---------------------》这个rss的值才是真正占用的物理内存rss_huge 0total_rss 45056total_rss_huge 0[root@test04 xxx]# cat memory.stat |grep cachecache 0 total_cache 0 # ---------------------》这个就是page cache的大小 所以控制组memory group里统计的是容器rss与page cache之和，而后者是可以释放给rss的，由于page cache的特性，一些读写频繁的容器会经常处于最大内存限制，这没问题，我们主要关注rss即可，只要rss没有接近最大限制，就不用担心，","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[]},{"title":"safe","slug":"Container/safe","date":"2025-09-10T04:00:37.000Z","updated":"2025-09-10T04:00:37.865Z","comments":true,"path":"Container/safe/","permalink":"https://aquapluto.github.io/Container/safe/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"network","slug":"Container/network","date":"2025-09-10T04:00:31.000Z","updated":"2025-09-10T04:00:31.641Z","comments":true,"path":"Container/network/","permalink":"https://aquapluto.github.io/Container/network/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"监控容器真实的cpu使用率","slug":"Container/process/cpu-usage","date":"2025-09-10T03:50:03.000Z","updated":"2025-09-10T14:00:32.728Z","comments":true,"path":"Container/process/cpu-usage/","permalink":"https://aquapluto.github.io/Container/process/cpu-usage/","excerpt":"","text":"1 在容器内无法通过top命令获取真实cpu使用率当容器云平台中一些节点出现负载过高、导致应用程序异常时，一个主要的解决方案就是找出对cpu占用过高的进程。容器云平台主要跑的都是容器进程，如何查看容器进程对cpu的真实使用率就成了一件重要的事情。 在宿主机我们可以通过top命令查看，但是在容器里，你通过top命令是无法查看到容器本身总共占用了多cpu资源的，看到的依然是宿主机的状态。我们进入容器内执行top查看docker run -ti –name test centos:7 sh 发现容器内进程消耗 0.7+0.7+0.3+0.0，而容器内的sh进程与top进程都没有消耗这么多cpu，这就验证了我们的说法：在容器内是无法通过top命令获取容器对cpu的占用率的 下面就让我们看看宿主机获取CPU使用率的原理 2 宿主机获取CPU使用率的原理2.1 获取单个进程对cpu的使用率cat &#x2F;proc&#x2F;pid&#x2F;stat 的内容有50多项，包括进程pid、名字、运行状态、ppid、优先级、内存使用等。要统计cpu使用率，主要关注第14项utime与第15项目stime utime代表进程的用户态部分在linux系统调度中获取的cpu的ticks time代表进程的内核态部分在linux系统调度中获取的cpu的ticks ticks是linux系统的一个时间单位，与人们熟悉的秒、毫秒一样都是时间单位，只不过在linux中一个tick代表一次中断的周期，这个周期需要耗费的时间由中断频率HZ决定，HZ在linux系统中默认为100，可以用如下命令查看 123# HZ为100代表1s内发生100次中断[root@test04 ~]# getconf CLK_TCK100 一个tick具体代表多长时间就是：1&#x2F;100 秒，即一次中断耗费的时间是1&#x2F;100秒，也就是一个tick所代表的的时间。如果utime等于150ticks，就相当于150*1&#x2F;100&#x3D;1.5秒，代表进程从启动那一刻到现在总共在用户态运行了1.5秒 统计进程对cpu的占用率的公式如下 123456789进程的 CPU 使用占比 =((utime_2 – utime_1) + (stime_2 – stime_1)) / (HZ * et * 1 )上述结果为一个小数，想要得到百分率，需要乘以100，如下进程的 CPU 使用百分率 = ((utime_2 – utime_1) + (stime_2 – stime_1)) * 100 / (HZ * et * 1 )(HZ et 1 )中1代表1颗cpu、et代表时间间隔，HZ代表中断频率最终简化为进程的 CPU 使用百分率 =（某段时间内进程占内核态与用户态ticks/ 该段时间内单个 CPU 总 ticks）*100.0 补充：一个cpu的完整工作时间，可能服务过多个进程，如下图颜色标注代表cpu服务过3个进程，红色框代表的进程可能占用的只是tick3、tick6、tick7这三个时间片，那么该段时间内，红色框进程对cpu的占用率（注意对cpu的占用率一定统计的是单颗）就是3&#x2F;7 用命令查看启动进程的cpu占用率为：7.6 123456[root@test04 ~]# sh a.sh &amp;[1] 63214[root@test04 ~]# ps aux |head -1USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND[root@test04 ~]# ps aux |grep 63214 |grep a.shroot 63214 7.6 0.1 113552 3316 pts/0 S 12:30 1:43 sh a.sh 编写脚本来统计该进程对cpu的占用率，与我们用ps命令或者top命令看到的7.6一致 123456789101112131415utime1=$(cat /proc/63214/stat | awk &#x27;&#123;print $14&#125;&#x27;)stime1=$(cat /proc/63214/stat | awk &#x27;&#123;print $15&#125;&#x27;)sleep 10 # 等待10s后utime2=$(cat /proc/63214/stat | awk &#x27;&#123;print $14&#125;&#x27;)stime2=$(cat /proc/63214/stat | awk &#x27;&#123;print $15&#125;&#x27;)user_ticks=$((utime2-utime1))sys_ticks=$((stime2-stime1))process_total_ticks=$((user_ticks+sys_ticks))cpu_total_ticks=$((100 * 10 * 1))echo $&#123;process_total_ticks&#125; / $&#123;cpu_total_ticks&#125; | bc -l 2.2 统计整个系统对cpu的使用率（指定CPU的使用率）top命令是如何统计出一个cpu上，所有的us进程对整颗cpu的占用率，以及sys、ni、id、wa、hi、si、st各自对本颗cpu的占用率呢？原理很简单，还是借助proc文件系统 123456[root@test04 ~]# cat /proc/stat |grep cpucpu 5086968 3 77767 29442463 63204 0 6255 0 0 0cpu0 1272310 0 20090 7376479 70 0 1606 0 0 0cpu1 1270320 0 19418 7373127 63 0 2209 0 0 0cpu2 1273176 0 17841 7377069 75 0 1327 0 0 0cpu3 1271161 2 20417 7315787 62994 0 1113 0 0 0 从cpu名开始数，关于数据列总共有10列，前八列对应的就是该cpu上，us、sys、ni、id、wa、hi、si、st累积的ticks数 如果计算某一项对整个cpu的占用率呢，原理还是与前面一样，但此处能简单一些，统计出一个时间段内所有项us、sys、ni、id、wa、hi、si、st的总ticks数加到一起，然后用某一项的个数除以这个总数就行 我们还是以10s为单位测试，统计cpu0上关于us的使用率 12345678910111213141516171819202122232425262728293031323334353637# 获取所有项的初始值，注意从第二列开始是数据列us1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $2&#125;&#x27;)sys1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $3&#125;&#x27;)ni1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $4&#125;&#x27;)id1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $5&#125;&#x27;)wa1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $6&#125;&#x27;)hi1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $7&#125;&#x27;)si1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $8&#125;&#x27;)st1=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $9&#125;&#x27;)# 间隔10ssleep 10# 再次获取所有项的ticks数us2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $2&#125;&#x27;)sys2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $3&#125;&#x27;)ni2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $4&#125;&#x27;)id2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $5&#125;&#x27;)wa2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $6&#125;&#x27;)hi2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $7&#125;&#x27;)si2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $8&#125;&#x27;)st2=$(cat /proc/stat |grep cpu0 |awk &#x27;&#123;print $9&#125;&#x27;)# 计算10s内所有项的总tick之和us_total=$((us2-us1))sys_total=$((sys2-sys1))ni_total=$((ni2-ni1))id_total=$((id2-id1))wa_total=$((wa2-wa1))hi_total=$((hi2-hi1))si_total=$((si2-si1))st_total=$((st2-st1))total=$((us_total + sys_total + ni_total + id_total + wa_total + hi_total + si_total + st_total))# 拿某一项除以总tick之和，就计算出该项对本颗cpu的占用率echo $us_total / $total | bc -l 3 获取容器内所有进程对cpu的真实使用率容器的本质就是进程，所以统计方式与前面一样，只不过针对容器来说获取tick的方式更为便捷，如下所示 123456789101112131415161718192021222324#启动容器docker container run -d --name test centos:7 sh -c &quot;i=0;while true;do let i++;done&quot;#获取容器id[root@test04 ~]# docker inspect --format=&quot;&#123;&#123;.Id&#125;&#125;&quot; testdaa5429122d890de00b208f802f6e9e4ed0690efa6f5d42067cce927e6fbbd95#进入宿主机下的容器目录cd /sys/fs/cgroup/cpucd system.slice/docker-daa5429122d890de00b208f802f6e9e4ed0690efa6f5d42067cce927e6fbbd95.scope/#然后查看[root@test04 docker-daa5429122d890de00b208f802f6e9e4ed0690efa6f5d42067cce927e6fbbd95.scope]# cat cpuacct.stat user 66842 # 这个就是当前控制组里所有进程的用户态tickssystem 2550 # 这个就是当前控制组里所有进程的内核态ticks[root@test04 docker-daa5429122d890de00b208f802f6e9e4ed0690efa6f5d42067cce927e6fbbd95.scope]# cat cpuacct.stat;sleep 3;cat cpuacct.stat user 120231system 4605user 120521system 4615#计算((120521 - 120231) + (4615 - 4605)) * 100 / (100 * 3 * 1) 补充1：/sys/fs/cgroup/cpu 是 /sys/fs/cgroup/cpu,cpuacct/ 的链接文件 12345[root@test04 docker-daa5429122d890de00b208f802f6e9e4ed0690efa6f5d42067cce927e6fbbd95.scope]# ll -d /sys/fs/cgroup/cpu,cpuacct/dr-xr-xr-x 7 root root 0 8月 10 12:52 /sys/fs/cgroup/cpu,cpuacct/[root@test04 docker-daa5429122d890de00b208f802f6e9e4ed0690efa6f5d42067cce927e6fbbd95.scope]# ll -d /sys/fs/cgroup/cpulrwxrwxrwx 1 root root 11 8月 10 12:52 /sys/fs/cgroup/cpu -&gt; cpu,cpuacct 补充2：如果不想采用上述统计方法，而想通过直接给容器定制自己的proc文件系统来获取容器真实的cpu占用情况，可以 参考文章 4 监控容器资源状态我们可以使用现成的指令或工具，比如docker stats或者CAdvisor+InfluxDB+Granfana，本质原理也都是通过统计cgroup来计算机容器对cpu资源占用的 4.1 原生命令docker stats123# docker stats 容器名或id号 CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS487ecf4fc7fd c3 398.82% 3.199MiB / 3.84GiB 0.08% 586B / 0B 0B / 0B 5 但是 docker stats 统计结果只能是当前宿主机的全部容器不能针对多台宿主机，并且数据资料是实时的，没有地方存储、没有健康指标过线预警等功能，没有界面，数据可视化还需要做大量的工作。 并且 docker stats 结果中一些指标不准确，比如上述查看结果中PIDS从字面看指的是pid数，理论将如果显示为5代表启动了5个进程，但实际上，启动的线程也被计算到里面，如下示例中只有一个进程，但是进程里包含主线程在内总线程数为5，则 docker stats 查看到的PIDS为5 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@test04 ~]# lsdockerfile libdead_loop.c test.py[root@test04 ~]# cat libdead_loop.cvoid DeadLoop() &#123; while (1) &#123; ; &#125;&#125; [root@test04 ~]# cat test.py #coding:utf-8from ctypes import cdllfrom threading import Threadimport sysif __name__ == &quot;__main__&quot;: if not (len(sys.argv) == 2 and sys.argv[1].isdigit()): print(&quot;Usage: python xxx.py cpu个数&quot;) exit() num = int(sys.argv[1]) lib = cdll.LoadLibrary(&quot;/opt/libdead_loop.so&quot;) for i in range(num): Thread(target=lib.DeadLoop).start() with open(&#x27;/tmp/log.txt&#x27;,mode=&#x27;at&#x27;) as f: f.write(&quot;%s %s\\n&quot; %(i,Thread(target=lib.DeadLoop).name))[root@test04 ~]# gcc libdead_loop.c -fPIC -shared -o ./libdead_loop.so[root@test04 ~]# cat dockerfile FROM centos:7ADD libdead_loop.so /optADD test.py /ENTRYPOINT [&quot;python&quot;,&quot;test.py&quot;]CMD 2 [root@test04 ~]# docker build -t egon_test:v1.0 ./[root@test04 ~]# docker run -d --name egon_test egon_test:v1.0 4 # 包括主线程在内共5个线程[root@test04 ~]# docker stats egon_test # 查看结果PIDS为5CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS138b0c3eea6a egon_test 205.38% 3.199MiB / 3.84GiB 0.08% 586B / 0B 0B / 0B 5 4.2 cadvisor由于 dokcer stats 有这些问题，所以 cadvisor 诞生了。 它使用Go语言开发，利用Linux的cgroups获取容器的资源使用信息，在K8S中集成在Kubelet里作为默认启动项，官方标配。cadvisor 不仅可以对节点机器上的资源及容器进行实时监控和性能数据采集，包括CPU使用情况、内存使用情况、网络吞吐量及文件系统使用情况，还提供基础查询界面和 http 接口，方便 Prometheus 进行数据抓取。正是因为 cadvisor 与 Prometheus 的完美结合，所以它成为了容器监控的第一选择。当然了cadvisor也可以与其他组件结合使用，如 cadvisor + influxdb + grafana 12345678910111213141516docker pull google/cadvisor:latestdocker run \\ --volume=/:/rootfs:ro \\ --volume=/var/run:/var/run:rw \\ --volume=/sys:/sys:ro \\ --volume=/var/lib/docker/:/var/lib/docker:ro \\ --volume=/dev/disk/:/dev/disk:ro \\ --publish=8080:8080 \\ --detach=true \\ --name=cadvisor \\ google/cadvisor:latest访问：http://172.16.10.14:8080/containers/ # 查看到宿主机上容器的资源占用情况http://172.16.10.14:8080/docker/ # 可以查看某个容器的情况","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"cpu cgroup使用","slug":"Container/process/cpu-cgroup","date":"2025-09-10T03:49:44.000Z","updated":"2025-09-10T14:02:45.288Z","comments":true,"path":"Container/process/cpu-cgroup/","permalink":"https://aquapluto.github.io/Container/process/cpu-cgroup/","excerpt":"","text":"1 容器的CPU限制介绍官方文档说明 一个宿主机，有几十个核心的CPU，但是宿主机上可以同时运行成百上千个不同的进程用以处理不同的任务，多进程共用一个 CPU 的核心为可压缩资源，即一个核心的 CPU 可以通过调度而运行多个进程，但是同一个单位时间内只能有一个进程在 CPU 上运行，那么这么多的进程怎么在 CPU 上执行和调度的呢？ Linux kernel 进程的调度基于CFS(Completely Fair Scheduler)，完全公平调度，cfs定义了进程调度的新模型，它给每一个进程安排一个虚拟时钟vruntime。如果一个进程得以执行，随着时间的增长，其vruntime将不断增大。没有得到执行的进程vruntime不变, 而调度器总是选择vruntime跑得最慢的那个进程来执行。这就是所谓的“完全公平”。为了区别不同优先级的进程，优先级高的进程vruntime增长得慢，以至于它可能得到更多的运行机会。 2 CFS相关参数CPU Cgroup中和CFS算法相关的参数 —》决定了对CPU的使用率 cpu.cfs_period_us：代表的是cpu的某段时间，以微妙为单位，代表的总时间，假设为100ms cpu.cfs_quota_us：代表的是该控制组内包含的进程，在某段时间内只能运行占用cpu的时间，假设为50ms 50ms &#x2F; 100ms &#x3D; 0.5 代表在100ms的总周期内，该控制组控制的进程最多使用0.5个cpu cpu.shares：控制的是在同一级下的多个控制组，控制在一个控制组目录树下，同一级控制组关于cpu的分配比例，如果其控制的进程对cpu的占用超过了宿主机的实际cpu个数，那么cpu.shares就会生效 例如上例中的group3与group4，如果group3下该值为1024，group4下该值为4096，则group3:group4比值为1:4，代表在一个5颗cpu的机器上，当group3与group4都需要5个cpu时，它们实际分配的cpu是：group3是1个，group4时候4个 123456[root@test04 cpu]# cat /sys/fs/cgroup/cpu/cpu.cfs_quota_us -1[root@test04 cpu]# cat /sys/fs/cgroup/cpu/cpu.cfs_period_us 100000 [root@test04 cpu]# cat /sys/fs/cgroup/cpu/cpu.shares 1024 2.1 cpu.cfs_period_us和cpu.cfs_quota_us开发一个能够占用cpu的程序，输入cpu个数，运行程序便占用n个cpu，此处我们用python编写，为了规避GIL对我们的影响，我们采用ctypes调用c语言库的方式，详见：https://egonlin.com/?p=7204 （1）编写一个libdead_loop.c 12345void DeadLoop() &#123; while (1) &#123; ; &#125;&#125; （2）编译为.so库，把so文件放到&#x2F;opt目录 12345gcc libdead_loop.c -fPIC -shared -o /opt/libdead_loop.so * -shared 为链接库,让编译器知道是要编译一个共享库* -fPIC（Position Independent Code） 编译生成代码与位置无关* 如果想能够调试可加上-g -Wall等参数 （3）编写py脚本test.py 1234567891011121314#coding:utf-8from ctypes import cdllfrom threading import Threadimport sys if __name__ == &quot;__main__&quot;: if not (len(sys.argv) == 2 and sys.argv[1].isdigit()): print(&quot;Usage: python xxx.py cpu个数&quot;) exit() num = int(sys.argv[1]) lib = cdll.LoadLibrary(&quot;/opt/libdead_loop.so&quot;) for i in range(num): Thread(target=lib.DeadLoop).start() 补充：如果不会编写脚本也可以直接使用负载模拟工具 12yum install stress -ystress -c 1 -t 60 # 对1个cpu，测试60秒 启动一个消耗2个cpu（200%）的程序，然后把该程序的pid加入控制组group3里 123[root@test04 ~]# python test.py 2 &amp;[1] 107612[root@test04 ~]# echo $! &gt; /sys/fs/cgroup/cpu/group2/group3/cgroup.procs # 该文件内容默认就为空，若杀死进程后则自动清空 最开始我们没有设置cpu.cfs_quota_us 值，默认为-1，代表无限制，所以启动上述python程序，消耗的cpu无限制，查看如下，占用接近2个cpu 然后我们设置cpu.cfs_quota_us 为150000（150ms），除以默认的cpu.cfs_period_us值100000（100ms），得到1.5，即限制使用1.5个cpu 1234567891011121314# 设置前，查看[root@test04 ~]# cat /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_quota_us -1[root@test04 ~]# cat /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_period_us 100000 # 设置echo 150000 &gt; /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_quota_us # 设置后，查看[root@test04 ~]# cat /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_quota_us 150000[root@test04 ~]# cat /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_period_us 100000 再次查看cpu消耗情况如下 2.2 cpu.shares总cpu个数调整为6个 启动两个进程，一个消耗2颗cpu，另外一个消耗4颗 1234567# 1、启动一个消耗2个cpu的进程[root@test04 ~]# python test.py 2 &amp;[1] 2990 # 2、启动一个消耗4个cpu的进程[root@test04 ~]# python test.py 4 &amp;[2] 2993 因为我们物理主机总cpu个数够用为6个，所以我们可以可以看到正常占用如下 把2990加入group3控制组 12345678# 1、把进程的pid加入控制组group3echo 2990 &gt; /sys/fs/cgroup/cpu/group2/group3/cgroup.procs # 2、限制cpu为1.5个cpuecho 150000 &gt; /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_quota_us # 3、设置cpu.sharesecho 1024 &gt; /sys/fs/cgroup/cpu/group2/group3/cpu.shares 把2993加入group4控制组 12345678# 1、把进程的pid加入控制组group4echo 2993 &gt; /sys/fs/cgroup/cpu/group2/group4/cgroup.procs # 2、限制cpu为3.5个cpuecho 350000 &gt; /sys/fs/cgroup/cpu/group2/group4/cpu.cfs_quota_us # 3、设置cpu.sharesecho 3072 &gt; /sys/fs/cgroup/cpu/group2/group4/cpu.shares 此时关于cpu.shares值，group3:group4的比值为1:3，从图中可以看出，在物理机cpu充足的情况下，cpu.shares并没有发生作用。 接下来，我们把测试机关机，然后将机器的cpu个数调整为4个，然后重新启动两个进程，重新加入group3与group4控制组。 效果会是什么？group3里的进程需要消耗2个cpu，group4里的进程需要消耗4个cpu，而物理机只有4个，明显不够用了，此时 cpu.shares 参数就开始发挥作用了，group3里的进程按比例获得1个cpu，group4里的cpu按比例获取3个cpu 12345678910111213141516171819# 重启主机后需要重建控制组[root@test04 ~]# cd /sys/fs/cgroup/cpu[root@test04 cpu]# mkdir group1[root@test04 cpu]# mkdir group2[root@test04 cpu]# cd group2[root@test04 group2]# mkdir group3 group4[root@test04 group2]# cd /root # group3python test.py 2 &amp;echo $! &gt; /sys/fs/cgroup/cpu/group2/group3/cgroup.procsecho 150000 &gt; /sys/fs/cgroup/cpu/group2/group3/cpu.cfs_quota_usecho 1024 &gt; /sys/fs/cgroup/cpu/group2/group3/cpu.shares # group4python test.py 4 &amp;echo $! &gt; /sys/fs/cgroup/cpu/group2/group4/cgroup.procsecho 350000 &gt; /sys/fs/cgroup/cpu/group2/group4/cpu.cfs_quota_usecho 3072 &gt; /sys/fs/cgroup/cpu/group2/group4/cpu.shares 思考：如果我们执行如下命令，会看到什么？ 1echo 3072 &gt; /sys/fs/cgroup/cpu/group2/group3/cpu.shares 答案：group3:group4关于cpu.shares的比例变为1:1，虽然此时资源不够用，但cpu.shares并不会产生什么本质的影响，具体group3里的进程受 cpu.cfs_quoa_us 与 cpu.cfs_period_us 的控制，对cpu的占用变为1.5，而group4里的进程对cpu的占用变为2.5，1.5+2.5&#x3D;4 2.3 总结cpu.cfs_quoa_us 与 cpu.cfs_period_us 这两个值决定了每个控制组所有进程可使用cpu资源的最大值 cpu.shares这个值决定了cpu cgrup子系统下控制组可用cpu的相对比例，不过只有当系统上cpu被占满时，这个比例才会在各个控制组间起作用 3 配置默认的 CFS 调度程序默认情况下，每个容器对主机的CPU周期的访问都是不受限制的。可以设置各种约束，以限制给定容器对主机CPU周期的访问。大多数用户使用并配置 默认的CFS调度程序。在Docker 1.13及更高版本中，还可以配置 realtime scheduler。 CFS是用于常规Linux进程的Linux内核CPU调度程序。通过几个运行时标志,可以配置对容器拥有的CPU资源的访问量。使用这些设置时，Docker会在主机上修改容器cgroup的设置。 选项 描述 --cpus= 指定一个容器可以使用多少个可用的CPU核心资源。例如，如果主机有两个CPU，如果设置了 –cpus&#x3D;”1.5” ，则可以保证容器最多使用1.5个的CPU(如果是4核CPU，那么还可以是4核心上每核用一点，但是总计是1.5核心的CPU)。这相当于设置 –cpu-period&#x3D;”100000” 和 –cpu-quota&#x3D;”150000” 。此设置可在Docker 1.13及更高版本中可用，目的是替代–cpu-period和–cpu-quota两个参数，从而使配置更简单，但是最大不能超出宿主机的CPU总核心数(在操作系统看到的CPU超线程后的数值)，此项较常用 --cpu-period= 过时选项,指定CPU CFS调度程序周期，必须与 –cpu-quota 一起使用 。默认为100微秒。大多数用户不会更改默认设置。如果使用Docker 1.13或更高版本，请改用 –cpus --cpu-quota= 过时选项,在容器上添加 CPU CFS 配额，计算方式为 cpu-quota &#x2F; cpu-period的结果值，docker1.13 及以上版本通常使用–cpus 设置此值 --cpuset-cpus 用于指定容器运行的 CPU 编号，也就是所谓的CPU绑定。如果一个或多个CPU，则容器可以使用逗号分隔的列表或用连字符分隔的CPU范围。第一个CPU的编号为0。有效值可能是 0-3 （使用第一，第二，第三和第四CPU）或1,3 （使用第二和第四CPU） --cpu-shares 用于设置 cfs 中调度的相对最大比例权重,cpu-share 的值越高的容器，将会分得更多的时间片(宿主机多核 CPU 总数为 100%，假如容器 A 为1024，容器 B 为2048，那么容器 B 将最大是容器 A 的可用 CPU 的两倍 )，默认的时间片1024，最大 262144。这是一个软限制。注意:进程数要多个CPU的核数才能看到效果，此值不能设置太小 4 使用 Stress-ng 测试 CPU配置范例: 查看 stress-n 关于cpu的帮助 1234567[root@ubuntu1804 ~]#docker run -it --rm --name c1 lorel/docker-stress-ng |grep cpu-c N, --cpu N #start N workers spinning on sqrt(rand()) --cpu-ops N #stop when N cpu bogo operations completed-l P, --cpu-load P #load CPU by P %%, 0=sleep, 100=full load (see -c) --cpu-method m #specify stress cpu method m, default is all Example: stress-ng --cpu 8 --io 4 --vm 2 --vm-bytes 128M --fork 4 --timeout 10s 范例: 不限制容器CPU 123456789101112131415161718[root@ubuntu1804 ~]#lscpu |grep CPUCPU op-mode(s): 32-bit, 64-bitCPU(s): 6On-line CPU(s) list: 0-5CPU family: 6Model name: Intel(R) Core(TM) i7-4710HQ CPU @ 2.50GHzCPU MHz: 2494.236NUMA node0 CPU(s): 0-5#占用4个CPU资源.但只是平均的使用CPU资源[root@ubuntu1804 ~]#docker run -it --rm --name c1 lorel/docker-stress-ng --cpu 4[root@ubuntu1804 ~]#docker stats --no-stream[root@ubuntu1804 ~]#cat /sys/fs/cgroup/cpuset/docker/818a85e1da2f9a4ef297178a9dc09b338b2308108195ad8d4197a1c47febcbff/cpuset.cpus0-5[root@ubuntu1804 ~]#top 范例: 限制使用CPU 1234567[root@ubuntu1804 ~]#docker run -it --rm --name c1 --cpus 1.5 lorel/docker-stress-ng --cpu 4stress-ng: info: [1] defaulting to a 86400 second run per stressorstress-ng: info: [1] dispatching hogs: 4 cpu, 4 vm[root@ubuntu1804 ~]#docker stats --no-stream[root@ubuntu1804 ~]#top 范例: 限制CPU 123[root@ubuntu1804 ~]#docker run -it --rm --name c1 --cpu-quota 2000 --cpu-period 1000 lorel/docker-stress-ng --cpu 4[root@ubuntu1804 ~]#docker stats --no-stream 范例: 绑定CPU 1234567891011121314151617#一般不建议绑在0号CPU上，因0号CPU一般会较忙[root@ubuntu1804 ~]#docker run -it --rm --name c1 --cpus 1.5 --cpuset-cpus 2,4-5 lorel/docker-stress-ng --cpu 4[root@ubuntu1804 ~]#ps axo pid,cmd,psr |grep stress 1964 /usr/bin/stress-ng --cpu 4 2 1996 /usr/bin/stress-ng --cpu 4 5 1997 /usr/bin/stress-ng --cpu 4 2 1998 /usr/bin/stress-ng --cpu 4 4 1999 /usr/bin/stress-ng --cpu 4 2 2002 grep --color=auto stress 1 [root@ubuntu1804 ~]#docker stats --no-stream[root@ubuntu1804 ~]#cat /sys/fs/cgroup/cpuset/docker/585879094e7382d2ef700947b4454426eee7f943f8d1438fe42ce34df789227b/cpuset.cpus2,4-5[root@ubuntu1804 ~]#top 范例: 多个容器的CPU利用率比例 123456789101112131415161718192021222324#同时开两个容器[root@ubuntu1804 ~]#docker run -it --rm --name c1 --cpu-shares 1000 lorel/docker-stress-ng --cpu 4stress-ng: info: [1] defaulting to a 86400 second run per stressorstress-ng: info: [1] dispatching hogs: 4 cpu, 4 vm[root@ubuntu1804 ~]#docker run -it --rm --name c2 --cpu-shares 500 lorel/docker-stress-ng --cpu 4stress-ng: info: [1] defaulting to a 86400 second run per stressorstress-ng: info: [1] dispatching hogs: 4 cpu, 4 vm#注意:进程数要多于CPU的核数才能看到效果,如果两个容器使用的CPU总数不超过CPU实际的核心数，两个容器都显示400%[root@ubuntu1804 ~]#docker stats --no-stream#查看c1容器的cpu利用比例[root@ubuntu1804 ~]#cat /sys/fs/cgroup/cpu,cpuacct/docker/d5944104aff40b7b76f536c45a68cd4b98ce466a73416b68819b9643e3f49da7/cpu.shares1000#查看c2容器的cpu利用比例[root@ubuntu1804 ~]#cat /sys/fs/cgroup/cpu,cpuacct/docker/a1d4c6e6802d1b846b33075f3c1e1696376009e85d9ff8756f9a8d93d3da3ca6/cpu.shares500#再打开新的容器，cpu分配比例会动态调整[root@ubuntu1804 ~]#docker run -it --rm --name c3 --cpu-shares 2000 lorel/docker-stress-ng --cpu 4[root@ubuntu1804 ~]#docker stats --no-stream 范例: 动态调整cpu shares值 123[root@ubuntu1804 ~]#echo 2000 &gt; /sys/fs/cgroup/cpu,cpuacct/docker/a1d4c6e6802d1b846b33075f3c1e1696376009e85d9ff8756f9a8d93d3da3ca6/cpu.shares[root@ubuntu1804 ~]#docker stats --no-stream","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"容器里的进程管理","slug":"Container/process/management","date":"2025-09-10T03:49:36.000Z","updated":"2025-09-11T05:45:07.073Z","comments":true,"path":"Container/process/management/","permalink":"https://aquapluto.github.io/Container/process/management/","excerpt":"","text":"1 容器内的两个特殊进程下列操作说明 tail -f /dev/null是容器内的1号进程，而1号进程的父进程则是0号进程 docker exec执行的命令是sh，该命令是在容器已经running之后运行的，产生的sh进程的父进程为0号进程 在sh环境里执行的命令，当然都是sh的儿子 123456789101112131415161718[root@test01 init5]# docker container run -d --name test centos:7 tail -f /dev/null[root@test01 init5]# docker exec -ti test shsh-4.2# ps -ef UID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:27 ? 00:00:00 tail -f /dev/nullroot 6 0 0 04:27 pts/0 00:00:00 shroot 13 6 0 04:27 pts/0 00:00:00 ps -efsh-4.2# sleep 1000 &amp;[1] 14sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:27 ? 00:00:00 tail -f /dev/nullroot 6 0 0 04:27 pts/0 00:00:00 shroot 14 6 0 04:27 pts/0 00:00:00 sleep 1000root 15 6 0 04:28 pts/0 00:00:00 ps -ef 1.1 0号进程containerd-shim进程是容器操作系统的进程的祖宗，具备回收僵尸儿子的功能，它挂掉整个容器也就退出了 containerd-shim调用runc来创建的容器，容器内的执行的指令都是由该shim进程接收处理的 一旦容器的1号进程结束，整个容器名称空间里的子孙进程都会被0号进程回收 如果容器的1号进程没有结束，但是1号进程的儿子进程结束了成为僵尸进程，0号进程无法处理，是由1号进程回收，而容器内的1号进程是我们自己指定的&#x2F;开发的，而我们自己指定的&#x2F;开发的这个1号进程很有可能没有开发定期发起回收僵尸儿子这种功能，那么儿子死掉后的僵尸进程状态就会累积 docker stop停止容器时，会向容器的1号进程发送一个 -15 信号 此时如果1号进程没有信号转发的能力，1号进程会向容器名称空间中的所有进程都发送一个 -9 强制杀死信号 如果1号进程有信号转发的能力，1号进程会向容器名称空间中的所有进程都转发 -15 信号 查看容器内0号进程对应宿主机的pid号 12345678[root@node1 ~]# docker container ls |grep test1 | awk &#x27;&#123;print $1&#125;&#x27;09e4114ddd9d # 注意,下面查看的是容器内部0号进程，对应宿主机上的pid号[root@node1 ~]# ps aux |grep containerd-shim |grep 09e4114ddd9droot 5439 0.0 0.6 712056 12836 ? Sl 12:27 0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 09e4114ddd9d9747244352637949ade8c61082627984b3381d37a589d92c4bc3 -address /run/containerd/containerd.sock 如果你杀死了容器的containerd-shim进程，那站在操作系统角度，容器下的所有进程都被操作系统的systemd进程收养然后回收了 1.1 1号进程容器内启动的第一个进程，该进程代表了整个容器的生命周期，1号进程结束容器则结束，0号进程完蛋当然容器也完蛋，而且容器内的1号进程必须是一个在前台一直运行的进程。1号进程的孙子进程一旦成为孤儿，1号进程会成为它爹 与操作系统功能区别： 容器内的1号进程并不一定是所有用户进程的祖先，所以exec进入容器里执行命令产生的新进程是0号进程的儿子，而不是1号进程的儿子 我们自己开发的1号进程的程序可能没有回收僵尸儿子的能力 我们自己开发的1号进程的程序可能没有转发信号的能力 所以1号进程应该具备两个能力： 回收僵尸儿子的能力 转发信号的能力 查看容器内1号进程对应宿主机的pid号 1234567[root@node1 ~]# docker container ls |grep test1 | awk &#x27;&#123;print $1&#125;&#x27;09e4114ddd9d[root@node1 ~]# docker inspect 09e4114ddd9d |grep -i pid &quot;Pid&quot;: 5458, &quot;PidMode&quot;: &quot;&quot;, &quot;PidsLimit&quot;: null, 2 其他进程查看容器内其他进程对应宿主机的pid号 12345678910111213141516171819202122232425262728293031323334[root@node1 ~]# docker container ls |grep test1 | awk &#x27;&#123;print $1&#125;&#x27;09e4114ddd9d # 注意，下面查看的是容器内部的进程对应宿主机对应的pid号，具体查看目录可能是system.slice或docker目录[root@node1 ~]# cat /sys/fs/cgroup/memory/system.slice/docker-09e4114ddd9d9747244352637949ade8c61082627984b3381d37a589d92c4bc3.scope/cgroup.procs 545854845485 # 也可以执行命令查看docker top 容器名/或ID # 上述结果包含了容器内1号进程在宿主机的映射，要确定其他进程号与容器内进程的对应关系，可以在宿主机上用ps aux |grep 号码，来过滤进行确认=============》宿主机[root@node1 ~]# ps aux |grep 5458 |grep -v greproot 5458 0.0 0.0 11688 1336 ? Ss 12:27 0:00 sh -c (sleep 10d &amp;) ; tail -f /dev/null[root@node1 ~]# ps aux |grep 5484 |grep -v greproot 5484 0.0 0.0 4364 356 ? S 12:27 0:00 sleep 10d[root@node1 ~]# ps aux |grep 5485 |grep -v greproot 5485 0.0 0.0 4400 352 ? S 12:27 0:00 tail -f /dev/null =============》在容器内，看一眼与上面的对应关系可以执行docker exec -ti test1 sh进入容器内执行ps -ef来查看与上面的的结果是一一对应的补充：由于容器采用了Linux的namespace机制, 对pid进行了隔离. 因此容器内的pid将会从1开始重新编号, 并且不会看到其他容器或宿主机的进程pid。本质上容器就是宿主机上的一个普通的Linux进程, 因此在宿主机中是可以看到容器内进程的pid, 只不过这个pid是在宿主机上显示的, 而非容器内的(因为隔离了) [root@node1 ~]# docker exec -ti test1 shsh-4.2# ps -elfF S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD4 S root 1 0 0 80 0 - 2922 do_wai 04:27 ? 00:00:00 sh -c (sleep 10d &amp;) ; tail -f /dev/null0 S root 8 1 0 80 0 - 1091 hrtime 04:27 ? 00:00:00 sleep 10d0 S root 9 1 0 80 0 - 1100 wait_w 04:27 ? 00:00:00 tail -f /dev/null4 S root 10 0 0 80 0 - 2956 do_wai 04:35 pts/0 00:00:00 sh4 R root 16 10 0 80 0 - 12933 - 04:35 pts/0 00:00:00 ps -elf docker top 显示的容器中的进程可能不太全，与是否该进程归属于1号进程没有任何关系；与进程是否最终归属于该容器的管理进程docker-containerd-shim也没有关系，而是进程是不是在容器的“进程命名空间”中启动的 如果是nsenter进入容器，则启动的进程在docker top中是看不到的，因为nsenter 是直接进入容器的命名空间，但它不属于 Docker 的 exec 管理机制。 虽然该进程在容器中显示的ppid也是0，但是其实同样是0的ppid却可能不是同一个进程，因为，只要父进程在容器外部（宿主机上），在容器内部看不到，则容器内部显示的ppid就统一为0 12345678910# 终端1docker container run -d --name test centos:7 tail -f /dev/nulldocker top test # 终端2docker exec -ti test sh # 进入容器后启动几个进程，可以在终端1用top命令看到 # 终端3# 进入后启动的新进程用top看不到，但是exec进入容器是可以看到的nsenter -t 容器的pid --mount --uts --ipc --net --pid 3 容器内的僵尸进程3.1 僵尸进程示例我们用python解释器作为1号进程来测试（强调，python解释器与bash解释器一样都具备定期回收僵尸儿子的功能，但是我们再次用sleep将python父进程停住，它不动弹了也就不会发起回收了，我们也就能看到僵尸儿子了） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# 1、创建工作目录mkdir /testcd /test # 2、创建脚本文件run1.shcat &gt;&gt; test.py &lt;&lt; EOF#coding:utf-8from multiprocessing import Processimport osimport time def task1(n): print(&quot;儿子,PID:%s PPID:%s&quot; %(os.getpid(),os.getppid())) pp1=Process(target=task2,args=(10,)) # 孙子进程 pp2=Process(target=task2,args=(10,)) # 孙子进程 pp3=Process(target=task2,args=(10,)) # 孙子进程 pp1.start() pp2.start() pp3.start() time.sleep(n) def task2(n): print(&quot;孙子,PID:%s PPID:%s&quot; %(os.getpid(),os.getppid())) time.sleep(n) if __name__ == &quot;__main__&quot;: # 主进程 p=Process(target=task1,args=(10000,)) # 子进程，主进程的儿子 p.start() print(&quot;爸爸,PID: %s&quot; %os.getpid()) time.sleep(10000) EOF # 3、创建dockerfilecat &gt; dockerfile &lt;&lt; EOFFROM centos:7ADD test.py /optCMD python /opt/test.pyEOF # 4、构建镜像并运行docker build -t test_defunct:v1 ./docker run -d --name test1 test_defunct:v1 # 5、进入容器会发现有僵尸进程产生： 因为1号进程一直sleep在原地，根本没有机会去回收僵尸的儿子，如此，效果就模拟出来了[root@test01 test]# docker exec -ti test1 shsh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 07:50 ? 00:00:00 python /opt/test.pyroot 7 1 0 07:50 ? 00:00:00 python /opt/test.pyroot 8 7 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 9 7 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 10 7 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 11 0 0 07:50 pts/0 00:00:00 shroot 18 11 0 07:51 pts/0 00:00:00 ps -ef 此时你是无法杀死那三个僵尸的，那我们杀死他们的爹，即pid为7的进程sh-4.2# kill -9 7sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 07:50 ? 00:00:00 python /opt/test.pyroot 7 1 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 8 1 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 9 1 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 10 1 0 07:50 ? 00:00:00 [python] &lt;defunct&gt;root 11 0 0 07:50 pts/0 00:00:00 shroot 19 11 0 07:52 pts/0 00:00:00 ps -ef 此时你会发现两个非常有趣的现象1、又多一个僵尸进程2、pid为7、8、9、10的进程的ppid都变成了1，也就是说他们都被1号进程收养了 原因是，当我们杀死pid为7的进程的时候，它爹也就是1号进程在原地sleep呢，并不会回收它，所以它停留在僵尸状态，而pid为7的进程一旦死掉，它的三个僵尸儿子们就成了孤儿，自然会被pid为1的进程收养，再说一句，这里的pid为1的进程被sleep住了，肯定不会回收他们，于是它们也残留着，这就是你看到的4个僵尸进程 最后，不能再杀了，而且你kill -9 1也是无效的，1号进程是无法被强制杀死的 如果我关掉容器，或者如果是k8s管理的容器，删掉k8s里的pod，容器里的僵尸进程还会残留吗，答案是肯定不会残留，为什么呢？ 1234567在容器平台上，无论你是用k8s去删除一个pod，或者用docker关闭一个容器，都会用到Containerd这个服务 1、创建容器时：kubelet调用`dockerDaemon`发起创建容器请求，然后由`containerd`接收并创建`containerd-shim`，`containerd-shim`即容器内的0号进程。所以实际的创建容器、容器内执行指令等都是此进程在做 2、同时，`containerd-shim`具有回收僵尸进程的功能，容器1号进程退出后，内核清理其下子孙进程，这些子孙进程被`containerd-shim`收养并清理。 注意：如果1号进程不被Kill，那么其下进程如果有僵尸进程，是无法被处理的。所以用户开发的容器首进程要注意回收退出进程。 ps: 在所有容器都清理后，k8s中的pod也就被删除了。 所以即便容器内的1号进程没有回收僵尸儿子的能力，0号进程是为其兜底的。虽然如此，我们还是要知道在1号进程活着的情况下，它若没有回收僵尸进程的能力，该怎么办 3.2 容器内的僵尸进程的影响虽然每个容器都是一个独立的个体，但容器里的进程pid毕竟还是需要映射到宿主机系统，说白了，对于宿主机linux系统而言，容器就是一组进程集合，如果容器内产生的进程过多（可能是正常产生、也可能是由于出现bug，累积过多的僵尸进程），就会产生类似fork bomb的行为。fork bomb是黑客攻击行为的一种，指的是不断创建新进程直到占满整个主机的pid号为止，此时，该主机便无法再创建任何新进程，宿主机及其上运行的所有容器都受到影响。综上，不管容器是正常的不断产生新的进程，还是说因为出现bug不断累积僵尸进程，都是一种不合理地、无节制地消耗宿主机pid资源的行为 3.3 解决方案站在容器外的角度： 从根本上限制住一个容器可用的pid数目，无论你是正常产生进程，还是出了bug不断累积僵尸进程，统统不允许无限占用pid号，具体操作我们可以在宿主机上配置PID CGROUP来限制每个容器的最大进程数目 站在容器内的角度： 如果你是代码的开发者，那么你应该尽量修正你所编写代码的逻辑，让你的主进程能够定期发起回收僵尸进程的操作 123（1）我们可以去查看p1.join()的代码，里面有一个关于wait的调用，在主进程里调用p1.join()的目的就是等子进程挂掉后而回收它的尸体，所以python代码多进程编程，在主进程里建议在主进程里一个个join子进程（2）而且即便你没有执行p.join(),只要你的主进程在运行着，它也会定期发起回收僵尸进程的操作，怕就怕你既没有调用join方法，还让主进程进入了类似sleep一样的阻塞状态，那僵尸进程的状态就残留住了 如果你只是代码的维护者，碰到了会残留僵尸进程的软件，那意味着该软件的主进程没有或者没做好回收僵尸进程的工作，你需要在容器内引入一个具备回收僵尸进程能力的进程作为1号进程，就好像操作系统拥有systemd进程一样，如此当僵尸进程产生时，你还可以杀掉它的父进程，之后该僵尸会被1号进程自动接管并回收 123456789101112131415161718192021有时候站在程序逻辑或架构的角度，残留僵尸进程是无法避免的，那么就只能为容器引入一个可以定期回收僵尸进程的进程作为容器的1号进程，并且该进程要有能够执行你原来主进程代码的能力才行，如此，你原来的主进程才能被其管理 说道为容器定制一个1号进程，具体可以有很多种，我们列举了如下两种，第一种只是为了辅助理解，并不推荐，第二种才是真正推荐的方案。 - 方案一、bash解释器：在linux系统中，我们可以把任意命令（当然包括你原本的主进程）放到一个bash脚本里，然后用bash解释器执行，如此bash就成了容器里的1号进程，你原本的主进程就成了它的子进程受其管理，并且bash是具备定期回收僵尸进程的能力的 - 方案二、tini：这是一个开源项目，tini是一个替代庞大复杂的systemd体系的解决方案，是一个最小化init系统，专门适用于充当容器内的1号进程，容器内所有其他进程都受其管理。 tini命令集成了类似操作系统init的功能，所以，tini不仅具备定期发起系统调用回收僵尸儿子的能力，还具备信号转发的能力。比如你想tini发送信号，那么它也会向它自己的子进程发送同样的信号，这在平滑关闭保证资源正常回收的操作非常有用。 tini项目地址：https://github.com/krallin/tini tini是如何定期回收僵尸进程的？# 1、关于wait与waitpid- wait()系统调用是一个阻塞的调用，也就是说，如果没有子进程是僵尸进程的话，这个调用就一直不会返回，那么整个进程就会被阻塞住，而不能去做别的事了。 - Linux还提供了一个类似的系统调用waitpid()，这个调用的参数更多。其中就有一个参数 WNOHANG，它的含义就是，如果在调用的时候没有僵尸进程，那么函数就马上返回了，而不会像 wait() 调用那样一直等待在那里。 # 2、在tini里会不断在调用带WNOHANG参数的waitpid()，通过这个方式清理它的僵尸儿子，注意了，只能是儿子 最后总结，如果只是站在回收僵尸儿子的角度，我们用bash做容器的1号进程就完全可以，而且很多语言比如python解释本身就有定期回收僵尸儿子的功能，但除此之外还有信号转发保证平滑关闭的需求，如此看来只有tini兼具二者，肯定是一个最优选择 3.3.1 在容器外使用pids CGROUP123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 在linux系统中可以创建的进程数目是有限的，可以在宿主机执行如下命令查看，如果超过了该值，则无法启动任何新进程，任何命令也都无法执行，因为执行命令也都是在产生进程，虽然执行完毕就销毁该进程，但因为进程数达到上限，你是无法开启任何新进程的cat /proc/sys/kernel/pid_max 该值可设置，linux操作系统支持的最大的pid范围是0-4194304个 无论容器内是正常创建新进程，还是因为bug而不断累积僵尸进程，我们总归应该对其进行限制，不能让一个容器无法无天地创建进程，此时就用到Cgoup机制，具体是是指pids Cgroup，一般Cgroup文件系统挂载点在/sys/fs/cgroup/pids 每创建一容器，创建容器的服务就会在宿主机的/sys/fs/cgroup/pids# 在宿主机执行[root@test01 pids]# cd /sys/fs/cgroup/pids[root@test01 pids]# df .文件系统 1K-块 已用 可用 已用% 挂载点cgroup 0 0 0 - /sys/fs/cgroup/pids 如果是docker命令直接启动的容器，可以后面跟着id号，用docker container ls | grep test2查看id，注意必须是存活的容器才可以[root@test01 pids]# ls docker/67c4f885b2a8e72c7ea7f4b8f7e28f413fdc7a8060881d8fc1a5ff484da801e5/cgroup.clone_children cgroup.procs pids.current taskscgroup.event_control notify_on_release pids.max 如果是kubelet调用容器引擎产生的容器，则查看，[root@test01 pids]# ls system.slice/data-docker-overlay2-8a0bb1b29a2e51278f278ac5f1e8aa4206ce817b767c908cedbb9d5cb6f7a8f5-merged.mount/cgroup.clone_children cgroup.procs pids.current taskscgroup.event_control notify_on_release pids.max 可以df | grep 8a0bb1b29a查看到对应的挂载 我们先打开一个终端，进入test2容器内，[root@test01 ~]# docker exec -ti test2 shsh-4.2# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 11692 1332 ? Ss 05:11 0:00 sh /opt/run.shroot 20 0.0 0.0 4400 352 ? S 05:12 0:00 tail -f /dev/nullroot 34 0.2 0.0 11824 1660 pts/0 Ss 13:53 0:00 shroot 40 0.0 0.0 51732 1704 pts/0 R+ 13:53 0:00 ps auxsh-4.2# 容器内目前总共有3个进程，两个容器自带的，一个我们执行sh进入容器产生的，还要一个ps aux命令的进程执行完毕后立刻结束了， 打开另外一个中断，然后修改test2容器的pid.max为3，再进入容器就会发现无法执行任何命令[root@test01 pids]# pwd/sys/fs/cgroup/pids[root@test01 pids]# echo 3 &gt; docker/67c4f885b2a8e72c7ea7f4b8f7e28f413fdc7a8060881d8fc1a5ff484da801e5/pids.max [root@test01 pids]# 然后回到上面那个终端，会发现任何命令都无法执行sh-4.2# ps auxsh: fork: retry: No child processessh: fork: retry: No child processessh: fork: retry: No child processessh: fork: retry: No child processessh: fork: Resource temporarily unavailablesh-4.2# 3.3.2 bash作为容器内的1号进程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293[root@test01 test1]# cd /test1/[root@test01 test1]# lsdockerfile run.sh test.py[root@test01 test1]# cat test.py#coding:utf-8from multiprocessing import Processimport osimport time def task1(n): print(&quot;儿子,PID:%s PPID:%s&quot; %(os.getpid(),os.getppid())) pp1=Process(target=task2,args=(10,)) pp2=Process(target=task2,args=(10,)) pp3=Process(target=task2,args=(10,)) pp1.start() pp2.start() pp3.start() time.sleep(n) def task2(n): print(&quot;孙子,PID:%s PPID:%s&quot; %(os.getpid(),os.getppid())) time.sleep(n) if __name__ == &quot;__main__&quot;: p=Process(target=task1,args=(10000,)) p.start() print(&quot;爸爸,PID: %s&quot; %os.getpid()) time.sleep(10000)#之前我们是直接创建dockerfile创建一个镜像，由于程序内没有回收僵尸进程的处理，所以导致出现了4个僵尸进程，让bash作为1号进程，可以写个sh脚本，执行脚本，而非直接python代码，run.sh内容如下#!/bin/bashpython /opt/test.py# 添加这一样是为了防止上面那一行进程被干掉后，本进程还有代码在运行着，否则就也结束了tail -f /dev/null #dockerfile内容如下FROM centos:7ADD test.py /opt ADD run.sh /optCMD sh /opt/run.sh#运行容器docker build -t t2:v2 ./docker run -d --name test2 t2:v2[root@test01 test1]# docker exec -ti test2 shsh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:57 ? 00:00:00 sh /opt/run.shroot 7 1 0 04:57 ? 00:00:00 python /opt/test.pyroot 8 7 0 04:57 ? 00:00:00 python /opt/test.pyroot 9 8 0 04:57 ? 00:00:00 [python] &lt;defunct&gt;root 10 8 0 04:57 ? 00:00:00 [python] &lt;defunct&gt;root 11 8 0 04:57 ? 00:00:00 [python] &lt;defunct&gt;root 20 0 1 04:58 pts/0 00:00:00 shroot 25 20 0 04:58 pts/0 00:00:00 ps -efsh-4.2# #先杀8号进程，会回收3僵尸，残留一个僵尸sh-4.2# kill -9 8sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:57 ? 00:00:00 sh /opt/run.shroot 7 1 0 04:57 ? 00:00:00 python /opt/test.pyroot 8 7 0 04:57 ? 00:00:00 [python] &lt;defunct&gt;root 20 0 0 04:58 pts/0 00:00:00 shroot 26 20 0 04:58 pts/0 00:00:00 ps -ef#然后杀7，父进程sh会回收僵尸7sh-4.2# kill -9 7sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:57 ? 00:00:00 sh /opt/run.shroot 20 0 0 04:58 pts/0 00:00:00 shroot 27 1 0 04:59 ? 00:00:00 tail -f /dev/nullroot 28 20 0 04:59 pts/0 00:00:00 ps -ef#然后我们还可以继续在容器内启动python程序，比如sh-4.2# (python /opt/test.py &amp;) # 会被0号进程管理，即containerd-shimsh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 04:57 ? 00:00:00 sh /opt/run.shroot 27 1 0 04:59 ? 00:00:00 tail -f /dev/nullroot 69 0 0 05:01 pts/0 00:00:00 shroot 79 0 0 05:02 pts/0 00:00:00 python /opt/test.pyroot 80 79 0 05:02 pts/0 00:00:00 python /opt/test.pyroot 81 80 0 05:02 pts/0 00:00:00 [python] &lt;defunct&gt;root 82 80 0 05:02 pts/0 00:00:00 [python] &lt;defunct&gt;root 83 80 0 05:02 pts/0 00:00:00 [python] &lt;defunct&gt;root 87 69 0 05:02 pts/0 00:00:00 ps -ef 但是bash不能转发信号，不能平滑关闭容器的内的所有进程 1234567891011121314151617181920212223242526272829[root@test01 test1]# docker run -d --name test3 t2:v2 [root@test01 test1]# docker exec -ti test3 shsh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 11:19 ? 00:00:00 sh /opt/run.shroot 6 1 0 11:19 ? 00:00:00 python /opt/test.pyroot 7 6 0 11:19 ? 00:00:00 python /opt/test.pyroot 8 7 0 11:19 ? 00:00:00 [python] &lt;defunct&gt;root 9 7 0 11:19 ? 00:00:00 [python] &lt;defunct&gt;root 10 7 0 11:19 ? 00:00:00 [python] &lt;defunct&gt;root 11 0 0 11:19 pts/0 00:00:00 shroot 18 11 0 11:20 pts/0 00:00:00 ps -ef [root@test01 ~]# docker container inspect test3 |grep -i pid # 获取容器的pid，对应容器内1号进程的pid &quot;Pid&quot;: 17771, &quot;PidMode&quot;: &quot;&quot;, &quot;PidsLimit&quot;: null, [root@test01 ~]# yum install strace -y [root@test01 ~]# strace -p 17771 # 追踪容器内信号，然后打开另外一个终端执行docker stop test3，会得到如下内容strace: Process 17771 attachedwait4(-1, 0x7ffc632b6990, 0, NULL) = ? ERESTARTSYS (To be restarted if SA_RESTART is set)# 1号进程收到了SIGTERM信号--- SIGTERM &#123;si_signo=SIGTERM, si_code=SI_USER, si_pid=0, si_uid=0&#125; ---wait4(-1, 0x7ffc632b6990, 0, NULL) = ? ERESTARTSYS (To be restarted if SA_RESTART is set)# 其他进程则收到了SIGKILL信号+++ killed by SIGKILL +++ 通过strace命令可以看到1号进程收到的是平滑关闭的信号SIGTERM，而容器内其他进程收到的则是强制杀死的信号SIGKILL，这是有问题的，如果容器内1号进程之外的子进程正在处理一些数据库连接、打开着文件，强制杀死后，这些资源短时间内都不会被释放掉，白白占用资源，如果碰上了高并发压力，这是绝对不允许的。我们应该保证容器在被stop时能够响应-15信号而平滑关闭 3.3.3 使用tini作为容器内的1号进程tini进程的特点如下 作为容器1号进程，并创建用户定义的业务进程 默认将信号传递给子进程，也支持更多传递方式 监听子进程退出并回收（注意只能回收儿子，孙子不行） 跟随最初创建的业务进程的退出而退出，并不是因为SIGTERM的信号 3.3.3.1 启用tini的两种方式方式一：docker默认集成了tini，在docker run的时候用 --init 参数，不需要在镜像中安装 tini，会自动注入tini程式 (/sbin/docker-init) 到容器中 1docker run -d --init --name test1 t1:v1 方式二：如果是 kubernetes 就不行了，还得安装 tini，或者是我们要定制 tini 的运行层级，可以去这里 https://github.com/krallin/tini 下载tini命令，然后基于dockerfile打包到容器里 12345678910111213141516171819202122# ===============需要先下载好一个tini命令，然后编写dockerfile文件如下，制作成一个容器===============FROM centos:7 ENV TINI_VERSION v0.19.0 # 1、从网上下载tini，官网地址https://github.com/krallin/tini/releases/# ADD https://github.com/krallin/tini/releases/download/$&#123;TINI_VERSION&#125;/tini /tini # 2、基本本地下载好的tini命令ADD tini /tini RUN chmod +x /tini ADD test.py /opt ENTRYPOINT [&quot;/tini&quot;,&quot;--&quot;] # 指定要运行的主程序# CMD [&quot;/your/program&quot;, &quot;-and&quot;, &quot;-its&quot;, &quot;arguments&quot;] # ===============然后启动时，就不需要指定--init参数了===============docker run -d --name test1 t1:v1 3.3.3.2 tini收养孤儿回收僵尸儿子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@test01 test2]# cd /test2[root@test01 test2]# lsdockerfile test.py[root@test01 test2]# cat test.py#coding:utf-8from multiprocessing import Processimport osimport time def task1(n): print(&quot;儿子,PID:%s PPID:%s&quot; %(os.getpid(),os.getppid())) pp1=Process(target=task2,args=(10,)) pp2=Process(target=task2,args=(10,)) pp3=Process(target=task2,args=(10,)) pp1.start() pp2.start() pp3.start() time.sleep(n) def task2(n): print(&quot;孙子,PID:%s PPID:%s&quot; %(os.getpid(),os.getppid())) time.sleep(n) if __name__ == &quot;__main__&quot;: p=Process(target=task1,args=(10000,)) p.start() print(&quot;爸爸,PID: %s&quot; %os.getpid()) time.sleep(10000)[root@test01 test2]# cat dockerfileFROM centos:7ADD test.py /optCMD [&quot;python&quot;,&quot;/opt/test.py&quot;][root@test01 test2]# docker build -t t1:v1 ./[root@test01 test2]# docker run -d --init --name test1 t1:v1sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 08:52 ? 00:00:00 /sbin/docker-init -- /bin/sh -c python /opt/test.pyroot 6 1 0 08:52 ? 00:00:00 python /opt/test.pyroot 7 6 0 08:52 ? 00:00:00 python /opt/test.pyroot 8 7 0 08:52 ? 00:00:00 [python] &lt;defunct&gt;root 9 7 0 08:52 ? 00:00:00 [python] &lt;defunct&gt;root 10 7 0 08:52 ? 00:00:00 [python] &lt;defunct&gt;root 11 0 0 08:52 pts/0 00:00:00 shroot 20 11 0 08:52 pts/0 00:00:00 ps -efsh-4.2# kill -9 7sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 08:52 ? 00:00:00 /sbin/docker-init -- /bin/sh -c python /opt/test.pyroot 6 1 0 08:52 ? 00:00:00 python /opt/test.pyroot 7 6 0 08:52 ? 00:00:00 [python] &lt;defunct&gt;root 11 0 0 08:52 pts/0 00:00:00 shroot 21 11 0 08:53 pts/0 00:00:00 ps -ef# tini随着业务进程的退出而退出sh-4.2# kill -9 6sh-4.2# [root@test01 test2]# 我们知道如果僵尸进程的父进程没有回收僵尸进程的能力，而且也没死，那么tini是不会回收的该父进程下的僵尸进程的，所以僵尸进程能被 tini 回收的前提条件是，僵尸进程的父进程挂掉了，也就说明了tini并不能管理所有的僵尸进程。 那么上述所例（kill -9 6）中，我们想要 tini 回收6号进程下的僵尸进程，就必须把6号进程杀死，但是因为因为 docker-init 或者 tini 只会监听它的直接子进程（6号进程），一旦直接子进程退出，则 tini 的生命周期也会结束，因为 tini 会跟随最初创建的业务进程的退出而退出，那么整个容器就停止了，所以我们不能杀掉该直接子进程。 那么我们如何解决这个问题呢？这需要特定的进程层级结构。如下图，当僵尸进程是 entrypint进程的子进程（进程1）的子进程（进程2），也就是 tini 的曾孙子辈，此时可以杀掉进程 1，然后进程2会成为孤儿，被tini收养并回收，同时由于 entrypoint 进程仍在运行，容器不会停止。可见，只有在这种曾孙子辈出现僵尸进程的场景中，tini 才能真正发挥作用。因为可以杀掉 tini 的孙子，容器也不会结束 基于此，要让 tini 能够管理到孙子辈产生的僵尸进程，就需要自定义进程层级：不再依赖 docker 的 –init 参数，而是手动将 tini 命令放入容器，让 tini 作为最顶层进程，其直接子进程是我们自己编写的 entrypoint bash 脚本进程，不再说主程序进程，该 bash 进程再启动主程序进程，主程序进程之下再运行具体的子进程。这样，当主程序进程产生僵尸子进程时，我们可以杀掉主程序进程，让其下的僵尸子进程被 tini 收养并回收，同时 entrypoint 进程保持运行，确保容器不会停止。 1tini ---》entrypoint为我们自己编写的bash进程 ----》主程序进程 ----》子进程1 -》子进程2 基于上面例子修改如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@test01 test]# cd /test2[root@test01 test2]# lsdockerfile run.sh test.py tinitest.py内容不变[root@test01 test2]# cat run.sh#!/bin/bashpython /opt/test.py# 添加这一行是为了防止上面那一行进程被干掉后，本进程还有代码在运行着，否则就也结束了tail -f /dev/null [root@test01 test2]# cat dockerfileFROM centos:7ADD tini /tiniRUN chmod +x /tiniADD test.py /opt ADD run.sh /optENTRYPOINT [&quot;/tini&quot;,&quot;--&quot;]CMD sh /opt/run.sh[root@test01 test2]# docker build -t t2:v2 ./[root@test01 test2]# docker run -d --name test2 t2:v2[root@test01 test2]# docker exec -ti test2 shsh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 09:18 ? 00:00:00 sh /opt/run.shroot 7 1 0 09:18 ? 00:00:00 python /opt/test.pyroot 8 7 0 09:18 ? 00:00:00 python /opt/test.pyroot 9 8 0 09:18 ? 00:00:00 [python] &lt;defunct&gt;root 10 8 0 09:18 ? 00:00:00 [python] &lt;defunct&gt;root 11 8 0 09:18 ? 00:00:00 [python] &lt;defunct&gt;root 12 0 0 09:19 pts/0 00:00:00 shroot 19 12 0 09:19 pts/0 00:00:00 ps -efsh-4.2# kill -9 8sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 09:18 ? 00:00:00 sh /opt/run.shroot 7 1 0 09:18 ? 00:00:00 python /opt/test.pyroot 8 7 0 09:18 ? 00:00:00 [python] &lt;defunct&gt;root 12 0 0 09:19 pts/0 00:00:00 shroot 20 12 0 09:19 pts/0 00:00:00 ps -efsh-4.2# kill -9 7sh-4.2# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 09:18 ? 00:00:00 sh /opt/run.shroot 12 0 0 09:19 pts/0 00:00:00 shroot 21 1 0 09:20 ? 00:00:00 tail -f /dev/nullroot 22 12 0 09:20 pts/0 00:00:00 ps -ef然后我们还可以在该容器里启动python进程，只不过新启动的进程都变成了0号进程的儿子，0号进程同样可以回收僵尸儿子 3.3.3.3 tini信号转发保障平滑关闭tini默认只会把信号转发给它的儿子，无法转发给孙子辈以后，那么就会发强制杀死信号给孙子辈，所以如果容器内还存在孙子辈的进程，这就有问题了，我们肯定希望是转发给所有的进程，怎么解决呢？ 需要在dockerfile文件里定义环境变量： TINI_KILL_PROCESS_GROUP：向子进程组发送信号 TINI_KILL_PROCESS_GROUP设置为1，如此tini便会把收到的信号转发给容器内所有子进程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# 1、工作目录[root@test01 init5]# cd /root/init5/[root@test01 init5]# lsdockerfile test.py # 2、下列程序我们没有自己控制信号转发，后续在dockerfile文件中设置环境变量TINI_KILL_PROCESS_GROUP为1来控制tini把信号转发给组内所有进程[root@test01 init5]# cat test.py#coding:utf-8import signalimport timeimport osfrom multiprocessing import Process def logger(msg): with open(&quot;/log/test.log&quot;,mode=&quot;at&quot;) as f: f.write(&quot;%s\\n&quot; %msg) def handler(signal_num,obj): logger(&quot;进程%s收到了信号%s，正在清理资源...&quot; %(os.getpid(),signal_num)) logger(&quot;进程%s清理资源完毕，关闭...&quot; %os.getpid()) exit() def task(): signal.signal(signal.SIGTERM,handler) while True: logger(&quot;子进程%s运行中...&quot; %os.getpid()) time.sleep(3) if __name__ == &quot;__main__&quot;: p=Process(target=task) p.start() signal.signal(signal.SIGTERM,handler) while True: logger(&quot;主进程%s运行中...&quot; %os.getpid()) time.sleep(3) if __name__ == &quot;__main__&quot;: p=Process(target=task) p.start() son_pids=[p.pid] signal.signal(signal.SIGTERM,handler_for_main) while True: logger(&quot;主进程%s运行中...&quot; %os.getpid()) time.sleep(3) # 3、设置环境变量了：ENV TINI_KILL_PROCESS_GROUP 为1，你可以先不加该变量，实验一下，在 docker stop 容器的情况下，孙子进程是无法收到转发来的TERM信号的[root@test01 init5]# cat dockerfileFROM centos:7 ADD test.py /optENV TINI_KILL_PROCESS_GROUP 1CMD python /opt/test.py # 4、制作镜像docker build -t tinitest:v1.0 ./ # 5、图简单，我们直接用--init产生启动tini容器mkdir /logtouch /log/test.logdocker run -d --init -v /log:/log --name test1 tinitest:v1.0 # 6、在物理机打开一个新终端可以看到如下日志[root@test01 ~]# tail -f /log/test.log 主进程7运行中...子进程8运行中... # 7、查看容器内1号进程对应宿主的pid，并且strace追踪[root@test01 ~]# dockerid=$(docker container ls |grep egontest1 | awk &#x27;&#123;print $1&#125;&#x27;)[root@test01 ~]# docker inspect $dockerid |grep -i pid &quot;Pid&quot;: 1821, &quot;PidMode&quot;: &quot;&quot;, &quot;PidsLimit&quot;: null,[root@test01 init5]# strace -p 1821 # 8、开启一个新终端[root@test01 init5]# docker stop test1#可以看到strace -p 1821的输出如下：strace: Process 1821 attachedrestart_syscall(&lt;... resuming interrupted call ...&gt;) = 0rt_sigprocmask(SIG_SIGTERM, [], NULL, 8) = 0...#也可以看到日志中有相应的输出主进程7运行中...子进程8运行中...进程8收到了信号15，正在清理资源...进程8清理资源完毕，关闭...进程7收到了信号15，正在清理资源...进程7清理资源完毕，关闭... 4 在容器内-9、-19、-15对1号进程的影响在k8s，如果我们已发布的镜像有bug，以此启动了一个pod，想修该镜像里的bug，但是因为网络配置问题，又不想重建pod去改变pod IP，即我们只想重启pod内的1号进程让它重新加载配置，而不想重建pod，怎么办？k8s里是没有restart pod这种命令的，你可能会想到exec到pod内执行强制杀死1号进程，容器不就重启了吗？但是没有那么容易 针对容器内的1号进程，我们在容器内部使用kill命令对其发送终止信号 无法用 kill -9 杀死&#x2F; kill -19 暂停容器内的1号进程，在容器外则是可以正常杀死&#x2F;暂停，前提是你找到容器内1号进程映射到物理机的pid号 -15信号对1号进程来说可能有效，至于是否有效需要看1号进程是否注册了关于 -15 信号的handler。 原理看此 kill命令的执行流程 go程序中，默认就有对 -15 信号的handler shell脚本中，可以使用trap命令捕获信号 trap &quot;echo &#39;捕捉到信号TERM可以在此执行一系列的清理工作&#39; &amp;&amp; exit 0&quot; TERM python程序中，示例如下 1234567891011121314151617#coding:utf-8import signalimport time def handler(signal_num,obj): print(&quot;资源清理操作...&quot;) print(signal_num) print(obj) exit() # 清理完资源后不要忘记关闭 def task(): print(&quot;程序运行中...&quot;) time.sleep(10000000000) if __name__ == &quot;__main__&quot;: signal.signal(signal.SIGTERM,handler) task() 5 不可中断睡眠对容器的影响当我们cgroup没有限制容器的资源使用量，有时候我们发现容器内运行的应用程序运行速度很慢，那么我们就要看宿主机剩余可用资源量，情况如下时 cpu：利用率很低，平均负载很高 内存：很充足 问题分析：与平均负载的统计方式有关 平均负载 &#x3D; 正在运行的进程数 + 就绪的进程数（可运行队列） + 不可中断睡眠的进程数（休眠队列里） 基于把D状态的进程也认为是活跃进程统计到平均负载里的这种统计方式 有可能会出现系统中：正在运行的进程数 + 就绪的进程数都很少 但是不可中断睡眠数很多，造成cpu与内存都很充足，但是平均负载很高 给我们的直观感受就是大量的进程都在进行IO操作，速度很慢 此时即便你把容器的cpu资源、内存资源都调高也没有用，因为问题出在D状态、出在IO上 当系统 load average 比较高时，首先我们需要去甄别，到底是 CPU 的问题还是 IO 的问题。有4颗cpu，满负载任务数就为4，超了就要分析原因了，看看是否是D状态带来的影响。因为当D状态过多，就可能程序出一种效果：所有进程的cpu占用率非常低，但是cpu的1分钟5分钟15分钟负载非常高，这个会影响k8s的调度 当进程处于D状态，一定是还没有拿到想要的资源，对于应用程序的用户来说看到的直观效果就是程序速度变慢，一旦出现这种情况，我们用cgroups是无法解决的，因为cgroups更多是以进程为单位进行隔离，而D状态的进程是内核中系统全局资源有引入的，所以cgroups影响不了它。 所以当我们遇到因为D状态而导致进程、如容器进程效率变慢，可以采用的解决方案就是监控宿主机D状态的数量，然后对那些D状态数量过多的节点做分析，如果磁盘硬件问题引起的D状态进程数量过多，那么就更新磁盘","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"harbor","slug":"Container/mirror/harbor","date":"2025-09-10T03:49:17.000Z","updated":"2025-09-10T03:49:17.611Z","comments":true,"path":"Container/mirror/harbor/","permalink":"https://aquapluto.github.io/Container/mirror/harbor/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"build","slug":"Container/mirror/build","date":"2025-09-10T03:49:11.000Z","updated":"2025-09-10T03:49:11.622Z","comments":true,"path":"Container/mirror/build/","permalink":"https://aquapluto.github.io/Container/mirror/build/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"dockerfile","slug":"Container/mirror/dockerfile","date":"2025-09-10T03:49:03.000Z","updated":"2025-09-10T03:49:03.400Z","comments":true,"path":"Container/mirror/dockerfile/","permalink":"https://aquapluto.github.io/Container/mirror/dockerfile/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"conspect","slug":"Container/mirror/conspect","date":"2025-09-10T03:48:51.000Z","updated":"2025-09-10T03:48:51.723Z","comments":true,"path":"Container/mirror/conspect/","permalink":"https://aquapluto.github.io/Container/mirror/conspect/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Docker-Compose","slug":"Container/docker/compose","date":"2025-09-10T03:48:38.000Z","updated":"2025-09-10T12:57:10.450Z","comments":true,"path":"Container/docker/compose/","permalink":"https://aquapluto.github.io/Container/docker/compose/","excerpt":"","text":"1 Docker Compose概念1.1 介绍当在宿主机启动较多容器的时候，如果都是手动操作会觉得比较麻烦而且容易出错，此时推荐使用 docker 单机编排工具 docker-compose。它是 docker 容器的一种单机编排服务，负责管理多个容器。比如可以解决容器之间的依赖关系，就像启动一个nginx前端服务的时候会调用后端的tomcat，那就得先启动tomcat，但是启动tomcat 容器还需要依赖数据库，那就还得先启动数据库，docker-compose 可以用来解决这样的嵌套依赖关系，并且可以替代docker命令对容器进行创建、启动和停止等手工的操作 因此如果说docker命令就像linux的命令，那么docker compose就像shell脚本，或者说docker命令相当于ansible命令，那么docker compose文件就相当于ansible-playbook的yaml文件，可以自动的执行容器批量操作，从而实现自动化的容器管理 github地址: https://github.com/docker/compose 官方地址: https://docs.docker.com/compose/ 1.2 文件格式官方文档: https://docs.docker.com/compose/compose-file/ docker compose 文件是一个 yaml 格式的文件，所以注意行首的缩进很严格 默认 docker-compose 命令会调用当前目录下的 docker-compose.yml 的文件，因此一般执行docker-compose命令前先进入docker-compose.yml文件所在目录 1.3 一键生成Docker Compose利用该网站可以将d ocker 命令自动生成 Docker Compse 1https://www.composerize.com/ 2 安装Docker Compose2.1 在线安装，通过 pip安装python-pip 包将安装一个 pip 的命令，pip 命令是一个python 安装包的安装工具，其类似于ubuntu 的 apt 或者 redhat 的 yum，但是pip 只安装 python 相关的安装包，可以在多种操作系统安装和使用pip 此方式当前安装的版本较新，为docker_compose-1.25.3，推荐使用 12345678Ubuntu: # apt update# apt install -y python-pipCentOS: # yum install epel-release# yum install -y python-pip# pip install --upgrade pip 范例: 基于python3 安装 docker-compose 1234567891011121314151617181920#配置加速[root@ubuntu2004 ~]#mkdir ~/.pip[root@ubuntu2004 ~]#cat &gt; ~/.pip/pip.conf &lt;&lt;-EOF[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simpleEOF[root@ubuntu2004 ~]#apt -y install python3-pip[root@ubuntu2004 ~]#pip3 install --upgrade pip[root@ubuntu2004 ~]#pip3 install docker-compose[root@ubuntu2004 ~]#docker-compose --versiondocker-compose version 1.27.4, build unknown#基于python2安装docker-compose[root@ubuntu1804 ~]#apt -y install python-pip[root@ubuntu1804 ~]#pip install docker-compose[root@ubuntu1804 ~]#docker-compose --versiondocker-compose version 1.25.3, build unknown 2.2 在线直接从包仓库安装此方法安装的版本较旧，不推荐使用 12345678910111213#ubuntu安装,此为默认版本[root@ubuntu2204 ~]#apt list docker-compose正在列表... 完成docker-compose/jammy 1.29.2-1 all[root@ubuntu1804 ~]#apt -y install docker-compose[root@ubuntu1804 ~]#docker-compose --versiondocker-compose version 1.17.1, build unknown#CentOS7安装，依赖EPEL源[root@centos7 ~]#yum -y install docker-compose[root@centos7 ~]#docker-compose --versiondocker-compose version 1.18.0, buil 8dd22a9 2.3 离线安装，直接从github或国内镜像站下载安装对应版本参看说明: https://github.com/docker/compose/releases 此方法安装版本可方便指定，推荐方法，但网络下载较慢 123456[root@ubuntu1804 ~]#curl -L https://github.com/docker/compose/releases/download/1.25.3/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose#从国内镜像站下载[root@ubuntu1804 ~]#curl -L https://get.daocloud.io/docker/compose/releases/download/1.25.3/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose[root@ubuntu1804 ~]#chmod +x /usr/local/bin/docker-compose 3 docker-compose命令官方文档: https://docs.docker.com/compose/reference/ 1234567891011121314151617181920212223242526272829303132333435363738394041docker-compose --help使用 Docker 定义和运行多容器应用程序Usage:docker-compose [-f &lt;arg&gt;...] [options] [COMMAND] [ARGS...]docker-compose -h|--help#选项说明: -f，–file FILE #指定Compose 模板文件，默认为docker-compose.yml-p，–project-name NAME #指定项目名称，默认将使用当前所在目录名称作为项目名。--verbose #显示更多输出信息--log-level LEVEL #定义日志级别 (DEBUG, INFO, WARNING, ERROR, CRITICAL)--no-ansi #不显示ANSI 控制字符-v, --version #显示版本#以下为命令选项，需要在docker-compose.yml|yaml 文件所在在目录里执行config -q #查看当前配置，没有错误不输出任何信息up #创建并启动容器build #构建镜像bundle #从当前docker compose 文件生成一个以&lt;当前目录&gt;为名称的json格式的Docker Bundle 备份文件create #创建服务down #停止和删除所有容器、网络、镜像和卷events #从容器接收实时事件，可以指定json 日志格式exec #进入指定容器进行操作help #显示帮助细信息images #显示镜像信息kill #强制终止运行中的容器logs #查看容器的日志pause #暂停服务port #查看端口ps #列出容器pull #重新拉取镜像，镜像发生变化后，需要重新拉取镜像push #上传镜像restart #重启服务rm #删除已经停止的服务run #一次性运行容器scale #设置指定服务运行的容器个数，新版已废弃start #启动服务stop #停止服务top #显示容器运行状态unpause #取消暂定 4 使用docker compose启动单个容器注意: 使用Docker compose之前，先要安装docker 4.1 创建docker compose文件docker compose 文件可在任意目录，创建文件名为docker-compose.yml 配置文件，要注意前后的缩进 123456789101112131415[root@ubuntu1804 ~]#docker-compose --versiondocker-compose version 1.25.4, build unknown[root@ubuntu1804 ~]#mkdir /data/docker-compose[root@ubuntu1804 ~]#cd /data/docker-compose[root@ubuntu1804 docker-compose]#vim docker-compose.ymlservice-nginx-web: image: harbor.wang.org/example/nginx-centos7-base:1.6.1 container_name: nginx-web expose: - 80 - 443 ports: - &quot;80:80&quot; - &quot;443:443&quot; 4.2 查看配置和格式检查123456[root@ubuntu1804 docker-compose]#docker-compose config[root@ubuntu1804 docker-compose]#docker-compose config -q#如果有错误[root@ubuntu1804 docker-compose]#docker-compose config -qERROR: yaml.scanner.ScannerError: mapping values are not allowed here in &quot;./docker-compose.yml&quot;, line 2, column 8 4.3 启动容器注意: 必须要在docker compose文件所在的目录执行 1234567891011#前台启动[root@ubuntu1804 docker-compose]#docker-compose up[root@ubuntu1804 docker-compose]#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --insecure-registry harbor.wang.org[root@ubuntu1804 docker-compose]#systemctl daemon-reload[root@ubuntu1804 docker-compose]#systemctl restart docker[root@ubuntu1804 docker-compose]#docker-compose up#以上是前台执行不退出 4.4 验证docker compose执行结果1234567891011121314151617181920212223242526272829#上面命令是前台执行，所以要查看结果，可以再开一个终端窗口进行观察[root@ubuntu1804 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd71030504f6a harbor.wang.org/example/nginx-centos7-base:1.6.1 &quot;/apps/nginx/sbin/ng…&quot; 15 seconds ago Up 13 seconds 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp nginx-web[root@ubuntu1804 ~]#docker-compose psERROR: Can&#x27;t find a suitable configuration file in this directory or any parent. Are you in the right directory? Supported filenames: docker-compose.yml, docker-compose.yaml [root@ubuntu1804 ~]#cd /data/docker-compose/[root@ubuntu1804 docker-compose]#docker-compose psName Command State Ports -------------------------------------------------------------------------------------nginx-web /apps/nginx/sbin/nginx Up 0.0.0.0:443-&gt;443/tcp, 0.0.0.0:80-&gt;80/tcp[root@ubuntu1804 docker-compose]#curl 127.0.0.1/app/Test Page in app[root@ubuntu1804 docker-compose]#docker-compose imagesContainer Repository Tag Image Id Size-----------------------------------------------------------------------------------nginx-web harbor.wang.org/example/nginx-centos7-base 1.6.1 ea3840c349e5 413.4 MB[root@ubuntu1804 docker-compose]#docker-compose exec service-nginx-web bash[root@17c17ad30193 /]# tail -f /apps/nginx/logs/access.log172.17.0.1 - - [04/Feb/2020:16:01:42 +0800] &quot;GET /app/ HTTP/1.1&quot; 200 17 &quot;-&quot;&quot;curl/7.58.0&quot;10.0.0.101 - - [04/Feb/2020:16:06:29 +0800] &quot;GET /app/ HTTP/1.1&quot; 200 17 &quot;-&quot;&quot;curl/7.58.0&quot;10.0.0.102 - - [04/Feb/2020:16:08:22 +0800] &quot;GET /app/ HTTP/1.1&quot; 200 17 &quot;-&quot;&quot;curl/7.58.0&quot; 4.5 结束前台执行12345[root@ubuntu1804 docker-compose]#docker-compose up.... #ctrl+c键，结束容器#关闭容器[root@ubuntu1804 docker-compose]#docker-compose kill 4.6 删除容器12345678#只删除停止的容器[root@ubuntu1804 docker-compose]#docker-compose rm#停止并删除容器及镜像[root@ubuntu1804 docker-compose]#docker-compose down#也会自动删除镜像[root@ubuntu1804 docker-compose]#docker-compose images 4.7 后台执行123456[root@ubuntu1804 docker-compose]#docker-compose up -d[root@ubuntu1804 docker-compose]#curl 127.0.0.1/app/Test Page in app[root@ubuntu1804 docker-compose]#curl http://127.0.0.1/app/Test Page in app 4.8 停止和启动与日志查看1234567891011121314[root@ubuntu1804 docker-compose]#docker-compose stop[root@ubuntu1804 docker-compose]#docker-compose ps[root@ubuntu1804 docker-compose]#docker-compose start[root@ubuntu1804 docker-compose]#docker-compose ps[root@ubuntu1804 docker-compose]#docker-compose restart[root@ubuntu1804 docker-compose]#docker-compose ps#执行上面操作时，可以同时开一个终端，观察日事件[root@ubuntu1804 docker-compose]#docker-compose events#以json格式显示日志[root@ubuntu1804 docker-compose]#docker-compose events --json 4.9 暂停和恢复12345678[root@ubuntu1804 docker-compose]#docker-compose pause[root@ubuntu1804 docker-compose]#curl -m 1 http://127.0.0.1/app/curl: (28) Operation timed out after 1002 milliseconds with 0 bytes received[root@ubuntu1804 docker-compose]#docker-compose unpause[root@ubuntu1804 docker-compose]#curl -m 1 http://127.0.0.1/app/Test Page in app 5 使用docker compose启动多个容器5.1 编辑docker-compose文件并使用数据卷注意: 同一个文件 ，数据卷的优先级比镜像内的文件优先级高 1234567891011121314151617181920212223242526272829303132[root@ubuntu1804 docker-compose]#vim docker-compose.ymlservice-nginx-web: image: 10.0.0.102/example/nginx-centos7-base:1.6.1 container_name: nginx-web volumes: - /data/nginx:/apps/nginx/html/ #指定数据卷，将宿主机/data/nginx挂载到容器/apps/nginx/html expose: - 80 - 443 ports: - &quot;80:80&quot; - &quot;443:443&quot; service-tomcat-app1: image: 10.0.0.102/example/tomcat-web:app1 container_name: tomcat-app1 expose: - 8080 ports: - &quot;8081:8080&quot; service-tomcat-app2: image: 10.0.0.102/example/tomcat-web:app2 container_name: tomcat-app2 expose: - 8080 ports: - &quot;8082:8080&quot; #在宿主机准备nginx测试页面文件[root@ubuntu1804 docker-compose]#mkdir /data/nginx[root@ubuntu1804 docker-compose]#echo Docker compose test page &gt; /data/nginx/index.html 5.2 启动容器并验证结果12345678[root@ubuntu1804 docker-compose]#docker-compose up -d[root@ubuntu1804 docker-compose]#curl http://127.0.0.1/Docker compose test page[root@ubuntu1804 docker-compose]#curl http://127.0.0.1:8081/app/Tomcat Page in app1[root@ubuntu1804 docker-compose]#curl http://127.0.0.1:8082/app/Tomcat Page in app2 5.3 指定同时启动容器的数量12345678910111213141516171819[root@ubuntu1804 docker-compose]#vim docker-compose.ymlservice-nginx-web: image: 10.0.0.102/example/nginx-centos7-base:1.6.1 # container_name: nginx-web #同时启动多个同一镜像的容器，不要指定容器名称，否则会冲突 expose: - 80 - 443 # ports: #同时启动多个同一镜像的容器，不要指定端口号，否则会冲突 # - &quot;80:80&quot; # - &quot;443:443&quot; service-tomcat: #再加一个service image: 10.0.0.102/example/tomcat-base:v8.5.50 [root@ubuntu1804 docker-compose]#docker-compose up -d --scale service-nginx-web=2[root@ubuntu1804 docker-compose]#docker-compose up -d --scale service-nginx-web=3 --scale service-tomcat=2[root@ubuntu1804 docker-compose]#docker-compose up -d 5.4 扩容和缩容注意: 新版中 scale 命令已废弃 12345#扩容[root@ubuntu1804 docker-compose]#docker-compose scale service-nginx-web=3#缩容为0，即删除容器[root@ubuntu1804 docker-compose]#docker-compose scale service-nginx-web=0 6 实现Wordpress应用12345678910111213141516171819202122232425262728293031323334353637383940414243444546version: &#x27;3&#x27;services: db: image: mysql:8.0.29-oracle container_name: db restart: unless-stopped environment: - MYSQL_DATABASE=wordpress - MYSQL_ROOT_PASSWORD=123456 - MYSQL_USER=wordpress - MYSQL_PASSWORD=123456 volumes: - dbdata:/var/lib/mysql networks: - wordpress-network wordpress: depends_on: - db #image: wordpress:5.8.3-apache image: wordpress:php7.4-apache container_name: wordpress restart: unless-stopped ports: - &quot;80:80&quot; environment: - WORDPRESS_DB_HOST=db:3306 - WORDPRESS_DB_USER=wordpress - WORDPRESS_DB_PASSWORD=123456 - WORDPRESS_DB_NAME=wordpress volumes: - wordpress:/var/www/html networks: - wordpress-networkvolumes: wordpress: dbdata:networks: wordpress-network: driver: bridge ipam: config: - subnet: 172.30.0.0/16","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"网络管理","slug":"Container/docker/network","date":"2025-09-10T03:48:32.000Z","updated":"2025-09-13T13:50:36.224Z","comments":true,"path":"Container/docker/network/","permalink":"https://aquapluto.github.io/Container/docker/network/","excerpt":"","text":"1 Docker默认的网络通信docker容器创建后，通常需要和其它主机或容器进行网络通信 官方文档：https://docs.docker.com/network/ 1.1 Docker安装后默认的网络设置Docker服务安装完成之后，默认在每个宿主机会生成一个名称为docker0的网卡，其IP地址都是172.17.0.1&#x2F;16 1234567891011121314151617181920212223[root@ubuntu2004 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:28:40:00 brd ff:ff:ff:ff:ff:ff inet 10.0.0.183/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe28:4000/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:13:93:5f:04 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:13ff:fe93:5f04/64 scope link valid_lft forever preferred_lft forever[root@ubuntu2004 ~]#brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024213935f04 no 1.2 创建容器后的网络配置每次新建容器后 宿主机多了一个虚拟网卡，和容器的网卡组合成一个网卡，比如: 113: veth8ca6d43@if112，而在容器内的网卡名为112，可以看出和宿主机的网卡之间的关联 容器会自动获取一个 172.17.0.0&#x2F;16 网段的随机地址，默认从 172.17.0.2 开始分配给第1个容器使用，第2个容器为172.17.0.3，以此类推 容器获取的地址并不固定，每次容器重启，可能会发生地址变化 当容器之间需要相互通信时，它们可以通过docker0桥接网络进行通信；当容器需要访问外部网络时，数据包会经过SNAT过程，使得外部网络看到的数据包源地址是宿主机的IP地址，而不是容器内部的IP地址。 evth-pair技术 evth-pair 就是一对的虚拟设备接口，它们都是成对出现，一段彼此连接。 正是因为这个特性，连接各种各样网络设备。 1.2.1 创建第一个容器后的网络状态范例: 创建容器，容器自动获取IP地址 1234567891011121314151617181920212223[root@ubuntu2004 ~]#docker run -it --rm alpine:3.18.0 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever130: eth0@if131: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever / # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 136d6155aff3 范例: 新建第一个容器，宿主机的网卡多了一个新网卡 12345678910111213141516171819202122232425[root@ubuntu2004 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:28:40:00 brd ff:ff:ff:ff:ff:ff inet 10.0.0.183/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe28:4000/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:13:93:5f:04 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:13ff:fe93:5f04/64 scope link valid_lft forever preferred_lft forever131: veth0a50f0f@if130: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 46:77:84:5b:b1:5e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::4477:84ff:fe5b:b15e/64 scope link valid_lft forever preferred_lft forever # 130: eth0@if131 和 131: veth0a50f0f@if130 一一对应 范例: 查看新建容器后桥接状态 123[root@ubuntu2004 ~]#brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024213935f04 no veth0a50f0f 1.2.2 创建第二个容器后面的网络状态范例: 再次创建第二个容器 1234567891011121314151617181920212223242526272829303132[root@ubuntu2004 ~]#docker run -it --rm alpine:3.18.0 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever132: eth0@if133: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.3 ca8b9979c12f/ # ping ca8b9979c12fPING ca8b9979c12f (172.17.0.3): 56 data bytes64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.220 ms64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.064 ms[root@ubuntu2004 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESca8b9979c12f alpine:3.18.0 &quot;sh&quot; 47 seconds ago Up 46 seconds gracious_kapitsa136d6155aff3 alpine:3.18.0 &quot;sh&quot; 3 minutes ago Up 3 minutes competent_golick 范例: 新建第二个容器后宿主机又多了一个虚拟网卡 123456789[root@ubuntu2004 ~]#ip a131: veth0a50f0f@if130: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 46:77:84:5b:b1:5e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::4477:84ff:fe5b:b15e/64 scope link valid_lft forever preferred_lft forever133: veth485fe04@if132: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 62:bb:2f:67:47:f3 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::60bb:2fff:fe67:47f3/64 scope link valid_lft forever preferred_lft forever 范例: 查看新建第二个容器后桥接状态 1234[root@ubuntu2004 ~]#brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024213935f04 no veth0a50f0f veth485fe04 1.3 容器间的通信1.3.1 同一个宿主机的不同容器可相互通信默认情况下 同一个宿主机的不同容器之间可以相互通信 12dockerd --icc #启用容器间通信（默认为 true）--icc=false #此配置可以禁止同一个宿主机的容器之间通信 不同宿主机之间的容器IP地址重复，默认不能相互通信 范例: 同一个宿主机的容器之间访问 1234567891011121314151617181920212223242526272829303132[root@ubuntu2004 ~]#docker run -it --rm alpine:3.18.0 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever124: eth0@if125: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:05 brd ff:ff:ff:ff:ff:ff inet 172.17.0.5/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:5/64 scope link valid_lft forever preferred_lft forever / # ping 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.328 ms64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.220 ms^C--- 172.17.0.2 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.220/0.274/0.328 ms/ # ping 172.17.0.3PING 172.17.0.3 (172.17.0.3): 56 data bytes64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.165 ms64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.084 ms^C--- 172.17.0.3 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.084/0.124/0.165 ms 1.3.2 禁止同一个宿主机的不同容器间通信范例: 同一个宿主机不同容器间禁止通信 12345678910111213141516#dockerd 的 --icc=false 选项可以禁止同一个宿主机的不同容器间通信[root@ubuntu1804 ~]#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --icc=false[root@ubuntu1804 ~]#systemctl daemon-reload[root@ubuntu1804 ~]#systemctl restart docker#创建两个容器,测试无法通信[root@ubuntu1804 ~]#docker run -it --name test1 --rm alpine sh/ # hostname -i172.17.0.2[root@ubuntu1804 ~]#docker run -it --name test2 --rm alpine sh/ # hostname -i172.17.0.3/ # ping 172.17.0.2 范例: 在第二个宿主机上创建容器，跨宿主机的容器之间默认不能通信 12345678910111213141516171819202122232425[root@ubuntu1804 ~]#docker pull alpine[root@ubuntu1804 ~]#ip a3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:1d:73:8b:71 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever [root@ubuntu1804 ~]#docker run -it --rm alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever4: eth0@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueuestate UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever / # ping -c 1 172.17.0.3PING 172.17.0.3 (172.17.0.3): 56 data bytes--- 172.17.0.3 ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss 1.4 修改默认docker0网桥的网络配置默认docker后会自动生成一个docker0的网桥，使用的IP是 172.17.0.1&#x2F;16，可能和宿主机的网段发生冲突，可以将其修改为其它网段的地址，避免冲突 范例: 将docker0的IP修改为指定IP 12345678910111213141516#方法1[root@ubuntu1804 ~]#vim /etc/docker/daemon.json&#123; &quot;bip&quot;: &quot;192.168.100.1/24&quot;, &quot;registry-mirrors&quot;: [&quot;https://si7y70hh.mirror.aliyuncs.com&quot;]&#125;[root@ubuntu1804 ~]#systemctl restart docker.service#方法2[root@ubuntu1804 ~]#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --bip=192.168.100.1/24[root@ubuntu1804 ~]#systemctl daemon-reload[root@ubuntu1804 ~]#systemctl restart docker.service#注意两种方法不可混用,否则将无法启动docker服务#注意如果要改回去，只能在配置文件将IP改回去，只是把文件删除或者将修改IP那一行删除是无效的 1.5 修改默认网络设置使用自定义网桥新建容器默认使用docker0的网络配置，可以修改默认指向自定义的网桥网络 范例: 用自定义的网桥代替默认的docker0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#查看默认网络[root@ubuntu1804 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:40:27:06 brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe40:2706/64 scope link valid_lft forever preferred_lft forever5: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:35:ba:e7:ce brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:35ff:feba:e7ce/64 scope link valid_lft forever preferred_lft forever [root@ubuntu1804 ~]#apt -y install bridge-utils[root@ubuntu1804 ~]#brctl addbr br0[root@ubuntu1804 ~]#ip a a 192.168.100.1/24 dev br0[root@ubuntu1804 ~]#brctl showbridge name bridge id STP enabled interfacesbr0 8000.000000000000 nodocker0 8000.024235bae7ce no[root@ubuntu1804 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:40:27:06 brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe40:2706/64 scope link valid_lft forever preferred_lft forever5: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:35:ba:e7:ce brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:35ff:feba:e7ce/64 scope link valid_lft forever preferred_lft forever18: br0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWNgroup default qlen 1000 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff inet 192.168.100.1/24 scope global br0 valid_lft forever preferred_lft forever inet6 fe80::9cf2:e0ff:fe4e:96b3/64 scope link valid_lft forever preferred_lft forever [root@ubuntu1804 ~]#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock -b br0[root@ubuntu1804 ~]#systemctl daemon-reload[root@ubuntu1804 ~]#systemctl restart docker[root@ubuntu1804 ~]#docker run -it alpine sh/ #hostname -i192.168.100.2/ # ping www.baidu.comPING www.baidu.com (220.181.38.150): 56 data bytes64 bytes from 220.181.38.150: seq=0 ttl=127 time=4.028 ms 2 容器名称互连新建容器时，docker会自动分配容器名称，容器ID和IP地址，导致容器名称，容器ID和IP都不固定，那么如何区分不同的容器，实现和确定目标容器的通信呢？解决方案是给容器起个固定的名称，容器之间通过固定名称实现确定目标的通信 有两种固定名称: 容器名称 容器名称的别名 注意: 两种方式都最少需要两个容器才能实现 2.1 通过容器名称互连2.1.1 容器名称介绍即在同一个宿主机上的容器之间可以通过自定义的容器名称相互访问，比如: 一个业务前端静态页面是使用nginx，动态页面使用的是tomcat，另外还需要负载均衡调度器，如: haproxy 对请求调度至nginx和tomcat的容器，由于容器在启动的时候其内部IP地址是DHCP 随机分配的，而给容器起个固定的名称，则是相对比较固定的，因此比较适用于此场景 注意: 如果被引用的容器地址变化，必须重启当前容器才能生效 2.1.2 容器名称实现docker run 创建容器，可使用--link选项实现容器名称的引用，其本质就是在容器内的/etc/hosts中添加--link后指定的容器的IP和主机名的对应关系，从而实现名称解析 12345--link list #Add link to another container格式: docker run --name &lt;容器名称&gt; #先创建指定名称的容器docker run --link &lt;目标通信的容器ID或容器名称&gt; #再创建容器时引用上面容器的名称 2.1.3 实战案例1: 使用容器名称进行容器间通信1、先创建第一个指定容器名称的容器 12345678910111213141516171819202122232425262728293031323334353637383940[root@ubuntu1804 ~]#docker run -it --name server1 --rm alpine:3.11 sh/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 cdb5173003f5/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever150: eth0@if151: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueuestate UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever / # ping 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.038 ms^C--- 172.17.0.2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.038/0.038/0.038 ms/ # ping server1ping: bad address &#x27;server1&#x27;/ # ping cdb5173003f5PING cdb5173003f5 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.040 ms64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.128 ms^C--- cdb5173003f5 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.040/0.084/0.128 ms 2、新建第二个容器时引用第一个容器的名称 会自动将第一个主机的名称加入&#x2F;etc&#x2F;hosts文件，从而可以利用第一个容器名称进行访问 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@ubuntu1804 ~]#docker run -it --rm --name server2 --link server1 alpine:3.11 sh/ # envHOSTNAME=395d8c3392eeSHLVL=1HOME=/rootTERM=xtermSERVER1_NAME=/server2/server1PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binPWD=// # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 server1 cdb5173003f5172.17.0.3 7ca466320980/ # ping server1PING server1 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.111 ms^C--- server1 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.111/0.111/0.111 ms/ # ping server2ping: bad address &#x27;server2&#x27;/ # ping 7ca466320980PING 7ca466320980 (172.17.0.3): 56 data bytes64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.116 ms64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.069 ms^C--- 7ca466320980 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.069/0.092/0.116 ms/ # ping cdb5173003f5 PING cdb5173003f5 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.072 ms64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.184 ms^C--- cdb5173003f5 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.072/0.128/0.184 ms 2.1.4 实战案例2: 实现busybox和alpine两个容器互连1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071[root@ubuntu2004 ~]#docker run -it --name server busybox sh/ # echo busybox website &gt; index.html/ # httpd --helpBusyBox v1.36.1 (2023-05-18 22:34:17 UTC) multi-call binary.Usage: httpd [-ifv[v]] [-c CONFFILE] [-p [IP:]PORT] [-u USER[:GRP]] [-r REALM] [-h HOME]or httpd -d/-e/-m STRINGListen for incoming HTTP requests -i Inetd mode -f Run in foreground -v[v] Verbose -p [IP:]PORT Bind to IP:PORT (default *:80) -u USER[:GRP] Set uid/gid after binding to port -r REALM Authentication Realm for Basic Authentication -h HOME Home directory (default .) -c FILE Configuration file (default &#123;/etc,HOME&#125;/httpd.conf) -m STRING MD5 crypt STRING -e STRING HTML encode STRING -d STRING URL decode STRING / # httpd -v -f -h /[::ffff:172.17.0.3]:41606: response:200[root@ubuntu2004 ~]#docker run -it --link server --name client alpine:3.18.0 sh/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 server 4fedff573361172.17.0.3 f9ce4ad9039b/ # ping serverPING server (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.200 ms^C--- server ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.200/0.200/0.200 ms/ # wget -qO - serverbusybox #现在将busybox停了/ # exit#跑另一个容器将原来busybox的ip占用[root@ubuntu2004 ~]#docker run -d busybox sleep 100#启动busybox[root@ubuntu2004 ~]#docker start server[root@ubuntu2004 ~]#docker inspect server&quot;IPAddress&quot;: &quot;172.17.0.4&quot;,#alpine的hosts文件也会自动寻回/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.4 server 6301ef5fc006172.17.0.3 7dca1c97784f#注意：如果是将原来的busybox删除，再重新跑是不能寻回的，解决方法可以通过自定义容器别名互连 2.1.5 实战案例3: 实现 wordpress 和 MySQL 两个容器互连1234567891011121314151617181920212223242526272829[root@centos7 ~]#tree lamp_docker/lamp_docker/├── env_mysql.list├── env_wordpress.list└── mysql └── mysql_test.cnf1 directory, 3 files[root@centos7 ~]#cat lamp_docker/env_mysql.listMYSQL_ROOT_PASSWORD=123456MYSQL_DATABASE=wordpressMYSQL_USER=wpuserMYSQL_PASSWORD=wppass[root@centos7 ~]#cat lamp_docker/env_wordpress.listWORDPRESS_DB_HOST=mysql:3306WORDPRESS_DB_NAME=wordpressWORDPRESS_DB_USER=wpuserWORDPRESS_DB_PASSWORD=wppassWORDPRESS_TABLE_PREFIX=wp_[root@centos7 ~]#cat lamp_docker/mysql/mysql_test.cnf[mysqld]server-id=100log-bin=mysql-bin[root@centos7 ~]#docker run --name mysql -v /root/lamp_docker/mysql/:/etc/mysql/conf.d -v /data/mysql:/var/lib/mysql --env-file=/root/lamp_docker/env_mysql.list -d -p 3306:3306 mysql:8:0[root@centos7 ~]#docker run -d --name wordpress --link mysql -v /data/wordpress:/var/www/html/wp-content --env-file=/root/lamp_docker/env_wordpress.list -p 80:80 wordpress:php7.4-apache 2.2 通过自定义容器别名互连2.2.1 容器别名介绍自定义的容器名称可能后期会发生变化，那么一旦名称发生变化，容器内程序之间也必须要随之发生变化，比如：程序通过固定的容器名称进行服务调用，但是容器名称发生变化之后再使用之前的名称肯定是无法成功调用，每次都进行更改的话又比较麻烦，因此可以使用自定义别名的方式解决，即容器名称可以随意更改，只要不更改别名即可 2.2.2 容器别名实现12345docker run --name &lt;容器名称&gt;#先创建指定名称的容器docker run --name &lt;容器名称&gt; --link &lt;目标容器名称&gt;:&quot;&lt;容器别名1&gt; &lt;容器别名2&gt; ...&quot;#给上面创建的容器起别名,来创建新容器 2.2.3 实战案例1: 使用容器别名范例: 创建第三个容器，引用前面创建的容器，并起别名 1234567891011121314151617181920212223242526272829303132333435[root@ubuntu1804 ~]#docker run -it --rm --name server3 --link server1:server1-alias alpine:3.11 sh/ # envHOSTNAME=395d8c3392eeSHLVL=1HOME=/rootTERM=xtermSERVER1-ALIAS_NAME=/server3/server1-aliasPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binPWD=// # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 server1-alias cdb5173003f5 server1172.17.0.4 d9622c6831f7/ # ping server1PING server1 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.101 ms^C--- server1 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.101/0.101/0.101 ms/ # ping server1-aliasPING server1-alias (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.073 ms^C--- server1-alias ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.073/0.073/0.073 ms 范例: 创建第四个容器，引用前面创建的容器，并起多个别名 12345678910111213141516171819202122232425262728293031323334[root@ubuntu1804 ~]#docker run -it --rm --name server4 --link server1:&quot;server1-alias1 server1-alias2&quot; alpine:3.11 sh/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 server1-alias1 server1-alias2 cdb5173003f5 server1172.17.0.5 db3d2f084c05/ # ping server1PING server1 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.107 ms^C--- server1 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.107/0.107/0.107 ms/ # ping server1-alias1PING server1-alias1 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.126 ms^C--- server1-alias1 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.126/0.126/0.126 ms/ # ping server1-alias2PING server1-alias2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.124 ms^C--- server1-alias2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.124/0.124/0.124 ms 2.2.4 实战案例2: 使用容器别名实现 wordpress 和 MySQL 两个容器互连123[root@centos7 ~]#docker run --name mysql -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d --restart=always mysql:8.0.29-oracle[root@centos7 ~]#docker run -d -p 80:80 --name wordpress --link mysql:&quot;mysql1 mysql2 mysql3&quot; --restart=always wordpress:php7.4-apache 3 网络模式docker的网络模式分为两大类 单机网络：指的是同一台宿主机上的多个容器的网络是如何组织，如何实现通信的 跨主机网络：指的是在单机网络的基础上，考虑分散于不同宿主机的多个容器，如何跨越宿主机进行通信 Docker的网络支持5种网络模式 none bridge container host network-name 范例: 查看默认的网络模式有三个 12345[root@ubuntu2004 ~]#docker network lsNETWORK ID NAME DRIVER SCOPEc29cd0fe5378 bridge bridge localf059ccc72b49 host host localcab6c739b857 none null local 默认新建的容器使用Bridge模式，创建容器时，docker run 命令使用以下选项指定网络模式 123456789docker run --network &lt;mode&gt;docker run --net=&lt;mode&gt;&lt;mode&gt;: 可是以下值nonebridgehostcontainer:&lt;容器名或容器ID&gt;&lt;自定义网络名称&gt; 3.1 Bridge模式3.1.1 Bridge 网络模式架构 原理：有一个网桥设备，默认叫docker0，本质上是一台虚拟机的二层交换机，每创建一个容器，都会生成一个veth对，veth对就好像一根网线，网线一端接到容器内，被命名为eth0，另外一端接到宿主机的docker0网桥上，名为vethxxx。容器的网关就是网桥的地址 本模式是docker的默认模式，即不指定任何模式就是bridge模式，也是使用比较多的模式，相当于VM的NAT模式，此模式创建的容器会为每一个容器分配自己的网络 IP 等信息，并将容器连接到一个虚拟网桥与外界通信，但默认情况下无法通过主机外部访问容器 可以和外部网络之间进行通信，通过SNAT访问外网，使用DNAT可以让容器被外部主机访问，所以此模式也称为NAT模式。同一台宿主机上的多个容器如果采用的是bridge模式，那么他们的通信直接走docker0（二层通信） 容器访问外网：容器网卡eth0 –&gt; vethxxx设备 –&gt; docker0网桥 –&gt; 宿主机网卡 外网访问容器内的服务：宿主机网卡+8000端口 –&gt; docker0网桥 –&gt; vethxxx设备 –&gt; 容器网卡eth0+容器内expose暴露的80端口 bridge网络模式特点 宿主机需要启动 ip_forward 功能 不同宿主机的容器无法直接通信，各自使用独立网络 容器内访问外网，容器的网关指向的就是docker0的地址 同一台宿主机上的多个容器直接处于一个二层网络，直接通过docker0这个二层交换机外部通信就可以 容器内expose暴露的端口，必须要映射到宿主机上的一个端口，然后才能被外部访问 bridge网络模式缺点 外部主机无法直接访问容器: 可以通过配置DNAT接受外网的访问 低性能较低: 因为可通过NAT，网络转换带来更的损耗 端口管理繁琐: 每个容器必须手动指定唯一的端口，容器产生端口冲容 查看虚拟机交换机即docker0网桥上的接口情况 123yum install -y bridge-utilsbrctl showbrctl showmacs docker0 # 查看docker0维护的mac地址情况 如何确定veth对的配对情况 123451、进入容器内，查看容器网卡配对的veth设备的id号cat /sys/class/net/eth0/iflink #假设输出的是612、然后在宿主机上查看veth设备的id号，找到对应的veth设备ip link show # 会找到一个编号为61的veth设备 3.1.2 Bridge 模式的默认设置范例: 查看bridge模式信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@ubuntu2004 ~]#docker network inspect bridge[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;c29cd0fe53785ab7b6135a1ec76bc70eac3667c4df0baf73a9f620c0f27cdd9a&quot;, &quot;Created&quot;: &quot;2024-04-15T14:57:08.134414438Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;6301ef5fc006dcdc9e3c509e3ff0cb7167a9d85dad66b76d7631f8f60b6dbbf8&quot;: &#123; &quot;Name&quot;: &quot;server&quot;, &quot;EndpointID&quot;: &quot;123263ca377908ab3bd85440122954554183531722d8029864a14c87f04bbeba&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:04&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.4/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;7dca1c97784f7cdc2b3fe815df42ca701b4bddcbc33f94409e4cb1ef49b126ce&quot;: &#123; &quot;Name&quot;: &quot;client&quot;, &quot;EndpointID&quot;: &quot;7e78cb3cb7e14d0085a52ccdacc457d79e4f01645ffc51a2540d96f7c8f3b0c3&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;] 范例: 宿主机的网络状态 1234567891011121314151617181920212223#安装docker后.默认启用ip_forward[root@ubuntu2004 ~]#cat /proc/sys/net/ipv4/ip_forward1[root@ubuntu2004 ~]#iptables -vnL -t natChain PREROUTING (policy ACCEPT 65 packets, 5022 bytes) pkts bytes target prot opt in out source destination 243 44752 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT 35 packets, 2730 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 79 packets, 5459 bytes) pkts bytes target prot opt in out source destination 0 0 DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT 83 packets, 5723 bytes) pkts bytes target prot opt in out source destination 131 8330 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 88 5280 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 范例: 通过宿主机的物理网卡利用SNAT访问外部网络 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#在另一台主机上建立httpd服务器[root@centos7 ~]#systemctl is-active httpdactive#启动容器，默认是bridge网络模式[root@ubuntu1804 ~]#docker run -it --rm alpine:3.11 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever166: eth0@if167: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever / # ping www.baidu.comPING www.baidu.com (61.135.169.125): 56 data bytes64 bytes from 61.135.169.125: seq=0 ttl=127 time=5.182 ms^C--- www.baidu.com ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 5.182/5.182/5.182 ms#可以访问其它宿主机/ # ping 10.0.0.7PING 10.0.0.7 (10.0.0.7): 56 data bytes64 bytes from 10.0.0.7: seq=0 ttl=63 time=0.764 ms64 bytes from 10.0.0.7: seq=1 ttl=63 time=1.147 ms^C--- 10.0.0.7 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.764/0.955/1.147 ms# 通过虚拟网桥去访问外部网络/ # traceroute 10.0.0.7traceroute to 10.0.0.7 (10.0.0.7), 30 hops max, 46 byte packets1 172.17.0.1 (172.17.0.1) 0.008 ms 0.008 ms 0.007 ms2 10.0.0.7 (10.0.0.7) 0.255 ms 0.510 ms 0.798 ms/ # wget -qO - 10.0.0.7Website on 10.0.0.7/ # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0[root@centos7 ~]#curl 127.0.0.1Website on 10.0.0.7[root@centos7 ~]#tail /var/log/httpd/access_log127.0.0.1 - - [01/Feb/2020:19:31:16 +0800] &quot;GET / HTTP/1.1&quot; 200 20 &quot;-&quot;&quot;curl/7.29.0&quot;10.0.0.100 - - [01/Feb/2020:19:31:21 +0800] &quot;GET / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;Wget&quot; 3.1.3 修改默认的 Bridge 模式网络配置有两种方法修改默认的bridge 模式的网络配置，但两种方式只能选一种，否则会导致冲容，docker服务无法启动 范例: 修改Bridge模式默认的网段方法1 123456#修改桥接地址[root@ubuntu1804 ~]#vim /lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --bip=10.100.0.1/24[root@ubuntu1804 ~]#systemctl daemon-reload[root@ubuntu1804 ~]#systemctl restart docker 范例: 修改Bridge网络配置方法2 123456789101112[root@ubuntu1804 ~]#vim /etc/docker/daemon.json&#123; &quot;hosts&quot;: [&quot;tcp://0.0.0.0:2375&quot;, &quot;fd://&quot;], &quot;bip&quot;: &quot;192.168.100.100/24&quot;, #分配docker0网卡的IP,24是容器IP的netmask &quot;fixed-cidr&quot;: &quot;192.168.100.128/26&quot;, #分配容器IP范围,26不是容器IP的子网掩码,只表示地址范围 &quot;fixed-cidr-v6&quot;: &quot;2001:db8::/64&quot;, #ipv6 &quot;mtu&quot;: 1500, &quot;default-gateway&quot;: &quot;192.168.100.200&quot;, #网关必须和bip在同一个网段 &quot;default-gateway-v6&quot;: &quot;2001:db8:abcd::89&quot;, &quot;dns&quot;: [ &quot;1.1.1.1&quot;, &quot;8.8.8.8&quot;]&#125;[root@ubuntu1804 ~]#systemctl restart docker 3.2 Host模式 此模式相当于VM的桥接模式，如果指定host模式启动的容器，那么新创建的容器不会创建自己的虚拟网卡，而是直接使用宿主机的网卡和IP地址，因此在容器里面查看到的IP信息就是宿主机的信息，访问容器的时候直接使用宿主机IP+容器端口即可，不过容器内除网络以外的其它资源，如: 文件系统、系统进程等仍然和宿主机保持隔离 此模式由于直接使用宿主机的网络无需转换，网络性能最高，只要宿主机与其他宿主机可以联通，就可以直接跨主机访问，但是各容器内使用的端口不能相同，因为使用的都是宿主机的端口，适用于运行容器端口比较固定的业务 Host 网络模式特点: 共享宿主机网络，所以只要该宿主机与其他宿主机是联通，那就可以直接跨主机访问 各容器网络无隔离，网络性能无损耗，网络故障排除相对简单 共享宿主机端口，容易产生端口冲突 不支持端口映射 范例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#查看宿主机的网络设置[root@ubuntu1804 ~]#ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 net6 fe80::42:2ff:fe7f:a8c6 prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:02:7f:a8:c6 txqueuelen 0 (Ethernet) RX packets 63072 bytes 152573158 (152.5 MB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 56611 bytes 310696704 (310.6 MB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.0.100 netmask 255.255.255.0 broadcast 10.0.0.255 inet6 fe80::20c:29ff:fe34:df91 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:34:df:91 txqueuelen 1000 (Ethernet) RX packets 2029082 bytes 1200597401 (1.2 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7272209 bytes 11576969391 (11.5 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 3533 bytes 320128 (320.1 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 3533 bytes 320128 (320.1 KB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@ubuntu1804 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0#打开容器前确认宿主机的80/tcp端口没有打开[root@ubuntu1804 ~]#ss -ntl|grep :80#创建host模式的容器[root@ubuntu1804 ~]#docker run -d --network host --name web1 nginx-centos7-base:1.6.141fb5b8e41db26e63579a424df643d1f02e272dc75e76c11f4e313a443187ed1#创建容器后，宿主机的80/tcp端口打开[root@ubuntu1804 ~]#ss -ntlp|grep :80LISTEN 0 128 0.0.0.0:80 0.0.0.0:* users:((&quot;nginx&quot;,pid=43762,fd=6),(&quot;nginx&quot;,pid=43737,fd=6))#进入容器[root@ubuntu1804 ~]#docker exec -it web1 bash#进入容器后仍显示宿主机的主机名提示符信息[root@ubuntu1804 /]# hostnameubuntu1804.wang.org[root@ubuntu1804 /]# ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:2ff:fe7f:a8c6 prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:02:7f:a8:c6 txqueuelen 0 (Ethernet) RX packets 63072 bytes 152573158 (145.5 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 56611 bytes 310696704 (296.3 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.0.0.100 netmask 255.255.255.0 broadcast 10.0.0.255 inet6 fe80::20c:29ff:fe34:df91 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:0c:29:34:df:91 txqueuelen 1000 (Ethernet) RX packets 2028984 bytes 1200589212 (1.1 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 7272137 bytes 11576960933 (10.7 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 3533 bytes 320128 (312.6 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 3533 bytes 320128 (312.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@ubuntu1804 /]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0#从容器访问远程主机[root@ubuntu1804 /]# curl 10.0.0.7Website on 10.0.0.7#查看远程主机的访问日志[root@centos7 ~]#tail -n1 /var/log/httpd/access_log10.0.0.100 - - [01/Feb/2020:19:58:06 +0800] &quot;GET / HTTP/1.1&quot; 200 20 &quot;-&quot;&quot;curl/7.29.0&quot;#远程主机可以访问容器的web服务[root@centos7 ~]#curl 10.0.0.100/app/Test Page in app 范例: host模式下端口映射无法实现 1234567891011[root@ubuntu1804 ~]#ss -ntl|grep :81[root@ubuntu1804 ~]#docker run -d --network host --name web2 -p 81:80 nginx-centos7-base:1.6.1WARNING: Published ports are discarded when using host network mode6b6a910d79d94b188f719bc6ad00c274acd76a4a2929212157cd49b5219d44ae#host模块下端口映射不成功，但是容器可以启动[root@ubuntu1804 ~]#docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6b6a910d79d9 nginx-centos7-base:1.6.1 &quot;/apps/nginx/sbin/ng…&quot; 6 seconds ago Exited (1) 2 seconds ago web2b27c0fd28b40 nginx-centos7-base:1.6.1 &quot;/apps/nginx/sbin/ng…&quot; About a minute ago Up About a minute web1 范例: 对比前面host模式的容器和bridge模式的端口映射 12345678[root@ubuntu1804 ~]#docker port web1[root@ubuntu1804 ~]#docker port web2[root@ubuntu1804 ~]#docker run -d --network bridge -p 8001:80 --name web3 nginx-centos7-base:1.6.14095372b9a561704eac98ccef8041a80a2cdc2aa7b57d2798dec1a8dcb00c377[root@ubuntu1804 ~]#docker port web380/tcp -&gt; 0.0.0.0:8001 3.3 None模式 在使用 none 模式后，Docker 容器不会进行任何网络配置，没有网卡、没有IP也没有路由，因此默认无法与外界通信，需要手动添加网卡配置IP等，所以极少使用 none模式特点 使用参数 --network none 指定 默认无网络功能，无法和外部通信 无法实现端口映射 适用于测试环境 范例: 启动none模式的容器 123456789101112131415161718192021222324252627282930313233[root@ubuntu1804 ~]#docker run -d --network none -p 8001:80 --name web1-none nginx-centos7-base:1.6.15207dcbd0aeea88548819267d3751135e337035475cf3cd63a5e1be6599c0208[root@ubuntu1804 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5207dcbd0aee nginx-centos7-base:1.6.1 &quot;/apps/nginx/sbin/ng…&quot; About a minute ago Up About a minute web1-none[root@ubuntu1804 ~]#docker port web1-none[root@ubuntu1804 ~]#docker exec -it web1-none bash[root@5207dcbd0aee /]# ifconfig -alo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@5207dcbd0aee /]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface[root@5207dcbd0aee /]# netstat -ntlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN [root@5207dcbd0aee /]# ping www.baidu.comping: www.baidu.com: Name or service not known[root@5207dcbd0aee /]# ping 172.17.0.1connect: Network is unreachable 3.4 Container模式 使用此模式创建的容器需指定和一个已经存在的容器共享一个网络，而不是和宿主机共享网络，新创建的容器不会创建自己的网卡也不会配置自己的IP，而是和一个被指定的已经存在的容器共享IP和端口范围，因此这个容器的端口不能和被指定容器的端口冲突，除了网络之外的文件系统、进程信息等仍然保持相互隔离，共享了网络资源的多个容器，可以直接使用lo网卡进行通信 Container 模式特点 使用参数 –-network container:名称或ID 指定 与宿主机网络空间隔离 容器间共享网络空间，直接使用对方的网络 第一个容器的网络可能是bridge，或none，或者host，而第二个容器模式依赖于第一个容器，它们共享网络 如果第一个容器停止，将导致无法创建第二个容器 第二个容器可以直接使用127.0.0.1访问第一个容器 适合频繁的容器间的网络通信 只有第一个容器支持端口映射，后续依赖的容器不支持 范例： 通过容器模式实现 wordpress 正常来说，先启动MySQL，再启动WordPress，因为WordPress需要连接MySQL。但是根据container模式的特点，如果先启动MySQL，必然要暴露MySQL的端口，而MySQL是不能暴露端口的，我们需要访问的是WordPress服务，需要暴露WordPress端口，所以先启动WordPress，再启动MySQL 12345[root@ubuntu2004 ~]#docker run -d -p 80:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php7.4-apache[root@ubuntu2004 ~]#docker run --network container:wordpress -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d -v /data/mysql:/var/lib/mysql --restart=always mysql:8.0.29-oracle#注意：数据库主机地址为127.0.0.1，不支持localhost 范例：实现LNMP的Wordpress 12345678910111213141516171819202122232425262728293031323334#准备配置文件[root@ubuntu2204 ~]#cat /data/nginx/conf.d/www.wang.org.confserver &#123; listen 80; server_name www.wang.org; root /var/www/html; index index.php index.html index.htm; client_max_body_size 100m; location ~ \\.php$ &#123; root /var/www/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125;[root@ubuntu2204 ~]#docker run -d -p 80:80 --name wordpress -v /data/www:/var/www/html -v /data/nginx/conf.d/:/apps/nginx/conf/conf.d/ wangxiaochun/nginx:1.24.0-alpine-3.18.0[root@ubuntu2204 ~]#docker run --name php-fpm --network container:wordpress -d -v /data/www:/var/www/html wordpress:6.2.2-php8.0-fpm-alpine[root@ubuntu2204 ~]#docker run --name mysql --network container:wordpress -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 -d -v /data/mysql:/var/lib/mysql --restart=always mysql:8.0.29-oracle#按下面浏览器访问初始化后，可以看到/data/www目录下自动生成文件[root@ubuntu2204 ~]#ll -d /data/www/drwxr-xr-x 5 82 82 4096 6月 17 21:20 /data/www//[root@ubuntu2204 ~]#ll /data/www/总计 256-rw-r--r-- 1 82 82 351 2月 6 2020 wp-blog-header.php-rw-r--r-- 1 82 82 2338 11月 10 2021 wp-comments-post.php-rw-rw-r-- 1 82 82 5492 6月 15 16:14 wp-config-docker.php-rw-rw-rw- 1 82 82 3292 6月 17 21:20 wp-config.php... 范例: 通过容器模式实现LNP架构 1234567891011121314151617181920212223242526272829#准备nginx连接php-fpm的配置文件[root@ubuntu2004 ~]#cat /data/nginx/conf.d/php.confserver &#123; listen 80; server_name www.wang.org; root /usr/share/nginx/html; index index.php; location ~ \\.php$ &#123; root /usr/share/nginx/html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125;#准备php的测试页文件[root@ubuntu2004 ~]#cat /data/nginx/html/index.php&lt;?phpphpinfo();?&gt;#启动nginx[root@ubuntu2004 ~]#docker run -d --name nginx -v /data/nginx/conf.d/:/etc/nginx/conf.d/ -v /data/nginx/html:/usr/share/nginx/html -p 80:80 wangxiaochun/nginx:1.20.0#启动php-fpm[root@ubuntu2004 ~]#docker run -d --network container:nginx --name php-fpm -v /data/nginx/html/:/usr/share/nginx/html php:8.1-fpm#注意:php:8.1-fpm镜像缺少连接数据库的相关包,无法直接连接MySQL 范例: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#创建第一个容器[root@ubuntu1804 ~]#docker run -it --name server1 -p 80:80 alpine:3.11 sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:9 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:766 (766.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # netstat -ntlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State #在另一个终端执行下面操作[root@ubuntu1804 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES4d342fac169f alpine:3.11 &quot;sh&quot; 29 seconds ago Up 28 seconds 0.0.0.0:80-&gt;80/tcp server1[root@ubuntu1804 ~]#docker port server180/tcp -&gt; 0.0.0.0:80#无法访问web服务[root@ubuntu1804 ~]#curl 127.0.0.1/app/curl: (52) Empty reply from server#创建第二个容器，基于第一个容器的container的网络模式[root@ubuntu1804 ~]#docker run -d --name server2 --network container:server1 nginx-centos7-base:1.6.17db90f38590ade11e1c833a8b2175810c71b3f222753c5177bb8b05952f08a7b#可以访问web服务[root@ubuntu1804 ~]#curl 127.0.0.1/app/Test Page in app[root@ubuntu1804 ~]#docker exec -it server2 bash#和第一个容器共享相同的网络[root@4d342fac169f /]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 172.17.0.2 netmask 255.255.0.0 broadcast 172.17.255.255 ether 02:42:ac:11:00:02 txqueuelen 0 (Ethernet) RX packets 29 bytes 2231 (2.1 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 12 bytes 1366 (1.3 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 10 bytes 860 (860.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 10 bytes 860 (860.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@4d342fac169f /]# netstat -ntlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN#可访问外网[root@4d342fac169f /]# ping www.baidu.comPING www.a.shifen.com (61.135.169.121) 56(84) bytes of data.64 bytes from 61.135.169.121 (61.135.169.121): icmp_seq=1 ttl=127 time=3.99 ms64 bytes from 61.135.169.121 (61.135.169.121): icmp_seq=2 ttl=127 time=5.03 ms^C--- www.a.shifen.com ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1002msrtt min/avg/max/mdev = 3.999/4.514/5.030/0.519 ms 范例: 第一个容器使用host网络模式,第二个容器与之共享网络 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@ubuntu1804 ~]#docker run -d --name c1 --network host nginx-centos7.8:v5.0-1.18.05a60804f3917d82dfe32db140411cf475f20acce0fe4674d94e4557e1003d8e0[root@ubuntu1804 ~]#docker run -it --name c2 --network container:c1 centos7.8:v1.0[root@ubuntu1804 /]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:63:8b:ac brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe63:8bac/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:24:86:98:fb brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:24ff:fe86:98fb/64 scope link valid_lft forever preferred_lft forever [root@ubuntu1804 ~]#docker exec -it c1 bash[root@ubuntu1804 /]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:63:8b:ac brd ff:ff:ff:ff:ff:ff inet 10.0.0.100/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe63:8bac/64 scope link valid_lft forever preferred_lft forever 3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:24:86:98:fb brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:24ff:fe86:98fb/64 scope link valid_lft forever preferred_lft forever 范例:第一个容器使用none网络模式,第二个容器与之共享网络 123456789[root@ubuntu1804 ~]#docker run -d --name c1 --network none nginx-centos7.8:v5.0-1.18.0caf5b57299c8359f21f30b8894c5f8496ff39b44ead6a732056000689cb0c91c[root@ubuntu1804 ~]#docker run -it --name c2 --network container:c1 centos7.8:v1.0[root@caf5b57299c8 /]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 3.5 自定义网络模式除了以上的网络模式，也可以自定义网络，使用自定义的网段地址，网关等信息 可以使用自定义网络模式实现不同集群应用的独立网络管理，而互不影响，而且在一个网络内，可以直接利用容器名相互访问，非常便利 注意: 自定义网络内的容器可以直接通过容器名进行相互的访问，而无需使用 --link 3.5.1 自定义网络实现123456789101112[root@ubuntu1804 ~]#docker network --helpUsage: docker network COMMANDManage networksCommands:connect #将容器连接到另一个容器的网络create #创建网络disconnect #断开容器与另一个容器网络的连接inspect #显示一个或多个网络的详细信息ls #列出网络prune #删除所有未使用的网络rm #移除一个或多个网络 创建自定义网络: 1234docker network create -d &lt;mode&gt; --subnet &lt;CIDR&gt; --gateway &lt;网关&gt; &lt;自定义网络名称&gt;#注意mode不支持host和none,默认是bridge模式-d &lt;mode&gt; 可省略，默认为bridge 查看自定义网络信息 1docker network inspect &lt;自定义网络名称或网络ID&gt; 引用自定义网络 123docker run --network &lt;自定义网络名称&gt; &lt;镜像名称&gt;docker run --net &lt;自定义网络名称&gt; --ip &lt;指定静态IP&gt; &lt;镜像名称&gt;#注意：静态IP只支持自定义网络模型 删除自定义网络 1doccker network rm &lt;自定义网络名称或网络ID&gt; 范例：内置的三个网络无法删除 1234567891011[root@ubuntu1804 ~]#docker network rm test-nettest-net[root@ubuntu1804 ~]#docker network rm noneError response from daemon: none is a pre-defined network and cannot be removed[root@ubuntu1804 ~]#docker network rm bridgeError response from daemon: bridge is a pre-defined network and cannot be removed[root@ubuntu1804 ~]#docker network rm hostError response from daemon: host is a pre-defined network and cannot be removed 3.5.2 实战案例: 自定义网络3.5.2.1 创建自定义的网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@ubuntu2004 ~]#docker network create -d bridge --subnet 172.27.0.0/16 --gateway 172.27.0.1 test-net f873ba3c38f8c05eb10263837c0b796b04ecb317dccf0d2a4f66ff59c22f71eb[root@ubuntu2004 ~]#docker network lsNETWORK ID NAME DRIVER SCOPEc29cd0fe5378 bridge bridge localf059ccc72b49 host host localcab6c739b857 none null localf873ba3c38f8 test-net bridge local[root@ubuntu2004 ~]#docker inspect test-net [ &#123; &quot;Name&quot;: &quot;test-net&quot;, &quot;Id&quot;: &quot;f873ba3c38f8c05eb10263837c0b796b04ecb317dccf0d2a4f66ff59c22f71eb&quot;, &quot;Created&quot;: &quot;2024-04-20T07:06:54.271353858Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.27.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.27.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123;&#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;][root@ubuntu2004 ~]#ip a#新添加了一个虚拟网卡154: br-f873ba3c38f8: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:93:7a:74:3d brd ff:ff:ff:ff:ff:ff inet 172.27.0.1/16 brd 172.27.255.255 scope global br-f873ba3c38f8 valid_lft forever preferred_lft forever #新加了一个网桥[root@ubuntu2004 ~]#brctl showbridge name bridge id STP enabled interfacesbr-f873ba3c38f8 8000.0242937a743d no docker0 8000.024213935f04 no [root@ubuntu2004 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0172.27.0.0 0.0.0.0 255.255.0.0 U 0 0 0 br-f873ba3c38f8 3.5.2.2利用自定义的网络创建容器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@ubuntu2004 ~]#docker run -it --network test-net alpine:3.18.0 sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever155: eth0@if156: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link valid_lft forever preferred_lft forever / # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.27.0.1 0.0.0.0 UG 0 0 0 eth0172.27.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0/ # ping -c1 www.baidu.comPING www.baidu.com (183.2.172.185): 56 data bytes64 bytes from 183.2.172.185: seq=0 ttl=127 time=10.913 ms[root@ubuntu2004 ~]#docker inspect test-net[ &#123; &quot;Name&quot;: &quot;test-net&quot;, &quot;Id&quot;: &quot;f873ba3c38f8c05eb10263837c0b796b04ecb317dccf0d2a4f66ff59c22f71eb&quot;, &quot;Created&quot;: &quot;2024-04-20T07:06:54.271353858Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.27.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.27.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;b68a25dfb739a1d907380c33ae46347eb20e0b6be74d2d61879d65d1bc13f620&quot;: &#123; &quot;Name&quot;: &quot;beautiful_feistel&quot;, &quot;EndpointID&quot;: &quot;c022e2c13a78d13b9b5d42214f3c15ca0fb1739157317332c01050554e0fc6c7&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:1b:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.27.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;] 3.5.3 实战案例: 自定义网络中的容器之间通信123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354root@ubuntu2004 ~]#docker network lsNETWORK ID NAME DRIVER SCOPEc29cd0fe5378 bridge bridge localf059ccc72b49 host host localcab6c739b857 none null localf873ba3c38f8 test-net bridge local[root@ubuntu2004 ~]#docker run -it --rm --network test-net --name test1 alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever157: eth0@if158: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link valid_lft forever preferred_lft forever[root@ubuntu2004 ~]#docker run -it --rm --network test-net --name test2 alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever159: eth0@if160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:03 brd ff:ff:ff:ff:ff:ff inet 172.27.0.3/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:3/64 scope link valid_lft forever preferred_lft forever#在test1中访问/ # ping -c1 test2PING test2 (172.27.0.3): 56 data bytes64 bytes from 172.27.0.3: seq=0 ttl=64 time=0.202 ms--- test2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.202/0.202/0.202 ms#在test2中访问/ # ping -c1 test1PING test1 (172.27.0.2): 56 data bytes64 bytes from 172.27.0.2: seq=0 ttl=64 time=0.077 ms--- test1 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.077/0.077/0.077 ms 结论: 自定义网络中的容器之间可以直接利用容器名进行通信 3.5.4 实战案例: 利用自定义网络实现 Wordpress1234567[root@ubuntu2004 ~]#docker network create -d bridge --subnet 172.27.0.0/16 --gateway 172.27.0.1 bridge2[root@ubuntu2004 ~]#docker run -d -p 8080:80 --network bridge2 --name wordpress2 -v /data/wordpress2:/var/www/html --restart=always wordpress:php7.4-apache[root@ubuntu2004 ~]#docker run -d --network bridge2 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql2 -v /data/mysql2:/var/lib/mysql --restart=always mysql:8.0.29-oracle#wordpress连接数据库时，数据库主机是容器的名称mysql2 3.5.5 实战案例: 利用自定义网络实现 Redis Cluster3.5.5.1 创建自定义网络1234[root@ubuntu1804 ~]#docker network create net-redis --subnet 172.18.0.0/1609b9dded99787835dccc029e16fa2782292d22c3e258f60a1db15d44e7a3bd93[root@ubuntu1804 ~]#docker inspect net-redis 3.5.5.2 创建6个redis容器配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 通过脚本创建六个redis容器配置[root@ubuntu1804 ~]#for port in &#123;1..6&#125;;do mkdir -p /data/redis/node-$&#123;port&#125;/conf cat &gt;&gt; /data/redis/node-$&#123;port&#125;/conf/redis.conf &lt;&lt; EOFport 6379bind 0.0.0.0masterauth 123456requirepass 123456cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.18.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379appendonly yesEOFdone[root@ubuntu1804 ~]#tree /data/redis//data/redis/├── node-1│ └── conf│ └── redis.conf├── node-2│ └── conf│ └── redis.conf├── node-3│ └── conf│ └── redis.conf├── node-4│ └── conf│ └── redis.conf├── node-5│ └── conf│ └── redis.conf└── node-6 └── conf └── redis.conf12 directories, 6 files[root@ubuntu1804 ~]#cat /data/redis/node-1/conf/redis.confport 6379bind 0.0.0.0masterauth 123456requirepass 123456cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.18.0.11cluster-announce-port 6379cluster-announce-bus-port 16379appendonly yes 3.5.5.3 创建6个 redis 容器123456789# 通过脚本运行六个redis容器[root@ubuntu1804 ~]#for port in &#123;1..6&#125;;do docker run -p 637$&#123;port&#125;:6379 -p 1667$&#123;port&#125;:16379 --name redis-$&#123;port&#125; \\ -v /data/redis/node-$&#123;port&#125;/data:/data \\ -v /data/redis/node-$&#123;port&#125;/conf/redis.conf:/etc/redis/redis.conf \\ -d --net net-redis --ip 172.18.0.1$&#123;port&#125; redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdone[root@ubuntu1804 ~]#docker ps 3.5.5.4 创建 redis cluster123456789101112131415161718#连接redis cluster[root@ubuntu1804 ~]#docker exec -it redis-1 /bin/sh/data # redis-cli -a 123456Warning: Using a password with &#x27;-a&#x27; or &#x27;-u&#x27; option on the command line interface may not be safe.127.0.0.1:6379&gt; exit#不支持 &#123; &#125; 扩展/data # echo &#123;1..10&#125;&#123;1..10&#125;/data # echo $-smi# 创建集群/data # redis-cli -a 123456 --cluster create 172.18.0.11:6379 172.18.0.12:6379 172.18.0.13:6379 172.18.0.14:6379 172.18.0.15:6379 172.18.0.16:6379 --cluster-replicas 1Warning: Using a password with &#x27;-a&#x27; or &#x27;-u&#x27; option on the command line interface may not be safe.&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Can I set the above configuration? (type &#x27;yes&#x27; to accept): #输入yes 3.5.5.5 测试访问 redis cluster123456789101112131415161718192021222324252627#连接redis cluster/data # redis-cli -a 123456 -c127.0.0.1:6379&gt; cluster nodes#看到172.18.0.&#123;11,12,13&#125;为master,172.18.0.&#123;14,15,16&#125;为slave#以下为master/slave关系#172.18.0.11&lt;---&gt;172.18.0.15#172.18.0.12&lt;---&gt;172.18.0.16#172.18.0.13&lt;---&gt;172.18.0.14#添加key到redis-2上127.0.0.1:6379&gt; set name wang-&gt; Redirected to slot [5798] located at 172.18.0.12:6379OK#添加key到redis-1上172.18.0.12:6379&gt; set title cto-&gt; Redirected to slot [2217] located at 172.18.0.11:6379OK172.18.0.11:6379&gt; get name-&gt; Redirected to slot [5798] located at 172.18.0.12:6379&quot;wang&quot;172.18.0.12:6379&gt; get title-&gt; Redirected to slot [2217] located at 172.18.0.11:6379&quot;cto&quot; 3.5.5.6 测试故障实现 redis cluster 高可用性123456789101112131415161718192021222324#模拟redis-2故障[root@ubuntu1804 ~]#docker stop redis-2redis-2#再次查看cluster状态,可以看到redis-2出错[root@ubuntu1804 ~]#docker exec -it redis-1 /bin/sh/data # redis-cli -a 123456 --cluster check 127.0.0.1:6379Warning: Using a password with &#x27;-a&#x27; or &#x27;-u&#x27; option on the command line interface may not be safe.Could not connect to Redis at 172.18.0.12:6379: Host is unreachable#查看到 172.18.0.16提升为新的master172.18.0.16:6379 (06295ce4...) -&gt; 1 keys | 5462 slots | 0 slaves.172.18.0.13:6379 (599f69b4...) -&gt; 0 keys | 5461 slots | 1 slaves.172.18.0.15:6379 (2f69287f...) -&gt; 1 keys | 5461 slots | 1 slaves./data # redis-cli -a 123456 -c127.0.0.1:6379&gt; cluster nodes127.0.0.1:6379&gt; get name-&gt; Redirected to slot [5798] located at 172.18.0.16:6379&quot;wang&quot;172.18.0.16:6379&gt; get title-&gt; Redirected to slot [2217] located at 172.18.0.15:6379&quot;cto&quot; 4 同一个宿主机之间不同网络的容器通信开两个容器，一个使用自定义网络容器，一个使用默认brideg网络的容器，默认因iptables规则导致无法通信 12345678910111213141516171819202122232425262728293031323334353637#使用默认brideg网络的容器[root@ubuntu2004 ~]#docker run -it --rm --name test1 alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever161: eth0@if162: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link tentative valid_lft forever preferred_lft forever #使用自定义网络的容器[root@ubuntu2004 ~]#docker run -it --rm --network test-net --name test2 alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever163: eth0@if164: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link tentative valid_lft forever preferred_lft forever / # ping 172.27.0.2 #无法ping通自定义网络容器PING 172.27.0.2 (172.27.0.2): 56 data bytes/ # ping 172.17.0.2 #无法ping通默认的网络容器PING 172.27.0.2 (172.17.0.2): 56 data bytes 4.1 方式1: 修改iptables实现同一宿主机上的不同网络的容器间通信这种方法不推荐，破坏了docker内部的规则 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#确认开启ip_forward[root@ubuntu2004 ~]#cat /proc/sys/net/ipv4/ip_forward1#默认网络和自定义网络是两个不同的网桥[root@ubuntu2004 ~]#brctl showbridge name bridge id STP enabled interfacesbr-f873ba3c38f8 8000.0242937a743d no veth0f52bdbdocker0 8000.024213935f04 no vethc0064b3#因为是跨网络通信，所以是在filter表的FORWARD链创造了规则[root@ubuntu2004 ~]#iptables -vnLChain INPUT (policy ACCEPT 3027 packets, 3826K bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy DROP 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 8 672 DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 8 672 DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 3 252 ACCEPT all -- * br-f873ba3c38f8 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 2 168 DOCKER all -- * br-f873ba3c38f8 0.0.0.0/0 0.0.0.0/0 1 84 ACCEPT all -- br-f873ba3c38f8 !br-f873ba3c38f8 0.0.0.0/0 0.0.0.0/0 2 168 ACCEPT all -- br-f873ba3c38f8 br-f873ba3c38f8 0.0.0.0/0 0.0.0.0/0 58472 116M ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 141 7996 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 30143 7463K ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 6 432 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 2258 packets, 151K bytes) pkts bytes target prot opt in out source destination Chain DOCKER (2 references) pkts bytes target prot opt in out source destination Chain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 1 84 DOCKER-ISOLATION-STAGE-2 all -- br-f873ba3c38f8 !br-f873ba3c38f8 0.0.0.0/0 0.0.0.0/0 30145 7464K DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 88762 123M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-ISOLATION-STAGE-2 (2 references) pkts bytes target prot opt in out source destination 2 168 DROP all -- * br-f873ba3c38f8 0.0.0.0/0 0.0.0.0/0 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 30144 7464K RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 88764 123M RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 [root@ubuntu2004 ~]#iptables-save # Generated by iptables-save v1.8.4 on Sat Apr 20 07:28:05 2024*nat:PREROUTING ACCEPT [7:576]:INPUT ACCEPT [2:156]:OUTPUT ACCEPT [46:3191]:POSTROUTING ACCEPT [48:3359]:DOCKER - [0:0]-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER-A POSTROUTING -s 172.27.0.0/16 ! -o br-f873ba3c38f8 -j MASQUERADE-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE-A DOCKER -i br-f873ba3c38f8 -j RETURN-A DOCKER -i docker0 -j RETURNCOMMIT# Completed on Sat Apr 20 07:28:05 2024# Generated by iptables-save v1.8.4 on Sat Apr 20 07:28:05 2024*filter:INPUT ACCEPT [3068:3827965]:FORWARD DROP [0:0]:OUTPUT ACCEPT [2295:156605]:DOCKER - [0:0]:DOCKER-ISOLATION-STAGE-1 - [0:0]:DOCKER-ISOLATION-STAGE-2 - [0:0]:DOCKER-USER - [0:0]-A FORWARD -j DOCKER-USER-A FORWARD -j DOCKER-ISOLATION-STAGE-1-A FORWARD -o br-f873ba3c38f8 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o br-f873ba3c38f8 -j DOCKER-A FORWARD -i br-f873ba3c38f8 ! -o br-f873ba3c38f8 -j ACCEPT-A FORWARD -i br-f873ba3c38f8 -o br-f873ba3c38f8 -j ACCEPT-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A FORWARD -o docker0 -j DOCKER-A FORWARD -i docker0 ! -o docker0 -j ACCEPT-A FORWARD -i docker0 -o docker0 -j ACCEPT-A DOCKER-ISOLATION-STAGE-1 -i br-f873ba3c38f8 ! -o br-f873ba3c38f8 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2-A DOCKER-ISOLATION-STAGE-1 -j RETURN-A DOCKER-ISOLATION-STAGE-2 -o br-f873ba3c38f8 -j DROP-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP-A DOCKER-ISOLATION-STAGE-2 -j RETURN-A DOCKER-USER -j RETURNCOMMIT# Completed on Sat Apr 20 07:28:05 2024[root@ubuntu2004 ~]#iptables-save &gt; iptables.rule[root@ubuntu2004 ~]#vim iptables.rule#方法1：修改下面两行的规则-A DOCKER-ISOLATION-STAGE-2 -o br-c90dee3b7937 -j ACCEPT-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j ACCEPT#方法2：在FORWARD链第一行添加下面一行-A FORWARD -j ACCEPT#方法3：执行下面命令[root@ubuntu2004 ~]#iptables -I DOCKER-ISOLATION-STAGE-2 -j ACCEPT[root@ubuntu2004 ~]#iptables-restore &lt; iptables.rule#再次两个容器之间可以相互通信/ # ping 172.27.0.2PING 172.27.0.2 (172.27.0.2): 56 data bytes64 bytes from 172.27.0.2: seq=896 ttl=63 time=0.502 ms64 bytes from 172.27.0.2: seq=897 ttl=63 time=0.467 ms64 bytes from 172.27.0.2: seq=898 ttl=63 time=0.227 ms/ # ping 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=63 time=0.163 ms64 bytes from 172.17.0.2: seq=1 ttl=63 time=0.232 ms 4.2 方式2: 通过解决docker network connect 实现同一个宿主机不同网络的容器间通信可以使用docker network connect命令实现同一个宿主机不同网络的容器间相互通信 12345678910111213141516171819#将container连入指定的NETWORK中,使此container可以与NETWORK中的其它容器进行通信docker network connect [OPTIONS] NETWORK CONTAINERConnect a container to a networkOptions: --alias strings #Add network-scoped alias for the container --driver-opt strings #driver options for the network --ip string #IPv4 address (e.g., 172.30.100.104) --ip6 string #IPv6 address (e.g., 2001:db8::33) --link list #Add link to another container --link-local-ip strings #Add a link-local address for the containerdocker network disconnect [OPTIONS] NETWORK CONTAINERDisconnect a container from a networkOptions: -f, --force #Force the container to disconnect from a network 将CONTAINER与指定的NETWORK断开连接，使此CONTAINER可以与CONTAINER中的其它容器无法通信 如果将容器从自定义的网络删除，将加入默认的网络，即docker0网桥中，获取172.17.0.0&#x2F;16 如果将容器从默认的网络docker0删除，将加入none网络 4.2.1 上面案例中test1和test2的容器间默认无法通信12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#每个网络中有属于此网络的容器信息[root@ubuntu2004 ~]#docker network inspect bridge[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;c29cd0fe53785ab7b6135a1ec76bc70eac3667c4df0baf73a9f620c0f27cdd9a&quot;, &quot;Created&quot;: &quot;2024-04-15T14:57:08.134414438Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;13bf8332f7a994336bdbd6640b6c658f2f25235e6da14b6730bef4565fe9cf3a&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;0bda6d6416515c79b593ad7ac865ca65ab6afab26a66aa9d0e788761a07a2e90&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;]#每个网络中有属于此网络的容器信息[root@ubuntu2004 ~]#docker network inspect test-net[ &#123; &quot;Name&quot;: &quot;test-net&quot;, &quot;Id&quot;: &quot;f873ba3c38f8c05eb10263837c0b796b04ecb317dccf0d2a4f66ff59c22f71eb&quot;, &quot;Created&quot;: &quot;2024-04-20T07:06:54.271353858Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.27.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.27.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;00ae9c265b790098f6fd2882fef814b7b61297d8231f716c1421f4494b6eb522&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;7377fb1379c1b990f69bd2d0bb05e6099769ada034e6c9f2ec9ede2bda31562e&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:1b:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.27.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;] 4.2.2 让默认网络中容器test1可以连通自定义网络test-net的容器test2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#这种方式只是实现了单向连接，原理是在test1中加入test-net网卡信息，实现了test1可以ping通test2，但是test2无法ping通test1[root@ubuntu2004 ~]#docker network connect test-net test1[root@ubuntu2004 ~]#docker network inspect test-net[ &#123; &quot;Name&quot;: &quot;test-net&quot;, &quot;Id&quot;: &quot;f873ba3c38f8c05eb10263837c0b796b04ecb317dccf0d2a4f66ff59c22f71eb&quot;, &quot;Created&quot;: &quot;2024-04-20T07:06:54.271353858Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.27.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.27.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;00ae9c265b790098f6fd2882fef814b7b61297d8231f716c1421f4494b6eb522&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;7377fb1379c1b990f69bd2d0bb05e6099769ada034e6c9f2ec9ede2bda31562e&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:1b:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.27.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;13bf8332f7a994336bdbd6640b6c658f2f25235e6da14b6730bef4565fe9cf3a&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;383d5bb9bb9d459ed2da6d3a946aa3f29121f4231c07aa50c70230b35fea4dd1&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:1b:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.27.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123;&#125;, &quot;Labels&quot;: &#123;&#125; &#125;]#在test1容器中可以看到新添加了一个网卡,并且分配了test-net网络的IP信息/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever161: eth0@if162: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever165: eth1@if166: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:03 brd ff:ff:ff:ff:ff:ff inet 172.27.0.3/16 brd 172.27.255.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:3/64 scope link valid_lft forever preferred_lft forever #test1可以连接test2容器/ # ping 172.27.0.2PING 172.27.0.2 (172.27.0.2): 56 data bytes64 bytes from 172.27.0.2: seq=0 ttl=64 time=0.265 ms64 bytes from 172.27.0.2: seq=1 ttl=64 time=0.160 ms^C--- 172.27.0.2 ping statistics ---2 packets transmitted, 2 packets received, 0% packet lossround-trip min/avg/max = 0.160/0.212/0.265 ms#在test2容器中没有变化,仍然无法连接test1/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever163: eth0@if164: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link valid_lft forever preferred_lft forever / # ping -c1 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes^C--- 172.17.0.2 ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss 4.2.3 让自定义网络中容器test2可以连通默认网络的容器test1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#将自定义网络中的容器test2也加入到默认网络中,使之和默认网络中的容器test1通信[root@ubuntu2004 ~]#docker network connect bridge test2[root@ubuntu2004 ~]#docker network inspect bridge[ &#123; &quot;Name&quot;: &quot;bridge&quot;, &quot;Id&quot;: &quot;c29cd0fe53785ab7b6135a1ec76bc70eac3667c4df0baf73a9f620c0f27cdd9a&quot;, &quot;Created&quot;: &quot;2024-04-15T14:57:08.134414438Z&quot;, &quot;Scope&quot;: &quot;local&quot;, &quot;Driver&quot;: &quot;bridge&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;172.17.0.0/16&quot;, &quot;Gateway&quot;: &quot;172.17.0.1&quot; &#125; ] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: false, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: &#123; &quot;00ae9c265b790098f6fd2882fef814b7b61297d8231f716c1421f4494b6eb522&quot;: &#123; &quot;Name&quot;: &quot;test2&quot;, &quot;EndpointID&quot;: &quot;11767a6a5fbc764fbae7ac92460820d1fa3978dfe288d076c453f6bc6f0efabc&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:03&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.3/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;13bf8332f7a994336bdbd6640b6c658f2f25235e6da14b6730bef4565fe9cf3a&quot;: &#123; &quot;Name&quot;: &quot;test1&quot;, &quot;EndpointID&quot;: &quot;0bda6d6416515c79b593ad7ac865ca65ab6afab26a66aa9d0e788761a07a2e90&quot;, &quot;MacAddress&quot;: &quot;02:42:ac:11:00:02&quot;, &quot;IPv4Address&quot;: &quot;172.17.0.2/16&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;, &quot;Options&quot;: &#123; &quot;com.docker.network.bridge.default_bridge&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_icc&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.enable_ip_masquerade&quot;: &quot;true&quot;, &quot;com.docker.network.bridge.host_binding_ipv4&quot;: &quot;0.0.0.0&quot;, &quot;com.docker.network.bridge.name&quot;: &quot;docker0&quot;, &quot;com.docker.network.driver.mtu&quot;: &quot;1500&quot; &#125;, &quot;Labels&quot;: &#123;&#125; &#125;]#确认自定义网络的容器test2中添加了新网卡,并设置默认网络的IP信息/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever163: eth0@if164: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link valid_lft forever preferred_lft forever167: eth1@if168: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever #test2可以连接test1容器/ # ping -c1 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.511 ms--- 172.17.0.2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.511/0.511/0.511 ms#在test1中可以利用test2容器名通信/ # ping -c1 test2PING test2 (172.27.0.2): 56 data bytes64 bytes from 172.27.0.2: seq=0 ttl=64 time=0.080 ms--- test2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.080/0.080/0.080 ms#在test2中可以利test1容器名通信/ # ping -c1 test1PING test1 (172.27.0.3): 56 data bytes64 bytes from 172.27.0.3: seq=0 ttl=64 time=0.139 ms--- test1 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.139/0.139/0.139 ms 4.2.4 断开不同网络中容器的通信123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#将test1 断开和网络test-net中其它容器的通信[root@ubuntu2004 ~]#docker network disconnect test-net test1#在容器test1中无法和test2通信/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever161: eth0@if162: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever / # ping -c1 172.27.0.2PING 172.27.0.2 (172.27.0.2): 56 data bytes^C--- 172.27.0.2 ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss#在容器test2中仍能和test1通信/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever163: eth0@if164: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link valid_lft forever preferred_lft forever167: eth1@if168: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff inet 172.17.0.3/16 brd 172.17.255.255 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:3/64 scope link valid_lft forever preferred_lft forever / # ping -c1 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.085 ms#将test2 断开和默认网络中其它容器的通信[root@ubuntu2004 ~]#docker network disconnect bridge test2#在容器test2中无法和test1通信/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever163: eth0@if164: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:1b:00:02 brd ff:ff:ff:ff:ff:ff inet 172.27.0.2/16 brd 172.27.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe1b:2/64 scope link valid_lft forever preferred_lft forever / # ping -c1 172.17.0.2PING 172.17.0.2 (172.17.0.2): 56 data bytes^C--- 172.17.0.2 ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss 5 跨主机网络同一个宿主机之间的各个容器之间是可以直接通信的，但是如果访问到另外一台宿主机的容器呢？接下来三种方式实现跨宿主机的容器之间网络互联 5.1 方式1: 利用桥接实现跨宿主机的容器间互联将物理网卡桥接在br0上，这种方式不推荐 123456789101112131415161718#分别将两个宿主机都执行下面操作[root@ubuntu1804 ~]#apt -y install bridge-utils[root@ubuntu1804 ~]#brctl addif docker0 eth0#在两个宿主机上各启动一个容器,需要确保IP不同,相互测试访问#第一个宿主机的容器[root@ubuntu1804 ~]#docker run -it --name b1 busybox/ # hostname -i172.17.0.2/ # httpd -h /data/html/ -f -v[::ffff:172.17.0.3]:42488:response:200#第二个宿主机的容器[root@ubuntu1804 ~]#docker run -it --name b2 busybox/ # hostname -i172.17.0.3/#wget-q0 - http://172.17.0.2httpd website in busybox 5.2 方式2: 利用NAT实现跨主机的容器间互联5.2.1 Docker跨主机互联实现说明跨主机互联是说A宿主机的容器可以访问B主机上的容器，但是前提是保证各宿主机之间的网络是可以相互通信的，然后各容器才可以通过宿主机访问到对方的容器 实现原理: 是在宿主机做一个网络路由就可以实现A宿主机的容器访问B主机的容器的目的 注意: 此方式只适合小型网络环境，复杂的网络或者大型的网络可以使用其它网络插件比如： Google 开源的 Kubernetes 网络插件Flannel 和 Calico 进行互联 5.2.2 修改各宿主机网段Docker默认网段是172.17.0.x&#x2F;24，而且每个宿主机都是一样的，因此要做路由的前提就是各个主机的网络不能一致 5.2.2.1 第一个宿主机A上更改网段1234567891011121314151617181920212223242526272829303132333435363738[root@ubuntu1804 ~]#vim /etc/docker/daemon.json[root@ubuntu1804 ~]#cat /etc/docker/daemon.json&#123; &quot;bip&quot;: &quot;192.168.100.1/24&quot;, &quot;registry-mirrors&quot;: [&quot;https://si7y70hh.mirror.aliyuncs.com&quot;]&#125;[root@ubuntu1804 ~]# systemctl restart docker[root@ubuntu1804 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:6b:54:d3 brd ff:ff:ff:ff:ff:ff inet 10.0.0.101/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe6b:54d3/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:e0:ef:72:05 brd ff:ff:ff:ff:ff:ff inet 192.168.100.1/24 brd 192.168.100.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:e0ff:feef:7205/64 scope link valid_lft forever preferred_lft forever [root@ubuntu1804 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 5.2.2.2 第二个宿主机B更改网段12345678910111213141516171819202122232425262728293031323334353637[root@ubuntu1804 ~]#vim /etc/docker/daemon.json&#123; &quot;bip&quot;: &quot;192.168.200.1/24&quot;, &quot;registry-mirrors&quot;: [&quot;https://si7y70hh.mirror.aliyuncs.com&quot;]&#125;[root@ubuntu1804 ~]#systemctl restart docker[root@ubuntu1804 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:01:f3:0c brd ff:ff:ff:ff:ff:ff inet 10.0.0.102/24 brd 10.0.0.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe01:f30c/64 scope link valid_lft forever preferred_lft forever3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:e8:c0:a4:d8 brd ff:ff:ff:ff:ff:ff inet 192.168.200.1/24 brd 192.168.200.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:e8ff:fec0:a4d8/64 scope link valid_lft forever preferred_lft forever [root@ubuntu1804 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 0 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0192.168.200.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 5.2.3 在两个宿主机分别启动一个容器第一个宿主机启动容器server1 1234567891011121314151617[root@ubuntu1804 ~]#docker run -it --name server1 --rm alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever16: eth0@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueuestate UP link/ether 02:42:c0:a8:64:02 brd ff:ff:ff:ff:ff:ff inet 192.168.100.2/24 brd 192.168.100.255 scope global eth0 valid_lft forever preferred_lft forever / # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.100.1 0.0.0.0 UG 0 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 第二个宿主机启动容器server2 1234567891011121314151617[root@ubuntu1804 ~]#docker run -it --name server2 --rm alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever8: eth0@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueuestate UP link/ether 02:42:c0:a8:c8:02 brd ff:ff:ff:ff:ff:ff inet 192.168.200.2/24 brd 192.168.200.255 scope global eth0 valid_lft forever preferred_lft forever / # route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.200.1 0.0.0.0 UG 0 0 0 eth0192.168.200.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 从第一个宿主机的容器server1无法和第二个宿主机的server2相互访问 12345678910111213141516[root@ubuntu1804 ~]#docker run -it --name server1 --rm alpine sh/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever14: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueuestate UP link/ether 02:42:0a:64:00:02 brd ff:ff:ff:ff:ff:ff inet 10.100.0.2/16 brd 10.100.255.255 scope global eth0 valid_lft forever preferred_lft forever / # ping -c1 192.168.200.2PING 192.168.200.2 (192.168.200.2): 56 data bytes--- 192.168.200.2 ping statistics ---1 packets transmitted, 0 packets received, 100% packet loss 5.2.4 添加静态路由和iptables规则在各宿主机添加静态路由，网关指向对方宿主机的IP 5.2.4.1 在第一台宿主机添加静态路由和iptables规则12345678#添加路由，A宿主机容器要访问B宿主机容器，下一跳地址就是B宿主机的IP[root@ubuntu1804 ~]#route add -net 192.168.200.0/24 gw 10.0.0.102#修改iptables规则[root@ubuntu1804 ~]#iptables -A FORWARD -s 10.0.0.0/24 -j ACCEPT#或者修改FORWARD默认规则[root@ubuntu1804 ~]#iptables -P FORWARD ACCEPT 5.2.4.2 在第二台宿主机添加静态路由和iptables规则12345678#添加路由[root@ubuntu1804 ~]#route add -net 192.168.100.0/24 gw 10.0.0.101#修改iptables规则[root@ubuntu1804 ~]#iptables -A FORWARD -s 10.0.0.0/24 -j ACCEPT#或者修改FORWARD默认规则[root@ubuntu1804 ~]#iptables -P FORWARD ACCEPT 5.2.5 测试跨宿主机之间容器互联宿主机A的容器server1访问宿主机B容器server2，同时在宿主机B上tcpdump抓包观察 12345678910111213/ # ping -c1 192.168.200.2PING 192.168.200.2 (192.168.200.2): 56 data bytes64 bytes from 192.168.200.2: seq=0 ttl=62 time=1.022 ms--- 192.168.200.2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 1.022/1.022/1.022 ms#宿主机B的抓包可以观察到[root@ubuntu1804 ~]#tcpdump -i eth0 -nn icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes16:57:37.912925 IP 10.0.0.101 &gt; 192.168.200.2: ICMP echo request, id 2560, seq0, length 6416:57:37.913208 IP 192.168.200.2 &gt; 10.0.0.101: ICMP echo reply, id 2560, seq 0,length 64 宿主机B的容器server2访问宿主机B容器server1，同时在宿主机A上tcpdump抓包观察 12345678910111213/ # ping -c1 192.168.100.2PING 192.168.100.2 (192.168.100.2): 56 data bytes64 bytes from 192.168.100.2: seq=0 ttl=62 time=1.041 ms--- 192.168.100.2 ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 1.041/1.041/1.041 ms#宿主机A的抓包可以观察到[root@ubuntu1804 ~]#tcpdump -i eth0 -nn icmptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes16:59:11.775784 IP 10.0.0.102 &gt; 192.168.100.2: ICMP echo request, id 2560, seq0, length 6416:59:11.776113 IP 192.168.100.2 &gt; 10.0.0.102: ICMP echo reply, id 2560, seq 0,length 64 5.2.6 创建第三个容器测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#在第二个宿主机B上启动第一个提供web服务的nginx容器server3#注意无需打开端口映射[root@ubuntu1804 ~]#docker run -d --name server3 centos7-nginx:1.6.169fc554fd00e4f7880c139283b64f2701feafb91047b217906b188c1f461b699[root@ubuntu1804 ~]#docker exec -it server3 bash[root@69fc554fd00e /]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.200.3 netmask 255.255.255.0 broadcast 192.168.200.255 ether 02:42:c0:a8:c8:03 txqueuelen 0 (Ethernet) RX packets 8 bytes 656 (656.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 #从server1中访问server3的页面可以成功/ # ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever14: eth0@if15: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueuestate UP link/ether 02:42:0a:64:00:02 brd ff:ff:ff:ff:ff:ff inet 10.100.0.2/16 brd 10.100.255.255 scope global eth0 valid_lft forever preferred_lft forever / # wget -qO - http://192.168.200.3/appTest Page in app#从server3容器观察访问日志，可以看到来自于第一个宿主机，而非server1容器[root@69fc554fd00e /]# tail -f /apps/nginx/logs/access.log10.0.0.101 - - [02/Feb/2020:09:02:00 +0000] &quot;GET /app HTTP/1.1&quot; 301 169 &quot;-&quot;&quot;Wget&quot;#用tcpdump抓包80/tcp的包，可以观察到以下内容[root@ubuntu1804 ~]#tcpdump -i eth0 -nn port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes17:03:35.885627 IP 10.0.0.101.43578 &gt; 192.168.200.3.80: Flags [S], seq 3672256868, win 29200, options [mss 1460,sackOK,TS val 4161963574 ecr0,nop,wscale 7], length 017:03:35.885768 IP 192.168.200.3.80 &gt; 10.0.0.101.43578: Flags [S.], seq 2298407060, ack 3672256869, win 28960, options [mss 1460,sackOK,TS val 3131173298 ecr 4161963574,nop,wscale 7], length 0 5.3 方式3：macvlan和overlaymacvlan：docker自带，相当于在两台物理主机之间创建了通道，这是好处，但也是局限性，因为只有建立了通道的主机才可以通信 12345678910# 每台docker物理主机上都需要执行下述命令，创建好通道主机1： docker network create --driver macvlan --subnet=10.0.0.0/24 --gateway=10.0.0.254 -o parent=eth0 macvlan_1 #ip link set eth0 promsic on (unbuntu或其他版本需要) docker run -it --network macvlan_1 --ip=10.0.0.11 centos:6.9 /bin/bash 主机2: docker network create --driver macvlan --subnet=10.0.0.0/24 --gateway=10.0.0.254 -o parent=eth0 macvlan_2 #ip link set eth0 promsic on (unbuntu或其他版本需要) docker run -it --network macvlan_2 --ip=10.0.0.12 centos:6.9 /bin/bash overlay：容器内有两块网卡，一块是桥接在docker0上，另外一块是建立了一个新的设备，专门用于容器间通信，为支持容器跨主机通信，Docker 提供了 overlay driver，使用户可以创建基于 VxLAN 的 overlay 网络。VxLAN 可将二层数据封装到 UDP 进行传输，VxLAN 提供与 VLAN 相同的以太网二层服务，拥有更强的扩展性和灵活性 解决macvlan的问题，macvlan只打通了容器与容器之间的网络，没有打通对外服务的网络，在bridge的基础上，又加了一层封装，打通容器间的隧道，还有专门用于记录分配ip的功能的服务器（启容器即可），保证不会出现maclan的ip地址冲突问题 Docerk overlay 网络需要一个 key-value 数据库用于保存网络状态信息，包括 Network、Endpoint、IP 等。Consul、Etcd 和 ZooKeeper 都是 Docker 支持的 key-vlaue 软件，我们这里使用 Consul。 1234567891011121314151617181920212223# 1、启动consul服务，实现网络的统一配置管理docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap# 参考命令 docker run -d -p 8400:8400 -p 8500:8500 -p 8600:53/udp -h consul progrium/consul -server -bootstrap -ui-dir /ui#2、配置文件中增加配置,所有主机都需要# 主机1vim /lib/systemd/system/docker.service # consul地址都一样，cluster-advertise不一样ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.15.100:8500 --cluster-advertise=192.168.15.100:2376 # 主机2ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --cluster-store=consul://192.168.15.100:8500 --cluster-advertise=192.168.15.101:2376 # 所有主机运行systemctl daemon-reloadsystemctl restart docker#3、创建overlay网络，在一台主机创建完毕后，其余主机都可以看到docker network create -d overlay --subnet 10.10.10.0/24 --gateway 10.10.10.254 xxxxdocker network create -d overlay xxx#4、两边都启动容器测试docker run -it --network xxx --name test1 busybox sh","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"数据卷","slug":"Container/docker/volume","date":"2025-09-10T03:48:26.000Z","updated":"2025-09-10T12:03:33.948Z","comments":true,"path":"Container/docker/volume/","permalink":"https://aquapluto.github.io/Container/docker/volume/","excerpt":"","text":"1 容器的数据管理Docker镜像由多个只读层叠加而成，启动容器时，Docker会加载只读镜像层并在镜像栈顶部添加一个读写层 如果运行中的容器修改了现有的一个已经存在的文件，那该文件将会从读写层下面的只读层复制到读写层，该文件的只读版本仍然存在，只是已经被读写层中该文件的副本所隐藏，此即“写时复制(COW copy on write)”机制 如果将正在运行中的容器修改生成了新的数据，那么新产生的数据将会被复制到读写层，进行持久化保存，这个读写层也就是容器的工作目录，也为写时复制(COW) 机制。 COW机制节约空间，但会导致性低下，虽然关闭重启容器，数据不受影响，但会随着容器的删除，其对应的可写层也会随之而删除，即数据也会丢失。如果容器需要持久保存数据，不影响性能可以用数据卷技术实现 如下图是将对根的数据写入到了容器的可写层，但是把 &#x2F;data 中的数据写入到了一个另外的 volume 中用于数据持久化，本质上是直接将宿主机目录挂载至容器的指定的目录 范例：查看指定容器数据分层 123456789101112131415161718192021222324252627282930[root@ubuntu1804 ~]#docker inspect 12959f2c152f[root@ubuntu1804 ~]#ll -i /var/lib/docker/overlay2/848d77064091ba3ddd25a10ea6e0065af15ee701fed06f82804cf9ed58751761total 28920832 drwx------ 5 root root 4096 Jan 31 19:02 ./917524 drwx------ 53 root root 4096 Jan 31 19:02 ../920843 drwxr-xr-x 3 root root 4096 Jan 31 19:02 diff/920846 -rw-r--r-- 1 root root 26 Jan 31 19:02 link920851 -rw-r--r-- 1 root root 57 Jan 31 19:02 lower920818 drwxr-xr-x 1 root root 4096 Jan 31 19:02 merged/920847 drwx------ 3 root root 4096 Jan 31 19:02 work/[root@ubuntu1804 ~]#docker run -it alpine:3.11 sh/ # dd if=/dev/zero of=/root/test.img bs=1M count=1010+0 records in10+0 records out/ ##每个镜像层目录中包含了一个文件link，文件内容则是当前层对应的短标识符，镜像层的内容则存放在diff目录[root@ubuntu1804 ~]#find /var/lib/docker/overlay2/848d77064091ba3ddd25a10ea6e0065af15ee701fed06f82804cf9ed58751761 -name test.img -ls920903 10240 -rw-r--r-- 1 root root 10485760 Jan 31 19:02/var/lib/docker/overlay2/848d77064091ba3ddd25a10ea6e0065af15ee701fed06f82804cf9ed58751761/merged/root/test.img920903 10240 -rw-r--r-- 1 root root 10485760 Jan 31 19:02/var/lib/docker/overlay2/848d77064091ba3ddd25a10ea6e0065af15ee701fed06f82804cf9ed58751761/diff/root/test.img#删除容器后，所有容器数据目录都随之而删除[root@ubuntu1804 ~]#docker rm -f 12959f2c152f[root@ubuntu1804 ~]#ls /var/lib/docker/overlay2/848d77064091ba3ddd25a10ea6e0065af15ee701fed06f82804cf9ed58751761ls: cannot access &#x27;/var/lib/docker/overlay2/848d77064091ba3ddd25a10ea6e0065af15ee701fed06f82804cf9ed58751761&#x27;: No such file or directory Docker容器删除后，在容器中产生的数据还在吗？ 当一个Docker容器被删除时，默认情况下所有在该容器内生成的数据也会一并丢失，除非这些数据被存储在持久化存储（如卷、绑定挂载）中 Docker容器和外部机器可以直接交换文件吗？ 容器之间能进行数据交互？ Docker 网络：Docker提供了内置的网络功能，允许容器之间通过自定义的网络互相通信。你可以创建一个用户定义的桥接网络，让连接在这个网络上的所有容器能够通过服务名称相互访问。 共享卷：多个容器可以挂载同一个卷，从而实现对同一组文件的同时访问。这对于日志收集、缓存共享等场景非常有用。 2 数据卷介绍数据卷实际上就是宿主机上的目录或者是文件，可以被直接mount到容器当中使用实际生成环境中，需要针对不同类型的服务、不同类型的数据存储要求做相应的规划，最终保证服务的可扩展性、稳定性以及数据的安全性 数据卷是宿主机中的一个目录或文件。 当容器目录和数据卷目录绑定后，对方修改会立即同步。 一个数据卷可以同时被多个容器同时挂载。 一个容器也可以被挂载多个数据卷。 启动容器时，可以指定使用数据卷实现容器数据的持久化，数据卷有三种 指定宿主机目录或文件：指定宿主机的具体路径和容器路径的挂载关系，此方式不会创建数据卷 匿名卷：不指定数据名称，只指定容器内目录路径充当挂载点，docker自动指定宿主机的路径进行挂载，此方式会创建匿名数据卷，Dockerfile中VOLUME指定的卷即为此种 命名卷：指定数据卷的名称和容器路径的挂载关系，此方式会创建命名数据卷（常用） 3 数据卷使用命令1234567-v, --volume=[host-src:]container-dest[:&lt;options&gt;]&lt;options&gt;ro #从容器内对此数据卷是只读，不写此项默认为可读可写rw #从容器内对此数据卷可读可写,此为默认值host-src #宿主机目录如果不存在,会自动创建docker run -v &lt;宿主机绝对路径的目录或文件&gt;:&lt;容器目录或文件&gt; 将宿主机目录挂载容器目录，两个目录都可自动创建 这种方式不是使用数据卷的方式，而是相当于挂载 如果初始容器中有旧数据，将被宿主机目录覆盖 1docker run -v &lt;容器内路径&gt; 创建匿名卷 宿主机自动生成/var/lib/docker/volumes/&lt;卷ID&gt;/_data目录，并挂载至容器指定路径 如果初始容器中有旧数据，将被复制到宿主机数据卷目录 12docker run --name nginx -v /etc/nginx nginxdocker run -v &lt;卷名&gt;:&lt;容器目录路径&gt; 创建命名卷 命名卷将固定的存放在宿主机的/var/lib/docker/volumes/&lt;卷名&gt;/_data 如果初始容器中有旧数据，将被复制到宿主机数据卷目录 如果没有事先创建卷名，docker run时也会自动创建卷 1docker run -d -p 80:80 --name nginx01 -v vol1:/usr/share/nginx/html nginx 查看数据卷的挂载关系 1docker inspect --format=&quot;&#123;&#123;.Mounts&#125;&#125;&quot; &lt;容器ID&gt; 管理数据卷：docker volume COMMAND create：创建卷 inspect：显示一个或多个卷的详细信息 ls：列出卷 prune：删除所有未使用的本地卷，-f强制删除 rm：删除一个或多个卷 删除所有数据卷 123[root@ubuntu1804 ~]#docker volume rm `docker volume ls -q`说明：docker rm 的-v选项，会一并删除相关联的匿名卷 4 挂载实现持久化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@ubuntu2004 nginx-web-entrypoint]#vim DockerfileFROM nginx-alpine:1.24.0-v1.0LABEL maintainer=&quot;wujunlin &lt;root@wujunlin.com&gt;&quot;RUN mkdir -p /data/websiteCOPY entrypoint.sh /CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]ENTRYPOINT [&quot;/entrypoint.sh&quot;][root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v /opt/html:/data/website -d -p 80:80 --name web01 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v1.0928a2dcfed111e8611ac8a6e8531f7f3b0fc6f53fc2733bf7395c930f2812b2f[root@ubuntu2004 nginx-web-entrypoint]#docker inspect web01&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/opt/html&quot;, &quot;Destination&quot;: &quot;/data/website&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; [root@ubuntu2004 nginx-web-entrypoint]#docker exec web01 dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/ubuntu--vg-ubuntu--lv 10218772 4351480 5326620 45% /data/website [root@ubuntu2004 nginx-web-entrypoint]#ls /opt/html/index.html[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wangxiaochun.com#在宿主机改，容器里的也改了，并做了持久化[root@ubuntu2004 nginx-web-entrypoint]#vim /opt/html/index.htmlwww.wangxiaochun.com v1.0[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wangxiaochun.com v1.0#现在把容器删了[root@ubuntu2004 nginx-web-entrypoint]#docker rm -f `docker ps -qa`#数据还在[root@ubuntu2004 nginx-web-entrypoint]#cat /opt/html/index.html www.wangxiaochun.com v1.0#现在重新跑一个容器[root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v /opt/html:/data/website -d -p 80:80 --name web02 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v2.0da29b6d29010a5a6109cbec0bd91dacf5431c2e99e8aadcc29ec83801d3edefa#本应该是www.wangxiaochun.com v1.0，原因可能是在创建镜像或者跑容器时有内容覆盖了[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wangxiaochun.com[root@ubuntu2004 nginx-web-entrypoint]#vim entrypoint.sh#echo $&#123;HOST:-&quot;www.wangxiaochun.com&quot;&#125; &gt; $&#123;DOC_ROOT:-/apps/nginx/html&#125;/index.html[root@ubuntu2004 nginx-web-entrypoint]#echo www.wangxiaochun.com v1.0 &gt; /opt/html/index.html[root@ubuntu2004 nginx-web-entrypoint]#docker build -t nginx-web-entrypoint:1.24.0-v2.0 .#再重新跑[root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v /opt/html:/data/website -d -p 80:80 --name web03 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v2.0ed8f3d4613c26e2590a488b461e2ceaa82592179f2b0f1ff35da2ebdd395fd3e[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wangxiaochun.com v1.0 5 匿名数据卷12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v /data/website -d -p 80:80 --name web06 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v3.0[root@ubuntu2004 ~]#docker inspect web06&quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;volume&quot;, &quot;Name&quot;: &quot;bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3/_data&quot;, &quot;Destination&quot;: &quot;/data/website&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125; [root@ubuntu2004 ~]#tree /var/lib/docker/volumes/bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3//var/lib/docker/volumes/bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3/└── _data └── index.html [root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wu.org[root@ubuntu2004 nginx-web-entrypoint]#echo www.wu.org v1.0 &gt; /var/lib/docker/volumes/bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3/_data/index.html[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wu.org v1.0[root@ubuntu2004 nginx-web-entrypoint]#docker rm -f `docker ps -qa`#原来的还在[root@ubuntu2004 ~]#ls /var/lib/docker/volumes/backingFsBlockDev bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3 metadata.db#重新跑[root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v /data/website -d -p 80:80 --name web06 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v3.0#生成新的目录，所以持久化不了，之前的就相当于没删除也没人使用的垃圾目录[root@ubuntu2004 ~]#ls /var/lib/docker/volumes/8441053f8dc740acfd576eb64c8088094a2418efc976b38672e2b2c767cd63afbackingFsBlockDevbf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3metadata.db[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wu.org#删除容器也同时删除数据目录[root@ubuntu2004 nginx-web-entrypoint]#docker rm -f -v web06[root@ubuntu2004 ~]#ls /var/lib/docker/volumes/backingFsBlockDev bf743fc2c80955c57c14981ee3de0d0f60b71c866f267c410b7163cb73f4f7b3 metadata.db 6 命名数据卷1234567891011121314151617181920212223[root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v mydata:/data/website -d -p 80:80 --name web07 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v3.0[root@ubuntu2004 ~]#ls /var/lib/docker/volumes/backingFsBlockDev metadata.db mydata[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wu.org[root@ubuntu2004 ~]#echo www.wu.org v1.0 &gt; /var/lib/docker/volumes/mydata/_data/index.html [root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wu.org v1.0[root@ubuntu2004 nginx-web-entrypoint]#docker rm -f `docker ps -qa`[root@ubuntu2004 ~]#cat /var/lib/docker/volumes/mydata/_data/index.html www.wu.org v1.0#只要还使用mydata，即使名字不同，也是使用上一次的数据[root@ubuntu2004 nginx-web-entrypoint]#docker run -d -v mydata:/data/website -d -p 80:80 --name web09 -e DOC_ROOT=/data/website nginx-web-entrypoint:1.24.0-v3.0[root@rocky8 ~]#curl -H&quot;host: www.wangxiaochun.com&quot; 10.0.0.183www.wu.org v1.0 7 实现 wordpress 持久化12345678910111213141516171819202122232425[root@ubuntu2004 ~]#docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d -v /data/mysql:/var/lib/mysql --restart=always mysql:8.0.29-oracle[root@ubuntu2004 nginx-web-entrypoint]#ls /data/mysql/ auto.cnf client-cert.pem ib_logfile0 mysql.sock sys binlog.000001 client-key.pem ib_logfile1 performance_schema undo_001 binlog.000002 &#x27;#ib_16384_0.dblwr&#x27; ibtmp1 private_key.pem undo_002 binlog.index &#x27;#ib_16384_1.dblwr&#x27; &#x27;#innodb_temp&#x27; public_key.pem wordpress ca-key.pem ib_buffer_pool mysql server-cert.pem ca.pem ibdata1 mysql.ibd server-key.pem [root@ubuntu2004 ~]#docker run -d -p 80:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php7.4-apache[root@ubuntu2004 ~]#ls /data/wordpress/index.php wp-admin wp-config-sample.php wp-links-opml.php wp-settings.phplicense.txt wp-blog-header.php wp-content wp-load.php wp-signup.phpreadme.html wp-comments-post.php wp-cron.php wp-login.php wp-trackback.phpwp-activate.php wp-config-docker.php wp-includes wp-mail.php xmlrpc.php[root@ubuntu2004 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES8e6723f49772 wordpress:php7.4-apache &quot;docker-entrypoint.s…&quot; About a minute ago Up About a minute 0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp wordpressc96bbd49950a mysql:8.0.29-oracle &quot;docker-entrypoint.s…&quot; 2 minutes ago Up 2 minutes 0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp mysql浏览器访问：10.0.0.183#注意：数据库服务器的地址要输入宿主机的IP 初始化完成后 1234[root@ubuntu2004 ~]#ls /data/mysql/wordpress/wp_commentmeta.ibd wp_options.ibd wp_termmeta.ibd wp_term_taxonomy.ibdwp_comments.ibd wp_postmeta.ibd wp_term_relationships.ibd wp_usermeta.ibdwp_links.ibd wp_posts.ibd wp_terms.ibd wp_users.ibd 发布文章后 123456789[root@ubuntu2004 ~]#tree /data/wordpress/wp-content/uploads//data/wordpress/wp-content/uploads/└── 2024 └── 04 ├── 微信图片_20240419173724-1024x588.jpg ├── 微信图片_20240419173724-150x150.jpg ├── 微信图片_20240419173724-300x172.jpg ├── 微信图片_20240419173724-768x441.jpg └── 微信图片_20240419173724.jpg 验证数据持久化 1234567[root@ubuntu2004 ~]#docker rm -f `docker ps -qa`[root@ubuntu2004 ~]#docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d -v /data/mysql:/var/lib/mysql --restart=always mysql:8.0.29-oracle[root@ubuntu2004 ~]#docker run -d -p 80:80 --name wordpress -v /data/wordpress:/var/www/html --restart=always wordpress:php7.4-apache 浏览器访问：http://10.0.0.183/2024/04/19/ggs/ 8 数据目录的迁移docker容器引擎默认的数据目录：&#x2F;var&#x2F;lib&#x2F;docker，数据目录里会放什么？ 本地的镜像 容器里新增的数据（没有关联存储卷） 什么场景下需要迁移数据目录？数据目录挂载磁盘大小快要用完了，并且磁盘还不能扩容 如何迁移docker容器引擎的数据目录？ 停掉当前主机运行的容器 停掉docker服务 新增一块大盘，制作文件系统，挂载到一个新目录&#x2F;data（做成lvm之后再挂载） 迁移数据（cp -ra &#x2F;var&#x2F;lib&#x2F;docker &#x2F;data&#x2F;docker） 修改docker的配置文件，将数据目录执行新目录&#x2F;data&#x2F;docker 重新启动docker服务 启动容器","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"容器管理命令","slug":"Container/docker/container","date":"2025-09-10T03:48:17.000Z","updated":"2025-09-10T12:13:06.948Z","comments":true,"path":"Container/docker/container/","permalink":"https://aquapluto.github.io/Container/docker/container/","excerpt":"","text":"1 容器说明运行一个 Docker 镜像时，它会创建一个叫做容器的独立运行环境，就好像是在一个虚拟的小计算机中运行应用程序。它包含了一个完整的应用程序以及运行这个应用程序所需要的一切。 容器和镜像一样，也是若干层的叠加，唯一区别是所有只读层的最上面一层，是一层可读可写层，可以记住这个简单的公式：容器 &#x3D; 容器镜像 + 可读可写层 容器生命周期 启动容器时运行的命令本质就是文件，文件一定存在于文件系统中，即启动容器时运行的命令一定是存在于容器的文件系统中的 一个正在运行的容器它的文件系统 &#x3D; lowerdir（镜像层、只读）+ upperdir（容器层，可写） 详细的说：run启动容器之初，因为容器内暂时没有任何新数据产生，所以容器内的文件系统的所有内容都来自于镜像，因为执行的命令本质就是文件，所以一定是存在于镜像中的 2 容器结构容器启动后，容器的进程如下，他们在宿主机中都会有映射 0号进程：containerd-shim-runc-v2 容器运行时守护进程，管理容器生命周期 1号进程：启动容器的命令 容器文件系统映射 cat /var/lib/docker/containers/&lt;ID&gt;/hostname 容器的主机名（6062eec79905） cat /var/lib/docker/containers/&lt;ID&gt;/resolv.conf 容器的DNS配置（继承宿主机的&#x2F;etc&#x2F;resolv.conf）。 如果容器被删除，upperdir层的内容会随之丢失，但如果容器只是退出而没有被删除，upperdir层的数据仍然会保留，因此数据不会丢失 3 管理容器3.1 启动容器 1docker run [OPTION] [镜像名] [shell命令] [参数] it：分配伪终端给容器，并支持标准输入 d：后台运行容器 h：指定容器主机名 -name：指定容器名 -rm：容器退出时自动移除容器 -entrypoint：覆盖ENTRYPOINT的指令 e：设置环境变量 -env-file：读取环境变量的行分隔文件 说明： shell命令和参数，必须是前台执行的命令 容器启动后，如果容器内没有前台运行的进程，将自动退出停止 从容器内退出会一并停止容器 从容器内退出，且容器不停止（同时按三个键，ctrl+p+q） 容器需要有一个前台运行的进程才能保持容器的运行，可以在构建镜像的时候指定容器启动时运行的前台命令，也可以通过启动容器时传递运行参数实现 容器里的PID为1的守护进程的实现方式 服务类: 如: Nginx，Tomcat，Apache ，服务不能停 命令类: 如: tail -f &#x2F;etc&#x2F;hosts ，主要用于测试环境，注意: 不要 tail -f &lt;服务访问日志&gt; 会产生不必要的磁盘IO run命令需要注意点 在同一台宿主机上，容器名具备唯一性，不能与其他容器名冲突 run启动容器可以指定容器启动的第一条命令，即第一个进程，如果不指定那就使用镜像内默认的配置 容器内的1号进程的生命周期决定了整个容器的生命周期，所以1号进程必须满足两个特点 1号进程运行的命令必须是一个前台命令 1号进程运行的命令必须是一个能够一直运行的命令 范例：运行docker 的 hello world 1234567891011121314151617181920#现在没有镜像[root@ubuntu2004 ~]#docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE#会先去查找本地有无hello-world镜像，没有就去官网下载[root@ubuntu2004 ~]#docker run hello-world[root@ubuntu2004 ~]#docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest d2c94e258dcb 11 months ago 13.3kB#容器是一次性的，相对于进程，执行完之后就看不到了[root@ubuntu2004 ~]#docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES#加选项可以看到刚刚运行的，这个容器只是程序退出了，本身还在磁盘上[root@ubuntu2004 ~]#docker container ls --allCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfcd45352d079 hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago bold_edisonb6f9554bf425 hello-world &quot;/hello&quot; 3 minutes ago Exited (0) 3 minutes ago awesome_chaplygin 范例：运行nginx服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283[root@ubuntu2004 ~]#ip a3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:0e:63:bb:ee brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:eff:fe63:bbee/64 scope link valid_lft forever preferred_lft forever#从互联网下载nginx镜像，默认前台运行[root@ubuntu2004 ~]#docker run nginx2024/04/08 13:47:47 [notice] 1#1: start worker processes2024/04/08 13:47:47 [notice] 1#1: start worker process 292024/04/08 13:47:47 [notice] 1#1: start worker process 302024/04/08 13:47:47 [notice] 1#1: start worker process 312024/04/08 13:47:47 [notice] 1#1: start worker process 32[root@ubuntu2004 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES57c225f87ed8 nginx &quot;/docker-entrypoint.…&quot; About a minute ago Up About a minute 80/tcp elated_shockley[root@ubuntu2004 ~]#docker inspect 57c225f87ed8#或者[root@ubuntu2004 ~]#docker inspect elated_shockley.....&quot;Gateway&quot;: &quot;172.17.0.1&quot;&quot;IPAddress&quot;: &quot;172.17.0.2&quot;[root@ubuntu2004 ~]#curl 172.17.0.2&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;#访问日志就在前台运行这[root@ubuntu2004 ~]#docker run nginx172.17.0.1 - - [08/Apr/2024:13:49:59 +0000] &quot;GET / HTTP/1.1&quot; 200 615 &quot;-&quot; &quot;curl/7.68.0&quot; &quot;-&quot;#在另一个终端再次运行也不会像虚拟机那样端口冲突，因为他们俩的IP不同[root@ubuntu2004 ~]#docker run nginx[root@ubuntu2004 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESd8bc9fd39f3c nginx &quot;/docker-entrypoint.…&quot; 10 seconds ago Up 9 seconds 80/tcp hopeful_hamilton57c225f87ed8 nginx &quot;/docker-entrypoint.…&quot; 6 minutes ago Up 6 minutes 80/tcp elated_shockley[root@ubuntu2004 ~]#curl 172.17.0.2 -IHTTP/1.1 200 OKServer: nginx/1.25.4Date: Mon, 08 Apr 2024 13:54:05 GMTContent-Type: text/htmlContent-Length: 615Last-Modified: Wed, 14 Feb 2024 16:03:00 GMTConnection: keep-aliveETag: &quot;65cce434-267&quot;Accept-Ranges: bytes[root@ubuntu2004 ~]#curl 172.17.0.3 -IHTTP/1.1 200 OKServer: nginx/1.25.4Date: Mon, 08 Apr 2024 13:54:14 GMTContent-Type: text/htmlContent-Length: 615Last-Modified: Wed, 14 Feb 2024 16:03:00 GMTConnection: keep-aliveETag: &quot;65cce434-267&quot;Accept-Ranges: bytes[root@ubuntu2004 ~]#du -sh /var/lib/docker/574M /var/lib/docker/#将一个nginx服务停止[root@ubuntu2004 ~]#docker run nginx^C2024/04/08 14:00:37 [notice] 1#1: exit[root@ubuntu2004 ~]#du -sh /var/lib/docker/384M /var/lib/docker/#重新启动[root@ubuntu2004 ~]#docker start d8bc9fd39f3cd8bc9fd39f3c[root@ubuntu2004 ~]#du -sh /var/lib/docker/574M /var/lib/docker/ 原因是容器启动时，是从镜像中复制数据过来来启动，而停止容器后，会删除从镜像复制过来的数据，但是容器自己新生成的数据，比如日志，是不会删除的，所有数据不会丢失。当我们重新启动后，容器又重新从镜像复制数据过来 范例：指定容器名称，不指定就随机字符作为容器名 1[root@ubuntu2004 ~]#docker run --name a1 alpine 范例: 运行交互式容器并退出 123456789101112131415161718#运行交互式容器并退出[root@ubuntu1804 ~]#docker run -it docker.io/busybox sh/ # exit#用exit退出后容器停止[root@ubuntu1804 ~]#docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMEScd8c2a7f39d5 busybox &quot;sh&quot; 8 seconds ago Exited (0) 1 second ago vigorous_leakey[root@ubuntu1804 ~]#docker run -it docker.io/busybox sh/ #同时按三个键:ctrl+p+q#用同时按三个键ctrl+p+q退出后容器不会停止[root@ubuntu1804 ~]#docker ps -lCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMEScd8c2a7f39d5 busybox &quot;sh&quot; 8 seconds ago Up 10 seconds silly_villani 范例: 设置容器内的主机名 123456789101112[root@ubuntu1804 ~]#docker run -it --name a1 -h a1.wang.org alpine/ # hostnamea1.wang.org/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 a1.wang.org a1 #设置了主机名和当前容器IP的对应关系 范例: 一次性运行容器，退出后立即删除，用于测试 123456[root@ubuntu1804 ~]#docker run --rm alpine cat /etc/issueWelcome to Alpine Linux 3.11Kernel \\\\r on an \\\\m (\\\\l)[root@ubuntu1804 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 范例: 启动前台守护式容器 1234[root@ubuntu1804 ~]#docker run nginx#为了让busybox在前台运行，加一个wget前台执行的命令[root@ubuntu1804 ~]#docker run --rm --name b1 busybox wget -qO - 172.17.0.3 范例: 启动后台守护式容器 12345[root@ubuntu1804 ~]#docker run -d nginx#有些容器后台启动不会持续运行[root@ubuntu1804 ~]#docker run -d --name alpine4 alpine[root@ubuntu1804 ~]#docker run -td --name alpine5 alpine 3.2 容器重启策略1docker run --restart=[policy] policy 说明 no 默认为 no，退出时不会自动重启容器。 on-failure[:max-retries] 仅当容器以非零状态码退出时才重新启动退出状态。（可选）限制重试 Docker 的重启次数守护程序尝试。 always 无论退出状态如何，始终重新启动容器。当指定为Docker守护程序时，总是会尝试无限期地重新启动容器。这容器也将始终在守护程序启动时启动，无论容器现在的状态，利用此项可以确保实现开机自动启动容器 unless-stopped Docker 守护进程重启或服务器重启后，只要该容器之前是运行状态（即不是手动停止的），Docker 将自动重启该容器。 范例：开机自动运行容器 1docker run -d --name nginx --restart=always -p 80:80 nginx 3.3 暴露所有容器端口容器启动后，默认处于预定义的NAT网络中，所以外部网络的主机无法直接访问容器中网络服务 1docker run -P 将事先容器预定义的所有端口映射宿主机的网卡的随机端口，默认从32768开始 使用随机端口时，当停止容器后再启动可能会导致端口发生变化 端口映射的本质就是利用NAT技术实现 查看容器的端口映射关系：容器的端口映射关系 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@ubuntu2004 ~]#docker run -P --name nginx1 -d nginxea40e4534fba94b93a6ea2fcafe58711dbab72a478f7b0e3c9a833a860d5e7f7#前台启动的会话窗口无法进行其他操作，除非退出，但是退出后容器也会退出[root@ubuntu2004 ~]#docker run -P --name nginx1 nginx#官方nginx镜像文件只允许暴露nginx的80端口，此处-P选项将容器的80端口映射到了宿主机的32769端口[root@ubuntu2004 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESea40e4534fba nginx &quot;/docker-entrypoint.…&quot; 9 seconds ago Up 8 seconds 0.0.0.0:32769-&gt;80/tcp, :::32769-&gt;80/tcp nginx1[root@ubuntu2004 ~]#docker port nginx180/tcp -&gt; 0.0.0.0:3276980/tcp -&gt; [::]:32769[root@ubuntu2004 ~]#ss -ntl | grep 32769LISTEN 0 4096 0.0.0.0:32769 0.0.0.0:*LISTEN 0 4096 [::]:32769 [::]:*#生成DNAT规则[root@ubuntu2004 ~]#iptables -vnL -t natChain PREROUTING (policy ACCEPT 20 packets, 1560 bytes) pkts bytes target prot opt in out source destination 9 492 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT 2 packets, 156 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 0 0 MASQUERADE tcp -- * * 172.17.0.2 172.17.0.2 tcp dpt:80Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:32769 to:172.17.0.2:80[root@rocky8 ~]#curl 10.0.0.182:32769&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt; 3.4 指定端口映射1docker run -p [宿主机端口]:[容器端口] 说明： 将容器的预定义的指定端口映射到宿主机的相应端口 多个容器映射到宿主机的端口不能冲突，但容器内使用的端口可以相同 方式1: 容器80端口映射宿主机本地随机端口 1docker run -p 80 --name nginx-test-port1 nginx 方式2: 容器80端口映射到宿主机本地端口81 1docker run -p 81:80 --name nginx-test-port2 nginx 方式3: 宿主机本地IP:宿主机本地端口:容器端口 1docker run -p 10.0.0.100:82:80 --name nginx-test-port3 docker.io/nginx 方式4: 宿主机本地IP:宿主机本地随机端口:容器端口，默认从32768开始 1docker run -p 10.0.0.100::80 --name nginx-test-port4 docker.io/nginx 方式5: 宿主机本机ip:宿主机本地端口:容器端口&#x2F;协议，默认为tcp协议 1docker run -p 10.0.0.100:83:80/udp --name nginx-test-port5 docker.io/nginx 方式6: 一次性映射多个端口+协议 1docker run -p 8080:80/tcp -p 8443:443/tcp -p 53:53/udp --name nginx-test-port6 nginx 范例 1234567891011121314151617181920212223[root@centos7 ~]#docker run -d -p 8080:80 -p 8443:443 -p 8053:53/udp nginxa902b177bb7135ad8a8a179dbf8ce02dcc4806a1136475e59c2310833d7434ab[root@centos7 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESa902b177bb71 nginx &quot;nginx -g &#x27;daemon of…&quot; 5 seconds ago Up 4 seconds 0.0.0.0:8053-&gt;53/udp, 0.0.0.0:8080-&gt;80/tcp,0.0.0.0:8443-&gt;443/tcp affectionate_aryabhata[root@centos7 ~]#ss -ntpulNetid State Recv-Q Send-Q Local Address:Port Peer Address:Portudp UNCONN 0 0 127.0.0.1:323 *:* users:((&quot;chronyd&quot;,pid=6292,fd=1))udp UNCONN 0 0 ::1:323 :::* users:((&quot;chronyd&quot;,pid=6292,fd=2))udp UNCONN 0 0 :::8053 :::* users:((&quot;docker-proxy&quot;,pid=32671,fd=4))tcp LISTEN 0 128 *:22 *:* users:((&quot;sshd&quot;,pid=6623,fd=3))tcp LISTEN 0 100 127.0.0.1:25 *:* users:((&quot;master&quot;,pid=6748,fd=13))tcp LISTEN 0 128 :::8080 :::* users:((&quot;docker-proxy&quot;,pid=32659,fd=4))tcp LISTEN 0 128 :::22 :::* users:((&quot;sshd&quot;,pid=6623,fd=4))tcp LISTEN 0 100 ::1:25 :::* users:((&quot;master&quot;,pid=6748,fd=14))tcp LISTEN 0 128 :::8443 :::* users:((&quot;docker-proxy&quot;,pid=32646,fd=4))[root@centos7 ~]#iptables -vnL -t nat#杀死nginx进程，nginx将关闭，相应端口也会关闭[root@centos7 ~]#kill &lt;NGINXPID&gt; 范例：实现 wordpress 应用 123[root@ubuntu2004 ~]#docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d --restart=always mysql:8.0.29-oracle[root@ubuntu2004 ~]#docker run -d -p 8080:80 --name wordpress -v /data/wordpess:/var/www/html --restart=always wordpress:php7.4-apache 实战案例: 修改已经创建的容器的端口映射关系 12345678910111213141516171819202122232425262728[root@ubuntu1804 ~]#docker run -d -p 80:80 --name nginx01 nginxdc5d7c1029e582a3e05890fd18565367482232c151bba09ca27e195d39dbcc24[root@ubuntu1804 ~]#docker port nginx0180/tcp -&gt; 0.0.0.0:80[root@ubuntu1804 ~]#lsof -i:80COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdocker-pr 2364 root 4u IPv6 35929 0t0 TCP *:http (LISTEN)[root@ubuntu1804 ~]#ls /var/lib/docker/containers/dc5d7c1029e582a3e05890fd18565367482232c151bba09ca27e195d39dbcc24/checkpointshostconfig.json mountsconfig.v2.jsonhostname resolv.confdc5d7c1029e582a3e05890fd18565367482232c151bba09ca27e195d39dbcc24-json.log hosts resolv.conf.hash[root@ubuntu1804 ~]#systemctl stop docker[root@ubuntu1804 ~]#vim /var/lib/docker/containers/dc5d7c1029e582a3e05890fd18565367482232c151bba09ca27e195d39dbcc24/hostconfig.json&quot;PortBindings&quot;:&#123;&quot;80/tcp&quot;:[&#123;&quot;HostIp&quot;:&quot;&quot;,&quot;HostPort&quot;:&quot;80&quot;&#125;]&#125;#PortBindings后80/tcp对应的是容器内部的80端口，HostPort对应的是映射到宿主机的端口80 修改此处为8000&quot;PortBindings&quot;:&#123;&quot;80/tcp&quot;:[&#123;&quot;HostIp&quot;:&quot;&quot;,&quot;HostPort&quot;:&quot;8000&quot;&#125;]&#125;[root@ubuntu1804 ~]#systemctl start docker[root@ubuntu1804 ~]#docker start nginx01[root@ubuntu1804 ~]#docker port nginx0180/tcp -&gt; 0.0.0.0:8000 3.5 修改容器内部的hosts文件容器会自动将容器的ID加入自已的 &#x2F;etc&#x2F;hosts 文件中，并解析成容器的IP 12345678910111213docker run --add-host [domain]:[port][root@ubuntu1804 ~]#docker run -it --rm --add-host www.wangxiaochun.com:6.6.6.6 --add-host www.wang.org:8.8.8.8 busybox/ # cat /etc/hosts127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters6.6.6.6 www.wangxiaochun.com8.8.8.8 www.wang.org172.17.0.2 449bf0468efd 3.6 修改容器内部DNS12docker run --dns [ip]docker run --dns-search [domain] 说明： 容器的DNS服务器，默认采用宿主机的DNS地址 也可以在 /etc/docker/daemon.json 文件中指定容器的DNS服务器 -dns优先级会比在文件中定义的高 范例: 指定DNS地址 1234567[root@ubuntu1804 ~]#docker run -it --rm --dns 1.1.1.1 --dns 8.8.8.8 centos bash[root@ef9cacc74b58 /]# cat /etc/resolv.confsearch magedu.com wang.orgnameserver 1.1.1.1nameserver 8.8.8.8[root@ef9cacc74b58 /]# exitexit 范例: 指定domain名 12345[root@ubuntu1804 ~]#docker run -it --rm --dns 1.1.1.1 --dns 8.8.8.8 --dns-search a.com --dns-search b.com busybox/ # cat /etc/resolv.confsearch a.com b.comnameserver 1.1.1.1nameserver 8.8.8.8 范例: 配置文件指定DNS和搜索domain名 1234567891011121314151617181920212223242526[root@ubuntu1804 ~]#vim /etc/docker/daemon.json&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;registry-mirrors&quot;: [&quot;&lt;https://si7y70hh.mirror.aliyuncs.com&gt;&quot;], &quot;dns&quot; : [ &quot;114.114.114.114&quot;, &quot;119.29.29.29&quot;], &quot;dns-search&quot;: [ &quot;magedu.com&quot;, &quot;wang.org&quot;]&#125;[root@ubuntu1804 ~]#systemctl restart docker[root@ubuntu1804 ~]#docker run -it --rm centos bash[root@7a2d8fac6f6b /]# cat /etc/resolv.confsearch magedu.com wang.orgnameserver 114.114.114.114nameserver 119.29.29.29[root@7a2d8fac6f6b /]# exitexit#用--dns指定优先级更高[root@ubuntu1804 ~]#docker run -it --rm --dns 8.8.8.8 --dns 8.8.4.4 centos bash[root@80ffe3547b87 /]# cat /etc/resolv.confsearch magedu.com wang.orgnameserver 8.8.8.8nameserver 8.8.4.4[root@80ffe3547b87 /]# exitexit 3.7 传递环境变量有些容器运行时，需要传递变量，可以使用 -e &lt;参数&gt; 或 --env-file &lt;参数文件&gt; 实现 变量参考链接: https://hub.docker.com/_/mysql 范例：命令中定义 1docker run --name mysql-test1 -v /data/mysql:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=wordpress -e MYSQL_USER=wpuser -e MYSQL_PASSWORD=123456 -d -p 3306:3306 mysql:5.7.30 范例：文件中定义 1234567[root@ubuntu1804 ~]#cat env.listMYSQL_ROOT_PASSWORD=123456MYSQL_DATABASE=wordpressMYSQL_USER=wpuserMYSQL_PASSWORD=wppassdocker run --name mysql-test2 -v /root/mysql/:/etc/mysql/conf.d -v /data/mysql:/var/lib/mysql --env-file=env.list -d -p 3307:3306 mysql:5.7.30 3.8 容器扩展权限1docker run --privileged 说明： 容器内的root默认只是外部的一个普通用户权限 使用该参数，容器内的root拥有宿主机真正的root权限 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[root@centos8 ~]#podman run -it centos[root@382ab09932a7 /]#cat /etc/redhat-releaseCentOS Linux release 8.1.1911 (Core)[root@382ab09932a7 /]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 200G 0 disk|-sda1 8:1 0 1G 0 part|-sda2 8:2 0 100G 0 part|-sda3 8:3 0 50G 0 part|-sda4 8:4 0 1K 0 part`-sda5 8:5 0 2G 0 part [SWAP]sr0 11:0 1 7G 0 rom[root@382ab09932a7 /]# mount /dev/sda3 /mntmount: /mnt: permission denied.[root@382ab09932a7 /]# exitexit#利用--privileged 选项运行容器[root@centos8 ~]#podman run -it --privileged centos#可以看到宿主机的设备[root@a6391a8f82e3 /]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 200G 0 disk|-sda1 8:1 0 1G 0 part|-sda2 8:2 0 100G 0 part|-sda3 8:3 0 50G 0 part|-sda4 8:4 0 1K 0 part`-sda5 8:5 0 2G 0 part [SWAP]sr0 11:0 1 7G 0 rom[root@a6391a8f82e3 /]# dfFilesystem 1K-blocks Used Available Use% Mounted onoverlay 104806400 2754832 102051568 3% /tmpfs 65536 0 65536 0% /devtmpfs 408092 5892 402200 2% /etc/hostsshm 64000 0 64000 0% /dev/shmtmpfs 408092 0 408092 0% /sys/fs/cgroup[root@a6391a8f82e3 /]# mount /dev/sda3 /mnt[root@a6391a8f82e3 /]# dfFilesystem 1K-blocks Used Available Use% Mounted onoverlay 104806400 2754832 102051568 3% /tmpfs 65536 0 65536 0% /devtmpfs 408092 5892 402200 2% /etc/hostsshm 64000 0 64000 0% /dev/shmtmpfs 408092 0 408092 0% /sys/fs/cgroup/dev/sda3 52403200 619068 51784132 2% /mnt[root@a6391a8f82e3 /]# touch /mnt/containter.txt[root@a6391a8f82e3 /]# echo container data &gt; /mnt/containter.txt[root@a6391a8f82e3 /]# cat /mnt/containter.txtcontainer data[root@a6391a8f82e3 /]##在宿主机查看是否生成文件[root@centos8 ~]#ll /data/containter.txt-rw-r--r-- 1 root root 25 Feb 29 12:26 /data/containter.txt[root@centos8 ~]#cat /data/containter.txtcontainer data[root@centos8 ~]#echo host data &gt;&gt; /data/containter.txt[root@centos8 ~]#cat /data/containter.txtcontainer datahost data#在容器内可看文件是否发生变化[root@a6391a8f82e3 /]# cat /mnt/containter.txtcontainer datahost data 3.9 指定网络模式默认新建的容器使用Bridge模式 12docker run --network &lt;mode&gt;docker run --net=&lt;mode&gt; none host container:&lt;容器名或容器ID&gt; &lt;自定义网络名称&gt; 创建自定义网络 1docker network create -d &lt;mode&gt; --subnet &lt;CIDR&gt; --gateway &lt;网关&gt; &lt;自定义网络名称&gt; mode不支持host和none，默认是bridge模式 查看自定义网络信息 1docker network inspect &lt;自定义网络名称或网络ID&gt; 删除自定义网络 1doccker network rm &lt;自定义网络名称或网络ID&gt; 更加详细请看该文章的 3.5 小节：网络管理 3.10 删除容器1docker rm [OPTIONS] CONTAINER [CONTAINER...] f：强制移除正在运行的容器 v：删除与容器关联的卷 范例: 删除所有容器 1docker rm `docker ps -qa` 范例: 删除指定状态的容器 12# 删除所有停止的容器docker rm `docker ps -qf status=exited` 3.11 启动和停止容器1docker start|stop|restart|pause|unpause 容器ID/容器name 范例: 启动和停止所有容器 12docker start `docker ps -qa`docker stop `docker ps -qa` 范例: 暂停和恢复容器 12[root@ubuntu1804 ~]#docker pause n1[root@ubuntu1804 ~]#docker unpause n1 3.12 给正在运行的容器发信号1docker kill [OPTIONS] CONTAINER [CONTAINER...] s：发送到容器的信号（默认为“KILL”） 范例：重新加载配置 1docker kill -s 1 web01 范例：关闭所有容器 1docker kill `docker ps -qa` 3.13 进入正在运行的容器在运行中的容器启动新进程进入容器 1docker exec [OPTIONS] CONTAINER COMMAND [ARG...] it：分配伪终端给容器，并支持标准输入 d：后台运行命令 e：设置环境变量 范例：执行一次性命令 1docker exec 2478 cat /etc/redhat-release 范例：进入容器，exit退出但容器不停止，注意和docker run进入容器的区别 1docker exec -it 2478 bash 3.14 容器内和宿主机之间复制文件12docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATHdocker cp [OPTIONS] SRC_PATH CONTAINER:DEST_PATH a：存档模式（复制所有 uid&#x2F;gid 信息） L：始终遵循SRC_PATH中的符号链接 范例： 复制容器的文件至宿主机 12docker run -it --name b1 busybox shdocker cp b1:/bin/busybox /usr/local/bin/ 范例：复制宿主机文件到容器内 123456789101112131415161718[root@ubuntu1804 ~]#docker run -itd centos1311fe67e6708dac71c01f7d1752a6dcb5e85c2f1fa4ac2efcef9edfe4fb6bb5[root@ubuntu1804 ~]#docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1311fe67e670 centos &quot;/bin/bash&quot; 2 seconds ago Up 2 seconds elegant_khorana#将容器内文件复制到宿主机[root@ubuntu1804 ~]#docker cp -a 1311:/etc/centos-release .[root@ubuntu1804 ~]#cat centos-releaseCentOS Linux release 8.1.1911 (Core)#将宿主机文件复制到容器内[root@ubuntu1804 ~]#docker cp /etc/issue 1311:/root/[root@ubuntu1804 ~]#docker exec 1311 cat /root/issueUbuntu 18.04.1 LTS \\\\n \\\\l 3.15 在宿主机传送命令给容器执行有时候容器的镜像比较精简，许多命令都没有，我们通过在宿主机传送命令给容器执行 12345#查看容器的piddocker inspect 容器名/容器id | grep pid#执行命令nsenter -t pid -n 命令 4 查看容器信息4.1 容器的简单信息1docker ps [OPTION] q：只显示容器ID a：显示所有容器 f：根据提供的条件筛选输出 -format：按格式输出信息 &#123;&#123;.ID&#125;&#125;：容器的ID &#123;&#123;.Image&#125;&#125;：容器使用的镜像名称 &#123;&#123;.Command&#125;&#125;：容器的启动命令 &#123;&#123;.CreatedAt&#125;&#125;：容器的创建时间 &#123;&#123;.RunningFor&#125;&#125;：容器运行的时间 &#123;&#123;.Ports&#125;&#125;：容器的端口映射信息 &#123;&#123;.Status&#125;&#125;：容器的状态 &#123;&#123;.Size&#125;&#125;：容器的大小 &#123;&#123;.Names&#125;&#125;：容器的名称 &#123;&#123;.Label&#125;&#125;：容器的标签 范例: 1234567891011121314#显示运行的容器[root@ubuntu1804 ~]#docker ps#显示全部容器，包括退出状态的容器[root@ubuntu1804 ~]#docker ps -a#只显示容器ID[root@ubuntu1804 ~]#docker ps -a -q#显示容器大小[root@ubuntu1804 ~]#docker ps -a -s#显示最新创建的容器(停止的容器也能显示)[root@ubuntu1804 ~]#docker ps -l 范例: 显示指定状态的容器 1docker ps -f &#x27;status=exited&#x27; 4.2 容器的详细信息1docker inspect [OPTIONS] NAME|ID [NAME|ID...] f：使用给定的模板设置输出格式 1234567891011# 查看元数据docker inspect -f &quot;&#123;&#123;.Metadata&#125;&#125;&quot; test:v1.0# 查看状态docker inspect -f &quot;&#123;&#123;.State.Status&#125;&#125;&quot; elasticsearch# 查看ipdocker inspect -f &quot; &#123;&#123;.NetworkSettings.Networks.bridge.IPAddress&#125;&#125;&quot; elasticsearch# 查看数据卷挂载关系docker inspect --format=&quot;&#123;&#123;.Mounts&#125;&#125;&quot; nginx 4.3 容器内正在运行的进程信息123456789101112131415161718docker top CONTAINER [ps OPTIONS][root@ubuntu1804 ~]#docker run -d httpddb144f1978148242dc20bd0be951628f1c00371b2c69dee53d84469c52995d8f[root@ubuntu1804 ~]#docker top db144f19UID PID PPID C STIME TTY TIME CMDroot 9821 9797 3 22:02 ? 00:00:00 httpd -DFOREGROUNDdaemon 9872 9821 0 22:02 ? 00:00:00 httpd -DFOREGROUNDdaemon 9873 9821 0 22:02 ? 00:00:00 httpd -DFOREGROUND[root@ubuntu1804 ~]#docker run -d alpine /bin/sh -c &#x27;i=1;while true;do echo hello$i;let i++;sleep 1;done&#x27;9997053f9766d4adf709d46161d7ec6739eacafbe8d4721133874b89112ad1a6[root@ubuntu1804 ~]#docker top 9997UID PID PPID C STIME TTY TIME CMDroot 10023 9997 3 22:03 ? 00:00:00 /bin/sh -c i=1;while true;do echo hello$i;let i++;sleep 1;doneroot 10074 10023 0 22:03 ? 00:00:00 sleep 1 4.4 容器资源使用情况1docker stats [OPTIONS] [CONTAINER...] a：显示所有容器 -format：按格式输出信息 -no-stream：禁用流式统计信息，仅提取第一个结果 -no-trunc：不截断输出 说明： 如果不限制内存大小，容器就会一次性的将机器最大内存全部占用 限制内存使用大小，注意只是限制容器运行时的内存 123[root@ubuntu1804 ~]#docker stats 251c7c7cf2aaCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDSe240ca8cbaee magical_shamir 0.00% 6.828MiB / 3.797GiB 0.18% 876B / 656B 0B / 16.4kB 5 4.5 容器的日志1docker logs [OPTIONS] CONTAINER f：日志持续输出 t：显示时间戳 -tail：从日志末尾开始显示的行数（默认为“all”） docker 日志是存放在宿主机的 /var/lib/docker/containers/XXXXX/YYYYY-json.log 文件中 说明：这里记录的日志内容不是传统日志的内容，而是这个容器运行的前台命令执行的结果 12345678910111213[root@ubuntu1804 ~]#docker run -d alpine /bin/sh -c &#x27;i=1;while true;do echo hello$i;let i++;sleep 2;done&#x27;512622b006c05673630eb04f081f8475400b1cda786b0a8a5d1c1c2fd6dc56a7[root@ubuntu1804 ~]#docker logs 5126hello1hello2hello3hello4hello5hello6[root@ubuntu1804 ~]#docker logs --tail 0 -t 51262020-02-25T13:30:07.321390731Z hello17 4.6 容器的端口映射关系12345678docker port CONTAINER [PRIVATE_PORT[/PROTO]][root@centos7 ~]#docker port nginx-c1443/tcp -&gt; 0.0.0.0:844353/udp -&gt; 0.0.0.0:805380/tcp -&gt; 0.0.0.0:8080[root@centos7 ~]#docker port nginx-c1 53/udp0.0.0.0:8053 4.7 查看 docker run 启动参数命令忘记之前启动一个容器的启动命令是什么，现在需要找回来 docker run 的运行参数，可以使用 runlike 工具实现 1&lt;https://github.com/lavie/runlike&gt; 安装 runlike 123456#安装方式1: pipapt install -y python3-pippip3 install runlike#安装方法2: by dockeralias runlike=&quot;docker run --rm -v /var/run/docker.sock:/var/run/docker.sock assaflavie/runlike&quot; 范例： 1234567891011121314151617181920212223[root@ubuntu2204 ~]#apt install -y python3-pip[root@ubuntu2204 ~]#pip3 install runlike#启动一个测试容器[root@ubuntu2204 ~]#docker run -e MYSQL_ROOT_PASSWORD=123456 -eMYSQL_DATABASE=wordpress -e MYSQL_USER=wordpress -e MYSQL_PASSWORD=123456 --name mysql -d -v /data/mysql:/var/lib/mysql --restart=always mysql:8.0.29-oracle#查看容器运行命令[root@ubuntu2204 ~]#runlike -p mysqldocker run --name=mysql \\\\ --hostname=7c0f6ba11f5a \\\\ --mac-address=02:42:ac:11:00:02 \\\\ --env=MYSQL_ROOT_PASSWORD=123456 \\\\ --env=MYSQL_DATABASE=wordpress \\\\ --env=MYSQL_USER=wordpress \\\\ --env=MYSQL_PASSWORD=123456 \\\\ --volume=/data/mysql:/var/lib/mysql \\\\ --expose=3306 \\\\ --expose=33060 \\\\ --restart=always \\\\ --runtime=runc \\\\ --detach=true \\\\ mysql:8.0.29-oracle \\\\ mysqld 5 容器内使用GPUcpu：擅长逻辑控制，串行运算，cpu就好像一个老教授，老教授的特点是啥数学题都能算 gpu：擅长大规模的并发计算机，gpu就好像是一群只会算简单的加减法的小学生 5.1 nvidia-docker2.0 1、nvidia-docker 2.0 nvidia-docker2.0 是一个简单的包，它主要通过修改docker的配置文件&#x2F;etc&#x2F;docker&#x2F;daemon.json来让docker使用NVIDIA Container runtime。 2、nvidia-container-runtime nvidia-container-runtime 才是真正的核心部分，它在原有的docker容器运行时runc的基础上增加一个prestart hook，用于调用libnvidia-container库。 3、libnvidia-container libnvidia-container 提供一个库和一个简单的CLI工具，使用这个库可以使NVIDIA GPU被Linux容器使用。 创建GPU容器的流程如下：docker –&gt; dockerd –&gt; containerd –&gt; containerd-shim –&gt; nvidia-container-runtime –&gt; nvidia-container-runtime-hook –&gt; libnvidia-container –&gt; runc — &gt; container-process 只是把docker默认的运行时替换成了NVIDIA自家的nvidia-container-runtime。这样当nvidia-container-runtime创建容器时，先执行nvidia-container-runtime-hook这个hook去检查容器是否需要使用GPU(通过环境变NVIDIA_VISIBLE_DEVICES来判断)。如果需要则调用libnvidia-container来暴露GPU给容器使用。否则走默认的runc逻辑。 5.2 docker使用GPU容器默认都能访问GPU，随着docker的不断升级，对GPU的支持也越来越友好，尤其是docker19.03之后，不再需要安装nvidia-docker了。只安装NVIDIA-CONTAINER-RUNTIME就可以使用了，并且支持docker-compose。 要想启动一个容器使用gpu需要具备以下条件 宿主机上必须插一块gpu卡 宿主机上需要为该gpu卡安装驱动程序 安装官方的容器引擎，例如docker19.03以上版本容器引擎 配套安装一个nvidia-container-runtime 容器镜像内必须有cuda环境才行 启动容器采用参数–gpus指定临时启动gpu，或者修改配置文件把默认的runc替换为nvidia-container-runtime永久启动 5.2.1 下载GPU的驱动在 NVIDIA驱动程序页面 下载对应的驱动。 5.2.2 安装NVIDIA-CONTAINER-RUNTIME在https://nvidia.github.io/nvidia-container-runtime/查看支持的操作系统和版本，并根据对应选项添加源，以下是centos7.6的添加方式 123distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L &lt;https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo&gt; | \\\\sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo Ubuntu 123456789curl -s -L &lt;https://nvidia.github.io/nvidia-container-runtime/gpgkey&gt; | \\\\ sudo apt-key add -distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L &lt;https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list&gt; | \\\\ sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.listsudo apt-get update 在参考 https://github.com/NVIDIA/nvidia-container-runtime 后，得知直接安装即可 12sudo yum install nvidia-container-runtimesudo apt-get install nvidia-container-runtime 5.2.3 直接使用此时就准备好了GPU环境，在docker下就能通过–gpus参数直接使用了 查看 –gpus 参数是否安装成功： 12docker run --help | grep -i gpus --gpus gpu-request GPU devices to add to the container (&#x27;all&#x27; to pass all GPUs) 从Docker 19.03开始，安装好docker之后，只需要使用 –gpus 即可指定容器使用显卡。 1234567891、# 如果镜像里没有cuda环境，那看不到cuda信息，显示结果为CUDA Version: N/Adocker run -it --rm --gpus all centos nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 440.64.00 Driver Version: 440.64.00 CUDA Version: N/A |2、 强调采用的镜像里必须包含cudadocker run -it --rm --gpus all nvidia/cuda:9.0-base nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 440.64.00 Driver Version: 440.64.00 CUDA Version: 10.2 | 以上就完成了GPU的准备，可以使用了。（如果指定某一张卡可以使用选项：–gpus &quot;device=0&quot;） 当你发现容器内部发现CUDA Version: N&#x2F;A，就是因为容器内没有cuda的driver接口，需要在镜像里定制好 ,如下制作一个带有cuda环境的java镜像（注意cuda版本）。 123456ROM nvidia/cuda:10.1-baseMAINTAINER scsynCOPY ./jre1.8.0_271 /jreENV JRE_HOME=/jreENV CLASSPATH=$JRE_HOME/lib/rt.jar:$JRE_HOME/lib/extENV PATH=$PATH:$JRE_HOME/bin 使用 docker build -t 打包镜像。之后再使用的话在此基础上进行封装就可以了 更多操作 123456789101112131415161718192021222324252627282930所有显卡都对容器可见：docker run --gpus all --name 容器名 -d -t 镜像id只有显卡1对容器可见：docker run --gpus=&quot;1&quot; --name 容器名 -d -t 镜像id如果不指定 --gpus ，运行nvidia-smi 会提示Command not found注意：1. 显卡驱动在所有方式中，都要先安装好，容器是不会有显卡驱动的，一台物理机的显卡只对应一个显卡驱动，当显卡驱动安装好后（即使未安装cuda），也可以使用命令nvidia-smi2. nvidia-smi显示的是显卡驱动对应的cuda版本，nvcc -V 显示的运行是cuda的版本启动容器时，容器如果想使用gpu，镜像里必须有cuda环境，就是说，针对想使用gpu的容器，镜像在制作时必须吧cuda环境打进去下面三个参数代表的都是是容器内可以使用物理机的所有gpu卡 --gpus all NVIDIA_VISIBLE_DEVICES=all --runtime=nvidaNVIDIA_VISIBLE_DEVICES=2 只公开两个gpu，容器内只能用两个gpu举例如下：# 使用所有GPU$ docker run --gpus all nvidia/cuda:9.0-base nvidia-smi# 使用两个GPU$ docker run --gpus 2 nvidia/cuda:9.0-base nvidia-smi# 指定GPU运行$ docker run --gpus &#x27;&quot;device=1,2&quot;&#x27; nvidia/cuda:9.0-base nvidia-smi$ docker run --gpus &#x27;&quot;device=UUID-ABCDEF,1&quot;&#x27; nvidia/cuda:9.0-base nvidia-smi 5.3 修改容器的runtime为nvdia-runtime123456789101112131415161718192021222324252627282930313233343536373839把运行时添加到docker中，这个只是临时添加dockerd --add-runtime=nvidia=/usr/bin/nvidia-container-runtime把运行时添加到docker中，永久添加[root@yq01-aip-aikefu19 ~]# systemctl stop docker[root@yq01-aip-aikefu19 ~]# cat /etc/docker/daemon.json&#123; &quot;insecure-registries&quot;:[&quot;xxx-registry:5000&quot;, &quot;xxx.xxx-int.com&quot;], &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;, # 指定路径，还有id为nvida &quot;runtimeArgs&quot;: [] &#125; &#125;, &quot;default-runtime&quot;: &quot;nvidia&quot;, # 然后设定nvidia-runtime &quot;data-root&quot;: &quot;/data/lib/docker&quot;&#125;生效前查看默认的Runc[root@yq01-aip-aikefu19 ~]# docker info |grep -iw runtime: Default Runtime: runc此时，启动容器如果不指定--gpus参数，那根本无法使用nvida-smi命令，会报exec format error执行格式错误[root@yq01-aip-aikefu19 ~]# docker run --rm --name test nvidia/cuda:9.0-base nvidia-smi重启让修改生效[root@yq01-aip-aikefu19 ~]# systemctl start docker重新查看，发现默认的runtime已经被替换为了nvidia-runtime[root@yq01-aip-aikefu19 ~]# docker info |grep -iw runtime: Default Runtime: nvidia 验证 无需加--gpus参数，在容器内也能使用gpu[root@yq01-aip-aikefu19 ~]# docker run --rm --name test nvidia/cuda:9.0-base nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 440.64.00 Driver Version: 440.64.00 CUDA Version: 10.2 ||-------------------------------+----------------------+----------------------+ 5.4 查看GPU的信息1234567891011121314显示所有GPU的当前信息状态：nvidia-smi查询所有GPU的当前详细信息：nvidia-smi -q设备监控命令，以滚动条形式显示GPU设备统计信息：nvidia-smi dmon进程监控命令，以滚动条形式显示GPU进程状态信息：nvidia-smi pmon实时监测并高亮显示状态：watch -n 1 -d nvidia-smi，1代表间隔1s刷新。","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"镜像管理命令","slug":"Container/docker/image","date":"2025-09-10T03:48:09.000Z","updated":"2025-09-10T11:36:09.869Z","comments":true,"path":"Container/docker/image/","permalink":"https://aquapluto.github.io/Container/docker/image/","excerpt":"","text":"1 查看镜像的分层结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@ubuntu1804 ~]#docker pull nginxUsing default tag: latestlatest: Pulling from library/nginx8ec398bc0356: Pull complete #一层一层镜像拉取a53c868fbde7: Pull complete79daf9dd140d: Pull completeDigest: sha256:70821e443be75ea38bdf52a974fd2271babd5875b2b1964f05025981c75a6717Status: Downloaded newer image for nginx:latestdocker.io/library/nginx:latest#查看镜像分层历史[root@ubuntu2004 ~]#docker image history nginx#获取镜像的详细信息[root@ubuntu1804 ~]#docker inspect nginx#将镜像导出到一个压缩文件[root@ubuntu1804 ~]#docker save nginx -o nginx.tar[root@ubuntu1804 ~]#ll -h nginx.tar-rw------- 1 root root 131M Jul 20 22:33 nginx.tar[root@ubuntu1804 ~]#tar xf nginx.tar -C /data[root@ubuntu1804 ~]#ll /data-rw-r--r-- 1 root root 509 Jan 1 1970 manifest.json....[root@ubuntu1804 ~]#cat /data/manifest.json[ &#123; &quot;Config&quot;: &quot;0901fa9da894a8e9de5cb26d6749eaffb67b373dc1ff8a26c46b23b1175c913a.json&quot;, &quot;RepoTags&quot;: [&quot;nginx:latest&quot;], &quot;Layers&quot;: [ # 包含了一组字符串，每个字符串代表了组成该镜像的不同层 &quot;d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd/layer.tar&quot;, &quot;f4bf863ecdbb8bddb4b3bb271bdd97b067dcb6c95c56f720018abec6af190c6e/layer.tar&quot;, &quot;517e3239147277447b60191907bc66168963e0ce8707a6a33532f7c63a0d2f12/layer.tar&quot;, &quot;0bb74fcd4b686412f7993916e58c26abd155fa10b10a4dc09a778e7c324c39a2/layer.tar&quot;, &quot;68c9e9da52d5a57ee196829ce4a461cc9425b0b920689da9ad547f1da13dbc9d/layer.tar&quot; ] &#125;][root@ubuntu1804 ~]#du -sh /data/*8.0K[root@ubuntu1804 ~]#cd /data/d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd/[root@ubuntu1804 d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd]#lsjson layer.tar VERSION[root@ubuntu1804 d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd]#tar xf layer.tar[root@ubuntu1804 d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd]#lsbin dev home layer.tar lib64 mnt proc run srv tmp varboot etc json lib media opt root sbin sys usr VERSION[root@ubuntu1804 d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd]#cat etc/iinit.d/ issue issue.net[root@ubuntu1804 d2cf0fc540bb3be33ee7340498c41fd4fc82c6bb02b9955fca2109e599301dbd]#cat etc/issueDebian GNU/Linux 10 \\\\n \\\\l 2 搜索镜像12345678910111213141516#官方网站进行镜像的搜索&lt;http://hub.docker.com&gt;&lt;http://dockerhub.com&gt;&lt;https://hub-stage.docker.com/&gt;#docker search命令Usage: docker search [OPTIONS] TERMOptions: -f, --filter filter Filter output based on conditions provided --format string Pretty-print search using a Go template --limit int Max number of search results (default 25) --no-trunc Don&#x27;t truncate output说明:OFFICIAL: 官方AUTOMATED: 使用第三方docker服务来帮助编译镜像，可以在互联网上面直接拉取到镜像，减少了繁琐的编译过程 范例: 选择性的查找镜像 12345678#搜索点赞100个以上的镜像#旧语法[root@ubuntu1804 ~]#docker search -s 100 centosFlag --stars has been deprecated, use --filter=stars=3 instead......#新语法[root@ubuntu1804 ~]#docker search --filter=stars=100 centos 3 下载镜像镜像下载保存的路径：/var/lib/docker/overlay2/镜像ID 12345678910docker pull [OPTIONS] NAME[:TAG|@DIGEST]Options: -a, --all-tags Download all tagged images in the repository --disable-content-trust Skip image verification (default true) --platform string Set platform if server is multi-platform capable -q, --quiet Suppress verbose outputNAME #镜像名,一般的形式 仓库服务器:端口/项目名称/镜像名称:TAG #版本号,如果不指定:TAG,则下载最新版镜像 镜像下载说明 1234567[root@ubuntu1804 ~]#docker pull hello-worldUsing default tag: latest #默认下载最新版本latest: Pulling from library/hello-world1b930d010525: Pull complete #分层下载Digest: sha256:9572f7cdcee8591948c2963463447a53466950b3fc15a247fcad1917ca215a2f #摘要Status: Downloaded newer image for hello-world:latestdocker.io/library/hello-world:latest #下载的完整地址 4 查看镜像信息12docker inspect [REPOSITORY[:TAG]]docker images [OPTIONS] [REPOSITORY[:TAG]] q：只显示镜像ID a：显示所有的镜像 f：根据提供的条件筛选输出 dangling=true dangling状态表示 REPOSITORY 和 TAG 为none的镜像 -format：指定在输出中显示映像信息的格式 &#123;&#123;.Repository&#125;&#125;：映像的仓库名称 &#123;&#123;.Tag&#125;&#125;：映像的标签 &#123;&#123;.ID&#125;&#125;：映像的ID &#123;&#123;.Digest&#125;&#125;：映像的摘要值 &#123;&#123;.CreatedAt&#125;&#125;：映像的创建时间 &#123;&#123;.Size&#125;&#125;：映像的大小 执行结果的显示信息说明 12345REPOSITORY #镜像所属的仓库名称TAG #镜像版本号（标识符），默认为latestIMAGE ID #镜像唯一ID标识,如果ID相同,说明是同一个镜像有多个名称CREATED #镜像在仓库中被创建时间VIRTUAL SIZE #镜像的大小 说明：每个Repository仓库可以包含多个Tag(标签)，每个标签对应一个镜像 12docker images --format &quot;&#123;&#123;.Repository&#125;&#125;\\\\t&#123;&#123;.Tag&#125;&#125;\\\\t&#123;&#123;.Size&#125;&#125;&quot;docker images --format &quot;&#123;&#123;.CreatedAt&#125;&#125;\\\\t&#123;&#123;.Repository&#125;&#125;:&#123;&#123;.Tag&#125;&#125;&quot; | sort -k 1 -r Repository仓库 由某特定的docker镜像的所有迭代版本组成的镜像仓库 一个Registry中可以存在多个Repository Repository可分为“顶层仓库”和“用户仓库” Repository用户仓库名称一般格式为“用户名&#x2F;仓库名” 每个Repository仓库可以包含多个Tag(标签),每个标签对应一个镜像 12345678910111213141516171819202122232425[root@ubuntu1804 ~]#docker images#只显示ID[root@ubuntu1804 ~]#docker images -qe7d92cdc71fe470671670cac6d5fcfe5ff17fce289e99eb9#显示完整的ImageID[root@ubuntu1804 ~]#docker images --no-trunc#只查看指定REPOSITORY的镜像[root@ubuntu1804 ~]#docker images tomcat#只查看镜像的REPOSITORY和TAG[root@ubuntu2204 ~]#docker image ls --format &quot;&#123;&#123;.Repository&#125;&#125;:&#123;&#123;.Tag&#125;&#125;&quot;#查看dangling状态的镜像[root@ubuntu2204 ~]#docker images -f dangling=trueREPOSITORY TAG IMAGE ID CREATED SIZE&lt;none&gt; &lt;none&gt; 0584b370e957 11 months ago 141MB#删除[root@ubuntu2204 ~]#docker images -f dangling=true -q | xargs docker rmi -f 查看指定镜像的详细信息 12[root@ubuntu2204 ~]#docker inspect alpine:3.16.2[root@centos8 ~]#podman image inspect alpine 5 镜像导出1docker save [OPTIONS] IMAGE [IMAGE...] o：写入文件 说明：如果只写 IMAGE，便使用 IMAGE ID 导出，在导入后的镜像没有REPOSITORY和TAG，显示为&lt;none&gt;，所以IMAGE要写成REPOSITORY:TAG格式 123456#导出为tar格式docker save -o /path/file.tar IMAGE1 IMAGE2 ...docker save IMAGE1 IMAGE2 ... &gt; /path/file.tar#导出为压缩格式docker save IMAGE1 IMAGE2 ... | gzip &gt; /path/file.tar.gz码片 范例: 导出指定镜像 1234567[root@ubuntu1804 ~]#docker images[root@ubuntu1804 ~]#docker save mysql:5.7.30 alpine:3.11.3 -o /data/myimages.tar#或者[root@ubuntu1804 ~]#docker save mysql:5.7.30 alpine:3.11.3 &gt; /data/myimages.tar[root@ubuntu1804 ~]#scp /data/myimages.tar 10.0.0.7:/data 范例：导出镜像并压缩 123[root@ubuntu2204 ~]#docker save rockylinux:9.1-minimal | gzip &gt; rockylinux-9.1-minimal.tar.gz[root@ubuntu2204 ~]#ll rockylinux-9.1-minimal.tar.gz -h-rw------- 1 root root 44M 12æ•• 30 09:33 rockylinux-9.1-minimal.tar.gz 范例：导出所有镜像至不同的文件中 1docker images | awk &#x27;NR!=1&#123;print $1,$2&#125;&#x27; | while read repo tag;do docker save $repo:$tag -o /opt/$repo-$tag.tar ;done 范例：导出所有镜像到一个打包文件 1docker save `docker images | awk &#x27;NR!=1&#123;print $1&quot;:&quot;$2&#125;&#x27;` -o all.tar 6 镜像导入1docker load [OPTION] i：从 tar 存档文件中读取 12docker load -i /path/file.tardocker load &lt; /path/file.tar.gz 7 镜像删除1docker rmi [OPTIONS] IMAGE [IMAGE...] f：强制删除 说明：不允许删除正在运行的容器对应的镜像 范例：删除所有镜像 1docker rmi `docker images -qa` 范例：删除dangling状态的镜像 1docker rmi `docker images -q -f dangling=true` 8 镜像打标签12docker tag [旧镜像名]:[旧版本号] [新镜像名]:[新版本号]docker tag alpine alpine:3.11 9 构建镜像1docker build [OPTIONS] PATH | URL -no-cache：不使用之前构建中创建的缓存 f：指定Dockerfile文件名，默认为 PATH&#x2F;Dockerfile t：设置注册名称、镜像名称、标签 1docker build -t nginx:v1 /usr/local/src/nginx 10 查看镜像的构建历史12docker history 镜像IDdocker history 90201858b1fc","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"部署与配置","slug":"Container/docker/deploy","date":"2025-09-10T03:48:00.000Z","updated":"2025-09-10T11:33:35.056Z","comments":true,"path":"Container/docker/deploy/","permalink":"https://aquapluto.github.io/Container/docker/deploy/","excerpt":"","text":"1 部署Docker官方文档 阿里云文档 1.1 Ubuntu范例：内置仓库安装docker 123456[root@ubuntu2004 ~]#apt -y install docker.io[root@ubuntu2004 ~]#docker versionClient:Version: 20.10.12[root@ubuntu2004 ~]#docker info 范例：安装指定版本 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# step 1: 安装必要的一些系统工具[root@ubuntu2004 ~]#sudo apt-get update[root@ubuntu2004 ~]#sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# step 2: 安装GPG证书[root@ubuntu2004 ~]#curl -fsSL &lt;https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg&gt; | sudo apt-key add -# Step 3: 写入软件源信息[root@ubuntu2004 ~]#sudo add-apt-repository &quot;deb [arch=amd64] &lt;https://mirrors.aliyun.com/docker-ce/linux/ubuntu&gt; $(lsb_release -cs) stable&quot;# Step 4: 更新并安装Docker-CE[root@ubuntu2004 ~]#sudo apt-get -y update# 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:[root@ubuntu2004 ~]#apt-cache madison docker-ce[root@ubuntu2004 ~]#apt install docker-ce=5:26.0.0-1~ubuntu.20.04~focal docker-ce-cli=5:26.0.0-1~ubuntu.20.04~focal[root@ubuntu2004 ~]#docker versionClient: Docker Engine - Community Version: 26.0.0 API version: 1.45 Go version: go1.21.8 Git commit: 2ae903e Built: Wed Mar 20 15:17:51 2024 OS/Arch: linux/amd64 Context: defaultServer: Docker Engine - Community Engine: Version: 26.0.0 API version: 1.45 (minimum version 1.24) Go version: go1.21.8 Git commit: 8b79278 Built: Wed Mar 20 15:17:51 2024 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.6.28 GitCommit: ae07eda36dd25f8a1b98dfbf587313b99c0190bb runc: Version: 1.1.12 GitCommit: v1.1.12-0-g51d5e94 docker-init: Version: 0.19.0 GitCommit: de40ad0 删除docker 12[root@ubuntu ~]#apt purge docker-ce[root@ubuntu ~]#rm -rf /var/lib/docker 1.2 CentOS官方rpm包下载地址 阿里镜像下载地址 范例：通过阿里源下载 12345678#CentOS 7 安装docker依赖三个yum源:Base,Extras,docker-cewget -O /etc/yum.repos.d/CentOS-Base.repo &lt;http://mirrors.aliyun.com/repo/Centos-7.repo&gt;wget -O /etc/yum.repos.d/epel.repo &lt;http://mirrors.aliyun.com/repo/epel-7.repo&gt;wget -O /etc/yum.repos.d/docker-ce.repo &lt;https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&gt;yum clean allyum -y install docker-cesystemctl enable --now docker 范例：安装指定版本 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@rocky8 ~]#yum install -y yum-utils device-mapper-persistent-data lvm2[root@rocky8 ~]#cat /etc/yum.repos.d/docker.repo[docker-ce]name=docker-cebaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/8/x86_64/stable/gpgcheck=0[root@rocky8 ~]#yum list docker-ce --showduplicates[root@rocky8 ~]#yum list docker-ce-cli --showduplicates#如果出现报错，加--allowerasing 或者 --skip-broken 或者 --nobest选项[root@rocky8 ~]#yum install docker-ce-3:26.0.0-1.el8 docker-ce-cli-1:26.0.0-1.el8[root@rocky8 ~]#systemctl enable --now docker.service[root@rocky8 ~]#docker versionClient: Docker Engine - Community Version: 26.0.0 API version: 1.45 Go version: go1.21.8 Git commit: 2ae903e Built: Wed Mar 20 15:19:04 2024 OS/Arch: linux/amd64 Context: defaultServer: Docker Engine - Community Engine: Version: 26.0.0 API version: 1.45 (minimum version 1.24) Go version: go1.21.8 Git commit: 8b79278 Built: Wed Mar 20 15:17:57 2024 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.6.31 GitCommit: e377cd56a71523140ca6ae87e30244719194a521 runc: Version: 1.1.12 GitCommit: v1.1.12-0-g51d5e94 docker-init: Version: 0.19.0 GitCommit: de40ad0 删除 docker 1234[root@centos7 ~]#yum remove docker-ce#删除docker资源存放的相关文件[root@centos7 ~]#rm -rf /var/lib/docker 1.3 二进制离线安装本方法适用于无法上网或无法通过包安装方式安装的主机上安装docker 安装文档：https://docs.docker.com/install/linux/docker-ce/binaries/ 二进制安装下载路径 https://download.docker.com/linux/ https://mirrors.aliyun.com/docker-ce/linux/static/stable/x86_64/ 范例: 在CentOS8上实现二进制安装docker 123456789101112#下载二进制包[root@centos8 ~]#wget &lt;https://download.docker.com/linux/static/stable/x86_64/docker-19.03.5.tgz&gt;[root@centos8 ~]#tar xvf docker-19.03.5.tgz#启动dockerd服务[root@centos8 ~]#dockerd &amp;&gt;/dev/null &amp;[root@centos8 ~]#docker versionClient: Docker Engine - CommunityVersion: 19.03.5[root@centos8 ~]#docker run hello-world 范例: 创建 service文件 123456789101112131415161718192021222324252627282930313233343536373839[root@centos8 ~]#cat &gt; /lib/systemd/system/docker.service &lt;&lt;-EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerd -H unix://var/run/docker.sockExecReload=/bin/kill -s HUP \\\\$MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of dockercontainersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process# restart the docker process if it exits prematurelyRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.targetEOF[root@centos8 ~]#systemctl daemon-reload[root@centos8 ~]#systemctl enable --now docker 1.4 安装docker脚本1.4.1 Ubuntu1234567891011121314151617181920212223242526272829303132333435#!/bin/bash#Description: Install docker on Ubuntu18.04 and 20.04#Version:1.0#Date:2020-01-22COLOR=&quot;echo -e \\\\\\\\033[1;31m&quot;END=&quot;\\\\033[m&quot;DOCKER_VERSION=&quot;5:19.03.5~3-0~ubuntu-bionic&quot;install_docker()&#123; dpkg -s docker-ce &amp;&gt; /dev/null &amp;&amp; $&#123;COLOR&#125;&quot;Docker已安装，退出&quot;$&#123;END&#125; &amp;&amp; exit apt update apt -y install apt-transport-https ca-certificates curl software-properties-common #curl -fsSL &lt;https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg&gt; | sudo apt-key add - #add-apt-repository &quot;deb [arch=amd64] &lt;https://mirrors.aliyun.com/docker-ce/linux/ubuntu&gt; $(lsb_release -cs) stable&quot; curl -fsSL &lt;https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu/gpg&gt; | sudo apt-key add - add-apt-repository &quot;deb [arch=amd64] &lt;https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu&gt; $(lsb_release -cs) stable&quot; apt update $&#123;COLOR&#125;&quot;Docker有以下版本&quot;$&#123;END&#125; apt-cache madison docker-ce $&#123;COLOR&#125;&quot;5秒后即将安装: docker-&quot;$&#123;DOCKER_VERSION&#125;&quot; 版本.....&quot;$&#123;END&#125; $&#123;COLOR&#125;&quot;如果想安装其它Docker版本，请按ctrl+c键退出，修改版本再执行&quot;$&#123;END&#125; sleep 5 apt -y install docker-ce=$&#123;DOCKER_VERSION&#125; docker-ce-cli=$&#123;DOCKER_VERSION&#125; mkdir -p /etc/docker tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;&lt;https://si7y70hh.mirror.aliyuncs.com&gt;&quot;]&#125;EOF systemctl daemon-reload systemctl enable --now docker docker version &amp;&amp; $&#123;COLOR&#125;&quot;Docker 安装成功&quot;$&#123;END&#125; || $&#123;COLOR&#125;&quot;Docker 安装失败&quot;$&#123;END&#125;&#125;install_docker 1.4.2 CentOS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596#!/bin/bash. /etc/init.d/functionsCOLOR_GREEN=&quot;echo -e \\\\\\\\E[1;32m&quot;COLOR_RED=&quot;echo -e \\\\\\\\E[1;31m&quot;END=&quot;\\\\\\\\E[0m&quot;check_os_version() &#123; if [ -f /etc/os-release ]; then . /etc/os-release if [ &quot;$ID&quot; = &quot;centos&quot; ]; then case $VERSION_ID in 7*) echo &quot;7&quot; ;; 8*) echo &quot;8&quot; ;; *) $&#123;COLOR_RED&#125;&quot;不支持的CentOS版本: $VERSION_ID&quot;$&#123;END&#125; exit 1 ;; esac else $&#123;COLOR_RED&#125;&quot;此脚本仅支持CentOS系统&quot;$&#123;END&#125; exit 1 fi else $&#123;COLOR_RED&#125;&quot;无法检测操作系统版本&quot;$&#123;END&#125; exit 1 fi&#125;install_docker() &#123; rpm -q docker-ce &amp;&gt; /dev/null &amp;&amp; action &quot;Docker已安装&quot; &amp;&amp; exit OS_VERSION=$(check_os_version) $&#123;COLOR_GREEN&#125;&quot;检测到CentOS $OS_VERSION，开始安装Docker...&quot;$&#123;END&#125; sleep 1 if [ &quot;$OS_VERSION&quot; = &quot;7&quot; ]; then VERSION=&quot;19.03.5-3.el7&quot; REPO_URL=&quot;https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo&quot; # REPO_URL=&quot;https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&quot; else VERSION=&quot;-19.03.13-3.el8&quot; REPO_CONTENT=&quot;[docker]name=dockergpgcheck=0baseurl=https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/8/x86_64/stable/#baseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/8/x86_64/stable/&quot; fi $&#123;COLOR_GREEN&#125;&quot;配置Docker yum源...&quot;$&#123;END&#125; if [ &quot;$OS_VERSION&quot; = &quot;7&quot; ]; then wget -P /etc/yum.repos.d/ $REPO_URL || &#123; $&#123;COLOR_RED&#125;&quot;下载repo文件失败，请检查网络配置!&quot;$&#123;END&#125; exit 1 &#125; else echo &quot;$REPO_CONTENT&quot; &gt; /etc/yum.repos.d/docker.repo fi $&#123;COLOR_GREEN&#125;&quot;开始安装Docker组件...&quot;$&#123;END&#125; yum clean all if [ &quot;$OS_VERSION&quot; = &quot;7&quot; ]; then yum -y install docker-ce-$VERSION docker-ce-cli-$VERSION || &#123; $&#123;COLOR_RED&#125;&quot;安装失败，请检查yum源配置&quot;$&#123;END&#125; exit 1 &#125; else yum -y install docker-ce$VERSION docker-ce-cli$VERSION || &#123; $&#123;COLOR_RED&#125;&quot;安装失败，请检查yum源配置&quot;$&#123;END&#125; exit 1 &#125; fi $&#123;COLOR_GREEN&#125;&quot;配置Docker镜像加速...&quot;$&#123;END&#125; mkdir -p /etc/docker cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://si7y70hh.mirror.aliyuncs.com&quot;]&#125;EOF systemctl enable --now docker if docker version &amp;&gt; /dev/null; then $&#123;COLOR_GREEN&#125;&quot;Docker安装成功&quot;$&#123;END&#125; else $&#123;COLOR_RED&#125;&quot;Docker安装失败&quot;$&#123;END&#125; exit 1 fi&#125;install_docker 1.4.3 二进制离线安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/bashDOCKER_VERSION=20.10.10URL=https://mirrors.aliyun.comprepare () &#123; if [ ! -e docker-$&#123;DOCKER_VERSION&#125;.tgz ];then wget $&#123;URL&#125;/docker-ce/linux/static/stable/x86_64/docker-$&#123;DOCKER_VERSION&#125;.tgz fi [ $? -ne 0 ] &amp;&amp; &#123; echo &quot;文件下载失败&quot;; exit; &#125;&#125;install_docker () &#123; tar xf docker-$&#123;DOCKER_VERSION&#125;.tgz -C /usr/local/ cp /usr/local/docker/* /usr/bin/ cat &gt; /lib/systemd/system/docker.service &lt;&lt;-EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network-online.target firewalld.serviceWants=network-online.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerExecStart=/usr/bin/dockerd -H unix://var/run/docker.sockExecReload=/bin/kill -s HUP \\\\$MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of dockercontainersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process# restart the docker process if it exits prematurelyRestart=on-failureStartLimitBurst=3StartLimitInterval=60s[Install]WantedBy=multi-user.targetEOF systemctl daemon-reload&#125;start_docker ()&#123; systemctl enable --now docker docker version&#125;prepareinstall_dockerstart_docker 2 Docker配置文件环境配置文件 123/etc/sysconfig/docker-network/etc/sysconfig/docker-storage/etc/sysconfig/docker Unit File 1/usr/lib/systemd/system/docker.service docker-ce 配置文件 1/etc/docker/daemon.json Docker Registry配置文件 1/etc/containers/registries.conf 范例: ubuntu 查看docker相关文件 12345#服务器端相关文件[root@ubuntu1804 ~]#dpkg -L docker-ce#客户端相关文件[root@ubuntu1804 ~]#dpkg -L docker-ce-cli 范例: CentOS7 查看docker相关文件 12[root@centos7 ~]#rpm -ql docker-ce[root@centos7 ~]#rpm -ql docker-ce-cli 3 Docker命令帮助docker 命令是最常使用的 docker 客户端命令，其后面可以加不同的参数以实现不同的功能 12345docker [OPTIONS] COMMANDCOMMAND分为Management Commands #指定管理的资源对象类型,较新的命令用法,将命令按资源类型进行分类,再进行command命令，方便使用Commands #对不同资源操作的命令不分类,使用容易产生混乱 范例 12345[root@ubuntu2004 ~]#docker image lsREPOSITORY TAG IMAGE ID CREATED SIZE[root@ubuntu2004 ~]#docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE docker 命令有很多子命令，可以用下面方法查看帮助 12345678#docker命令帮助man dockerdockerdocker --help#docker子命令帮助man docker-COMMANDdocker COMMAND --help 官方文档: 12&lt;https://docs.docker.com/reference/&gt;&lt;https://docs.docker.com/engine/reference/commandline/cli/&gt; 4 Docker相关信息和优化配置4.1 查看docker版本1[root@ubuntu1804 ~]#docker version 4.2 查看docker详解信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@ubuntu1804 ~]#docker infoClient:Debug Mode: false #client 端是否开启 debugServer: Containers: 2 #当前主机运行的容器总数 Running: 0 #有几个容器是正在运行的 Paused: 0 #有几个容器是暂停的 Stopped: 2 #有几个容器是停止的 Images: 4 #当前服务器的镜像数Server Version: 19.03.5 #服务端版本Storage Driver: overlay2 #正在使用的存储引擎Backing Filesystem: extfs #后端文件系统，即服务器的磁盘文件系统Supports d_type: true #是否支持 d_typeNative Overlay Diff: true #是否支持差异数据存储Logging Driver: json-file #日志类型,每个容器的标准输出以日志存放在/var/lib/docker/containers/&lt;CONTAINER ID&gt;/&lt;CONTAINER ID&gt;-json.logCgroup Driver: cgroupfs #Cgroups 类型Plugins: #插件Volume: local #卷Network: bridge host ipvlan macvlan null overlay #overlay 跨主机通信Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog #日志类型Swarm: inactive #是否支持 swarmRuntimes: runc #已安装的容器运行时Default Runtime: runc #默认使用的容器运行时Init Binary: docker-init #初始化容器的守护进程，即 pid 为 1 的进程containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339 #版本runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 #runc 版本init version: fec3683 #init 版本Security Options: #安全选项 apparmor #安全模块，&lt;https://docs.docker.com/engine/security/apparmor/&gt; seccomp #安全计算模块，即制容器操作，&lt;https://docs.docker.com/engine/security/seccomp/&gt; Profile: default #默认的配置文件Kernel Version: 4.15.0-29-generic #宿主机内核版本Operating System: Ubuntu 18.04.1 LTS #宿主机操作系统OSType: linux #宿主机操作系统类型Architecture: x86_64 #宿主机架构CPUs: 1 #宿主机 CPU 数量Total Memory: 962MiB #宿主机总内存Name: ubuntu1804.wang.org #宿主机 hostnameID: IZHJ:WPIN:BRMC:XQUI:VVVR:UVGK:NZBM:YQXT:JDWB:33RS:45V7:SQWJ #宿主机 IDDocker Root Dir: /var/lib/docker #宿主机关于docker数据的保存目录,建议使用独立SSD的磁盘,保证性能和空间Debug Mode: false #server 端是否开启 debugRegistry: &lt;https://index.docker.io/v1/&gt; #仓库路径Labels:Experimental: false #是否测试版Insecure Registries: 127.0.0.0/8 : #非安全的镜像仓库Registry Mirrors:&lt;https://si7y70hh.mirror.aliyuncs.com/&gt; #镜像仓库地址，可以在/etc/docker/daemon.json文件中指定Live Restore Enabled: false #是否开启活动重启 (重启docker-daemon 不关闭容器 )WARNING: No swap limit support #系统警告信息 (没有开启 swap 资源限制 ) 范例: 解决上述SWAP报警提示 官方文档 1234567891011121314[root@ubuntu1804 ~]#docker info......WARNING: No swap limit support[root@ubuntu1804 ~]# vim /etc/default/grubGRUB_DEFAULT=0GRUB_TIMEOUT_STYLE=hiddenGRUB_TIMEOUT=2GRUB_DISTRIBUTOR=`lsb_ release -i -s 2&gt; /dev/null || echo Debian`GRUB_CMDLINE_LINUX_DEFAULT=&quot;&quot;GRUB_CMDLINE_LINUX=&quot;net.ifnames=0 biosdevname=0 swapaccount=1&quot; #修改此行[root@ubuntu1804 ~]# update-grub[root@ubuntu1804 ~]# reboot 4.3 查看docker0网卡在docker安装启动之后，默认会生成一个名称为docker0的网卡并且默认IP地址为172.17.0.1的网卡 12345678910111213141516171819202122232425#ubuntu18.04安装docker后网卡配置[root@ubuntu1804 ~]#ip a4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UPgroup default link/ether 02:42:d3:26:ed:4e brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:d3ff:fe26:ed4e/64 scope link valid_lft forever preferred_lft forever#CentOS 7.6 安装docker后网卡配置[root@centos7 ~]#ip a3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:d2:81:c2:e0 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever#CentOS 8.1 安装docker后网卡配置[root@centos8 ~]#ip a3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue stateDOWN group default link/ether 02:42:f5:3e:65:b6 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever 4.4 docker存储引擎官方文档关于存储引擎的相关文档： https://docs.docker.com/storage/storagedriver/ https://docs.docker.com/storage/storagedriver/select-storage-driver/ Overlay2：Overlay 的升级版，到目前为止，所有 Linux 发行版推荐使用的存储类型，也是docker默认使用的存储引擎，需要磁盘分区支持d-type功能，因此需要系统磁盘的额外支持，相对AUFS来说，Overlay2资源消耗更少 修改存储引擎参考文档 范例：在CentOS7.2修改存储引擎 1234567891011[root@centos7 ~]#vim /lib/systemd/system/docker.service.....ExecStart=/usr/bin/dockerd -s overlay2 -H fd:// --containerd=/run/containerd/containerd.sock......#创建新的xfs分区,添加ftype特性,否则默认无法启动docker服务[root@centos7 ~]#mkfs.xfs -n ftype=1 /dev/sdb[root@centos7 ~]#mount /dev/sdb /var/lib/docker[root@centos7 ~]#systemctl daemon-reload[root@centos7 ~]#systemctl restart docker 注意：修改存储引擎会导致所有容器丢失，所以先备份再修改 12345678910#查看Ubuntu1804的默认存储引擎[root@ubuntu1804 ~]#docker info |grep StorageWARNING: No swap limit supportStorage Driver: overlay2#查看CentOS7.6的默认存储引擎[root@centos7 ~]#docker info |grep StorageWARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabledStorage Driver: overlay2 4.5 docker优化配置注意：这种方式只对新建的容器有效的，之前的容器不生效 12345678910111213141516171819202122232425262728293031323334353637[root@ubuntu2004 ~]#vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ #指向国内的镜像源 &quot;&lt;https://registry.docker-cn.com&gt;&quot;, &quot;&lt;http://hub-mirror.c.163.com&gt;&quot;, &quot;&lt;https://docker.mirrors.ustc.edu.cn&gt;&quot;, &quot;&lt;https://si7y70hh.mirror.aliyuncs.com/&gt;&quot; #私有阿里云账号自动生成的地址（控制台-容器镜像服务中心-镜像加速） ],#开启远程：&lt;https://docs.docker.com/config/daemon/remote-access/&gt; 或者 ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H fd://#tcp://10.0.0.10:2375这里指定远程的端口和IP地址 &quot;hosts&quot;: [&quot;unix:///var/run/docker.sock&quot;, &quot;tcp://10.0.0.10:2375&quot;], &quot;insecure-registries&quot;: [&quot;harbor.wang.org&quot;], #允许使用私有仓库，默认docker不允许从私有仓库拉取镜像 &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],#指定docker数据目录,新数据往新目录写，旧数据还在原来的目录，要想把旧数据迁移到新目录，先systemctl stop docker和systemctl stop docker.socket，然后将使用cp或者mv等命令将旧数据迁移到新目录，最后启动#新版24.0.0不支持，实现：ExecStart=/usr/bin/dockerd --data-root=/data/docker &quot;graph&quot;: &quot;/data/docker&quot;, &quot;max-concurrent-downloads&quot;: 10, #最大可以并发运行容器的数量 &quot;max-concurrent-uploads&quot;: 5, #最大可以并发下载镜像的数量 &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;300m&quot;, #指定容器日志文件的最大值 &quot;max-file&quot;: &quot;2&quot; #指定容器日志文件的个数，循环写入日志文件，即一个日志满，会写入第二个文件 &#125;, &quot;live-restore&quot;: true, #容器的运行不因为docker服务的重启而停止，默认重启服务后，容器会停止运行 &quot;dns&quot;: [&quot;8.8.8.8&quot;, &quot;8.8.4.4&quot;] #配置DNS &quot;proxies&quot;: &#123; #代理 &lt;https://docs.docker.com/network/proxy/&gt; &quot;default&quot;: &#123; &quot;httpProxy&quot;: &quot;&lt;http://proxy.example.com:3128&gt;&quot;, &quot;httpsProxy&quot;: &quot;&lt;https://proxy.example.com:3129&gt;&quot;, &quot;noProxy&quot;: &quot;*.test.example.com,.example.org,127.0.0.0/8&quot; &#125; &quot;tcp://docker-daemon1.example.com&quot;: &#123; &quot;noProxy&quot;: &quot;*.internal.example.net&quot; &#125; &#125;&#125;[root@ubuntu2004 ~]#systemctl daemon-reload ;systemctl restart docker.service 范例：Docker 实现代理功能 12345678910[root@ubuntu2204 ~]#mkdir -p /etc/systemd/system/docker.service.d[root@ubuntu2204 ~]#cat &gt;&gt; /etc/systemd/system/docker.service.d/http-proxy.conf &lt;&lt;EOF[Service]Environment=&quot;HTTP_PROXY=http://10.145.8.236:7890&quot;Environment=&quot;HTTPS_PROXY=http://10.145.8.236:7890&quot;Environment=&quot;NO_PROXY=127.0.0.0/8,172.17.0.0/16,10.0.0.0/24,10.244.0.0/16,192.168.0.0/16,wang.org,cluster.local&quot;EOF[root@ubuntu2204 ~]#systemctl daemon-reload &amp;&amp; systemctl restart docker.service 4.6 docker镜像源Docker可用的镜像源合集 镜像加速器 加速地址 是否是公共 Docker中国官方镜像 https://registry.docker-cn.com 是 阿里云 https:&#x2F;&#x2F;.mirror.aliyuncs.com 否 腾讯云 https://mirror.ccs.tencentyun.com 是 网易云 https://hub-mirror.c.163.com 是 Azure中国镜像 https://dockerhub.azk8s.cn 是 DaoCloud镜像站 http://f1361db2.m.daocloud.io 否 七牛云 https://reg-mirror.qiniu.com 是 百度 https://mirror.baidubce.com 是 4.7 docker相关目录12345678910111213141516#这个目录非常重要，存放了所有镜像和容器的相关文件#目录空间要大，而且要在高速固态磁盘上，提高docker运行的速度[root@ubuntu2004 ~]#ls /var/lib/docker/buildkit containers engine-id image network overlay2 plugins runtimes swarm tmp volumesbuildkit #存储与 BuildKit 构建工具相关的缓存和其他临时文件，包含构建过程中生成的中间层和其他资源。containers #保存容器的具体信息，每个容器都有一个对应的子目录，其中包含了该容器的配置文件、日志文件以及状态信息engine-id #存储 Docker 引擎的唯一标识符，在多节点集群环境中识别不同的 Docker 实例image #存放镜像元数据network #管理 Docker 网络配置overlay2 #存储使用 Overlay2 存储驱动的镜像层和容器文件系统，每个镜像层和容器都会在此目录下有相应的子目录。plugins #存放插件相关数据runtimes #配置和管理不同运行时环境swarm #存储 Docker Swarm 模式下的集群管理数据tmp #存放 Docker 在执行某些操作时所需的临时文件volumes #管理持久化卷 4.8 docker默认防火墙规则123456789101112131415161718192021222324252627282930313233#跟KVM一样，会生成大量的规则[root@ubuntu2004 ~]#iptables -vnLChain INPUT (policy ACCEPT 5422 packets, 77M bytes) pkts bytes target prot opt in out source destinationChain FORWARD (policy DROP 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 DOCKER-USER all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 DOCKER-ISOLATION-STAGE-1 all -- * * 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- * docker0 0.0.0.0/0 0.0.0.0/0 ctstate RELATED,ESTABLISHED 0 0 DOCKER all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 ACCEPT all -- docker0 docker0 0.0.0.0/0 0.0.0.0/0Chain OUTPUT (policy ACCEPT 4636 packets, 288K bytes) pkts bytes target prot opt in out source destinationChain DOCKER (1 references) pkts bytes target prot opt in out source destinationChain DOCKER-ISOLATION-STAGE-1 (1 references) pkts bytes target prot opt in out source destination 0 0 DOCKER-ISOLATION-STAGE-2 all -- docker0 !docker0 0.0.0.0/0 0.0.0.0/0 0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0Chain DOCKER-ISOLATION-STAGE-2 (1 references) pkts bytes target prot opt in out source destination 0 0 DROP all -- * docker0 0.0.0.0/0 0.0.0.0/0 0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0Chain DOCKER-USER (1 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 FORWARD链：用于处理 Docker 容器间的通信 DOCKER-USER: 将所有协议、所有端口的流量导向 DOCKER-USER 链。 DOCKER-ISOLATION-STAGE-1: 为容器间网络隔离的第一阶段规则，确保不同 Docker 网络之间的适当隔离。 接受来自任意接口但目标是 docker0 网桥且状态为 RELATED 或 ESTABLISHED 的所有流量 DOCKER: 将所有目的接口为 docker0 的流量导向 DOCKER 链。 接受源接口为 docker0 且目的接口不为 docker0 的所有流量。 接受源与目的接口均为 docker0 的所有流量。 Chain DOCKER 这是一个自定义链，用来处理发往 Docker 容器的数据包，特别是那些需要通过 docker0 网桥进入或离开容器的流量。 目前没有特定的规则，这意味着它会遵循调用它的链的默认行为。 应用：容器端口映射，不过一般不修改，因为这些规则是由 Docker 动态管理的 Chain DOCKER-ISOLATION-STAGE-1 第一阶段的隔离规则，确保不同 Docker 网络之间的适当隔离。 如果流量从 docker0 网桥出发且目的不是 docker0，则将其转发给 DOCKER-ISOLATION-STAGE-2 链。 否则返回到调用它的链继续处理。 Chain DOCKER-ISOLATION-STAGE-2 第二阶段的隔离规则。 如果流量的目的接口是 docker0，则丢弃该流量。 否则返回到调用它的链继续处理。 Chain DOCKER-USER 这是一条用户自定义链，用户可以在此链中添加额外的规则来定制 Docker 流量的行为，比如容器间通信 当前只有一个规则，即返回到调用它的链，意味着没有额外的用户自定义规则应用于 Docker 流量。 总结上述防火墙规则的作用 Docker 网络隔离：通过 DOCKER-ISOLATION-STAGE-1 和 DOCKER-ISOLATION-STAGE-2 实现不同 Docker 网络间的隔离，防止容器间不必要的直接通信。 容器通信控制：允许相关联的流量（如已建立的连接或相关连接）进出 Docker 网桥 docker0，同时允许容器之间以及容器到主机的流量。","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"Docker概论","slug":"Container/docker/conspect","date":"2025-09-10T03:47:51.000Z","updated":"2025-09-10T11:14:38.524Z","comments":true,"path":"Container/docker/conspect/","permalink":"https://aquapluto.github.io/Container/docker/conspect/","excerpt":"","text":"1 Docker概念docker 官网 帮助文档链接 docker 镜像 docker 中文网站 Docker 是基于 Linux 内核实现，最早采用 LXC 技术 ，LXC 是 Linux 原生支持的容器技术 ，可以提供轻量级的虚拟化 。 后来Docker 改为自己研发并开源的 runc 技术运行容器，彻底抛弃了LXC。它的三大理念是build(构建)、ship(运输)、 run(运行)，Docker遵从apache 2.0协议，并通过（namespace及cgroup等）来提供容器的资源隔离与安全保障等，所以Docke容器在运行时不需要类似虚拟机（空运行的虚拟机占用物理机6-8%性能）的额外资源开销，因此可以大幅提高资源利用率。 统一基础设施环境-docker环境 硬件的组成配置 操作系统的版本 运行时环境的异构 统一程序打包（装箱）方式-docker镜像 java程序 python程序 node.js程序 统一程序部署（运行）方式-docker容器 java-jar… → docker run… python manage.py runserver… → docker run… npm run dev … → docker run… 2 Docker的组成Docker1.11版之后将单体引擎分为了5大组成部分 docker-client： 客户端命令 dockerd守护进程： 全称docker daemon，主要提供客户端命令接口 用来响应Docker客户端（命令行CLI工具）发来的各种容器、镜像管理的任务 containerd服务： containerd独立负责容器运行时的生命周期（如创建、启动、停止、暂停、信号处理、删除） 其他一些比如镜像构建、存储卷管理、日志操作都是dockerd（docker daemon）的一些模块管理 containerd-shim： 该进程由containerd服务创建 每创建一个容器，都会启动一个containerd-shim进程，然后由该进程调用runc来具体创建容器 runc： 最早期docker只是把Runc单拿出来捐赠给了OCI来作为容器运行时的标准 即runc造出来的容器自然就符合OCI标准，使用runc创造出来的就是一个符合oci规范的标准容器 3 为何需要有containerd-shim这个进程？Runc创建出的容器需要依赖于某个进程或服务，现在创建一个docker容器的时候，Docker Daemon 并不能直接帮我们创建了，而是请求 containerd 服务来创建一个容器，当containerd 收到请求后，也不会直接去操作容器，而是创建一个叫做 containerd-shim 的进程。让这个进程去操作容器，我们指定容器进程是需要一个父进程来做状态收集、维持 stdin 等 fd 打开等工作的 如果容器直接依赖于containerd服务，即父进程是containerd，那意味着一台机器上启动的多个容器都统一依附于containerd，一旦containernd退出（例如重启docker服务），该主机上的所有容器都跟着一起退出了 于是引入了containerd-shim进程这种设计，由该进程调用runc来创建容器，容器创建出来之后就依赖于该进程，并且该进程虽然是由containerd服务创建的，但是containerd-shim进程的父进程是1号进程，即containerd只负责创建containerd-shim这个进程，创建出来之后containerd-shim进程就与containerd服务无关了，live-restore就是基于该设计而来。 所以说，此时containerd服务挂掉了，根本影响不到containerd-shim进程，而containerd-shim进程作为容器的依赖它不出问题就影响不到容器。 需要强调的是：runc 启动完容器后本身会直接退出，containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程。 总的来说，containerd-shim翻译为垫片，主要起到了解耦的作用，即containterd和runc独立开发，互不干扰，比如containterd是2.0版本，runc是1.0版本，只要两者遵守了containerd-shim的规则，就可以互相调用 4 containerd、containerd-shim及容器进程的关系 层级关系总结如下： 12345dockerd服务 containerd服务 containerd-shim进程---》runc---&gt;容器的1号进程 containerd-shim进程---》runc---&gt;容器的1号进程 containerd-shim进程---》runc---&gt;容器的1号进程 宿主机中依赖关系： dockerd服务没有依赖，父进程是1号进程 containerd服务没有依赖，父进程是1号进程 chontainerd-shim是由containerd创建的，但是containerd-shim父进程也是1号进程 容器中依赖关系： 容器内的1号进程是有依赖，依赖于containerd-shim，即父进程是containerd-shim 容器一旦结束，进入僵尸进程状态，会由containerd-shim来负责回收 容器的一些如stdin、fd等都需要依赖于containerd-shim管理，所以containerd-shim一旦结束，容器就跟着一块完蛋了 5 镜像和容器镜像和容器的区别 镜像：模板，文件，静态文件，只占磁盘空间，国际标准 容器：镜像副本，实例，进程，多实例，占内存，占磁盘，国际标准，具有隔离性 在生产中，管理虚拟机和管理容器是不一样的，虚拟机的生命周期很长，而容器的生命周期平均只有一两天，docker的生产流程如下 先搭建一个仓库，将所有需要用到的服务做成镜像，放到镜像仓库里 客户端机器可以通过docker bulid生成镜像，然后通过docker push放到镜像仓库中 客户端机器运行docker命令，连接到服务器端机器的守护进程(docker daemon)，服务器端机器的守护进程接收到指令后，进行各种操作 从互联网或者私有仓库拉取镜像(docker pull) 将镜像加载到内存中，变成一个容器，相当于有了进程(docker run)，注意：一个镜像可以跑多个容器 6 容器运行机制当我们使用docker run运行一个命令在容器中时，在容器运行时层面会发生什么？ 如果本地没有镜像，则从镜像 登记仓库(registry)拉取镜像 镜像被提取到一个写时复制（COW）的文件系统上，所有的容器层相互堆叠以形成一个合并的文件系统 为容器准备一个挂载点 从容器镜像中设置元数据，包括诸如覆盖 CMD、来自用户输入的ENTRYPOINT、设置SECCOMP规则等设置，以确保容器按预期运行 提醒内核为该容器分配某种隔离，如进程、网络和文件系统（ 命名空间(namespace)） 提醒内核为该容器分配一些资源限制，如 CPU 或内存限制（ 控制组(cgroup)） 传递一个 系统调用(syscall)给内核用于启动容器 设置 SELinux&#x2F;AppArmor","categories":[{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"}]},{"title":"Grafana","slug":"Monitor/grafana","date":"2025-08-27T10:35:42.000Z","updated":"2025-09-13T09:44:17.896Z","comments":true,"path":"Monitor/grafana/","permalink":"https://aquapluto.github.io/Monitor/grafana/","excerpt":"","text":"一、部署Grafana是一款基于go语言开发的通用可视化工具，支持从多种不同的数据源加载并展示数据，可作为其数据源的部分存储系统如下所示 TSDB：Prometheus、IfluxDB、OpenTSDB和Graphit 日志和文档存储：Loki和ElasitchSearch 分布式请求跟踪：Zipkin、Jaeger和SkyWalking等 SQL DB: MySQL、PostgreSQL和Microsoft SQL Server Grafana基础 默认监听于TCP协议的3000端口，支持集成其他认证服务，且能够通过&#x2F;metrics输出内建指标； 数据源（Data Source）：提供用于展示的数据的存储系统； 仪表盘（Dashboard）：组织和管理数据的可视化面板（Panel）； 团队和用户：提供了面向企业组织层级的管理能力； 选择合适的社区版 12https://grafana.com/grafana/download?pg=graf&amp;plcmt=deploy-box-1https://mirrors.aliyun.com/grafana/apt/pool/main/g/grafana/?spm=a2c6h.25603864.0.0.4cfa7dfbtmLGbG Ubuntu&#x2F;Debian系统上的部署步骤 12345~# apt-get install -y adduser libfontconfig1 musl~# VERSION=11.4.0~# wget https://dl.grafana.com/oss/release/grafana_$&#123;VERSION&#125;_amd64.deb~# wget https://mirrors.aliyun.com/grafana/apt/pool/main/g/grafana/grafana_$&#123;VERSION&#125;_amd64.deb~# dpkg -i grafana_$&#123;VERSION&#125;_amd64.deb RHEL&#x2F;CentOS系统上的部署步骤 123~]# VERSION=11.4.0~]# wget https://dl.grafana.com/oss/release/grafana-$&#123;VERSION&#125;-1.x86_64.rpm~]# yum install grafana-$&#123;VERSION&#125;-1.x86_64.rpm Docker容器运行方式 12345~# VERSION=11.4.0基于Alpine的Grafana Image~# docker run -d --name=grafana -p 3000:3000 grafana/grafana-oss基于Ubuntu的Grafana Image~# docker run -d --name=grafana -p 3000:3000 grafana/grafana:$&#123;VERSION&#125;-ubuntu docker-compose 123456789101112131415161718192021222324252627# 配置文件config.monitoringGF_SECURITY_ADMIN_PASSWORD=adminGF_USERS_ALLOW_SIGN_UP=false# yamlversion: &#x27;3.6&#x27;volumes: grafana_data: &#123;&#125;networks: monitoring: driver: bridgeservices: grafana: image: grafana/grafana:11.4.0 ports: - 3000:3000 volumes: - grafana_data:/var/lib/grafana - ./grafana/dashboards:/var/lib/grafana/dashboards - ./grafana/provisioning:/etc/grafana/provisioning env_file: - config.monitoring networks: - monitoring 默认账号和密码：admin k8s集群部署 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# grafana.yaml# 为grafana创建持久存储，用于存放插件等数据，挂载到容器的/var/lib/grafana下apiVersion: v1kind: PersistentVolumeClaimmetadata: name: grafana-pvc namespace: monitor labels: app: grafanaspec: storageClassName: nfs-client accessModes: - ReadWriteOnce resources: requests: storage: 2Gi---apiVersion: apps/v1kind: Deploymentmetadata: name: grafana namespace: monitorspec: selector: matchLabels: app: grafana template: metadata: labels: app: grafana spec: volumes: - name: storage persistentVolumeClaim: claimName: grafana-pvc securityContext: runAsUser: 0 # 必须以root身份运行 containers: - name: grafana image: grafana/grafana # 默认lastest最新，也可以指定版本grafana/grafana:10.4.4 imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: grafana env: # 配置 grafana 的管理员用户和密码的， - name: GF_SECURITY_ADMIN_USER value: admin - name: GF_SECURITY_ADMIN_PASSWORD value: admin readinessProbe: failureThreshold: 10 httpGet: path: /api/health port: 3000 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 30 livenessProbe: failureThreshold: 3 httpGet: path: /api/health port: 3000 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: cpu: 150m memory: 512Mi requests: cpu: 150m memory: 512Mi volumeMounts: - mountPath: /var/lib/grafana name: storage---apiVersion: v1kind: Servicemetadata: name: grafana namespace: monitorspec: type: NodePort ports: - port: 3000 selector: app: grafana 配置Prometheus采集Grafana metrics 1234root@ubuntu2004:/usr/local/prometheus# vim prometheus.yml - job_name: &quot;grafana&quot; static_configs: - targets: [&quot;10.0.0.200:3000&quot;] 二、添加数据源并导入Dashboard2.1 添加Prometheus数据源 2.2 导入内建的Dashboard 2.3 Grafana Dashboard示例 2.4 其他的Dashboard其他更多的Dashboard，下载 json文件，然后import，导入的时候要选择数据源Data Source 1https://grafana.com/grafana/dashboards/ 1234567891011121314151617181920212223# mysqlhttps://grafana.com/grafana/dashboards/17320-1-mysqld-exporter-dashboard/# redishttps://grafana.com/grafana/dashboards/11835-redis-dashboard-for-prometheus-redis-exporter-helm-stable-redis-ha/# mongodbhttps://grafana.com/grafana/dashboards/12079-mongodb/# nginxhttps://grafana.com/grafana/dashboards/12708# springboothttps://grafana.com/grafana/dashboards/4701-jvm-micrometer/# rabbitmqhttps://grafana.com/grafana/dashboards/4279-rabbitmq-monitoring/# 进程https://grafana.com/grafana/dashboards/8378/# blackboxhttps://grafana.com/grafana/dashboards/9965/ 2.5 参考模板node_exporter：8919、1860、15172(基于11074模板做的优化) redis_exporter：14615 mysqld_exporter：7362、 11323 配置主从复制推荐模板ID：11323 如果安装的是Mariadb推荐模板ID：13106 haproxy_exporter：367和2428 Nginx_exporter：14900 三、自定义图表 12# node01的cpu使用率(1 - sum(increase(node_cpu_seconds_total&#123;mode=&quot;idle&quot;, instance=~&quot;k8s-node-01&quot;&#125;[1m])) by (instance) / sum(increase(node_cpu_seconds_total&#123;instance=~&quot;k8s-node-01&quot;&#125;[1m])) by (instance)) * 100 这里的 instance 是写死的，但实际上我们是有很多机器的，需要展示多台服务器，可以使用变量的方式获取对应的主机组，以及主机，然后基于变量来完成图形创建 1(1 - sum(increase(node_cpu_seconds_total&#123;mode=&quot;idle&quot;, instance=~&quot;$node&quot;&#125;[1m])) by (instance) / sum(increase(node_cpu_seconds_total&#123;instance=~&quot;$node&quot;&#125;[1m])) by (instance)) * 100","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"https://aquapluto.github.io/tags/Grafana/"}]},{"title":"实现告警功能","slug":"Monitor/alertmanager/alarm","date":"2025-08-27T10:29:44.000Z","updated":"2025-09-13T10:05:02.243Z","comments":true,"path":"Monitor/alertmanager/alarm/","permalink":"https://aquapluto.github.io/Monitor/alertmanager/alarm/","excerpt":"","text":"添加告警规则更多告警规则参考 告警规则（Alert rule）是另一种定义在Prometheus配置文件中的PromQL表达式，它通常是一个基于查询语句的布尔表达式，该表达式负责触发告警 当告警规则中用的查询语句较为复杂时，可将其保存为记录规则 而后通过查询该记录规则生成的时间序列来参与比较，从而避免实时查询导致的较长时间延迟 类似于记录规则，告警规则（Alerting rule）也定义在独立的文件中，而后由Prometheus在rule_files配置段中加载 123rule_files: - &quot;rules/recording/*.yaml&quot; - &quot;rules/alert/*.yaml&quot; # 专用于加载告警规则相关的文件； Alerting rule配置参数类似于记录规则，有着类似或相关联功能的告警规则同样可以组织为group，从而为规则名称提供“名称空间”；一个组内的每个告警规则必须有个名称，且在该组内其名称必须唯一 12345678910111213141516171819202122232425262728293031323334353637383940# 每个规则文件，都是含有一至多个规则组（rule_group）的列表，# 这些列表项定义在顶级字段groups之下；groups: # 每个规则组都要有一个名字，且在当前文件中必须惟一； - name: &lt;string&gt; # 组内的规则每多长时间评估（计算）一次； [ interval: &lt;duration&gt; | default = global.evaluation_interval ] # 限制条件；对于告警规则，用于限制其最多可生成的告警数量， # 对于记录规则，用于限制其最多可生成的序列数量； [ limit: &lt;int&gt; | default = 0 ] # 该规则组中的规则列表，对于每条告警规则，要遵循规则语法 rules: # 告警规则的名称，其格式要遵循label取值的语法要求 - alert: &lt;string&gt; # 使用的PromQL表达式作为告警触发条件（布尔表达式）；每次评估周期到达时都会基于当前时间进行表达式计算， # 评估的结果即会成为pending/firing状态的告警； # 可以使用由Recording Rule定义的指标 expr: &lt;string&gt; # 到达触发告警的条件后，在真正发送告警之前要持续的时长； # 在满足该时长条件之前，告警状态为pending，满足时长后，转为firing； # 表达式的值为false时，告警将转为inactive状态 [ for: &lt;duration&gt; | default = 0s ] # 在告警实例上添加的标签， # 若已添加，则每次告警都会覆盖前一次的标签值； # 支持使用模板化字串； labels: [ &lt;labelname&gt;: &lt;tmpl_string&gt; ] # 在告警上添加的注解信息，支持使用模板化字串； # 不能被用于标识告警实例，经常用于存储告警摘要 annotations: [ &lt;labelname&gt;: &lt;tmpl_string&gt; ] blackbox-exporter的告警规则123456789101112131415161718192021222324252627282930groups:- name: blackbox rules: # Blackbox probe failed - alert: BlackboxProbeFailed expr: probe_success == 0 for: 0m labels: severity: critical annotations: summary: Blackbox probe failed (instance &#123;&#123; $labels.instance &#125;&#125;) description: &quot;Probe failed\\n VALUE = &#123;&#123; $value &#125;&#125;\\n LABELS = &#123;&#123; $labels &#125;&#125;&quot; # Blackbox probe slow HTTP - alert: BlackboxProbeSlowHttp expr: avg_over_time(probe_http_duration_seconds[1m]) &gt; 1 for: 1m labels: severity: warning annotations: summary: Blackbox probe slow HTTP (instance &#123;&#123; $labels.instance &#125;&#125;) description: &quot;HTTP request took more than 1s\\n VALUE = &#123;&#123; $value &#125;&#125;\\n LABELS = &#123;&#123; $labels &#125;&#125;&quot; # Blackbox probe slow ping - alert: BlackboxProbeSlowPing expr: avg_over_time(probe_icmp_duration_seconds[1m]) &gt; 1 for: 1m labels: severity: warning annotations: summary: Blackbox probe slow ping (instance &#123;&#123; $labels.instance &#125;&#125;) description: &quot;Blackbox ping took more than 1s\\n VALUE = &#123;&#123; $value &#125;&#125;\\n LABELS = &#123;&#123; $labels &#125;&#125;&quot; 主机CPU利用率 &gt; 85%；主机MEM利用率 &gt; 70%1234567891011121314151617181920groups:- name: hostStatsAlert rules: - alert: hostCpuUsageAlert expr: (1 - avg(irate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) by (instance))*100 &gt; 85 for: 1m labels: severity: critical annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; CPU usage high&quot; #摘要 description: &quot;&#123;&#123; $labels.instance &#125;&#125; CPU usage above 85% (current value: &#123;&#123; $value &#125;&#125;)&quot; #描述 - alert: hostMemUsageAlert expr: (1 - (node_memory_MemAvailable_bytes&#123;&#125; / (node_memory_MemTotal_bytes&#123;&#125;)))* 100 &gt; 70 for: 1m labels: severity: critical annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; MEM usage high&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125; MEM usage above 70% (current value: &#123;&#123; $value &#125;&#125;)&quot; 磁盘使用率 &gt; 70%12345678910111213groups:- name: general.rules rules: - alert: NodeFilesystemUsage expr: 100 - (node_filesystem_free_bytes&#123;mountpoint=&quot;/&quot;,fstype=~&quot;ext4|xfs&quot;&#125; / node_filesystem_size_bytes&#123;fstype=~&quot;ext4|xfs&quot;&#125; * 100) &gt; 70 for: 1m labels: severity: warning annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; : &#123;&#123; $labels.mountpoint &#125;&#125; 分区使用率过高&quot; description: &quot;&#123;&#123; $labels.instance &#125;&#125; : &#123;&#123; $labels.job &#125;&#125; : &#123;&#123;$labels.mountpoint&#125;&#125; 这个分区使用大于百分之70% (当前值: &#123;&#123; $value &#125;&#125;)&quot;#在相关主机上使用命令创建占用磁盘空间的大文件，以达到测试的目的。dd if=/dev/zero of=/test bs=1M count=10000 告警路由Alertmanager的route配置段支持定义“树”状路由表，入口位置称为根节点，每个子节点可以基于匹配条件定义出一个独立的路由分支； 所有告警都将进入路由根节点，而后进行子节点遍历； 若路由上的continue字段的值为false，则遇到第一个匹配的路由分支后即终止；否则，将继续匹配后续的子节点； route配置段的可用配置参数123456789101112131415161718192021222324252627282930313233343536373839# 分组时使用的标签，可通过prometheus告警规则中的标签名称将告警进行分组# 默认情况下所有的告警都组织在一起，而一旦指定分组标签，则Alertmanager将按这些标签进行分组，具有相同标签值的告警会被归为同一组# 例如group_by: [&#x27;alertname&#x27;]，如果有两个告警，一个alertname=&quot;HighCPU&quot;，另一个alertname=&quot;DiskFull&quot;，它们会被分成两组# group_by:[&#x27;…&#x27;] 禁用分组聚合[ group_by: &#x27;[&#x27; &lt;labelname&gt;, ... &#x27;]&#x27; ]# 警报是否继续匹配后续的子route规则，允许一个告警被多个接收者（receivers）处理[ continue: &lt;boolean&gt; | default = false ]# 匹配器，匹配prometheus告警规则中的labelmatch: [ &lt;labelname&gt;: &lt;labelvalue&gt;, ... ] # 正则匹配器，使用正则表达式匹配prometheus告警规则中的labelmatch_re: [ &lt;labelname&gt;: &lt;regex&gt;, ... ]# 一个匹配器列表，用于筛选哪些告警应该匹配当前的路由规则（该匹配哪个receiver）# 告警必须满足该列表所有匹配规则# 三个 token：一个有效的 Prometheus 标签名称；运算符：=、！=、=~ 或 ！~ 之一；UTF-8 字符串。可以用双引号括起来# matchers: [ &quot;foo = bar,baz&quot;, &quot;dings != bums&quot; ]matchers: [ - &lt;matcher&gt; ... ]# 指定告警接受器，对应receiver配置中的name[ receiver: &lt;string&gt; ]# 发出一组告警通知的初始等待时长；允许等待一个抑制告警到达或收集属于同一组的更多初始告警，通常是0到数分钟；[ group_wait: &lt;duration&gt; | default = 30s ]# 发送关于同一组新告警的消息之前，需要等待多久；新告警将被添加到已经发送了初始通知的告警组中；一般在5分钟或以上；[ group_interval: &lt;duration&gt; | default = 5m ]# 成功发送了告警后再次发送告警信息需要等待的时长，一般至少为4个小时；[ repeat_interval: &lt;duration&gt; | default = 4h ]# 子路由配置routes: [ - &lt;route&gt; ... ] 多团队协作： 使用 continue: true 将同一个告警发送给多个团队（如开发团队和运维团队）。 使用 matchers 根据告警的标签（如 team 或 service）将告警路由到不同的团队。 分级通知： 使用 matchers 匹配不同严重性的告警（如 severity = &quot;critical&quot;），并将它们发送给不同的接收者。 使用 continue: true 实现分级通知，例如先发送给值班人员，再发送给管理层。 告警路由示例第一步，在Prometheus上新增一些告警规则，例如下面的两个 1234567891011121314151617181920groups:- name: nodes_alerts rules: - alert: DiskWillFillIn12Hours expr: predict_linear(node_filesystem_free_bytes&#123;mountpoint=&quot;/&quot;&#125;[1h], 12*3600) for: 1m labels: severity: critital annotations: discription: Disk on &#123;&#123; $label.instance &#125;&#125; will fill in approximately 12 hours. - name: prometheus_alerts rules: - alert: PrometheusConfigReloadFailed expr: prometheus_config_last_reload_successful == 0 for: 3m labels: serverity: warning annotations: description: Reloading Prometheus configuration has failed on &#123;&#123; $labels.instance &#125;&#125;. 第二步，添加路由信息 123456789101112131415161718192021222324252627282930global: smtp_smarthost: &#x27;localhost:25&#x27; smtp_from: &#x27;alertmanager@magedu.com&#x27; smtp_require_tls: false templates:- &#x27;templates/*.yaml&#x27;route: group_by: [‘instance’] group_wait: 30s group_interval: 2m repeat_interval: 5m receiver: email-receiver #默认receiver routes: - match: severity: critical reciver: support-team - match_re: severity: ^(warning|critical)$ receiver: email-receiver receivers:- name: &#x27;email-receiver&#x27; email_configs: - to: ‘root@magedu.com‘, ‘root@localhost’- name: ‘support-team&#x27; email_configs: - to: ‘root@magedu.com‘, ‘root@localhost 告警模板一般来说，在告警规则文件的annotations中使用summary描述告警的概要信息， description用于描述告警的详细信息。同时Alertmanager的UI也会根据这两个标签值，显示告警信息。为了让告警信息具有更好的可读性，Prometheus支持模板化label和annotations的中标签的值。 Prometheus支持在告警中使用模板； 告警模板是指在告警中的标签和注解上引用时间序列的标签和样本值的方法； 它使用标准的Go语法，并暴露一些包含时间标签和值的变量； 引用当前告警实例中指定标签的值： &#123;&#123; $label. &#125;&#125;- 引用当前 PromQL表达式计算的样本值： &#123;&#123; $value &#125;&#125; 1、基于模板字符串。用户可以直接在Alertmanager的配置文件中使用模板字符串 若要在description注解中引用触发告警的时间序列上的instance和 job标签的值，可分别使用 “&#123;&#123; $label.instance &#125;&#125;” 和 “&#123;&#123; $label.job &#125;&#125;”； 例如，我们可以把此前的告警规则改为如下模板格式 1234567891011groups:- name: AllInstances rules: - alert: InstanceDown expr: up == 0 for: 1m annotations: title: &#x27;Instance &#123;&#123; $labels.instance &#125;&#125; down&#x27; description: &#x27;&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 1 minute.&#x27; labels: severity: &#x27;critical&#x27; 2、自定义可复用的模板文件 （1）创建自定义模板文件 email_template.tmpl 12345678910111213141516171819202122232425262728&#123;&#123; define &quot;email.default.html&quot; &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;告警主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;告警主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;恢复时间: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- end &#125;&#125; （2）修改alertmanager配置文件，将模版文件配置到alertmanager，并应用 1234567891011templates: - &#x27;/usr/local/alertmanager/email_template.tmpl&#x27;receivers:- name: &#x27;email-receiver&#x27; email_configs: headers: subject: &quot;&#123;&#123; .Status | toUpper &#125;&#125; &#123;&#123; .CommonLabels.env &#125;&#125;:&#123;&#123; .CommonLabels.cluster &#125;&#125; &#123;&#123; .CommonLabels.alertname &#125;&#125;&quot; # 邮件主题 html: &#x27;&#123;&#123; template &quot;email.default.html&quot; . &#125;&#125;&#x27; # 与模板文件中的define定义的模板名保持一致 send_resolved: true require_tls: false 告警接收者告警媒介及告警接收者 告警媒介：可用于发送告警信息的媒介，例如某个特定的SMTP Server、SMS、企业微信、钉钉等 告警接收者：特定告警媒介的一个信息目的地，例如SMTP Server场景中的邮件地址 mage@magedu.com 、企业微信场景中的某个微信号等 AlertManager支持多种告警媒介 Email: &lt;email_config&gt; WeChat: &lt;wechat_config&gt; WebHook: &lt;webhook_config&gt; 邮件告警123456789101112131415161718192021222324252627282930313233343536# 故障恢复后，是否发送通知；[ send_resolved: &lt;boolean&gt; | default = false ]# 邮件接收人，支持使用模板字串；to: &lt;tmpl_string&gt;# 告警邮件中使用的发件人[ from: &lt;tmpl_string&gt; | default = global.smtp_from ]# SMTP 服务器[ smarthost: &lt;string&gt; | default = global.smtp_smarthost ]# 发送邮件时将hello信息发往的目标SMTP Server；[ hello: &lt;string&gt; | default = global.smtp_hello ]# SMTP authentication information.[ auth_username: &lt;string&gt; | default = global.smtp_auth_username ][ auth_password: &lt;secret&gt; | default = global.smtp_auth_password ][ auth_secret: &lt;secret&gt; | default = global.smtp_auth_secret ][ auth_identity: &lt;string&gt; | default = global.smtp_auth_identity ]# SMTP TLS 要求。请注意，Go 不支持到远程 SMTP 终端节点的未加密连接。[ require_tls: &lt;bool&gt; | default = global.smtp_require_tls ]# TLS configuration.tls_config: [ &lt;tls_config&gt; ]# HTML格式的邮件内容，支持使用模板，默认将引用名为email.default.html的模板[ html: &lt;tmpl_string&gt; | default = &#x27;&#123;&#123; template &quot;email.default.html&quot; . &#125;&#125;&#x27; ]# 纯文本格式的邮件，支持使用模板字串[ text: &lt;tmpl_string&gt; ]# Further headers 电子邮件标头键/值对。覆盖通知实现之前设置的任何标头。[ headers: &#123; &lt;string&gt;: &lt;tmpl_string&gt;, ... &#125; ] 告警信息和故障恢复信息模板 email_template.tmpl 12345678910111213141516171819202122232425262728&#123;&#123; define &quot;email.default.html&quot; &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;告警主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123; range .Alerts &#125;&#125;=========start==========&lt;br&gt;告警程序: prometheus_alert &lt;br&gt;告警级别: &#123;&#123; .Labels.severity &#125;&#125; &lt;br&gt;告警类型: &#123;&#123; .Labels.alertname &#125;&#125; &lt;br&gt;告警主机: &#123;&#123; .Labels.instance &#125;&#125; &lt;br&gt;告警主题: &#123;&#123; .Annotations.summary &#125;&#125; &lt;br&gt;告警详情: &#123;&#123; .Annotations.description &#125;&#125; &lt;br&gt;触发时间: &#123;&#123; .StartsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;恢复时间: &#123;&#123; .EndsAt.Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt;=========end==========&lt;br&gt;&#123;&#123; end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- end &#125;&#125; 修改alertmanager配置文件，将模版文件配置到alertmanager，然后配置headers、html、send_resolved字段 1234567891011121314151617templates: - &#x27;/usr/local/alertmanager/email_template.tmpl&#x27;receivers:- name: &#x27;email-me&#x27; email_configs: - to: &#x27;298155013@qq.com&#x27; from: &#x27;298155013@qq.com&#x27; headers: subject: &quot;&#123;&#123; .Status | toUpper &#125;&#125; &#123;&#123; .CommonLabels.env &#125;&#125;:&#123;&#123; .CommonLabels.cluster &#125;&#125; &#123;&#123; .CommonLabels.alertname &#125;&#125;&quot; # 邮件主题 html: &#x27;&#123;&#123; template &quot;email.default.html&quot; . &#125;&#125;&#x27; # 与模板文件中的define定义的模板名保持一致 send_resolved: true smarthost: &#x27;smtp.qq.com:465&#x27; auth_username: &#x27;298155013@qq.com&#x27; auth_identity: &#x27;298155013@qq.com&#x27; auth_password: &#x27;liodvniebsbtbgii&#x27; require_tls: false 字段 含义 .Status 告警的当前状态 .CommonLabels.env 告警中所有实例共享的 env 标签值 .CommonLabels.cluster 告警中所有实例共享的 cluster 标签值 .CommonLabels.alertname 告警的名称，在告警规则中定义的 rules.alert 企业微信告警1234567891011121314151617181920212223242526272829# 故障恢复后是否发送通知；[ send_resolved: &lt;boolean&gt; | default = false ]# 调用WeChat API时使用的API Key，可在企业微信后台中添加的告警机器人上获取该信息；[ api_secret: &lt;secret&gt; | default = global.wechat_api_secret ]# The WeChat API URL.[ api_url: &lt;string&gt; | default = global.wechat_api_url ]# 企业微信中的企业ID[ corp_id: &lt;string&gt; | default = global.wechat_api_corp_id ]# 微信消息内容，支持使用模板格式化告警信息，默认使用名为“wechat.default.message”的模板[ message: &lt;tmpl_string&gt; | default = &#x27;&#123;&#123; template &quot;wechat.default.message&quot; . &#125;&#125;’ ]# 消息格式，支持`text` 和`markdown`；[ message_type: &lt;string&gt; | default = &#x27;text’ ]# 企业微信应用的唯一标识符[ agent_id: &lt;string&gt; | default = &#x27;&#123;&#123; template &quot;wechat.default.agent_id&quot; . &#125;&#125;&#x27; ]# 接收告警通知的用户，支持 @all 发送给所有人[ to_user: &lt;string&gt; | default = &#x27;&#123;&#123; template &quot;wechat.default.to_user&quot; . &#125;&#125;&#x27; ]# 接收告警通知的部门，支持多个部门 ID（逗号分隔）[ to_party: &lt;string&gt; | default = &#x27;&#123;&#123; template &quot;wechat.default.to_party&quot; . &#125;&#125;&#x27; ]# 接收告警通知的标签，支持多个标签（逗号分隔）[ to_tag: &lt;string&gt; | default = &#x27;&#123;&#123; template &quot;wechat.default.to_tag&quot; . &#125;&#125;&#x27; ] 告警信息和故障恢复信息模板 123456789101112131415161718192021222324252627282930313233343536&#123;&#123; define &quot;wechat.default.message&quot; &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 &#125;&#125;========= 监控报警 =========告警状态：&#123;&#123; .Status &#125;&#125;告警级别：&#123;&#123; .Labels.severity &#125;&#125;告警类型：&#123;&#123; $alert.Labels.alertname &#125;&#125;故障主机: &#123;&#123; $alert.Labels.instance &#125;&#125;告警主题: &#123;&#123; $alert.Annotations.summary &#125;&#125;告警详情: &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125;;触发阈值：&#123;&#123; .Annotations.value &#125;&#125;故障时间: &#123;&#123; ($alert.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;========= = end = =========&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 &#125;&#125;========= 异常恢复 =========告警类型：&#123;&#123; .Labels.alertname &#125;&#125;告警状态：&#123;&#123; .Status &#125;&#125;告警主题: &#123;&#123; $alert.Annotations.summary &#125;&#125;告警详情: &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125;;故障时间: &#123;&#123; ($alert.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;恢复时间: &#123;&#123; ($alert.EndsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;&#123;&#123;- if gt (len $alert.Labels.instance) 0 &#125;&#125;实例信息: &#123;&#123; $alert.Labels.instance &#125;&#125;&#123;&#123;- end &#125;&#125;========= = end = =========&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125; 将模版文件配置到alertmanager，配置headers、html、send_resolved字段 1234567891011templates: - &#x27;/etc/alertmanager/wechat_template.tmpl&#x27; receivers:- name: &#x27;team-devops-wechat&#x27; wechat_configs: - corp_id: ww4c893118fbf4d07c to_user: &#x27;@all&#x27; agent_id: 1000003 api_secret: WTepmmaqxbBOeTQOuxa0Olzov_hSEWsZWrPX1k6opMk send_resolved: true 钉钉告警我们可将alertmanager报警提醒通过自定义机器人聚合到钉钉群中，以实现钉钉告警。 群机器人是钉钉群的高级扩展功能，群机器人可以将第三方服务的信息聚合到群聊中，实现自动化的信息同步 群机器人支持Webhook协议的自定义接入，支持更多可能性 alertmanager需要和 prometheus-webhook-dingtalk 配合才能将告警到群机器人，所以需要将其部署在Prometheus服务器上 123456789101112131415161718192021222324https://github.com/timonwong/prometheus-webhook-dingtalk$ wget https://githubfast.com/timonwong/prometheus-webhook-dingtalk/releases/download/v1.4.0/prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz$ tar -xvf prometheus-webhook-dingtalk-1.4.0.linux-amd64.tar.gz -C /usr/local/$ cd /usr/local$ mv prometheus-webhook-dingtalk-1.4.0.linux-amd64/ prometheus-webhook-dingtalk$ vim /usr/lib/systemd/system/webhook-dingtalk.service[Unit]Description=prometheus-webhook-dingtalkDocumentation=https://github.com/timonwong/prometheus-webhook-dingtalkAfter=network.target[Service]User=rootGroup=rootExecStart=/usr/local/prometheus-webhook-dingtalk/prometheus-webhook-dingtalk --config.file=/usr/local/prometheus-webhook-dingtalk/config.ymlExecReload=/bin/kill -HUP $MAINPIDKillMode=processRestart=on-failure[Install]WantedBy=multi-user.target$ systemctl daemon-reload$ systemctl start webhook-dingtalk &amp;&amp; systemctl enable webhook-dingtalk webhook_configs配置字段说明 123456789101112# 故障恢复后是否发送通知；[ send_resolved: &lt;boolean&gt; | default = true ]# HTTP POST请求发向的目标地址url: &lt;string&gt;# HTTP客户端配置，主要包括认证信息等[ http_config: &lt;http_config&gt; | default = global.http_config ]# 在单次webhook调用中允许发送的告警信息的个数# 多出的部分将自动被移除，默认值0表示不限制；[ max_alerts: &lt;int&gt; | default = 0 ] 范例：alertmanager中调用钉钉机器人的配置 12345receivers:- name: &#x27;team-devops-dingtalk&#x27; webhook_configs: - url: http://prometheus-webhook-dingtalk:8060/dingtalk/webhook1/send send_resolved: true prometheus-webhook-dingtalk配置文件说明 123456789101112131415161718192021222324252627282930313233343536## Request timeout# timeout: 5s## Customizable templates path# templates:# - contrib/templates/legacy/template.tmpl## You can also override default template using `default_message`## The following example to use the &#x27;legacy&#x27; template from v0.3.0# default_message:# title: &#x27;&#123;&#123; template &quot;legacy.title&quot; . &#125;&#125;&#x27;# text: &#x27;&#123;&#123; template &quot;legacy.content&quot; . &#125;&#125;&#x27;## Targets, previously was known as &quot;profiles&quot;targets: webhook1: url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx # secret for signature secret: SEC000000000000000000000 webhook2: url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx webhook_legacy: url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx # Customize template content message: # Use legacy template title: &#x27;&#123;&#123; template &quot;legacy.title&quot; . &#125;&#125;&#x27; text: &#x27;&#123;&#123; template &quot;legacy.content&quot; . &#125;&#125;&#x27; webhook_mention_all: url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx mention: all: true webhook_mention_users: url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx mention: mobiles: [&#x27;156xxxx8827&#x27;, &#x27;189xxxx8325&#x27;] 范例 12345678910111213141516171819202122templates: - /etc/prometheus-webhook-dingtalk/dingtalk_template.tmpl# 定义默认的消息格式，当没有特定的消息格式时将使用这些设置default_message: title: &#x27;&#123;&#123; template &quot;legacy.title&quot; . &#125;&#125;&#x27; #使用 Go 模板语法定义消息标题 text: &#x27;&#123;&#123; template &quot;legacy.content&quot; . &#125;&#125;&#x27; #使用 Go 模板语法定义消息正文内容targets: webhook1: url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx secret: SEC000000000000000000000 message: text: &#x27;&#123;&#123; template &quot;dingtalk_template.tmpl&quot; . &#125;&#125;&#x27; webhook_mention_all: # 发给所有人 url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx mention: all: true webhook_mention_users: # 根据手机号来发 url: https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx mention: mobiles: [&#x27;156xxxx8827&#x27;, &#x27;189xxxx8325&#x27;] 在DingTalk Webhook中使用模板，模板内容与WeChat中使用模板类似 参考模版文件存放目录：/usr/local/prometheus-webhook-dingtalk/contrib/templates 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;&#123;&#123;- if eq $index 0 &#125;&#125;========= 监控报警 =========告警状态：&#123;&#123; .Status &#125;&#125;告警级别：&#123;&#123; .Labels.severity &#125;&#125;告警类型：&#123;&#123; $alert.Labels.alertname &#125;&#125;故障主机: &#123;&#123; $alert.Labels.instance &#125;&#125;告警主题: &#123;&#123; $alert.Annotations.summary &#125;&#125;告警详情: &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125;;触发条件：&#123;&#123; .Annotations.value &#125;&#125;触发时间: &#123;&#123; ($alert.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125;========= = end = =========&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123;- end &#125;&#125;&#123;&#123; define &quot;dingtalk.tmpl&quot; &#125;&#125;&#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;============ = **&lt;font color=&#x27;#FF0000&#x27;&gt;告警&lt;/font&gt;** = =============**告警名称:** &#123;&#123; $alert.Labels.alertname &#125;&#125; **告警级别:** &#123;&#123; $alert.Labels.severity &#125;&#125; 级 **告警状态:** &#123;&#123; .Status &#125;&#125; **告警实例:** &#123;&#123; $alert.Labels.instance &#125;&#125; &#123;&#123; $alert.Labels.device &#125;&#125; **告警概要:** &#123;&#123; .Annotations.summary &#125;&#125; **告警详情:** &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125; **故障时间:** &#123;&#123; ($alert.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; ============ = end = ============= &#123;&#123;- end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125;&#123;&#123;- range $index, $alert := .Alerts -&#125;&#125;============ = &lt;font color=&#x27;#00FF00&#x27;&gt;恢复&lt;/font&gt; = =============**告警实例:** &#123;&#123; .Labels.instance &#125;&#125; **告警名称:** &#123;&#123; .Labels.alertname &#125;&#125; **告警级别:** &#123;&#123; $alert.Labels.severity &#125;&#125; 级 **告警状态:** &#123;&#123; .Status &#125;&#125; **告警概要:** &#123;&#123; $alert.Annotations.summary &#125;&#125; **告警详情:** &#123;&#123; $alert.Annotations.message &#125;&#125;&#123;&#123; $alert.Annotations.description&#125;&#125; **故障时间:** &#123;&#123; ($alert.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; **恢复时间:** &#123;&#123; ($alert.EndsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; ============ = **end** = =============&#123;&#123;- end &#125;&#125;&#123;&#123; end -&#125;&#125;&#123;&#123;- end &#125;&#125; 触发告警告警状态 Inactive: 什么都没发生 Pending：已触发阈值，但未满足告警持续时间 Firing：已触发阈值，并满足for定义的持续时间。告警发送给接受者 Prometheus以一个固定的周期来评估所有告警规则，其值由evaluate_interval参数定义，默认为15s；在每个评估周期内，Prometheus会计算每条告警规则中的布尔表达式并更新告警状态； 未使用for子句的告警，会自动从Inactive状态转为Firing，它只需要一个评估周期便能触发；而带有for子句的告警状态将先转Pending，而后才到Firing，因而至少需要两个评估周期才能触发； 处于Pending状态的告警，在其持续时长满足for子句的定义之前，若布尔表达式的值转回了false，则告警状态将转回Inactive； 由此可见，经由Pending再到Firing的转换，可以确保告警更有效，且不会来回浮动； Prometheus将为Pending和Firing状态中的每个告警创建指标，该指标名称为ALERT，其值固定为1，并且在告警处于Pending和Firing状态期间存在；在此之后，它将不接收任何更新，直到过期。 同时对于已经Pending或者Firing的告警，Prometheus也会将它们存储到时间序列 ALERTS&#123;&#125; 中，可以通过表达式，查询告警实例 1ALERTS&#123;alertname=&quot;&lt;alert name&gt;&quot;, alertstate=&quot;pending|firing&quot;, &lt;additional alert labels&gt;&#125; 范例：若某个Instance的up指标的值转为0持续超过1分钟后，将触发告警 12345678910111213141516root@ubuntu2004:/usr/local/prometheus/rules/alert# vim instance_down.yamlgroups:- name: AllInstances rules: - alert: InstanceDown # Condition for alerting expr: up == 0 for: 1m annotations: title: &#x27;Instance down&#x27; description: Instance has been down for more than 1 minute.&#x27; summary: &#x27;Instance down&#x27; labels: severity: &#x27;critical&#x27;root@ubuntu2004:/usr/local/prometheus/rules/alert# curl -XPOST localhost:9090/-/reload 现在我们把 grafana 停掉 1root@ubuntu2004:/usr/local/prometheus/rules/alert# systemctl stop grafana-server.service 刚开始为 pending 状态 1分钟过后，为 firing 状态 已发出的告警，告警将被Alertmanager接收到，便可在Alertmanager的Web UI上看到相关的信息，而且也将告警信息发送到目标email 抑制规则Alertmanager提供了方式可以帮助用户控制告警抑制通知的行为，包括预先定义的抑制机制和临时定义的静默规则 1234567891011121314151617181920212223# 定义源告警（触发抑制的告警）的匹配标签器source_match: # 匹配器 [ &lt;labelname&gt;: &lt;labelvalue&gt;, ... ] source_match_re: # 正则匹配器 [ &lt;labelname&gt;: &lt;regex&gt;, ... ] source_matchers: # 匹配器列表，需要全满足 [ - &lt;matcher&gt; ... ] # 定义目标告警（可能被抑制的告警）的匹配标签器target_match: [ &lt;labelname&gt;: &lt;labelvalue&gt;, ... ] target_match_re: [ &lt;labelname&gt;: &lt;regex&gt;, ... ] target_matchers: [ - &lt;matcher&gt; ... ] # 定义在源和目标告警中必须具有相等值的标签，才能应用抑制规则equal: [ &lt;labelname&gt;, ...] 以下3个条件同时满足，则启动抑制机制，新的告警不会发送。 当已经发送的告警通知匹配 source_match 规则 有新的告警满足 target_match 匹配规则 并且已发送的告警与新产生的告警中，equal定义的标签值完全相同 注意： 在语义上，缺失的标签和空值的标签是相同的意思。因此，如果在源警报和目标警报中， equal 中列出的所有标签名称都不存在，也会启动抑制规则。 为了防止警报抑制自身，建议 target_match 和 source_match 一般不要设置的一样。 123456inhibit_rules: - source_match: severity: &#x27;critical&#x27; target_match: severity: &#x27;warning&#x27; equal: [&#x27;alertname&#x27;, &#x27;dev&#x27;, &#x27;instance&#x27;] 假设某个服务的实例因内存耗尽而崩溃，系统会生成两个告警： 高优先级告警：HighMemoryUsage（severity: &#39;critical&#39;），表示该实例内存耗尽 低优先级告警：LowCPUUsage（severity: &#39;warning&#39;），表示该实例的 CPU 使用率较低 如果没有抑制规则，这两个告警会同时发送给运维团队，可能导致信息过载。使用上述配置后： 当 HighMemoryUsage（severity: &#39;critical&#39;）触发时，如果它与 LowCPUUsage（severity: &#39;warning&#39;）的 alertname、dev 和 instance 标签值一致，则 LowCPUUsage 告警会被抑制。 运维团队只需关注 HighMemoryUsage 告警即可。 equal 的作用是确保抑制规则只适用于相关联的告警。如果没有 equal，可能会导致不相关的告警被错误地抑制。例如： 如果只有 severity 匹配，而不检查 instance，那么某个实例的 HighMemoryUsage 告警可能会抑制另一个实例的 LowCPUUsage 告警，这显然是不合理的。 范例 12345678inhibit_rules:- source_match: alertname: NodeDown severity: critical target_match: severity: critical equal: - node 例如当集群中的某一个主机节点异常宕机导致告警NodeDown被触发，同时在告警规则中定义了告警级别severity&#x3D;critical。由于主机异常宕机，该主机上部署的所有服务，中间件会不可用并触发报警。根据抑制规则的定义，如果有新的告警级别为severity&#x3D;critical，并且告警中标签node的值与NodeDown告警的相同，则启动抑制机制停止发送告警通知 临时静默配置除了基于抑制机制可以控制告警通知的行为以外，用户或者管理员还可以直接通过Alertmanager的UI页面临时屏蔽特定的告警通知，专用于在维护期的时候。通过定义标签的匹配规则（字符串或者正则表达式），如果新的告警通知满足静默规则的设置，则停止向receiver发送通知。 进入Alertmanager UI，点击”New Silence”显示如下内容 用户可以通过该UI定义新的静默规则的开始时间以及持续时间 通过Matchers部分可以设置多条匹配规则(字符串匹配或者正则匹配)。 填写当前静默规则的创建者以及创建原因后，点击”Create”按钮即可。 通过”Preview Alerts”可以查看预览当前匹配规则匹配到的告警信息。静默规则创建成功后，Alertmanager会加载该规则设置状态为Pending，规则生效后进入Active状态。 静默规则生效后，Alerts页面下不能看到静默规则匹配的告警信息。 对于已经生效的规则，可以手动点击“Expire”使当前规则过期","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Alertmanager","slug":"Alertmanager","permalink":"https://aquapluto.github.io/tags/Alertmanager/"}]},{"title":"部署与配置","slug":"Monitor/alertmanager/deploy","date":"2025-08-27T10:29:08.000Z","updated":"2025-09-13T10:03:54.022Z","comments":true,"path":"Monitor/alertmanager/deploy/","permalink":"https://aquapluto.github.io/Monitor/alertmanager/deploy/","excerpt":"","text":"1 部署Alertmanager1.1 二进制部署1https://prometheus.io/download/#alertmanager Altermanager是一个独立的 go 二进制程序，需要独立部署及维护； 按需下载相应的程序包，展开至特定的目录下即可运行 123~]# wget https://github.com/prometheus/alertmanager/releases/download/v0.28.0/alertmanager-0.28.0.linux-amd64.tar.gz~]# tar xf alertmanager-0.28.0.linux-amd64.tar.gz -C /usr/local/~]# ln -s /usr/local/alertmanager-0.28.0.linux-amd64 /usr/local/alertmanager 编辑我们所需要的配置文件（alertmanager.yml），这里以局部配置QQ邮箱为例 123456789101112131415161718192021global: resolve_timeout: 5mroute: group_by: [&#x27;alertname&#x27;] group_wait: 10s group_interval: 10s repeat_interval: 10s receiver: &#x27;email-me&#x27;receivers:- name: &#x27;email-me&#x27; email_configs: - to: &#x27;xxx@qq.com&#x27; from: &#x27;xxx@qq.com&#x27; smarthost: &#x27;smtp.qq.com:465&#x27; auth_username: &#x27;xxx@qq.com&#x27; auth_identity: &#x27;xxx@qq.com&#x27; auth_password: &#x27;xxxxxx&#x27; require_tls: false send_resolved: true 或者也可以使用全局配置 12345678910111213global: resolve_timeout: 5m smtp_smarthost: &#x27;smtp.qq.com:465&#x27; smtp_from: &#x27;xxx@qq.com&#x27; smtp_auth_username: &#x27;xxx@qq.com&#x27; smtp_auth_password: &#x27;xxxxxx&#x27; smtp_require_tls: falsereceivers: - name: &#x27;email-me&#x27; email_configs: - to: &#x27;xxx@qq.com&#x27; send_resolved: true 编写system来管理服务 123456789101112131415161718192021222324252627root@ubuntu2004:~# useradd -r prometheusroot@ubuntu2004:~# chown -R prometheus.prometheus /usr/local/alertmanagerroot@ubuntu2004:~# vim /usr/lib/systemd/system/alertmanager.service[Unit]Description=node_exporterDocumentation=https://prometheus.io/docs/introduction/overview/After=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/alertmanager/alertmanager \\ --config.file=&quot;/usr/local/alertmanager/alertmanager.yml&quot; \\ --storage.path=&quot;/usr/local/alertmanager/data/&quot; \\ --data.retention=120h \\ --log.level=infoExecReload=/bin/kill -HUP $MAINPIDTimeoutStopSec=20sRestart=always[Install]WantedBy=multi-user.targetroot@ubuntu2004:~# systemctl daemon-reloadroot@ubuntu2004:~# systemctl start alertmanager.service root@ubuntu2004:~# systemctl enable alertmanager.service 配置参数说明 123456789--web.route-prefix=/alertmanager # 指定基础路径，默认无，加上后访问web UI则为 ip:9093/alertmanager --config.file=/etc/alertmanager/alertmanager.yml # 指定配置文件--storage.path=/alertmanager/data/ # 指定存储文件路径--data.retention=120h # 数据保留时间--web.listen-address=:9093 # 指定端口--web.external-url=https://xxx.com/alertmanager/ # 外部访问目录--web.enable-lifecycle # 设置热加载 开启后可通过“/-/reload”接口加载配置--web.enable-admin-api # 开启管理接口(不常用)--cluster.listen-address=&quot;0.0.0.0:9094&quot; # 监听集群地址。设置为空字符串表示关闭HA模式 内建的HTTP服务默认监听于TCP协议的9093端口，且 /metrics 路径暴露了其内建的指标； 1.2 k8s部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173# alertmanager-config.yamlapiVersion: v1kind: ConfigMapmetadata: name: alert-config namespace: monitordata: template_email.tmpl: |- &#123;&#123; define &quot;email.html&quot; &#125;&#125; &#123;&#123;- if gt (len .Alerts.Firing) 0 -&#125;&#125; @报警&lt;br&gt; &#123;&#123;- range .Alerts &#125;&#125; &lt;strong&gt;实例:&lt;/strong&gt; &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt; &lt;strong&gt;概述:&lt;/strong&gt; &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt; &lt;strong&gt;详情:&lt;/strong&gt; &#123;&#123; .Annotations.description &#125;&#125;&lt;br&gt; &lt;strong&gt;时间:&lt;/strong&gt; &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt; &#123;&#123;- end -&#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123;- if gt (len .Alerts.Resolved) 0 -&#125;&#125; @恢复&lt;br&gt; &#123;&#123;- range .Alerts &#125;&#125; &lt;strong&gt;实例:&lt;/strong&gt; &#123;&#123; .Labels.instance &#125;&#125;&lt;br&gt; &lt;strong&gt;信息:&lt;/strong&gt; &#123;&#123; .Annotations.summary &#125;&#125;&lt;br&gt; &lt;strong&gt;恢复:&lt;/strong&gt; &#123;&#123; (.StartsAt.Add 28800e9).Format &quot;2006-01-02 15:04:05&quot; &#125;&#125; &lt;br&gt; &#123;&#123;- end -&#125;&#125; &#123;&#123;- end &#125;&#125; &#123;&#123; end &#125;&#125; config.yml: |- templates: # 1、增加 templates 配置，指定模板文件 - &#x27;/etc/alertmanager/template_email.tmpl&#x27; inhibit_rules: - source_match: # prometheus配置文件中的报警规则1产生的所有报警信息都带着下面2个标签，第一个标签是promethus自动添加，第二个使我们自己加的 alertname: NodeMemoryUsage severity: critical target_match: severity: normal # prometheus配置文件中的报警规则2产生的所有报警信息都带着该标签 equal: - instance # instance是每条报警规则自带的标签，值为对应的节点名 # 一、全局配置 global: # （1）当alertmanager持续多长时间未接收到告警后标记告警状态为 resolved（解决了） resolve_timeout: 5m # （2）配置发邮件的邮箱 smtp_smarthost: &#x27;smtp.163.com:25&#x27; smtp_from: &#x27;18611453110@163.com&#x27; smtp_auth_username: &#x27;18611453110@163.com&#x27; smtp_auth_password: &#x27;YKQQBTNSCEXOONLH&#x27; # 填入你开启pop3时获得的码 smtp_hello: &#x27;163.com&#x27; smtp_require_tls: false # 二、设置报警的路由分发策略 route: # 定义用于告警分组的标签。当有多个告警消息有相同的 alertname 和 cluster 标签时，这些告警消息将会被聚合到同一个分组中 # 例如，接收到的报警信息里面有许多具有 cluster=XXX 和 alertname=YYY 这样的标签的报警信息将会批量被聚合到一个分组里面 group_by: [&#x27;alertname&#x27;, &#x27;cluster&#x27;] # 当一个新的报警分组被创建后，需要等待至少 group_wait 时间来初始化通知， # 这种方式可以确保您能有足够的时间为同一分组来获取/累积多个警报，然后一起触发这个报警信息。 group_wait: 30s # 短期聚合: group_interval 确保在短时间内，同一分组的多个告警将会合并/聚合到一起等待被发送，避免过于频繁的告警通知。 group_interval: 30s # 长期提醒: repeat_interval确保长时间未解决的告警不会被遗忘，Alertmanager每隔一段时间定期提醒相关人员，直到告警被解决。 repeat_interval: 120s # 实验环境想快速看下效果，可以缩小该时间，比如设置为120s # 上述两个参数的综合解释： #（1）当一个新的告警被触发时，会立即发送初次通知 #（2）然后开始一个 group_interval 窗口（例如 30 秒）。 # 在 group_interval 窗口内，任何新的同分组告警会被聚合到一起，但不会立即触发发送。 #（3）聚合窗口结束后， # 如果刚好抵达 repeat_interval 的时间点，聚合的告警会和原有未解决的告警一起发送通知。 # 如果没有抵达 repeat_interval 的时间点，则原有未解决的报警不会重复发送，直到到达下一个 repeat_interval 时间点。 # 这两个参数一起工作，确保短时间内的警报状态变化不会造成过多的重复通知，同时在长期未解决的情况下提供定期的提醒。 # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器，与下面receivers中定义的name呼应 receiver: default routes: # 子路由规则。子路由继承父路由的所有属性，可以进行覆盖和更具体的规则匹配。 - receiver: email # 匹配此子路由的告警将发送到的接收器，该名字也与下面的receivers中定义的name呼应 group_wait: 10s # 等待时间，可覆盖父路由的 group_by: [&#x27;instance&#x27;] # 根据instance做分组 match: # 告警标签匹配条件，只有匹配到特定条件的告警才会应用该子路由规则。 team: node # 只有拥有 team=node 标签的告警才会路由到 email 接收器。 continue: true - receiver: mywebhook # 匹配此子路由的告警将发送到的接收器，该名字也与下面的receivers中定义的name呼应 group_wait: 10s # 等待时间，可覆盖父路由的 group_by: [&#x27;instance&#x27;] # 根据instance做分组 match: # 告警标签匹配条件，只有匹配到特定条件的告警才会应用该子路由规则。 team: node # 只有拥有 team=node 标签的告警才会路由到 email 接收器。 # 三、定义接收器，与上面的路由定义中引用的介receiver相呼应 receivers: - name: &#x27;default&#x27; # 默认接收器配置，未匹配任何特定路由规则的告警会发送到此接收器。 email_configs: - to: &#x27;378533872@qq.com&#x27; send_resolved: true # : 当告警恢复时是否也发送通知。 - name: &#x27;email&#x27; # 名为 email 的接收器配置，与之前定义的子路由相对应。 email_configs: - to: &#x27;18611453110@163.com&#x27; send_resolved: true html: &#x27;&#123;&#123; template &quot;email.html&quot; . &#125;&#125;&#x27; - name: &#x27;mywebhook&#x27; # 默认接收器配置，未匹配任何特定路由规则的告警会发送到此接收器。 webhook_configs: - url: &#x27;http://promoter:8080/dingtalk/webhook1/send&#x27; send_resolved: true # : 当告警恢复时是否也发送通知。---# alertmanager-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: alertmanager namespace: monitor labels: app: alertmanagerspec: selector: matchLabels: app: alertmanager template: metadata: labels: app: alertmanager spec: volumes: - name: alertcfg configMap: name: alert-config containers: - name: alertmanager # 版本去查看官网https://github.com/prometheus/alertmanager/releases/ # 1、官网镜像地址，需要你为containerd配置好镜像加速 #image: prom/alertmanager:v0.27.0 # 2、搞成了国内的地址 image: registry.cn-hangzhou.aliyuncs.com/egon-k8s-test/alertmanager:v0.27.0 imagePullPolicy: IfNotPresent args: - &#x27;--config.file=/etc/alertmanager/config.yml&#x27; ports: - containerPort: 9093 name: http volumeMounts: - mountPath: &#x27;/etc/alertmanager&#x27; name: alertcfg resources: requests: cpu: 100m memory: 256Mi limits: cpu: 100m memory: 256Mi---# alertmanager-svc.yamlapiVersion: v1kind: Servicemetadata: name: alertmanager namespace: monitor labels: app: alertmanagerspec: selector: app: alertmanager type: NodePort ports: - name: web port: 9093 targetPort: http 2 Alertmanager配置段说明123456789101112131415161718192021222324252627282930# 全局配置global: [ smtp_from: &lt;tmpl_string&gt; ] # 默认的SMTP From报头字段，标识了邮件的实际发件人 [ smtp_smarthost: &lt;string&gt; ] # 用于发送邮件的默认SMTP智能主机，包括端口号。端口号一般是25 [ smtp_auth_username: &lt;string&gt; ] # SMTP认证使用CRAMM-MD5, LOGIN和PLAIN。如果为空，则Alertmanager不向SMTP服务器进行身份验证 [ smtp_auth_password: &lt;secret&gt; ] # SMTP认证使用LOGIN和PLAIN [ smtp_auth_password_file: &lt;string&gt; ] # SMTP认证使用LOGIN和PLAIN [ smtp_auth_identity: &lt;string&gt; ] # SMTP认证使用PLAIN [ smtp_auth_secret: &lt;secret&gt; ] # SMTP认证使用CRAM-MD5 [ smtp_require_tls: &lt;bool&gt; | default = true ] # 默认的SMTP TLS要求。注意Go不支持未加密的远程SMTP端点连接 [ resolve_timeout: &lt;duration&gt; | default = 5m ] # 当Alertmanager持续多长时间未接收到告警后，标记告警状态从firing转为resolved(已解决)。该参数的定义可能会影响到告警恢复通知的接收时间 # 定义告警模板文件。可以使用通配符匹配器，例如”templates/*.tmpl“templates: [ - &lt;filepath&gt; ... ] # 路由树的根节点，指定如何处理传入的告警route: &lt;route&gt; # 告警接收者列表receivers: - &lt;receiver&gt; ... # 抑制规则列表，合理设置抑制规则可以减少垃圾告警的产生inhibit_rules: [ - &lt;inhibit_rule&gt; ... ] # 静默/激活路由的时间间隔列表。time_intervals: [ - &lt;time_interval&gt; ... ] 提示：上面配置中用于处理和发送告警信息的媒介是运行于本地节点上的SMTP服务，因而需要安装配置postfix一类的程序以完成测试 3 组合Prometheus与AltermanagerPrometheus调用的Alertmanager实例的配置信息定义在顶级的alerting配置段中，它同样支持静态配置和服务发现的机制 静态配置示例 12345alerting: alertmanagers: - static_configs: - targets: - 10.0.0.204:9093 基于文件发现的动态配置示例 12345678910alerting: alertmanagers: - file_sd_configs: - files: - &quot;targets/alertmanager*.yaml&quot;- targets: - 10.0.0.204:9093 labels: app: alertmanager job: alertmanager 还应该将Alertmanager程序作为监控目标 1234- job_name: &#x27;alertmanager&#x27; file_sd_configs: - files: - targets/prometheus/alertmanager*.yaml","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Alertmanager","slug":"Alertmanager","permalink":"https://aquapluto.github.io/tags/Alertmanager/"}]},{"title":"告警功能概述","slug":"Monitor/alertmanager/introduce","date":"2025-08-27T10:28:57.000Z","updated":"2025-09-13T09:58:21.119Z","comments":true,"path":"Monitor/alertmanager/introduce/","permalink":"https://aquapluto.github.io/Monitor/alertmanager/introduce/","excerpt":"","text":"1 告警功能概述通过在 Prometheus 中定义告警规则，Prometheus会周期性的对告警规则进行计算，抓取到异常值后，Prometheus支持通过“告警（Alert）”机制向用户发送反馈或警示，以触发用户能够及时采取应对措施； Prometheus对指标的收集、存储同告警能力分属于Prometheus Server和AlertManager两个独立的组件，前者仅负责基于“告警规则”生成告警通知，具体的告警操作则由后者完成； 告警指示由Prometheus Server基于用户提供的“告警规则”周期性计算生成；客户端通常是Prometheus Server，但AlertManager也支持接收来自其它工具的告警 Alertmanager接收到Prometheus Server发来的告警指示后，对告警通知进行分组、去重后，基于用户定义的告警路由（route）向告警接收人（receivers）发送告警信息；即根据路由规则将其路由到不同的receiver，如Email、短信或PagerDuty等 2 Alertmanager的特性除了基本的告警通知能力外，Altermanager还支持对告警进行去重、分组、抑制、静默和路由等功能； 分组（Grouping）：将相似告警合并为单个告警通知的机制，在系统因大面积故障而触发告警潮时，分组机制能避免用户被大量的告警噪声淹没，进而导致关键信息的隐没； 抑制（Inhibition）：系统中某个组件或服务故障而触发告警通知后，那些依赖于该组件或服务的其它组件或服务可能也会因此而触发告警，抑制便是避免类似的级联告警的一种特性，从而让用户能将精力集中于真正的故障所在；Inhibition的关键作用在于，同时存在的两组告警条件中，其中一组的生效，能使得另一组失效 静默（Silent）：是指在一个特定的时间窗口内，即便接收到告警通知，Alertmanager也不会真正向用户发送告警信息的行为；通常，在系统例行维护期间，需要激活告警系统的静默特性； 路由（route）：用于配置Alertmanager如何处理传入的特定类型的告警通知，其基本逻辑是根据路由匹配规则的匹配结果来确定处理当前告警通知的路径和行为； 3 监控系统的告警逻辑首先要配置Prometheus成为Alertmanager的告警客户端；反过来，Alertmanager也是应用程序，它自身同样应该纳入prometheus的监控目标 在Alertmanager上定义receiver，他们通常是能够基于某个媒介接收告警消息的特定用户； Email、WeChat、Pagerduty、Slack和Webhook等是为常见的发送告警信息的媒介； 在不同的媒介上，代表告警消息接收人的地址表示方式也会有所不同； 在Alertmanager上定义路由规则（route），以便将收到的告警通知按需分别发送给特定的receiver； 在Prometheus上定义告警规则生成告警通知，发送给Alertmanager；","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Alertmanager","slug":"Alertmanager","permalink":"https://aquapluto.github.io/tags/Alertmanager/"}]},{"title":"指标重新打标","slug":"Monitor/prometheus/indicators-remarked","date":"2025-08-27T10:18:07.000Z","updated":"2025-09-12T16:00:06.975Z","comments":true,"path":"Monitor/prometheus/indicators-remarked/","permalink":"https://aquapluto.github.io/Monitor/prometheus/indicators-remarked/","excerpt":"","text":"1 指标抓取的生命周期发现 -&gt; 配置 -&gt; relabel -&gt; 指标数据抓取 -&gt; metrics relabel 1.1 服务发现阶段在每个scrape_interval期间，Prometheus都会执行作业（Job）以检查和更新其监控的目标列表。服务发现根据作业中指定的配置生成一个目标列表。每个目标都附带有一组元数据标签，这些标签以__meta_为前缀，提供了关于目标的额外信息。 除了元数据标签外，对于发现的每个target，Prometheus默认会自动添加以下标签，这些标签可在 relabel_configs 中被引用或修改。需要说明的是，如果有任何参数存在于URI路径中，则它们以前缀__param_的形式被设置。 job的标签设定为其所属的 job_name 的值 __address__ 标签的值为该target的套接字地址 “&lt;host&gt;:&lt;port&gt;” instance标签的值为 __address__ 的值 __scheme__ 标签的值为抓取该target上指标时使用的协议（http或https） __metrics_path__ 标签的值为抓取该target上的指标时使用URI路径，默认为/metrics __param_&lt;name&gt; 标签的值为传递的URL参数中第一个名称为 &lt;name&gt; 的参数的值 1.2 重新标记阶段relabel_configs在将目标列表返回给Prometheus之前，可以通过配置中的 relabel_configs 来对目标进行重新标记。这一步骤通常用于： 将服务发现过程中得到的元数据标签信息附加到指标的标签上。 根据特定条件过滤目标列表，例如排除某些不符合要求的目标。 重新标记期间，还可以使用该target上以 “__meta_” 开头的元标签，由服务发现机制自动添加，可用于 relabel 中提取信息 各服务发现机制为其target添加的元标签会有所不同 重新标记完成后，该target上以 “__” 开头的所有标签都会被移除 若在relabel的过程中需要临时存储标签值，则要使用 __tmp 标签名称为前缀进行保存，以避免同Prometheus的内建标签冲突 1.3 数据抓取与指标处理metric_relabel_configs完成重新标记后，Prometheus开始从各个目标处抓取数据。抓取的数据可能会包含不需要的信息或者需要修改的部分。为此，Prometheus允许通过 metric_relabel_configs 在保存指标之前对其进行最后的调整，包括但不限于： 删除不必要的指标，减少存储开销和查询压力 从指标中删除敏感或不需要的标签，隐私保护、减少基数 对指标的标签值或格式进行添加、编辑或修改 但是要注意的是，更改或添加标签会创建新的时间序列，因为Prometheus中，时间序列是由指标名称 + 所有标签唯一确定，应该明确地使用各个标签，并尽可能保持不变，以避免创建出一个动态的数据环境 更改标签会导致原时间序列终结，新时间序列开始，可能会造成查询断点 原序列: http_requests_total&#123;job=&quot;api&quot;, status=&quot;200&quot;&#125; 修改后: http_requests_total&#123;job=&quot;api&quot;, status=&quot;success&quot;&#125; —&gt; 全新序列 添加标签会导致标签组合爆炸，可能引发内存爆炸，TSDB 写入性能下降，查询超时，应避免使用连续值或高基数字段作为标签 如果系统有 100 万注册用户，而使用用户 ID 作为标签（高基数字段） 使用连续值作为标签（如时间戳、自增 ID） 也需要注意的是，标签是时间序列的唯一性约束，删除标签并导致时间序列重复时，可能会导致系统出现问题，所以删除标签前确保剩余标签仍能唯一标识时间序列 删除 instance 标签前 http_requests_total&#123;instance=&quot;a&quot;, job=&quot;web&quot;&#125; http_requests_total&#123;instance=&quot;b&quot;, job=&quot;web&quot;&#125; 删除 instance 后 —&gt; 两者变为同一序列！冲突！ 1.4 两者区别 配置项 作用阶段 作用对象 核心作用 relabel_configs 数据采集之前 target（目标） 决定是否采集该目标（过滤或调整采集对象） metric_relabel_configs 数据采集之后、保存之前 指标本身 决定是否保存该指标（过滤或调整具体指标） 2 重新标记配置1234567891011- source_labels: [label1, label2] # 指定要操作的源标签（值拼接） separator: &#x27;;&#x27; # 指定在串联源标签值之间的分隔符，默认是 ; regex: (.*) # 正则匹配源标签的串连值 target_label: instance # 要设置的目标标签名（action为replace/hashmod时有效） replacement: $1 # $1表示正则匹配的第一个捕获组 # action为replace时，作为target_label指定标签的值 # action为labelmap时，作为新标签名 action: replace # 替换操作，更多动作类型见下文 modulus: 2 # hashmod动作专用，指定对源标签值的哈希值进行取模的模数 &lt;action&gt; 字段用于定义重新标记的行为，其可用取值如下 1、替换标签值 replace：首先将source_labels中指定的各标签的值以separator进行连接起来，然后用regex字段中指定的正则表达式进行匹配判定，如果匹配上了，则将target_label字段中指定标签的值替换为replacement字段中保存的值。 hashmod：计算源标签串联值的哈希值，然后对 modulus 取模，取模后的结果作为值赋给 target_label 指定标签的值 2、创建或删除标签 labelmap：将regex对所有的标签名进行匹配判定，而后将匹配到的标签的值赋给 replacement 指定的模版作为新标签名 labelkeep：将regex对所有的标签名进行匹配判断，不能够匹配到的标签从该target的标签集中删除 labeldrop：将 regex 对所有的标签名进行匹配判断，能够匹配到的标签从该target的标签集中删除 3、删除指标 keep：使用regex不能匹配到target上的source_labels上的各标签的串联值时，则删除该target drop：使用regex能匹配到target上的source_labels上的各标签的串联值时，则删除该target。 2.1 replace示例将三个源标签的值接顺序串联后，由指定的正则表达式进行模式匹配，而后由replacement引用模式匹配的结果，并加以改造后，将其赋值给 target_label 的endpoint标签 123456789101112131415- job_name: &#x27;nodes&#x27; file_sd_configs: - files: - targets/prometheus/node*.yaml relabel_configs: - source_labels: - __scheme__ - __address__ - __metrics_path__ regex: &quot;(http|https)(.*)&quot; separator: &quot;&quot; target_label: &quot;endpoint&quot; replacement: &quot;$&#123;1&#125;://$&#123;2&#125;&quot; action: replace 2.2 hashmod示例123456789101112131415161718192021假设我们有以下的 relabel 配置：- action: hashmod modulus: 10 source_labels: [instance] target_label: shard 在这个示例中，我们使用 hashmod 操作，并指定 modulus 为 10，source_labels 为 [instance]，target_label 为 shard。这将对 instance 标签的值进行哈希运算，并将哈希值对 10 取模，然后将结果存储到 shard 标签中。假设我们有以下的指标样本：metric_name&#123;instance=&quot;server1&quot;&#125; 42.0metric_name&#123;instance=&quot;server2&quot;&#125; 57.0metric_name&#123;instance=&quot;server3&quot;&#125; 68.0应用以上的 relabel 配置后，我们将得到：metric_name&#123;instance=&quot;server1&quot;, shard=&quot;1&quot;&#125; 42.0metric_name&#123;instance=&quot;server2&quot;, shard=&quot;2&quot;&#125; 57.0metric_name&#123;instance=&quot;server3&quot;, shard=&quot;3&quot;&#125; 68.0可以看到，对于每个样本，instance 标签的值被哈希运算，并且哈希结果对 10 取模后存储到了 shard 标签中。这样，我们可以将指标样本分散到 10 个不同的分片中，以实现负载均衡或其他分片策略。#应用场景：比如收集10台服务器的日志，数据量太大。只需要采样其中一台的服务器日志做详细收集即可。这样可以应用hashmod的10，采样其中1台服务器做详细日志采集 2.3 labelmap示例下面的示例，将regex指定的模式对target上的所有标签进行匹配判定，对于匹配到的标签名，它将以该标签名中匹配的部分为前缀，指定的 “_name” 为后缀生成新的标签名，而新标签的值则与其原标签的值相同； 123456789- job_name: &#x27;nodes&#x27; file_sd_configs: - files: - targets/prometheus/node*.yaml relabel_configs: - regex: &quot;(job|app)&quot; replacement: $&#123;1&#125;_name action: labelmap 2.4 labelkeep示例1234567891011121314假设有以下relabel 配置：- action: labelkeep regex: &quot;^(app|env|version)$&quot; 在这个示例中，我们使用 labelkeep 操作，并指定正则表达式 ^(app|env|version)$。该正则表达式将与所有标签名称进行匹配，只有与 &quot;app&quot;、&quot;env&quot; 或 &quot;version&quot; 匹配的标签将被保留。假设我们有以下的指标样本：metric_name&#123;app=&quot;myapp&quot;, env=&quot;production&quot;, version=&quot;1.2.3&quot;, region=&quot;us-west&quot;&#125; 42.0应用以上的 relabel 配置后，我们将得到：metric_name&#123;app=&quot;myapp&quot;, env=&quot;production&quot;, version=&quot;1.2.3&quot;&#125; 42.0可以看到，只有与正则表达式匹配的标签 &quot;app&quot;、&quot;env&quot; 和 &quot;version&quot; 被保留下来，而不匹配正则表达式的标签 &quot;region&quot; 被删除了。 2.5 labeldrop示例1234567891011121314假设我们有以下的 relabel 配置：- action: labeldrop regex: &quot;^(app|env|version)$&quot; 在这个示例中，我们使用 labeldrop 操作，并指定正则表达式 ^(app|env|version)$。该正则表达式将与所有标签名称进行匹配，任何匹配的标签都将从标签集合中删除。假设我们有以下的指标样本：metric_name&#123;app=&quot;myapp&quot;, env=&quot;production&quot;, version=&quot;1.2.3&quot;, region=&quot;us-west&quot;&#125; 42.0应用以上的 relabel 配置后，我们将得到：metric_name&#123;region=&quot;us-west&quot;&#125; 42.0可以看到，与正则表达式匹配的标签 &quot;app&quot;、&quot;env&quot; 和 &quot;version&quot; 被从标签集合中删除，而不匹配正则表达式的标签 &quot;region&quot; 被保留了。 2.6 drop示例在source_labels字段上，通过指标上元标签 “__name__” 引用指标名称，而后由regex进行匹配判定，可使用drop action删除匹配的指标，或使用keep action仅保留匹配的指标； 下面的示例，用于在相应的 job 上，在发现的各target之上，删除以 “go_info” 为前名称前缀的指标 12345678910- job_name: &#x27;nodes&#x27; file_sd_configs: - files: - targets/prometheus/node*.yaml metric_relabel_configs: - source_labels: - __name__ regex: &quot;go_info.*&quot; action: drop go_info为前缀的指标删除之前的查询结果 删除相关指标之后的查询结果 提示：若删除的指标此前曾由Prometheus抓取并存储过相关的样本数据，则删除操作的需要经过一定的时长后才会反映至查询结果中； 2.7 keep示例1234567891011121314151617假设我们有以下的 relabel 配置：- action: keep source_labels: [status] regex: &quot;success&quot; 在这个示例中，我们使用 keep 操作，并指定 source_labels 为 [status]，regex 为 &quot;success&quot;。这将保留与正则表达式匹配的目标，并删除与正则表达式不匹配的目标。假设我们有以下的指标样本：metric_name&#123;status=&quot;success&quot;&#125; 42.0metric_name&#123;status=&quot;failure&quot;&#125; 57.0metric_name&#123;status=&quot;success&quot;&#125; 68.0应用以上的 relabel 配置后，我们将得到：metric_name&#123;status=&quot;success&quot;&#125; 42.0metric_name&#123;status=&quot;success&quot;&#125; 68.0可以看到，只有与正则表达式 &quot;success&quot; 匹配的目标样本被保留下来，而与正则表达式不匹配的目标样本被删除了。","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"服务发现","slug":"Monitor/prometheus/service-discovery","date":"2025-08-27T10:17:40.000Z","updated":"2025-09-12T09:41:29.931Z","comments":true,"path":"Monitor/prometheus/service-discovery/","permalink":"https://aquapluto.github.io/Monitor/prometheus/service-discovery/","excerpt":"","text":"1 服务发现的概念1.1 为何要进行服务发现Prometheus Server的数据抓取工作于Pull模型，因而，它必需要事先知道各Target的位置，然后才能从相应的Exporter或Instrumentation中抓取数据 对于小型的系统环境来说，通过static_configs指定各Target便能解决问题，这也是最简单的配置方法；每个Targets用一个网络端点（ip:port）进行标识； 对于中大型的系统环境或具有较强动态性的云计算环境来说，静态配置显然难以适用，手动添加客户端非常耗时间； 因此，Prometheus为此专门设计了一组服务发现机制，以便于能够基于服务注册中心（服务总线）自动发现、检测、分类可被监控的各Target，以及更新发生了变动的Target； 1.2 可集成的服务发现机制不同场景中，服务注册中心的指代也会有所不同 公有或私有IaaS云自身保存有平台上的所有资源信息，其API Server便可作为Prometheus的服务发现媒介； azure、ec2、digitalocean、gce、hetzner Prometheus也可以集成到多种不同的开源服务发现工具上，以动态发现需要监控的目标； Consul、Eureka、Zookeeper、Serverset或Airbnb Nerve等 Prometheus也可以很好地集成到Kubernetes平台上，通过其API Server动态发现各类被监控的Pod（容器集）、Service、Endpoint、Ingress和Node对象； 它也支持基于dockerswarm和marathon两款编排工具进行服务发现； Prometheus还支持基于DNS或文件的动态发现机制； 2 基于文件的服务发现基于文件的服务发现是仅仅略优于静态配置的服务发现方式，它不依赖于任何平台或第三方服务，因而也是最为简单和通用的实现方式； Prometheus Server定期从文件中加载Target信息 文件使用 JSON 或 YAML 格式，它仅含有定义的Target列表，以及可选的标签信息； 它可以被prometheus动态获取到，而不需要重启； 如果主机太多，不好管理，这些文件可由另一个系统生成，例如Puppet、Ansible或Saltstack等配置管理系统，也可以是由脚本基于CMDB定期查询生成； 待监测的targets信息写入专门的配置文件，比较好管理；但是和Prometheus服务器的耦合是比较深的，需要考虑如果Prometheus服务器挂了&#x2F;重新部署一台，那么这些targets文件也要定期备份、及时还原出来，这一点是比较麻烦的。 需要注意的是：如果prometheus.yml文件有变动，需要刷新配置的； 如果是targets目录中的文件内容有变动，不需要刷新配置即可生效。 2.1 target信息配置12345678910- targets: - localhost:9090 labels: app: prometheus job: prometheus- targets: - localhost:9100 labels: app: node-exporter job: node 发现 target 的配置，定义在Prometheus配置文件的 job 之中 123456789101112scrape_configs:- job_name: ‘prometheus&#x27; file_sd_configs: - files: # 指定要加载的文件列表； - targets/prometheus*.yaml # 文件加载支持glob通配符； refresh_interval: 2m # 每隔2分钟重新加载一次文件中定义的Targets，默认为5m； - job_name: ‘nodes&#x27; file_sd_configs: - files: - targets/node*.yaml refresh_interval: 2m 2.2 基于文件的服务发现监控node export1234567891011121314151617181920212223#部署node export略root@ubuntu2004:/usr/local/prometheus# mkdir -p targets/noderoot@ubuntu2004:/usr/local/prometheus# cd targets/noderoot@ubuntu2004:/usr/local/prometheus/targets/node# vim node-01.yaml- targets: - 10.0.0.204:9100 labels: app: node-exporter job: noderoot@ubuntu2004:/usr/local/prometheus/targets/node# vim ../../prometheus.yml - job_name: &quot;linux&quot; static_configs: - targets: [&quot;localhost:9100&quot;] file_sd_configs: - files: - targets/node/*.yaml refresh_interval: 2mroot@ubuntu2004:/usr/local/prometheus/targets/node# curl -XPOST localhost:9090/-/reload 3 基于DNS的服务发现基于DNS的服务发现针对一组DNS域名进行定期查询，以发现待监控的目标，了解即可 查询时使用的DNS服务器由 &#x2F;etc&#x2F;resolv.conf 文件指定； 该发现机制依赖于A、AAAA和SRV资源记录，且仅支持该类方法，尚不支持RFC6763中的高级DNS发现方式； 在prometheus.yml配置中，使用DNS记录返回目标列表（dns_sd_config） 123456789101112# 需要查询的 DNS 域名列表。此处要指定SRV资源记录的名称，例如“_prometheus._tcp.magedu.com”names: [ - &lt;string&gt; ]# 要执行的 DNS 查询的类型。SRV、A 或 AAAA 之一[ type: &lt;string&gt; | default = &#x27;SRV&#x27; ]# 如果查询类型不是 SRV，则使用的端口号。[ port: &lt;int&gt; ]# The time after which the provided names are refreshed.[ refresh_interval: &lt;duration&gt; | default = 30s ] 元数据标签： __meta_dns_name __meta_dns_srv_record_target __meta_dns_srv_record_port 4 基于Consul的服务发现4.1 Consul简介一款基于golang开发的开源工具，主要面向分布式，服务化的系统提供服务注册、服务发现和配置管理的功能 提供服务注册&#x2F;发现、健康检查、Key&#x2F;Value存储、多数据中心和分布式一致性保证等功能，不再需要依赖其他工具（比如ZooKeeper等），微服务场景用的比较多 Consul服务发现的原理：将可以进行数据采集的服务注册到 consul 中，用于自动发现，同时使用 prometheus 做为 client 端获取 consul 上注册的服务，从而进行动态获取数据。 官网：https://www.consul.io 部署consul查阅文章6.1或6.3小节 4.2 Consul命令1234567891011121314151617181920212223242526272829#启动开发者模式，正常应该使用server模式，测试可以使用client模式consul agent -dev -ui -data-dir=/consul/data/ -config-dir=/etc/consul/ -client=0.0.0.0#查看集群成员consul memberscurl localhost:8500/v1/catalog/nodes#在Server上添加其它agentconsul join [options] address ...#在agent主机上，设置该agent离开集群并关闭agentconsul leave#注册Serviceconsul services register nodes.jsoncurl -XPUT --data @nodes.json http://localhost:8500/v1/agent/service/register#注销Serviceconsul services deregister -id &lt;SERVICE_ID&gt;curl -XPUT http://localhost:8500/v1/agent/service/deregister/&lt;SERVICE_ID&gt;#让consul重新加载配置信息consul reload#列出已经注册的服务curl -XGET http://localhost:8500/v1/agent/services#获取某个特定服务的配置信息curl -XGET http://localhost:8500/v1/agent/service/&lt;SERVICE_ID&gt; 4.3 在consul上注册services在配置目录中添加 json 文件，在文件中定义单个Service 1234567891011121314# nodes.json&#123; &quot;service&quot;: &#123; &quot;id&quot;: &quot;node_exporter&quot;, &quot;name&quot;: &quot;node01&quot;, &quot;address&quot;: &quot;172.29.1.11&quot;, &quot;port&quot;: 9100, &quot;tags&quot;: [&quot;nodes&quot;], &quot;checks&quot;: [&#123; &quot;http&quot;: &quot;http://172.29.1.11:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot; &#125;] &#125;&#125; 或者在文件中定义多个Services 12345678910111213141516171819202122232425# nodes.json&#123; &quot;services&quot;: [&#123; &quot;id&quot;: &quot;node_exporter-node01&quot;, &quot;name&quot;: &quot;node01&quot;, &quot;address&quot;: &quot;172.29.1.11&quot;, &quot;port&quot;: 9100, &quot;tags&quot;: [&quot;nodes&quot;], &quot;checks&quot;: [&#123; &quot;http&quot;: &quot;http://172.29.1.11:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot; &#125;] &#125;, &#123; &quot;ID&quot;: &quot;node_exporter-node02&quot;, &quot;Name&quot;: &quot;node02&quot;, &quot;Address&quot;: &quot;172.29.1.12&quot;, &quot;Port&quot;: 9100, &quot;Tags&quot;: [&quot;nodes&quot;], &quot;Checks&quot;: [&#123; &quot;http&quot;: &quot;http://172.29.1.12:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot; &#125;] &#125;]&#125; 4.4 编辑Prometheus配置文件1234567scrape_configs: - job_name: &quot;linux&quot; consul_sd_configs: #指定使用 consul 服务发现 - server: &quot;10.0.0.204:8500&quot; #指定 consul 服务的端点列表 tags: #指定 consul 服务发现的 services 中哪些 service 能够加入到 prometheus 监控的标签 - &quot;nodes&quot; refresh_interval: 2m 4.5 consul_sd_configs配置段的可用参数12345678910111213141516171819202122232425262728293031# 访问 Consul API 的信息。它将根据 Consul 文档的要求进行定义。[ server: &lt;host&gt; | default = &quot;localhost:8500&quot; ][ token: &lt;secret&gt; ][ datacenter: &lt;string&gt; ][ scheme: &lt;string&gt; | default = &quot;http&quot; ][ username: &lt;string&gt; ][ password: &lt;secret&gt; ]tls_config: [ &lt;tls_config&gt; ]# 检索目标的服务列表。如果省略，则会抓取所有服务。services: [ - &lt;string&gt; ] # 用于过滤给定服务的节点的可选标签列表。必须包含consul配置文件中的所有标签，才会被 Prometheus 发现并抓取tags: [ - &lt;string&gt; ] # 节点元数据键/值对，用于过滤给定服务的节点。[ node_meta: [ &lt;string&gt;: &lt;string&gt; ... ] ] # Consul 标签加入到标签标签中的字符串。[ tag_separator: &lt;string&gt; | default = , ]# 允许陈旧的 Consul 结果（请参阅 https://www.consul.io/api/features/consistency.html）。这将减少 Consul 的负载。[ allow_stale: &lt;boolean&gt; | default = true ]# 刷新提供的名称的时间。在大型设置中，增加此值可能是一个好主意，因为目录将一直更改。[ refresh_interval: &lt;duration&gt; | default = 30s ] 4.6 Consul上的meta标签Prometheus的Consul服务发现机制将通过Consul的Catalog API来发现target 在发现的target上执行relabel时，它支持使用如下meta标签 123456789101112__meta_consul_address #目标的地址__meta_consul_dc #目标的数据中心名称__meta_consul_health #服务的健康状态__meta_consul_metadata_&lt;key&gt; #目标的每个节点元数据键值__meta_consul_node #为目标定义的节点名称__meta_consul_service_address #目标的服务地址__meta_consul_service_id #目标的服务 ID__meta_consul_tags #由标签分隔符连接的目标标签列表__meta_consul_service_port #目标的服务端口__meta_consul_service #目标所属服务的名称__meta_consul_tagged_address_&lt;key&gt; #每个节点标记目标地址的键值__meta_consul_service_metadata_&lt;key&gt; #目标的每个服务元数据键值 4.7 基于Consul服务发现监控物理主机前提：在物理主机已经部署了 node exporter 123456789101112131415161718192021222324252627282930313233343536373839404142434445#Consul上配置Serviceroot@ubuntu2004:~/learning-prometheus/08-prometheus-components-compose/consul-and-exporter# vim consul_configs/nodes.json&#123; &quot;services&quot;: [ &#123; &quot;id&quot;: &quot;node_exporter-prometheus&quot;, &quot;name&quot;: &quot;prometheus-server&quot;, &quot;address&quot;: &quot;10.0.0.200&quot;, &quot;port&quot;: 9100, &quot;tags&quot;: [&quot;nodes&quot;], &quot;checks&quot;: [&#123; &quot;http&quot;: &quot;http://10.0.0.200:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot; &#125;] &#125;, &#123; &quot;id&quot;: &quot;node_exporter-consul&quot;, &quot;name&quot;: &quot;consul-server&quot;, &quot;address&quot;: &quot;10.0.0.204&quot;, &quot;port&quot;: 9100, &quot;tags&quot;: [&quot;nodes&quot;], &quot;checks&quot;: [&#123; &quot;http&quot;: &quot;http://10.0.0.204:9100/metrics&quot;, &quot;interval&quot;: &quot;5s&quot; &#125;] &#125;]&#125;访问 http://10.0.0.204:8500#配置Prometheus的配置文件 - job_name: &quot;linux&quot; #static_configs: # - targets: [&quot;localhost:9100&quot;] #file_sd_configs: #- files: # - targets/node/*.yaml # refresh_interval: 2m consul_sd_configs: - server: &quot;10.0.0.204:8500&quot; tags: - &quot;nodes&quot; refresh_interval: 2m","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"查询结果持久化","slug":"Monitor/prometheus/query-persist","date":"2025-08-27T10:17:21.000Z","updated":"2025-09-12T09:33:07.392Z","comments":true,"path":"Monitor/prometheus/query-persist/","permalink":"https://aquapluto.github.io/Monitor/prometheus/query-persist/","excerpt":"","text":"1 为什么需要查询结果持久化在Prometheus的表达式中浏览器进行的查询会生成新的数据序列，但其结果仅会临时保存于Prometheus Server上；在样本数据量较大、工作较为繁忙的Prometheus Server上，对于那些查询频率较高且运算较为复杂的查询来说，实时查询可能会存在一定程度的响应延迟。 这时需要一种能够类似于后台批处理的机制能够在后台完成这些复杂运算的计算，对于使用者而言只需要查询这些运算结果即可；Prometheus通过Recoding Rule规则，支持这种后台计算的方式，可以实现对复杂查询的性能优化，提高查询效率。 记录规则（Recording rule）能够预先运行频繁用到或计算消耗较大的表达式，并将其结果保存为一组新的时间序列，因而其名称必须是规范的指标名称格式；记录规则必须定义在规则组（rule group）中，各规则按给定的顺序依次运行 记录规则是定义在Prometheus配置文件中的查询语句，由Server加载后，它能够于以类似批处理任务的方式在后台周期性的执行并记录查询结果； 客户端只需要查询由记录规则生成的结果序列上的样本数据即可，速度远快于实时查询； 常用于跨多个时间序列生成聚合数据，或者计算消耗较大的查询等场景中； 多见于同可视化工具结合使用的需求中，也可用于生成可产生告警信息的时间序列； 2 Recording Rule语法12345678910111213141516171819202122232425# 每个规则文件，都是含有一至多个规则组（rule_group）的列表# 这些列表项定义在顶级字段groups之下groups: # 每个规则组都要有一个名字，且在当前文件中必须唯一 name: &lt;string&gt; # 组内的规则每多长时间评估（计算）一次 [ interval: &lt;duration&gt; | default = global.evaluation_interval ] # 限制记录规则和告警规则可以产生的数量。0表示无限制。 # 对于告警规则，用于限制其最多可生成的告警数量；对于记录规则，用于限制其最多可生成的序列数量 [ limit: &lt;int&gt; | default = 0 ] # 该规则组中的规则列表，对于每条告警规则，要遵循规则语法 rules: # 要输出的时间序列的名称。必须是有效的指标名称。 record: &lt;string&gt; # 要评估的 PromQL 表达式。每次评估周期到达时都会基于当前时间进行表达式计算 # 结果会记录为一组新的以 &#x27;record&#x27; 给定的指标名称的时间序列，即会生成一个新的时间序列 expr: &lt;string&gt; # 在存储结果之前要添加或覆盖的标签，即在记录规则上添加的标签 labels: [ &lt;labelname&gt;: &lt;labelvalue&gt; ] 3 Recording Rule示例通常要保存于单独的文件中 123456789101112131415root@ubuntu2004:/usr/local/prometheus# mkdir -p rules/recordingroot@ubuntu2004:/usr/local/prometheus# cd rules/recording/root@ubuntu2004:/usr/local/prometheus/rules/recording# vim recording_rules.yamlgroups:- name: custom_rules interval: 5s rules: - record: instance:node_cpu:avg_rate5m expr: 100 - avg(irate(node_cpu_seconds_total&#123;job=&quot;node&quot;, mode=&quot;idle&quot;&#125;[5m])) by (instance) * 100 - record: instace:node_memory_MemFree_percent expr: 100 - (100 * node_memory_MemFree_bytes / node_memory_MemTotal_bytes) - record: instance:root:node_filesystem_free_percent expr: 100 * node_filesystem_free_bytes&#123;mountpoint=&quot;/&quot;&#125; / node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;&#125; 而后在prometheus.yml中通过 rule_files 加载 12rule_files: - &quot;rules/recording/*.yaml&quot; Web UI 中的 &#x2F;rules 路径下可展示加载的所有rules 相关指标的使用方式与常规指标并无不同 4 mysqld Recording rule 示例123456789101112131415groups:- name: mysqld_rules rules: # 记录预先计算的时间序列的从属滞后秒数 # 考虑 `mysql_slave_status_sql_delay` - record: instance:mysql_slave_lag_seconds expr: mysql_slave_status_seconds_behind_master - mysql_slave_status_sql_delay # 通过心跳方式记录从站滞后 - record: instance:mysql_heartbeat_lag_seconds expr: mysql_heartbeat_now_timestamp_seconds - mysql_heartbeat_stored_timestamp_seconds - record: job:mysql_transactions:rate5m expr: sum without (command) (rate(mysql_global_status_commands_total&#123;command=~&quot;(commit|rollback)&quot;&#125;[5m]))","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"PromQL","slug":"Monitor/prometheus/promql","date":"2025-08-27T10:16:50.000Z","updated":"2025-09-12T09:32:17.572Z","comments":true,"path":"Monitor/prometheus/promql/","permalink":"https://aquapluto.github.io/Monitor/prometheus/promql/","excerpt":"","text":"1 时间序列数据时间序列数据是在一段时间内重复测量而获得的观测值的集合，即按照相同时序(相同的名字和标签)，以时间维度存储连续的数据的集合。然后将这些观测值绘制在图形中，它会有一个时间轴和数据轴。可以就是一个变量在不同时间点上的取值序列。 每一个观测值，即数据称为样本。时间序列的核心在于它记录的是随时间变化的数据点，每个数据点包含一个时间戳和对应的值。序列也称为向量，当多个序列数据放在同一个坐标系内时，就会形成一个由数据点组成的矩阵。 1.1 采集样本的数据格式上游采集的指标样本数据，他们由两部分构成 指标：指标名+标签 指标值 指标名称：代表着监控目标上某类可测量属性的基本特征标识，支持使用字母、数字、下划线和冒号，且必须能匹配RE2规范的正则表达式； 标签：键值型数据，附加在指标名称之上，同一指标可能会适配到多个目标或设备，因而给指标再次细分的多个可测量维度，用于对同类 metric 进行分类，可选项 可使用字母、数字和下划线，且必须能匹配RE2规范的正则表达式 以 “__” 为前缀的名称为Prometheus系统预留使用，是不会显示在 /metrics 页面里面的； 1.2 时间序列模型Prometheus将这些指标样本数据以时间序列的方式保存在TSDB时序数据库，并且定时保存到硬盘上，这些样本在TSDB中是按照时间序列组成的，所以一个完整的样本的构成中必然需要挟带时间，他们由三部分构成 指标：指标名+标签 时间戳(timestamp)：一个精确到毫秒的时间戳 样本值：一个 float64 的浮点型数据表示当前样本的值 所有这些指标都是 Prometheus 定期从 /metrics 接口那里采集过来的。采集的间隔时间的设置由prometheus.yaml 配置中的 scrape_interval 指定，如果scrape_interval: 15s，则意味着每 15 秒就会有一个带有新时间戳记录的新数据点 也就说对于同一个指标来说，prometheus会按照时间顺序存好它的一个个的值，因此我们可以将某一个指标按时间顺序存放的多个值称之为该指标的时间序列值，或者简称为时间序列，也称之为向量 123456^│ . . . . . . . . . . . . . . . . . . node_cpu_seconds_total&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125;│ . . . . . . . . . . . . . . . . . . node_cpu_seconds_total&#123;cpu=&quot;cpu0&quot;,mode=&quot;system&quot;&#125;│ . . . . . . . . . . . . . . . . . . node_load1&#123;&#125;│ . . . . . . . . . . . . . . . . . .v &lt;------------------ 时间 ----------------&gt; 横向的X轴是时间，纵向的Y轴是对应的值，这种按时间序列存放的值也称之为向量(vector) 选取X轴上的同一个时间点（精确到毫秒的时间戳），如上图所示你会发现同一时间节点，纵向的Y轴会有很多值，这些值分别对应不同的指标。 1.3 指标名称及标签使用注意事项指标名称和标签的特定组合代表着一个时间序列； 指标名称相同，但标签不同的组合分别代表着不同的时间序列； 不同的指标名称自然更是代表着不同的时间序列； PromQL支持基于定义的指标维度进行过滤和聚合 更改任何标签值，包括添加或删除标签，都会创建一个新的时间序列 应该尽可能地保持标签的稳定性，否则，则很可能创建新的时间序列，更甚者会生成一个动态的数据环境，并使得监控的数据源难以跟踪，从而导致建立在该指标之上的图形、告警及记录规则变得无效 2 PromQL概念PromQL (Prometheus Query Language)是Prometheus Server内置数据查询语言，基于PromQL表达式，用户可以针对指定的特征及其细分的纬度进行过滤、聚合、统计等运算从而产生期望的计算结果，进行实时的数据查询及聚合操作 PromQL使用表达式（expression）来表述查询需求 根据其使用的指标和标签，以及时间范围，表达式的查询请求可灵活地覆盖在一个或多个时间序列的一定范围内的样本之上，甚至是只包含单个时间序列的单个样本 PromQL支持处理两种向量，并内置提供了一组用于数据处理的函数 即时向量：最近一次的时间戳上跟踪的数据指标； 时间范围向量：指定时间范围内的所有时间戳上的数据指标； 2.1 数据类型PromQL的表达式中支持4种数据类型 即时向量（Instant Vector）：特定或全部的时间序列集合上，具有相同时间戳的一组样本值称为即时向量；表达式的返回值中只会包含该时间序列中的最新的一个样本值。简单来说，一个时间点的数据性能。 查出的是一组&#x2F;多个时间序列值，但每一个时间序列值中只包含最新的一个样本值 实时查询：用于获取某个时间点的监控数据，这通常用于手动调试或故障排查 即时分析：用于实时的统计分析，如计算当前时刻各指标的平均值、最大值、最小值等 数据呈现：系统仪表板中展示当前的系统状态或最新的指标值 范围向量（Range Vector）：特定或全部的时间序列集合上，在指定的同一时间范围内的所有样本值。简单来说，一段时间的数据性能值 查出的也是一组&#x2F;多个时间序列值，但每一个时间序列包含的是一段时间范围内的样本数据 时间序列分析：用于计算一段时间内的性能指标，如速率、增量或者百分比变化，这对于趋势分析和历史数据分析非常有用 趋势监控：绘制一段时间内的指标变化曲线，帮助识别潜在问题或验证优化效果 聚合计算：对一段时间内的数据进行聚合操作，如计算总和、平均值等 1sum_over_time(container_cpu_usage_seconds_total[1h]) # 该查询表示每个容器在过去1小时内累计消耗的CPU时间总量 标量（Scalar）：一个浮点型的数据值，只有一个数字，没有时序。 阈值对比：用于设置报警阈值，某个指标是否超过或低于某个值 node_load1 &gt; 1 数值计算：与瞬时向量、区间向量结合进行数值运算 (node_memory_MemFree_bytes / node_memory_MemTotal_bytes) * 100 聚合结果转换：使用 scalar() 函数可以将瞬时向量转换为标量 确保返回单一数值，防止多值返回 字符串（String）：支持使用单引号、双引号或反引号进行引用，但反引号中不会对转义字符进行转义 scalar()用法示例及解释 需求：将过去5分钟内，每个kubelet_http_requests_total 的速率与所有这些速率的平均值进行比较，留下超过平均值的 1234567# 1、没有使用scalar转换，查出结果为空# 分析：因为rate(kubelet_http_requests_total[5m]) 和avg(rate(kubelet_http_requests_total[5m]))返回的类型不匹配。rate(kubelet_http_requests_total[5m]) &gt; avg(rate(kubelet_http_requests_total[5m]))# 2、使用scalar转换，可以正常查询# 分析：在这个查询中，使用scalar将 avg(rate(kubelet_http_requests_total[5m]))强制转换为标量，确保类型匹配，因此能顺利执行。rate(kubelet_http_requests_total[5m]) &gt; scalar(avg(rate(kubelet_http_requests_total[5m]))) 2.2 时间序列选择器PromQL的查询操作需要针对有限个时间序列上的样本数据进行，挑选出目标时间序列是构建表达式时最为关键的一步 用户可使用向量选择器表达式来挑选出给定指标名称下的所有时间序列或部分时间序列的即时（当前）样本值或现在至过去某个时间范围内的样本值，前者称为即时向量选择器，后者称为范围向量选择器 2.2.1 向量选择器表达式使用要点表达式的返回值类型亦是即时向量、范围向量、标题或字符串4种数据类型其中之一，但是，有些使用场景要求表达式返回值必须满足特定的条件，例如 需要将返回值绘制成图形时，仅支持即时向量类型的数据； 对于诸如rate一类的速率函数来说，其要求使用的却又必须是范围向量型的数据； 由于范围向量选择器的返回的是范围向量型数据，它不能用于表达式浏览器中图形绘制功能，否则，表达式浏览器会返回以下错误：Error executing query: invalid expression type &quot;range vector&quot; for range query, must be Scalar or instant Vector 。事实上，范围向量选择几乎总是结合速率类的函数rate一同使用 2.2.2 即时向量选择器2.2.2.1 即时向量即时向量选择器（Instant Vector Selectors）：返回0个、1个或多个时间序列上在给定时间戳（instant）上的各自的一个样本，该样本也可称为即时样本 12http_requests_totalhttp_requests_total&#123;job=&quot;prometheus&quot;&#125; 即时向量选择器由两部分组成； 指标名称：用于限定特定指标下的时间序列，即负责过滤指标；可选； 匹配器（Matcher）：或称为标签选择器，用于过滤时间序列上的标签；定义在&#123;&#125;之中；可选； 显然，定义即时向量选择器时，以上两个部分应该至少给出一个；于是，这将存在以下三种组合； 仅给定指标名称，或在标签名称上使用了空值的匹配器：返回给定的指标下的所有时间序列各自的即时样本； 例如 http_requests_total 和 http_requests_total&#123;&#125; 的功能相同，都是用于返回 http_requests_total 指标下各时间序列的即时样本； 仅给定匹配器：返回所有符合给定的匹配器的所有时间序列上的即时样本； 注意：这些时间序列可能会有着不同的指标名称； 例如， &#123;job=&quot;.*&quot;, method=&quot;get&quot;&#125; 指标名称和匹配器的组合：返回给定的指定下的，且符合给定的标签过滤器的所有时间序列上的即时样本； 例如， http_requests_total&#123;method=&quot;get&quot;&#125; Prometheus会周期性的对Exporter的target进行Pull。我们查询最新的Metric信息，会返回距离服务器当前时间最近的采样点的Metric信息。 2.2.2.2 匹配器（Matcher）匹配器用于定义标签过滤条件，目前支持如下4种匹配操作符 1234= #选择与提供的字符串完全相等的标签。!= #选择不等于所提供字符串的标签=~ #选择与提供的字符串正则表达式匹配的标签。!~ #选择与提供的字符串不正则匹配的标签。 注意事项 匹配到空标签值的匹配器时，所有未定义该标签的时间序列同样符合条件； 例如，http_requests_total&#123;env= &quot;&quot;&#125; ，则该指标名称上所有未使用该标签（env）的时间序列也符合条件，比如时间序列http_requests_total&#123;method =&quot;get&quot;&#125; 正则表达式将执行完全锚定机制，它需要匹配指定的标签的整个值； 向量选择器至少要包含一个指标名称，或者至少有一个不会匹配到空字符串的匹配器； 例如，&#123;job=&quot;&quot;&#125; 为非法的选择器； 使用 “__name__” 做为标签名称，还能够对指标名称进行过滤； 例如，&#123;__name__=~&quot;http_requests_.*&quot;&#125; 能够匹配所有以 “ http_requests_ ” 为前缀的所有指标 2.2.3 范围向量选择器2.2.3.1 范围向量范围向量选择器（Range Vector Selectors）：返回0个、1个或多个时间序列上在给定时间范围内的各自的一组样本； 1http_requests_total[5m] 同即时向量选择器的唯一不同之处在于，范围向量选择器需要在表达式后紧跟一个方括号 [ ] 来表达需在时间时序上返回的样本所处的时间范围； 时间范围：以当前时间为基准时间点，指向过去一个特定的时间长度；例如 [5m] 便是指过去5分钟之内； 时间格式：一个整数后紧跟一个时间单位，例如“5m”中的“m”即是时间单位； 可用的时间单位有ms（毫秒）、s（秒）、m（分钟）、h（小时）、d（天）、w（周）和y（年）; 必须使用整数时间，且能够将多个不同级别的单位进行串联组合，以时间单位由大到小为顺序，例如1h30m，但不能使用1.5h； 需要注意的是，不管是即时向量还是范围向量，虽然不同时间序列的数据抓取时间点相同，但它们的时间戳并不会严格对齐 Prometheus在同一轮查询中去同时拉取多个Target上的数据，但是为了以均衡Prometheus Server的负载，多个Target上的数据抓取需要分散在抓取时间点前后一定的时间范围内，那么就会导致不同时间序列的实际数据抓取时间点可能不会严格对齐； 例如，一个时间序列可能在某秒的第一毫秒采集了数据，而另一个时间序列则在同秒的最后毫秒采集了数据； 因而，Prometheus在趋势上准确，但并非绝对精准； 2.2.3.2 偏移量修改器默认情况下，即时向量选择器和范围向量选择器都以当前时间为基准时间点，而偏移量修改器能够修改该基准； 偏移量修改器的使用方法是紧跟在选择器表达式之后使用“offset”关键字指定 12345#表示获取以http_requests_total为指标名称的所有时间序列在过去5分钟之时的即时样本http_requests_total offset 5m#表示获取距此刻1天时间之前的5分钟之内的所有样本http_requests_total[5m] offset 1d 3 指标类型3.1 Counter型指标及常用函数计数器，单调递增，除非重置（例如服务器或进程重启） 用于保存单调递增型的数据，例如站点访问次数等；不能为负值，也不支持减少，但可以重置回0； 需要做的是速率计算。比如网卡每秒发送包的速率，上一个时间点发送了9000字节，过了30秒，发送了9900字节，那么速率就是(9900-9000)/30 = 30b/s 通常，Counter的总数并没有直接作用，而是需要借助于rate、increase和irate等函数来生成样本数据的变化状况（增长率） 1rate(nginx_http_requests_total[1m]) 获取1分钟内，该指标各相关时间序列上的http总请求数的平均增长速率； 指定时间范围内的样本的最后一个样本值减去第一个样本值，而后除以这两个样本之间的间隔时长 1irate(nginx_http_requests_total[1m]) 高灵敏度函数，用于计算指标的瞬时速率 基于样本范围内的最后两个样本计算增长的速率，即最后一个样本减去其前面的一个样本，并除以间隔的时长 相较于rate函数来说，irate更适用于短期时间范围内的变化速率分析 1icrease(nginx_http_requests_total[1m]) 计算指定时间范围内样本值的增加量 可能会引用时间范围边界之前的样本值，以便于整个计算能覆盖指定的整个时间范围 1topk(3, kubelet_http_requests_total) 统计访问排名前3的http请求 用Prometheus客户端库在Python中创建和增加一个计数器指标 12345678from prometheus_client import Counterapi_requests_counter = Counter( &#x27;http_requests_total&#x27;, &#x27;Total number of http api requests&#x27;, [&#x27;api&#x27;])api_requests_counter.labels(api=&#x27;add_product&#x27;).inc() 3.2 Gauge型指标及常用函数仪表盘，可增可减的数据（上下波动），用于表示瞬时值 用于存储有着起伏特征的指标数据，例如当前内存使用量或 CPU 利用率等 Gauge是Counter的超集，但存在指标数据丢失的可能性，Counter能让用户确切了解指标随时间的变化状态，而Gauge则可能随时间流逝而精准度越来越低； 所以经常用于进行求和、取平均值、最小值、最大值等聚合计算；也会经常结合PromQL的 predict_linear 和 delta 函数使用 1predict_linear(v range-vector, t, scalar) 预测时间序列 v 在 t 秒后的值，它通过线性回归的方式来预测样本数据的Gauge变化趋势 predict_linear(node_filesystem_free_bytes[1h], 4 * 3600) 基于过去 1 小时node_filesystem_free_bytes 指标的线性预测值，预测 4 小时后的磁盘可用空间 predict_linear(node_filesystem_free_bytes&#123;kubernetes_io_hostname=&quot;master01&quot;, device=&quot;/dev/sda3&quot;&#125;[24h], 5 * 86400) 1delta(v range-vector) 计算范围向量中每个时间序列上的第一个样本值与最后一个样本值之差 其计算结果与increase函数相同 返回具有给定增量和等效标签的即时向量 可以获取样本在一段时间范围内的变化情况 delta(http_requests_total[1h]) 1idelta(v range-vector) 计算范围向量中每个时间序列上最后两个样本之差，返回具有给定增量和等效标签的即时向量 范例 12345678#基于2小时的样本数据，来预测主机可用磁盘空间在4个小时之后的剩余情况predict_linear(node_filesystem_free&#123;job=&quot;node&quot;&#125;[2h], 4 * 3600) #返回该服务器上的CPU温度与2小时之前的差异delta(cpu_temp_celsius&#123;host=&quot;server01.wu.com&quot;&#125;[2h])#JVM堆内存用量排名前三的时间序列topk(3, jvm_memory_bytes_used&#123;area=&quot;heap&quot;&#125;) 使用Prometheus客户端库在Python中创建一个Gauge指标 12345678from prometheus_client import Gaugememory_used = Gauge( &#x27;node_memory_used_bytes&#x27;, &#x27;Total memory used in the node in bytes&#x27;, [&#x27;hostname&#x27;])memory_used.labels(hostname=&#x27;host1.domain.com&#x27;).set(943348382) 3.3 Histogram直方数据与summary摘要3.3.1 长尾问题长尾问题：指的是大部分数据集中在某个较小范围内，但仍有一部分数据分布在远离中心的尾部。这些尾部的数据虽然数量少，但可能对整体分析产生重要影响，特别是在性能分析中。 例如，以系统的API调用的平均响应时间为例，如果大部分 API 请求的响应时间都在 100ms 左右，但有个别请求的响应时间达到了 5秒，这些极慢的请求会显著影响平均响应时间的统计结果，使得平均值不能真实反映大部分请求的体验。这种现象被称为长尾问题。 为了规避长尾问题的影响，单纯用平均值必然不行，为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组，例如，统计延迟在 0 ~ 10 ms 之间的请求数有多少，而 10 ~ 20 ms 之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因，所以prometheus引入了 Histogram 和 Summary 类型的指标来处理和分析长尾问题 3.3.2 Histogram直方图，将指定时间范围内的样本数据值切分成不同时间的数据段，并评估出整体的样本个数及所有样本值之和，以便按需计算分位数，是一种对数据分布情况的图形表示。 计算样本平均值：以值的总和除以值的数量； 计算样本分位值：分位数有助于了解符合特定标准的数据个数；例如评估响应时长超过1秒钟的请求比例，若超过20%即发送告警等； 直方图将整个测量范围划分为一组区间，称为bucket(桶)，在一段时间范围内对数据进行采样，并将其计入可配置的bucket之中，由一系列高度不等的长条图（bar）或线段表示，用于展示单个测度的值的分布。所以直方图会存储包括样本值分布在每个bucket（bucket自身的可配置）中的数量、所有样本值之和以及总的样本数量。 它一般用横轴表示某个指标维度的数据取值区间，用纵轴表示样本统计的频率或频数，从而能够以二维图的形式展现数值的分布状况 为了构建Histogram，首先需要将值的范围进行分段，即将所有值的整个可用范围分成一系列连续、相邻（相邻处可以是等同值）但不重叠的间隔，而后统计每个间隔中有多少值 捕捉长尾：通过设置更细的桶，你可以捕捉到那些延迟较长的请求，从而分析长尾问题。 从统计学的角度看，分位数不能被聚合，也不能进行算术运算 与常规方式略有不同的是，Prometheus取值间隔的划分采用的是累积（Cumulative）区间间隔机制，即每个bucket中的样本均包含了其前面所有bucket中的样本，因而也称为累积直方图，如下图，只有绿色部分才是每个区间真实的样本数据总数 Histogram类型的每个指标有一个基础指标名称 &lt;basename&gt; ，它会提供多个时间序列 _bucket&#123;le=“&lt;上边界&gt;”&#125; ：观测桶的上边界，即样本统计区间，表示样本值小于等于上边界的所有样本数量 _bucket&#123;le=“+Inf”&#125; ：最大区间（包含所有样本）的样本数量 123456789#观测桶的上边界（upper inclusive bound），即样本统计区间#最大区间（包含所有样本）的名称为&lt;basename&gt;_bucket&#123;le=&quot;+Inf&quot;&#125;&lt;basename&gt;_bucket&#123;le=&quot;&lt;upper inclusive bound&gt;&quot;&#125;#所有样本观测值的总和&lt;basename&gt;_sum#总的观测次数，它自身本质上是一个Counter类型的指标&lt;basename&gt;_count 如下直方图通过将观测值分组到不同的“桶”中来记录分布情况，每个桶有一个上限 1234567891011121314151617# http 请求响应时间 &lt;= 0.005 秒的请求次数为 10prometheus_http_request_duration_seconds_bucket&#123;handler=&quot;/metrics&quot;,le=&quot;0.005&quot;&#125; 10# http 请求响应时间 &lt;= 0.01 秒的请求次数为 15prometheus_http_request_duration_seconds_bucket&#123;handler=&quot;/metrics&quot;,le=&quot;0.01&quot;&#125; 15# http 请求响应时间 &lt;= 0.025 秒的请求次数为 18prometheus_http_request_duration_seconds_bucket&#123;handler=&quot;/metrics&quot;,le=&quot;0.025&quot;&#125; 18# http 请求响应时间 &lt;= 0.05 秒的请求次数为 18prometheus_http_request_duration_seconds_bucket&#123;handler=&quot;/metrics&quot;,le=&quot;0.05&quot;&#125; 18# 所有请求的响应时间的总和，命名为 _sumprometheus_http_request_duration_seconds_sum&#123;handler=&quot;/metrics&quot;&#125; 10.107670803000001# 请求总数，命名为 _count ，效果与 _bucket&#123;le=“+Inf”&#125; 相同prometheus_http_request_duration_seconds_count&#123;handler=&quot;/metrics&quot;&#125; 20 从上述信息可以推断，在0.01秒到0.025秒之间的桶有3个额外的请求被计数（即18 - 15 &#x3D; 3），而在0.025秒到0.05秒之间没有新的请求被添加（因为两个桶的计数值都是18）。这表明最长的响应时间不超过0.025秒。 从服务启动以来的累计平均 HTTP 请求响应时间：总时间/总数 = 10.107670803000001/20 = 0.50538354015s 范例：基于单个时间序列（即特定的 api 和 instance），计算过去5分钟的HTTP请求平均响应时间（适合分析某个具体服务或实例的性能表现） 1rate(http_request_duration_seconds_sum&#123;api=&quot;add_product&quot;, instance=&quot;host1.domain.com&quot;&#125;[5m]) / rate(http_request_duration_seconds_count&#123;api=&quot;add_product&quot;, instance=&quot;host1.domain.com&quot;&#125;[5m]) 范例：基于所有时间序列（即所有API和实例），计算过去5分钟的HTTP请求平均响应时间（适合分析整个系统的整体性能表现） 1sum(rate(http_request_duration_seconds_sum[5m])) / sum(rate(http_request_duration_seconds_count[5m])) 累积间隔机制生成的样本数据需要额外使用内置的 histogram_quantile() 函数，即可根据Histogram指标来计算相应的分位数（quantile），即根据某个分位数去求某个bucket的样本数在所有样本数中占据的该分位数的比例（某个bucket的样本数在所有样本数中占据的比例）。 1histogram_quantile(φ scalar, b instant-vector) 基于histogram类型的样本数据计算 φ 分位数 (0 ≤ φ ≤ 1) φ 为期望计算的分位数，例如0.9，或者0.99等 b 是histogram类型的时间序列，其指标名称通常为原指标名称后跟一个“_bucket”后缀 histogram_quantile() 函数在计算分位数时会假定每个区间内的样本满足线性分布状态，因而它的结果仅是一个预估值，并不完全准确 预估的准确度取决于bucket区间划分的粒度；粒度越大，准确度越低 范例：这个值表示 99% 的 HTTP 请求的响应时间不超过 4.95 毫秒。换句话说，只有最慢的 1% 请求的响应时间超过了这个值，即这1%的请求是长尾，应该忽略 12345histogram_quantile( 0.99, sum(rate(kubelet_http_requests_duration_seconds_bucket[5m])) by (le)) # 假设结果为4.95毫秒 rate(kubelet_http_requests_duration_seconds_bucket[5m]): 计算过去5分钟内每个桶的每秒请求增长的数量 sum(rate(...)) by (le)：对所有实例的桶按 le 标签进行求和，得到每个桶的总请求速率，这里有很多请求速率，到底哪一个才是符合大多数情况的指标呢？往下看 histogram_quantile(0.99, ...)：计算99%分位数，即99%请求的响应时间不超过的值，最终得到一个合理的请求速率 这个查询的目的是计算在过去 5 分钟内，99% 的 HTTP 请求的响应时间不超过的最大值。通过汇总所有实例的请求数据，并计算 99% 分位数，你可以了解绝大多数请求的响应时间性能，而不受极端值的影响。 总结：Histogram 通过将样本数据分段（buckets），统计每个段内的样本数量，使得可以清楚地看到各个范围内的请求分布。这样可以精确地了解不同延迟范围的请求数量，从而判断出是否存在长尾问题 使用Prometheus的Python客户端库创建一个带有自定义桶的直方图指标 123456789101112from prometheus_client import Histogramapi_request_duration = Histogram( name=&#x27;http_request_duration_seconds&#x27;, documentation=&#x27;Api requests response time in seconds&#x27;, labelnames=[&#x27;api&#x27;, &#x27;instance&#x27;], buckets=(0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 25))api_request_duration.labels( api=&#x27;add_product&#x27;, instance=&#x27;host1.domain.com&#x27;).observe(0.3672) 3.3.3 Summary摘要，Histogram的扩展类型，但它是直接由被监控端自行聚合计算出分位数，即客户端会直接计算并上报分位数，并将计算结果响应给Prometheus Server的样本采集请求，所以Summary统计的不是区间的个数而是统计分位数；不支持sum或avg一类的聚合运算，而且其分位数由客户端计算并生成，Server端无法获取客户端未定义的分位数。 对于每个指标，Summary以指标名称 &lt;basename&gt; 为前缀，生成如下几个个指标序列 1&lt;basename&gt;&#123;quantile=&quot;&lt;φ&gt;&quot;&#125; 其中φ是分位点，其取值范围是(0 ≤ φ ≤ 1)；计数器类型指标；如下是几种典型的常用分位点； 0、0.25、0.5、0.75和1几个分位点 0.5、0.9和0.99几个分位点 0.01、0.05、0.5、0.9和0.99几个分位点 也提供了 _sum 和 _count 指标，像直方图一样，可用于计算随时间的平均值以及不同时间序列的平均值。 12345#抓取到的所有样本值之和&lt;basename&gt;_sum#抓取到的所有样本总数&lt;basename&gt;_count 范例：测量在host1.domain.com上运行的add_productAPI端点实例的响应时间的Summary指标 12345678http_request_duration_seconds_sum&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot;&#125; 8953.332http_request_duration_seconds_count&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot;&#125; 27892http_request_duration_seconds&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot; quantile=&quot;0&quot;&#125;http_request_duration_seconds&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot; quantile=&quot;0.5&quot;&#125; 0.232227334http_request_duration_seconds&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot; quantile=&quot;0.90&quot;&#125; 0.821139321http_request_duration_seconds&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot; quantile=&quot;0.95&quot;&#125; 1.528948804http_request_duration_seconds&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot; quantile=&quot;0.99&quot;&#125; 2.829188272http_request_duration_seconds&#123;api=&quot;add_product&quot; instance=&quot;host1.domain.com&quot; quantile=&quot;1&quot;&#125; 34.283829292 上面这个例子包括 sum 和 count 以及五个分位数。分位数0相当于最小值，分位数1相当于最大值。分位数0.5是中位数，分位数0.90、0.95和0.99相当于在 host1.domain.com 上运行的 add_product API 端点响应时间的第90、95和99个百分位。 在分布式系统中使用Summary不合适。假设有 10 台服务器提供 add_product 接口服务，每台服务器独立收集请求延迟数据，并使用Summary指标计算百分位数（如 P99），Prometheus 分别采集每台机器的指标。问题就在于分位数不能合并，因为它是非线性统计量，每台机器的 P99 是独立计算的，直接对他们平均没有意义，因为不知道每台机器实际处理了多少请求、请求分布是怎样的，最终是无法知道整个系统的实际P99延迟，只能看到局部的情况 总结：Summary预先计算和暴露请求的百分位值（如 50%, 90%, 99% 百分位），提供实时的延迟分布情况，从而快速指示长尾现象。 3.3.4 Histogram 与 Summary 的比较Histgram由于是在服务端集成的所有数据，所以可以对所有数据进行计算，也就可以对所有实例的数据进行聚合，最终得出分位数，反映出整体系统的情况；而Summary是在客户端计算的分位数，然后上报给Prometheus并保存在数据库，所以服务端只有每个实例的分位数数据，直接对不同实例上报的分位数取平均值并不能得到整体系统的实际分位数，因为忽略了每台实例的实际请求分布情况 特性 Histogram（直方图） Summary（摘要） 采集方式 客户端记录原始观测值，服务端统计分位数 客户端直接计算分位数并上报 数据结构 记录观测值落入不同 bucket 的计数 直接保存 count、sum、quantile 数据 聚合能力 ✅ 可以对多个实例进行聚合后计算整体分位数 ❌ 不能跨实例聚合 计算位置 Prometheus 服务端计算分位数 客户端自己计算分位数 延迟 vs 精度 延迟略高，但精度可控 实时快，但无法准确聚合 4 PromQL表达式4.1 聚合函数和聚合表达式一般说来，单个指标的价值不大，监控场景中往往需要联合并可视化一组指标，这种联合机制即是指“聚合”操作，例如，将计数、求和、平均值、分位数、标准差及方差等统计函数应用于时间序列的样本之上生成具有统计学意义的结果等； 对查询结果事先按照某种分类机制进行分组（groupby）并将查询结果按组进行聚合计算也是较为常见的需求，例如分组统计、分组求平均值、分组求和等； 聚合操作由聚合函数针对一组值进行计算并返回单个值或少量几值作为结果 Prometheus内置提供的11个聚合函数也称为聚合运算符 这些运算符仅支持应用于单个即时向量的元素，其返回值也是具有少量元素的新向量或标量 这些聚合运行符既可以基于向量表达式返回结果中的时间序列的所有标签维度进行分组聚合，也可以仅基于指定的标签维度分组后再进行分组聚合 4.1.1 聚合表达式PromQL中的聚合操作语法格式可采用如下面两种格式之一 12&lt;aggr-op&gt;([parameter,] &lt;vector expression&gt;) [without|by (&lt;label list&gt;)]&lt;aggr-op&gt; [without|by (&lt;label list&gt;)] ([parameter,] &lt;vector expression&gt;) 分组聚合：先分组、后聚合 without：从结果向量中删除由without子句指定的标签，未指定的那部分标签则用作分组标准； by：功能与without刚好相反，它仅使用by子句中指定的标签进行聚合，结果向量中出现但未被by子句指定的标签则会被忽略； 为了保留上下文信息，使用by子句时需要显式指定其结果中原本出现的 job、instance等一类的标签，事实上，各函数工作机制的不同之处也仅在于计算操作本身，PromQL对于它们的执行逻辑相似 1234567891011121314# 每台主机CPU在最近5分钟内的平均使用率 avg(...) by (instance)(1 - avg(rate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) by (instance)) * 100# 每个实例的1分钟平均负载是否超过了动态生成的阈值（2 * CPU核心数）node_load1 &gt; on (instance) 2 * count (node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;) by (instance)# 计算主机内存使用率，可用内存空间：空闲内存、buffer、cache 指标之和(node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100# 计算K8s所有node节点的所有容器内存使用量（以 GB 为单位） KB -&gt; MB -&gt; GBsum by (instance) (container_memory_usage_bytes&#123;instance=~&quot;node*&quot;&#125;) / 1024 / 1024 / 1024# 计算最近 1m 所有容器 cpu 使用率sum by (id) (rate(container_cpu_usage_seconds_total&#123;id!=&quot;/&quot;&#125;[1m])) 4.1.2 11个聚合函数其他更多内置函数参考官网：Prometheus_Function 聚合函数 作用 sum() 对样本值求和 avg() 对样本值求平均值，这是进行指标数据分析的标准方法 count() 对分组内的时间序列进行数量统计 stddev() 对样本值求标准差，以帮助用户了解数据的波动大小（或称之为波动程度） stdvar() 对样本值求方差，它是求取标准差过程中的中间状态 min() 求取样本值中的最小者 max() 求取样本值中的最大者 topk() 逆序返回分组内的样本值最大的前k个时间序列及其值 bottomk ( ) 顺序返回分组内的样本值最小的前k个时间序列及其值 quantile ( ) 分位数用于评估数据的分布状态，该函数会返回分组内指定的分位数的值，即数值落在小于 等于指定的分位区间的比例 count_values ( ) 对分组内的时间序列的样本值进行数量统计 4.2 二元运算符PromQL支持基本的算术运算和逻辑运算，这类运算支持使用操作符连接两个操作数，因而也称为二元运算符或二元操作符； 支持的运算 两个标量间运算； 即时向量和标量间的运算：将运算符应用于向量上的每个样本； 两个即时向量间的运算：遵循向量匹配机制； 将运算符用于两个即时向量间的运算时，可基于向量匹配模式（Vector Matching）定义其运算机制； 4.2.1 算术运算符+（加）、-（减）、*（乘）、&#x2F;（除）、%（取模）和 ^（幂运算） 12345#查看Prometheus的HTTP响应字节总和（以MB为单位）prometheus_http_response_size_bytes_sum/8/1024#获取内存可用率node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 4.2.2 比较运算符&#x3D;&#x3D;（等值比较）、!&#x3D;（不等）、&gt;、&lt;、&gt;&#x3D; 和 &lt;&#x3D; 12345#查询每个接口的请求次数超过20次的，value为1。不符合条件的数据，value为0prometheus_http_requests_total &gt; bool 20#待监测域名的监测code比200小，或者code大于等于400，返回1probe_http_status_code &lt;= bool 199 or probe_http_status_code &gt;= bool 400 4.2.3 逻辑运算符and（并且）、or（或者）和unless（除了） 目前，该运算仅允许在两个即时向量间进行，尚不支持标量参与运算 举例说明 and（并且）：vector1 and vector2 进行一个与操作，会产生一个新的集合。该集合中的元素同时在 vector1 和 vector2 中都存在。例如：我们有 vector1 为 A B C，vector2 为 B C D，那么 vector1 and vector2 的结果为：B C or（或者）：vector1 and vector2 进行一个或操作，会产生一个新的集合。该集合中包含 vector1 和 vector2 中的所有元素。例如：我们有 vector1 为 A B C，vector2 为 B C D，那么 vector1 or vector2 的结果为：A B C D unless（除了）：vector1 and vector2 进行一个或操作，会产生一个新的集合。该集合首先取 vector1 集合的所有元素，然后排除掉所有在 vector2 中存在的元素。例如：我们有 vector1 为 A B C，vector2 为 B C D，那么 vector1 unless vector2 的结果为：A 12345678#仅显示user和system的node节点CPU使用率结果node_cpu_seconds_total&#123;mode=~&quot;idle|user|system&quot;&#125; and node_cpu_seconds_total&#123;mode=~&quot;user|system|iowait&quot;&#125;#显示四种mode的node节点CPU使用率node_cpu_seconds_total&#123;mode=~&quot;idle|user|system&quot;&#125; or node_cpu_seconds_total&#123;mode=~&quot;user|system|iowait&quot;&#125;#仅显示idle的性能检测数据node_cpu_seconds_total&#123;mode=~&quot;idle|user|system&quot;&#125; unless node_cpu_seconds_total&#123;mode=~&quot;user|system|iowait&quot;&#125; 4.3 向量匹配在标量和即时向量之间使用运算符可以满足很多需求，但是在两个即时向量之间使用运算符时，哪些样本应该适用于哪些其他样本？这种瞬时向量的匹配称为向量匹配。即时向量间运算时，PromQL为会左侧向量中的每个元素在右侧向量中找到匹配的元素，其匹配行为有两种基本类型 一对一 （One-to-One） 一对多或多对一 （Many-to-One, One-to-Many） 向量与向量之间的运算，意味着两个向量必须具有“相同的标签”，且对应的“标签值也必须完全相同” 4.3.1 向量一对一匹配即时向量的一对一匹配 从运算符的两边表达式所获取的即时向量间依次比较，并找到唯一匹配（标签完全一致）的样本值 找不到匹配项的值则不会出现在结果中 在操作符两边表达式标签不一致的情况下，可以使用 on(labe1 list) 或者 ignoring(label list) 来修改便签的匹配行为 匹配表达式语法：&lt;vector expr&gt; &lt;bin-op&gt; on|ignoring(&lt;label list&gt;) &lt;vector expr&gt; ignoring：定义匹配检测时要忽略的标签 on：定义匹配检测时只使用的标签 范例：查询 HTTP 单个错误的监控数据 12345678910# 示例输入method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 24method_code:http_errors:rate5m&#123;method=&quot;get&quot;, code=&quot;404&quot;&#125; 30method_code:http_errors:rate5m&#123;method=&quot;put&quot;, code=&quot;501&quot;&#125; 3method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 6method_code:http_errors:rate5m&#123;method=&quot;post&quot;, code=&quot;404&quot;&#125; 21method:http_requests:rate5m&#123;method=&quot;get&quot;&#125; 600method:http_requests:rate5m&#123;method=&quot;del&quot;&#125; 34method:http_requests:rate5m&#123;method=&quot;post&quot;&#125; 120 比如我们想要计算使用GET方法中出现500错误的占比是多少 理想的计算公式是： GET请求出现500错误的总数 &#x2F; 总的GET请求数 * 100 &#x3D; GET请求出现500错误所占的比例 但由于两个向量的标签不完全相同（一个有code标签，一个没有），因此无法直接进行计算 12# 示例查询method_code:http_errors:rate5m&#123;code=&quot;500&quot;&#125; / ignoring(code) method:http_requests:rate5m 使用 ignoring(code) 后，可以计算出以下比例 &#123;method=&quot;get&quot;&#125; 0.04 &#123;method=&quot;post&quot;&#125; 0.05 4.3.2 向量一对多&#x2F;多对一匹配多对一和一对多的匹配模式，可以理解为向量元素中的一个样本数据匹配到了多个样本数据标签 “一” 侧的每个元素，可与“多”侧的多个元素进行匹配； 必须使用 group_left 或 group_right 明确指定哪侧为“多”侧； 多对一和一对多两种模式一定是出现在操作符两侧表达式返回的向量标签不一致的情况。因此也需要使用 ignoring 和 on 修饰符来排除或者限定匹配的标签列表。 匹配表达式语法：&lt;vector expr&gt; &lt;bin-op&gt; on|ignoring(&lt;label list&gt;) group_right|group_left(&lt;label list&gt;) &lt;vector expr&gt; group_left：用于在连接操作时保留左侧向量的所有时间序列，即使右侧向量中没有匹配的时间序列。如果右侧没有匹配的时间序列，则结果中的对应字段将为空 group_right：与 group_left 相反 范例：查询 HTTP 全部错误的监控数据 12# 示例查询method_code:http_errors:rate5m / ignoring(code) group_left method:http_requests:rate5m 返回一个结果向量，包含每个 HTTP 方法在过去的 5 分钟内出现请求错误的比例 &#123;method=&quot;get&quot;, code=&quot;500&quot;&#125; 0.04 &#123;method=&quot;get&quot;, code=&quot;404&quot;&#125; 0.05 &#123;method=&quot;post&quot;, code=&quot;500&quot;&#125; 0.05 &#123;method=&quot;post&quot;, code=&quot;404&quot;&#125; 0.175 5 Prometheus API中使用PromQL5.1 API响应格式Prometheus当前稳定的HTTP API可以通过 /api/v1 访问 Prometheus API使用了JSON格式的响应内容。 当API调用成功后将会返回2xx的HTTP状态码。 反之，当API调用失败时可能返回以下几种不同的HTTP状态码： 404 Bad Request:当参数错误或者缺失时。 422 Unprocessable Entity 当表达式无法执行时。 503 Service Unavailiable 当请求超时或者被中断时。 所有的API请求均使用以下的JSON格式： 1234567&#123; &quot;status&quot;: &quot;success&quot; | &quot;error&quot;, // 请求状态 &quot;data&quot;: &lt;data&gt;, // 实际返回的数据 &quot;errorType&quot;: &quot;&lt;string&gt;&quot;, // 错误类型 &quot;error&quot;: &quot;&lt;string&gt;&quot; // 错误描述信息 &quot;warnings&quot;: &quot;&lt;string&gt;&quot; // 警告信息（可选，如查询语法存在不影响执行的警告）&#125; 5.2 在HTTP API中使用PromQL通过HTTP API我们可以分别通过 /api/v1/query 和 /api/v1/query_range 查询PromQL表达式当前或者一定时间范围内的计算结果。 5.2.1 即时数据查询通过使用query API我们可以查询PromQL在特定时间点下的计算结果 1GET /api/v1/query URL请求参数 query &#x3D;: PromQL表达式。 time &#x3D;: 用于指定用于计算PromQL的时间戳。可选参数，默认情况下使用当前系统时间。 timeout &#x3D;: 超时设置。可选参数，默认情况下使用全局设置。 例如使用以下表达式查询表达式up在时间点 2015-07-01T20:10:51.781Z 的计算结果 1curl &#x27;http://localhost:9090/api/v1/query?query=up&amp;time=2015-07-01T20:10:51.781Z&#x27; 当API调用成功后，Prometheus会返回JSON格式的响应内容，并且在data节点中返回查询结果。data节点格式如下 1234&#123; &quot;resultType&quot;: &quot;matrix&quot; | &quot;vector&quot; | &quot;scalar&quot; | &quot;string&quot;, // 数据类型 &quot;result&quot;: &lt;value&gt; // 实际数据数组，value是一个瞬时值&#125; 类型 描述 vector 瞬时向量，表示某一时刻的指标快照（如 http_requests_total 当前值）。 matrix 范围向量，表示一段时间内的指标变化（需搭配 query_range API 使用）。 scalar 标量值（如 count(http_requests_total) 返回的单个数值）。 string 字符串值（如 time() 返回的时间字符串）。 5.2.2 范围数据查询通过使用 query_range API 我们可以查询PromQL在一段时间内下的计算结果 1GET /api/v1/query_range URL请求参数 query &#x3D;: PromQL表达式。 start &#x3D;: 起始时间。 end &#x3D;: 结束时间。 step &#x3D;: 查询步长。 timeout &#x3D;: 超时设置。可选参数，默认情况下使用全局设置。 当使用QUERY_RANGE API查询PromQL表达式时，返回结果一定是一个范围向量。 需要注意的是，在QUERY_RANGE API中PromQL只能使用瞬时向量选择器类型的表达式 例如使用以下表达式查询表达式up在30秒范围内以15秒为间隔计算PromQL表达式的结果。 12345curl &#x27;http://localhost:9090/api/v1/query_range?query=up&amp;start=2015-07-01T20:10:30.781Z&amp;end=2015-07-01T20:11:00.781Z&amp;step=15s&#x27;&#123; &quot;resultType&quot;: &quot;matrix&quot; | &quot;vector&quot; | &quot;scalar&quot; | &quot;string&quot;, // 数据类型 &quot;result&quot;: &lt;value&gt; // 范围值数组（时间戳 + 值）&#125;","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"Exporter","slug":"Monitor/prometheus/exporter","date":"2025-08-27T10:16:32.000Z","updated":"2025-09-12T09:16:45.147Z","comments":true,"path":"Monitor/prometheus/exporter/","permalink":"https://aquapluto.github.io/Monitor/prometheus/exporter/","excerpt":"","text":"1 Exporter基础Exporter是Prometheus的指标数据收集组件。它负责从目标Jobs收集数据，并把收集到的数据转换为Prometheus支持的时序数据格式。 和传统的指标数据收集组件不同的是，他只负责收集，并不向Server端发送数据，而是等待Prometheus Server 主动抓取(pull) 对于那些未内建Instrumentation，且也不便于自行添加该类组件以暴露指标数据的应用程序来说，常用的办法是于待监控的目标应用程序外部运行一个独立指标暴露程序，该类型的程序即统称为Exporter；换句话说，Exporter负责从目标应用程序上采集和聚合原始格式的数据，并转换或聚合为Prometheus格式的指标向外暴露； 对于那些非由用户可直接控制的应用代码来说，为其添加客户端库以进行直接测量很难实现 操作系统内核就是一个典型的示例，它显然不大可能易于实现添加自定义代码通过HTTP协议输出Prometheus格式的指标 但这一类的程序一般都会通常某种接口输出其内在的指标，只不过这些指标可能有着特殊的格式，例如Linux内核的特有指标格式，或者SNMP指标格式等 这些指标需要对它进行适当的解析和处理以转换为合用的目标格式，Exporter（指标暴露器）是完成此类转换功能的应用程序 Exporter独立运行于要获取其测量指标的应用程序之外，负责接收来自于Prometheus Server的指标获取请求，它通过目标应用程序（真正的目标）内置的指标接口获取指标数据，并将这些指标数据转换为合用的目标格式后响应给Prometheus Exporter更像是“一对一”的代理，它作为Prometheus Server的target存在，工作于应用程序的指标接口和Prometheus的文本指标格式之间转换数据格式 但Exporter不存储也不缓存任何数据 2 Node Export2.1 二进制部署每个被监控的主机节点上均应该部署node-exporter，用于监控系统级指标 123456789101112131415161718192021222324252627282930313233343536373839404142434445root@ubuntu2004:~# wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gzroot@ubuntu2004:~# tar xf node_exporter-1.8.2.linux-amd64.tar.gz -C /usr/local/root@ubuntu2004:~# ln -sv /usr/local/node_exporter-1.8.2.linux-amd64/ /usr/local/node_exporter&#x27;/usr/local/node_exporter&#x27; -&gt; &#x27;/usr/local/node_exporter-1.8.2.linux-amd64/&#x27;root@ubuntu2004:~# ls /usr/local/node_exporterLICENSE node_exporter NOTICEroot@ubuntu2004:~# vim /usr/lib/systemd/system/node_exporter.service[Unit]Description=node_exporterDocumentation=https://prometheus.io/docs/introduction/overview/After=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/node_exporter/node_exporter \\ --collector.ntp \\ --collector.mountstats \\ --collector.systemd \\ --collector.ethtool \\ --collector.tcpstatExecReload=/bin/kill -HUP $MAINPIDTimeoutStopSec=20sRestart=always[Install]WantedBy=multi-user.targetroot@ubuntu2004:~# systemctl daemon-reloadroot@ubuntu2004:~# systemctl start node_exporter.serviceroot@ubuntu2004:~# systemctl enable node_exporter.serviceroot@ubuntu2004:~# ss -tnlp | grep &#x27;9100&#x27;LISTEN 0 4096 *:9100 *:* users:((&quot;node_exporter&quot;,pid=6011,fd=7)) # 查看指标数据，HELP解释当前指标的含义，TYPE说明当前指标的数据类型root@ubuntu2004:~#curl localhost:9100/metrics | less# HELP node_cpu Seconds the cpus spent in each mode.# TYPE node_cpu counternode_cpu&#123;cpu=&quot;cpu0&quot;,mode=&quot;idle&quot;&#125; 362812.7890625# HELP node_load1 1m load average.# TYPE node_load1 gaugenode_load1 3.0703125 选项 --collector.&lt;name&gt; 用于指定启用的collector 默认即启用的collector，可使用选项 --no-collector.&lt;name&gt; 禁用 编辑Prometheus配置文件，去采集Node Exporter暴露出来的指标数据 1234567891011121314151617root@ubuntu2004:~# vim /usr/local/prometheus/prometheus.ymlscrape_configs: - job_name: &quot;prometheus&quot; static_configs: - targets: [&quot;localhost:9090&quot;] # 添加以下三项 - job_name: &quot;linux&quot; static_configs: - targets: [&quot;localhost:9100&quot;]root@ubuntu2004:~# /usr/local/prometheus/promtool check config /usr/local/prometheus/prometheus.ymlroot@ubuntu2004:~# systemctl reload prometheus.service#或者curl -XPOST localhost:9090/-/reload访问http://localhost:9090/config 2.2 容器部署12345678910111213141516171819202122232425version: &#x27;3.6&#x27;networks: monitoring: driver: bridgeservices: node-exporter: image: bitnami/node-exporter:1.8.2 container_name: node-exporter volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - &#x27;--path.procfs=/host/proc&#x27; - &#x27;--path.sysfs=/host/sys&#x27; - &#x27;--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)&#x27; #- &#x27;--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker)($$|/)&#x27; - &#x27;--path.rootfs=/rootfs&#x27; ports: - 9100:9100 networks: - monitoring restart: always --path.procfs=/host/proc: 指定 /proc 文件系统的路径。 --path.sysfs=/host/sys: 指定 /sys 文件系统的路径。 --collector.filesystem.mount-points-exclude 和 --collector.filesystem.ignored-mount-points: 排除某些特定的挂载点，避免不必要的文件系统指标被收集。 --path.rootfs=/rootfs: 指定根文件系统的路径。 2.3 Node Exporter的指标启用或禁用Node Exporter内生支持的指标 通过提供 --collector.&lt;name&gt; 标志来启用收集器。 可以通过提供 --no-collector.&lt;name&gt; 标志来禁用默认启用的收集器。 12345678910node_boot_time #系统启动时间node_cpu #系统CPU使用量nodedisk* #磁盘IOnodefilesystem* #文件系统用量node_load1 #系统负载nodememeory* #内存使用量nodenetwork* #网络带宽node_time #当前系统时间go_* #node exporter中go相关指标process_* #node exporter自身进程相关运行指标 常用指标 123456node_cpu_seconds_totalnode_memory_MemTotal_bytesnode_filesystem_size_bytes&#123;mount_point=PATH&#125;node_system_unit_state&#123;name=&#125;node_vmstat_pswpin #系统每秒从磁盘读到内存的字节数；node_vmstat_pswpout #系统每秒钟从内存写到磁盘的字节数； 相关文档 1https://github.com/prometheus/node_exporter 2.3.1 CPU监控CPU的监控项名称是：node_cpu_seconds_total，使用总量 直接执行node_cpu_seconds_total查询后会出现很多监控指标，显然不是想要的 node_cpu_seconds_total执行后会出现很多监控指标，其中各种类型的比如系统态、用户态都会由mode标签来区分 我们想要查询CPU的使用率的思路是：查出当前空闲的CPU百分比，最后用100减去，mode标签值idle就表示当前空闲的CPU值 cpu&#x3D;”0”代表第1核cpu，cpu&#x3D;”1”代表第2核cpu 12345678910111213141516171819#获取空闲cpu监控数据，mode标签值为idle的为空闲（user代表mode为用户，system代表mode为系统...）node_cpu_seconds_total&#123;mode=&#x27;idle&#x27;&#125;#获取某台监控机器的cpu数据，instance标签值为centos2服务器的cpu数据node_cpu_seconds_total&#123;instance=&#x27;centos2服务器&#x27;&#125;#获取1分钟/5分钟/15分钟的cpu负载node_load1node_load5node_load15#获取5分钟内的监控数据node_cpu_seconds_total&#123;mode=&#x27;idle&#x27;&#125;[5m]#获取5分钟内的cpu平均空闲状况avg(irate(node_cpu_seconds_total&#123;mode=&#x27;idle&#x27;&#125;[5m])) by (instance)#获取cpu5分钟内的使用率100 - (avg(irate(node_cpu_seconds_total&#123;mode=&#x27;idle&#x27;&#125;[5m])) by (instance) *100) 2.3.2 内存监控由于内存的监控项没有像CPU一样区分了很多标签，因此内存监控相较于CPU则需要结合很多个监控项 1234node_memory_MemFree_bytes #空闲内存node_memory_MemTotal_bytes #总内存node_memory_Cached_bytes #缓存node_memory_Buffers_bytes #缓冲区内存 监控内存使用的思路： 空闲内存+缓存+缓冲区内存得出空闲总内存 得出的空闲总内存再除总内存大小再乘100，得出空闲率 再用100 - 空闲率就得出使用率 12345678#获取空闲内存(node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes)#获取空闲内存率(node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100#获取内存使用率100 - ((node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes) / node_memory_MemTotal_bytes * 100) 2.3.3 磁盘使用率关于磁盘使用率，这里我们用到的主要有： 123node_filesystem_free_bytes #剩余磁盘空间node_filesystem_size_bytes #磁盘空间总大小node_disk相关 这两个监控项中都有相同的标签可以关联，我们这里用到的标签有fstype，fstype标签值是关于磁盘的文件系统类型，对于磁盘监控，我们主要对xfs、ext4等文件系统的磁盘进行监控，像tmpfs这种的不必要监控，另一个主要的标签是mountpoint，这个标签值主要用来储存磁盘的挂载点，我们可以通过标签来选择要对那个挂载点的磁盘进行监控 磁盘使用率实现思路： 1.由磁盘空闲容量除磁盘总容量乘100即可得到磁盘空闲率 2.用100减磁盘空闲率即可得到磁盘使用率 123456#获取磁盘空闲率node_filesystem_free_bytes&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint=&quot;/&quot;&#125; / node_filesystem_size_bytes&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint=&quot;/&quot;&#125; *100df -hT #linux中获取磁盘空闲率命令#获取磁盘使用率100 - (node_filesystem_free_bytes&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint=&quot;/&quot;&#125; / node_filesystem_size_bytes&#123;fstype=~&quot;ext4|xfs&quot;,mountpoint=&quot;/&quot;&#125; *100) 2.3.4 网络采集node_network_ 相关都属于网络采集数据 1234#网络流出流量node_network_transmit_bytes_total#网络流入流量 node_network_receive_bytes_total 2.3.5 系统服务状态监控服务的状态，例如nginx、docker这种服务器的启动状态 node_exporter是根据systemd去监控的，因此只有能用systemctl启动的服务器才能被监控到 配置非常简单，只需要在启动时开启system监控，并指定监控什么服务即可 配置system监控的参数： --collector.systemd --collector.systemd.unit-whitelist=&quot;.+&quot; 12345vi /usr/lib/systemd/system/node_exporter.service ExecStart=/data/node_exporter/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(docker|sshd|node_exporter).servicesystemctl daemon-reload systemctl restart node_exporter.service 以docker为例，我们查询docker存活状态 node_systemd_unit_state 使用这个监控项查看，里面也有很多标签，name=“docker.service”，标签name表示服务的名称， state=“active”，state表示服务的状态，active表示活动的，对应的监控值也是1，如果为1则表示正常，不为1表示异常 1node_systemd_unit_state&#123;name=&quot;docker.service&quot;, state=&quot;active&quot;&#125; 2.4 适用于主机监控的USE方法对CPU来说，USE通常意味着如下概念 CPU使用率随时间的百分比； CPU饱和度，等待CPU的进程数； 错误，通常对CPU不太有影响； 对内存来说，USE的意义相似 内存使用率随时间的百分比； 内存饱和度，可通过监控swap进行测量； 错误，通常不太关键； 2.4.1 CPU使用率每台主机CPU在5分钟内的平均使用率 2.4.2 CPU的饱和度跟踪CPU的平均负载就能获取到相关主机的CPU饱和度，实际上，它是将主机上的CPU数量考虑在内的一段时间内的平均运行队列长度 平均负载少于CPU的数量是正常状况，而长时间内超过CPU数量则表示CPU已然饱和； 12#查询1分钟平均负载超过主机CPU数量两倍的时间序列node_load1 &gt; on (instance) 2 * count (node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;) by (instance) 2.4.3 内存使用率node_exporter暴露了多个以node_memory为前缀的指标，我们重点关注如下几个 node_memory_MemTotal_bytes node_memory_MemFree_bytes node_memory_Buffers_bytes node_memory_Cached_bytes 计算使用率 可用空间：上面后三个指标之和； 已用空间：总空间减去可用空间； 使用率：已用空间除以总空间 3 MySQL Exporter3.1 容器部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172root@ubuntu2004:~# cd learning-prometheus/08-prometheus-components-compose/mysqld-and-exporter/root@ubuntu2004:mysqld-and-exporter# vim docker-compose.yml version: &#x27;3.6&#x27;volumes: mysqld_data: &#123;&#125;networks: monitoring: driver: bridge ipam: config: - subnet: 192.168.0.0/24services: mysqld: image: mysql:8.0 volumes: - ./mysql:/etc/mysql/conf.d - mysqld_data:/var/lib/mysql environment: - MYSQL_ALLOW_EMPTY_PASSWORD=true networks: - monitoring ports: - 3306:3306 # CREATE USER &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27; IDENTIFIED BY &#x27;exporter&#x27;; # GRANT PROCESS, REPLICATION CLIENT ON *.* TO &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27;; # GRANT SELECT ON performance_schema.* TO &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27;; mysqld-exporter: image: prom/mysqld-exporter:v0.16.0 command: - --mysqld.username=exporter:exporter - --mysqld.address=10.0.0.204:3306 - --collect.info_schema.innodb_metrics - --collect.info_schema.innodb_tablespaces - --collect.perf_schema.eventsstatementssum - --collect.perf_schema.memory_events - --collect.global_status - --collect.engine_innodb_status - --collect.binlog_size environment: - DATA_SOURCE_NAME=exporter:exporter@(mysqld:3306)/ ports: - 9104:9104 networks: - monitoring depends_on: - mysqld root@ubuntu2004:~# docker-compose pullroot@ubuntu2004:~# docker-compose uproot@ubuntu2004:~# docker-compose psNAME IMAGE COMMAND SERVICE CREATED STATUS PORTSroot-mysqld-1 mysql:8.0 &quot;docker-entrypoint.s…&quot; mysqld 11 hours ago Up 11 seconds 0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcproot-mysqld-exporter-1 prom/mysqld-exporter:v0.16.0 &quot;/bin/mysqld_exporte…&quot; mysqld-exporter 11 seconds ago Up 10 seconds 0.0.0.0:9104-&gt;9104/tcp, :::9104-&gt;9104/tcproot@ubuntu2004:~# ss -ntl | grep -E &#x27;(3306|9104)&#x27;LISTEN 0 4096 0.0.0.0:3306 0.0.0.0:* LISTEN 0 4096 0.0.0.0:9104 0.0.0.0:* LISTEN 0 4096 [::]:3306 [::]:* LISTEN 0 4096 [::]:9104 [::]:* root@ubuntu2004:~# docker-compose exec mysqld /bin/shsh-5.1# mysqlmysql&gt; CREATE USER &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27; IDENTIFIED BY &#x27;exporter&#x27;;mysql&gt; GRANT PROCESS, REPLICATION CLIENT ON *.* TO &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27;;mysql&gt; GRANT SELECT ON performance_schema.* TO &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27;;# 查看指标root@ubuntu2004:~# curl localhost:9104/metrics 编辑Prometheus配置文件 12345678root@ubuntu2004:~# vim /usr/local/prometheus/prometheus.yml scrape_configs: # 添加下列 - job_name: &quot;mysql&quot; static_configs: - targets: [&quot;10.0.0.204:9104&quot;] root@ubuntu2004:~# systemctl reload prometheus.service 3.2 二进制部署MySQL 12345# 安装略# 创建用户并授权mysql&gt; CREATE USER &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27; IDENTIFIED BY &#x27;exporter&#x27;;mysql&gt; GRANT PROCESS, REPLICATION CLIENT ON *.* TO &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27;;mysql&gt; GRANT SELECT ON performance_schema.* TO &#x27;exporter&#x27;@&#x27;192.168.%.%&#x27;; MySQL Exporter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@rocky8 ~]# wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.16.0/mysqld_exporter-0.16.0.linux-amd64.tar.gz[root@rocky8 ~]#cd /usr/local/[root@rocky8 local]#ln -sv mysqld_exporter-0.16.0.linux-amd64 mysqld_exporter[root@rocky8 local]#cd mysqld_exporter[root@rocky8 mysqld_exporter]#lsLICENSE mysqld_exporter NOTICE[root@rocky8 mysqld_exporter]# vim .my.cnf[client]user=exporterpassword=exporter[root@rocky8 mysqld_exporter]#vim /usr/lib/systemd/system/mysqld_exporter.service[Unit]Description=consul_exporterDocumentation=https://prometheus.io/docs/introduction/overview/After=network.target[Service]Type=simpleUser=mysqlEnvironmentFile=-/etc/default/mysqld_exporter# 具体使用时，若mysql_exporter与mysql server不在同一主机时，mysql server要指向实际的地址；# mysql_exporter连接mysql server使用的用户名和密码均为exporter，该用户要获得正确的授权；Environment=&#x27;DATA_SOURCE_NAME=exporter:exporter@(localhost:3306)&#x27;ExecStart=/usr/local/mysqld_exporter/mysqld_exporter \\ --config.my-cnf=/usr/local/mysqld_exporter/.my.cnf \\ --web.listen-address=&quot;:9104&quot; \\ --web.telemetry-path=&quot;/metrics&quot; \\ --collect.info_schema.innodb_tablespaces \\ --collect.info_schema.innodb_metrics \\ --collect.global_status \\ --collect.global_variables \\ --collect.slave_status \\ --collect.engine_innodb_status \\ $ARGSExecReload=/bin/kill -HUP $MAINPIDTimeoutStopSec=20sRestart=always[Install]WantedBy=multi-user.target[root@rocky8 mysqld_exporter]# systemctl daemon-reload[root@rocky8 mysqld_exporter]# systemctl start mysqld_exporter.service[root@rocky8 mysqld_exporter]# systemctl enable mysqld_exporter.service 3.3 mysql服务器指标查询123456789101112131415161718192021222324252627282930313233mysql_up # 服务器是否在线mysql_global_status_uptime # 运行时长，单位 sdelta(mysql_global_status_bytes_received[1m]) # 网络接收的 bytesdelta(mysql_global_status_bytes_sent[1m]) # 网络发送的 bytesmysql_global_status_threads_connected # 当前的客户端连接数mysql_global_variables_max_connections # 允许的最大连接数mysql_global_status_threads_running # 正在执行命令的客户端连接数，即非 sleep 状态delta(mysql_global_status_aborted_connects[1m]) # 客户端建立连接失败的连接数，比如登录失败delta(mysql_global_status_aborted_clients[1m]) # 客户端连接之后，未正常关闭的连接数delta(mysql_global_status_commands_total&#123;command=&quot;xx&quot;&#125;[1m]) &gt; 0 # 每分钟各种命令的次数delta(mysql_global_status_handlers_total&#123;handler=&quot;xx&quot;&#125;[1m]) &gt; 0 # 每分钟各种操作的次数delta(mysql_global_status_handlers_total&#123;handler=&quot;commit&quot;&#125;[1m]) &gt; 0 # 每分钟 commit 的次数delta(mysql_global_status_table_locks_immediate[1m]) # 请求获取锁，且立即获得的请求数delta(mysql_global_status_table_locks_waited[1m]) # 请求获取锁，但需要等待的请求数。该值越少越好delta(mysql_global_status_queries[1m]) # 每分钟的查询数delta(mysql_global_status_slow_queries[1m]) # 慢查询数。如果未启用慢查询日志，则为 0mysql_global_status_innodb_page_size # innodb 数据页的大小，单位 bytesmysql_global_variables_innodb_buffer_pool_size # innodb_buffer_pool 的限制体积mysql_global_status_buffer_pool_pages&#123;state=&quot;data&quot;&#125; # 包含数据的数据页数，包括洁页、脏页mysql_global_status_buffer_pool_dirty_pages # 脏页数mysql_global_status_innodb_row_lock_current_waits #当前正在等待的 InnoDB 行锁数量。mysql_global_status_innodb_row_lock_time #从服务器启动以来的总 InnoDB 行锁等待时间（以毫秒为单位）。mysql_global_status_innodb_row_lock_time_avg #每次等待 InnoDB 行锁的平均时间（以毫秒为单位）mysql_global_status_innodb_row_lock_time_max #单次等待 InnoDB 行锁的最长时间（以毫秒为单位）。mysql_global_status_innodb_row_lock_waits #从服务器启动以来的总 InnoDB 行锁等待次数。 4 Mongodb Exporter4.1 容器部署docker-compose.yaml 123456789101112131415161718192021222324version: &#x27;3.3&#x27;services: mongo: image: mongo:4.2.5 container_name: mongo restart: always volumes: - /data/mongo/db:/data/db command: [--auth] ports: - 27017:27017 environment: MONGO_INITDB_ROOT_USERNAME: root MONGO_INITDB_ROOT_PASSWORD: 123456 mongodb_exporter: image: ccr.ccs.tencentyun.com/rig-agent/mongodb-exporter:0.10.0 container_name: mongodb_exporter restart: always environment: MONGODB_URI: &quot;mongodb://exporter:password@192.168.28.110:27017/admin?ssl=false&quot; ports: - &quot;9216:9216&quot; 创建监控用户 12345docker exec -it mongo mongo admin&gt; db.auth(&#x27;root&#x27;,&#x27;123456&#x27;)&gt;db.createUser(&#123; user: &#x27;exporter&#x27;,pwd : &#x27;password&#x27;,roles:[&#123;role: &#x27;readAnyDatabase&#x27;,db : &#x27;admin&#x27;&#125;,&#123;role: &#x27;clusterMonitor&#x27;,db : &#x27;admin&#x27;&#125;]&#125;);#测试 使用上面创建的用户信息进行连接&gt; db.auth(&#x27;exporter&#x27;,&#x27;password&#x27;) 4.2 mongodb服务器指标查询123456mongodb_connections&#123;state=&quot;available&quot;&#125; #可用的连接数mongodb_connections&#123;state=&quot;current&quot;&#125; #当前连接数 #关于server statusmongodb_up #服务器是否在线mongodb_instance_uptime_seconds #服务器的运行时长,单位为秒 5 Nginx Exporter5.1 容器部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354https://github.com/nginx/nginx-prometheus-exporterroot@ubuntu2004:~/learning-prometheus/08-prometheus-components-compose/nginx-and-exporter# vim docker-compose.yml version: &#x27;3.6&#x27;networks: monitoring: driver: bridge ipam: config: - subnet: 192.168.100.0/24services: nginx: image: nginx:1.26.2 volumes: - ./nginx/stub_status-server.conf:/etc/nginx/conf.d/stub_status-server.conf:ro networks: - monitoring expose: - 8080 - 80 ports: - 80:80 nginx-exporter: image: nginx/nginx-prometheus-exporter:1.4.0 command: - &#x27;-nginx.scrape-uri=http://nginx:8080/stub_status&#x27; networks: - monitoring ports: - &#x27;9113:9113&#x27; depends_on: - nginxroot@ubuntu2004:~/learning-prometheus/08-prometheus-components-compose/nginx-and-exporter# cat nginx/stub_status-server.conf server &#123; listen 8080; server_name localhost; location /stub_status &#123; stub_status; access_log off; #allow 172.31.0.0/16; #deny all; &#125;&#125;root@ubuntu2004:~/nginx# docker-compose pullroot@ubuntu2004:~/nginx# docker-compose psNAME IMAGE COMMAND SERVICE CREATED STATUS PORTSnginx-nginx-1 nginx:1.26.2 &quot;/docker-entrypoint.…&quot; nginx 7 seconds ago Up 7 seconds 0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp, 8080/tcpnginx-nginx-exporter-1 nginx/nginx-prometheus-exporter:1.4.0 &quot;/usr/bin/nginx-prom…&quot; nginx-exporter 24 minutes ago Up 6 seconds 0.0.0.0:9113-&gt;9113/tcp, :::9113-&gt;9113/tcp 编辑Prometheus配置文件 12345678root@ubuntu2004:~# vim /usr/local/prometheus/prometheus.yml scrape_configs: # 添加下列 - job_name: &quot;nginx&quot; static_configs: - targets: [&quot;10.0.0.204:9113&quot;] root@ubuntu2004:~# systemctl reload prometheus.service 5.2 nginx监控指标1234567nginx_connections_accepted #接收请求数nginx_connections_active #活动连接数nginx_connections_handled #成功处理请求数nginx_connections_reading #正在进行读操作的请求数nginx_connections_waiting #正在等待的请求数nginx_connections_writing #正在进行写操作的请求数nginx_connections_requests #总请求数 6 Consul Exporter6.1 部署Consul1https://github.com/hashicorp/consul 下载Consul并展开，创建用户 12345wget https://github.com/hashicorp/consul/archive/refs/tags/v1.20.2.tar.gzmkdir -p /usr/local/consul/config/datatar xf consul_1.20.2_linux_amd64.tar.gz -C /usr/local/consuluseradd -r consulchown consul.consul /usr/local/consul/&#123;data,config&#125; 创建Systemd Unitfile，保存于 &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;consul.service 文件中: 12345678910111213141516171819202122232425[Unit]Description=&quot;HashiCorp Consul - A service mesh solution&quot;Documentation=https://www.consul.io/Requires=network-online.targetAfter=network-online.target[Service]EnvironmentFile=-/etc/consul.d/consul.envUser=consulGroup=consulExecStart=/usr/bin/consul agent -dev -bootstrap \\ -config-dir /usr/local/consul/config \\ -data-dir /usr/local/consul/data \\ -ui \\ -log-level INFO \\ -bind 127.0.0.1 \\ -client 0.0.0.0ExecReload=/bin/kill --signal HUP $MAINPIDKillMode=processKillSignal=SIGTERMRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target 配置参数说明 12345678910111213-server #提供此标志指定您希望代理以服务器模式启动。-dev #启动开发者模式-ui #打开内置的web界面-bootstrap-expect #这告诉Consul服务器数据中心应该总共有多少服务器。所有服务器将等待此数量的服务器加入后再引导复制日志，这保持所有服务器上的数据一致性。因为您正在设置一个单服务器数据中心，您将其值设置为1。-node #数据中心中的每个节点必须有一个唯一的名称。默认情况下，Consul使用机器的主机名，但您将手动覆盖它，并将其设置为agent-one。 -bind #这是此代理将在其上监听来自其他Consul成员通信的地址。它必须可以被数据中心中的所有其他节点访问。如果您不设置绑定地址，Consul将尝试监听所有IPv4接口，如果找到多个私有IP，将无法启动。由于生产服务器通常具有多个接口，您应始终提供一个绑定地址。在这种情况下，它是172.20.20.10，这是您在Vagrantfile中指定的第一个虚拟机的地址。-data-dir #此标志告诉Consul代理它们应该在哪里存储其状态，其中可能包含ACL令牌等敏感数据，用于服务器和客户端。在生产部署中，您应该注意此目录的权限。-config-dir #此标志告诉Consul在哪里查找其配置。您将其设置为标准位置：/etc/consul.d。 启动服务并验证监听的端口 123systemctl daemon-reloadsystemctl start consul.servicesystemctl enable consul.service 随后即可访问Consul的Web UI，其使用的URL如下: http:&#x2F;&#x2F;:8500&#x2F; 6.2 部署Consul Exporter1https://github.com/prometheus/consul_exporter 提示：仅需要为每个Consul实例部署consul-exporter，它负责将Consul的状态信息转为Prometheus兼容的指标格式并予以暴露。 下载Consul Exporter并展开，创建用户 1234wget https://github.com/prometheus/consul_exporter/releases/download/v0.13.0/consul_exporter-0.13.0.linux-amd64.tar.gztar xf consul_exporter-0.13.0.linux-amd64.tar.gz -C /usr/local/ln -sv /usr/local/consul_exporter-0.13.0.linux-amd64 /usr/local/consul_exporteruseradd -r consul 创建Systemd Unitfile，保存于&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;consul_exporter.service文件中 12345678910111213141516171819202122[Unit]Description=consul_exporterDocumentation=https://prometheus.io/docs/introduction/overview/After=network.target[Service]Type=simpleUser=consulEnvironmentFile=-/etc/default/consul_exporter# 具体使用时，若consul_exporter与consul server不在同一主机时，consul server要指向实际的地址；ExecStart=/usr/local/consul_exporter/consul_exporter \\ --consul.server=&quot;http://localhost:8500&quot; \\ --web.listen-address=&quot;:9107&quot; \\ --web.telemetry-path=&quot;/metrics&quot; \\ --log.level=info \\ $ARGSExecReload=/bin/kill -HUP $MAINPIDTimeoutStopSec=20sRestart=always[Install]WantedBy=multi-user.target 启动服务并验证监听的端口，测试访问其暴露的指标 12ss -tnlp | grep &#x27;9107&#x27;curl localhost:9107/metrics 编辑Prometheus配置文件 12345678root@ubuntu2004:~# vim /usr/local/prometheus/prometheus.yml scrape_configs: # 添加下列 - job_name: &quot;consul&quot; static_configs: - targets: [&quot;10.0.0.204:9107&quot;] root@ubuntu2004:~# systemctl reload prometheus.service 6.3 容器化运行1234567891011121314151617181920212223242526272829303132root@ubuntu2004:~/learning-prometheus/08-prometheus-components-compose/consul-and-exporter# cat docker-compose.yml version: &#x27;3.6&#x27;volumes: consul_data: &#123;&#125;networks: monitoring: driver: bridgeservices: consul: image: hashicorp/consul:1.20 volumes: - ./consul_configs:/consul/config - consul_data:/consul/data/ networks: - monitoring ports: - 8500:8500 command: [&quot;consul&quot;,&quot;agent&quot;,&quot;-dev&quot;,&quot;-bootstrap&quot;,&quot;-config-dir&quot;,&quot;/consul/config&quot;,&quot;-data-dir&quot;,&quot;/consul/data&quot;,&quot;-ui&quot;,&quot;-log-level&quot;,&quot;INFO&quot;,&quot;-bind&quot;,&quot;127.0.0.1&quot;,&quot;-client&quot;,&quot;0.0.0.0&quot;] consul-exporter: image: prom/consul-exporter:v0.13.0 networks: - monitoring ports: - 9107:9107 command: - &quot;--consul.server=consul:8500&quot; depends_on: - consul 7 Redis Exporter7.1 容器部署12345678910111213141516171819202122version: &#x27;3.3&#x27;services: redis: image: redis:6 container_name: redis restart: always volumes: - /data/redis/data:/data command: redis-server --maxmemory 512mb ports: - 6379:6379 redis_exporter: image: oliver006/redis_exporter container_name: redis_exporter restart: always environment: REDIS_ADDR: &quot;192.168.28.110:6379&quot; REDIS_PASSWORD: ports: - 9121:9121 7.2 k8s部署如果 redis-server 跑在k8s中，那我们通常不会裸部署一个redis_exporter，而是会以 sidecar 的形式将 redis_exporter 和主应用 redis_server 部署在同一个 Pod 中 redis 这个 Pod 中包含了两个容器，一个就是 redis 本身的主应用，另外一个容器就是 redis_exporter。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293apiVersion: apps/v1kind: Deploymentmetadata: name: redis namespace: promspec: selector: matchLabels: app: redis template: metadata: annotations: prometheus.io/scrape: &quot;true&quot; prometheus.io/port: &quot;9121&quot; labels: app: redis spec: containers: - name: redis image: redis:4 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 - name: redis-exporter image: oliver006/redis_exporter:latest resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 9121---kind: ServiceapiVersion: v1metadata: name: redis namespace: promspec: selector: app: redis ports: - name: redis port: 6379 targetPort: 6379 - name: prom port: 9121 targetPort: 9121 [root@master prom]# kubectl apply -f prome-redis.yamldeployment.apps/redis createdservice/redis created[root@master prom]# kubectl get pods -n promNAME READY STATUS RESTARTS AGEprometheus-cfc6c98f-dshdt 1/1 Running 1 (152m ago) 20hredis-65c96d4bb4-fz55l 2/2 Running 0 39s[root@master prom]# kubectl get svc -n prom NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEprometheus NodePort 10.98.38.77 &lt;none&gt; 9090:32197/TCP 20hredis ClusterIP 10.111.11.242 &lt;none&gt; 6379/TCP,9121/TCP 41s# 通过 9121 端口来校验是否能够采集到数据[root@master prom]# curl 10.111.11.242:9121/metrics# 更新 Prometheus 的配置文件apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: promdata: prometheus.yml: | global: scrape_interval: 15s scrape_timeout: 15s scrape_configs: - job_name: &#x27;prometheus&#x27; static_configs: - targets: [&#x27;localhost:9090&#x27;] - job_name: &#x27;redis&#x27; static_configs: - targets: [&#x27;redis:9121&#x27;][root@master prom]# kubectl apply -f prometheus-cm.yamlconfigmap/prometheus-config configured[root@master prom]# curl -X POST &quot;http://10.98.38.77:9090/-/reload&quot; Redis服务器指标查询1234567891011redis_up #服务器是否在线redis_uptime_in_seconds #运行时长,单位srate(redis_cpu_sys_seconds_total[1m])+rate(redis_cpu_user_seconds_total[1m]) #占用CPU核数redis_memory_used_bytes #占用内存量redis_memory_max_bytes #限制的最大内存,如果没限制则为0delta(redis_net_input_bytes_total[1m]) #网络接受的bytesdelta(redis_net_output_bytes_total[1m]) #网络发送的bytesredis_connected_clients #客户端连接数redis_connected_clients / redis_config_maxclients #连接数使用率redis_rejected_connections_total #拒绝的客户端连接数redis_connected_slaves #slave连接数 8 黑盒监控Blackbox Exporter“白盒监控”是需要把对应的Exporter程序安装到被监控的目标主机上，从而实现对主机各种资源及其状态的数据采集工作。 但是由于某些情况下操作技术或其他原因，不是所有的Exporter都能部署到被监控的主机环境中，最典型的例子是监控全国网络质量的稳定性，通常的方法是使用ping操作，对选取的节点进行ICMP测试，此时不可能在他人应用环境中部署相关的Exporter程序。针对这样的应用的场景，Prometheus社区提供了黑盒解决方案，Blackbox Exporter无须安装在被监控的目标环境中，用户只需要将其安装在与Prometheus和被监控目标互通的环境中，通过HTTP、HTTPS、DNS、TCP、ICMP等方式（探针）进行探测监控，还可以探测SSL证书过期时间，Blackbox Exporter去获取被监控目标的暴露出来的数据，返回给Prometheus 1https://github.com/prometheus/blackbox_exporter 8.1 二进制部署提示：仅需要部署的Blackbox Exporter实例数据，取决于黑盒监控的任务量及节点的可用资源。 blackbox_exporter组件，需要一个config.yml，用于指示当前黑盒监控可以具备哪些监控能力 123456789101112131415161718192021222324252627282930313233# wget https://github.com/prometheus/blackbox_exporter/releases/download/v0.25.0/blackbox_exporter-0.25.0.linux-amd64.tar.gz# tar xf blackbox_exporter-0.25.0.linux-amd64.tar.gz -C /usr/local/# ln -sv /usr/local/blackbox_exporter-0.25.0.linux-amd64 /usr/local/blackbox_exporter# useradd -r prometheus# /usr/lib/systemd/system/blackbox_exporter.service[Unit]Description=blackbox_exporterDocumentation=https://prometheus.io/docs/introduction/overview/After=network.target[Service]Type=simpleUser=prometheusEnvironmentFile=-/etc/default/blackbox_exporterExecStart=/usr/local/blackbox_exporter/blackbox_exporter \\ --web.listen-address=&quot;:9115&quot; \\ --config.file=&quot;/usr/local/blackbox_exporter/blackbox.yml&quot; \\ --config.check \\ $ARGSExecReload=/bin/kill -HUP $MAINPIDTimeoutStopSec=20sRestart=always[Install]WantedBy=multi-user.targetsystemctl daemon-reloadsystemctl start blackbox_exporter.servicesystemctl enable blackbox_exporter.servicess -tnlp | grep &#x27;9115&#x27;curl localhost:9115/metrics 随后即可访问Blackbox Exporter的Web UI，其使用的URL如下: http://localhost:9115/ 配置Prometheus使用黑盒监控，编辑prometheus的主配置文件prometheus.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#http配置- job_name: &#x27;blackbox_http&#x27; metrics_path: /probe params: module: [http_2xx] # Look for a HTTP 200 response. static_configs: - targets: - &quot;www.magedu.com&quot; - &quot;www.google.com&quot; relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: &quot;prometheus.magedu.com:9115&quot; # 指向实际的Blackbox exporter. - target_label: region replacement: &quot;local&quot; #tcp配置- job_name: &quot;blackbox_tcp&quot; metrics_path: /probe params: module: [tcp_connect] static_configs: - targets: - 192.168.28.100:9090 - 192.168.28.110:9100 relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: blackbox_exporter:9115#icmp配置 ping- job_name: &quot;blackbox_icmp&quot; metrics_path: /probe params: module: [icmp] static_configs: - targets: - 192.168.28.100 - 192.168.28.110 relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: blackbox_exporter:9115 8.2 容器化部署123456789101112131415161718192021root@ubuntu2004:~/learning-prometheus/08-prometheus-components-compose/blackbox-exporter# cat docker-compose.ymlversion: &#x27;3.6&#x27;networks: monitoring: driver: bridge ipam: config: - subnet: 192.168.200.0/24services: blackbox_exporter: image: prom/blackbox-exporter:v0.25.0 volumes: - ./configs/:/etc/blackboxexporter/ command: - &#x27;--config.file=/etc/blackboxexporter/blackbox.yml&#x27; networks: - monitoring ports: - 9115:9115 8.3 blackbox的监控项1234567891011121314probe_success # 是否探测成功（取值 1、0 分别表示成功、失败）probe_duration_seconds # 探测的耗时# 关于 DNSprobe_dns_lookup_time_seconds # DNS 解析的耗时probe_ip_protocol # IP 协议，取值为 4、6probe_ip_addr_hash # IP 地址的哈希值，用于判断 IP 是否变化# 关于 HTTPprobe_http_status_code # HTTP 响应的状态码。如果发生重定向，则取决于最后一次响应probe_http_content_length # HTTP 响应的 body 长度，单位 bytesprobe_http_version # HTTP 响应的协议版本，比如 1.1probe_http_ssl # HTTP 响应是否采用 SSL ，取值为 1、0probe_ssl_earliest_cert_expiry # SSL 证书的过期时间，为 Unix 时间戳 修复检测总耗时图形，可检测各个target的耗时情况。探测不通的target往往耗时很久 12options选项，&#123;&#123;env&#125;&#125;_&#123;&#123;name&#125;&#125;修改为：&#123;&#123;instance&#125;&#125; ICMP&#x2F;HTTPS检测类 阶段耗时 12options选项，&#123;&#123;env&#125;&#125;_&#123;&#123;name&#125;&#125; &#123;&#123;phase&#125;&#125;修改为：&#123;&#123;instance&#125;&#125; &#123;&#123;phase&#125;&#125; 网站HTTP状态检查 12options选项，&#123;&#123;env&#125;&#125;_&#123;&#123;name&#125;&#125;修改为：&#123;&#123;instance&#125;&#125; 9 监控springbootprometheus中配置springboot的监控节点 1234567- job_name: &#x27;SpringBoot&#x27; scrape_interval: 5s metrics_path: &#x27;/actuator/prometheus&#x27; # springboot暴露出的指标api static_configs: - targets: [&#x27;192.168.28.110:8081&#x27;] # springboot所在主机 labels: instance: springboot服务器 springboot监控指标查询 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#http请求http_server_requests_seconds_count：请求此时http_server_requests_seconds_max ：http请求数峰值http_server_requests_seconds_sum： 请求n次花费的时间#jvm缓冲区jvm_buffer_count_buffers：计数缓冲jvm_buffer_memory_used_bytes：缓冲内存使用大小jvm_buffer_total_capacity_bytes：缓冲容量大小#类信息jvm_classes_loaded_classes：已加载类个数jvm_classes_unloaded_classes_total：已卸载类总数#内存信息jvm_memory_committed_bytes：已提交内存jvm_memory_max_bytes： 最大内存jvm_memory_used_bytes：已使用内存#gc信息jvm_gc_live_data_size_bytes：gc存活数据大小jvm_gc_max_data_size_bytes：gc最大数据大小jvm_gc_memory_allocated_bytes_total：gc分配的内存大小jvm_gc_memory_promoted_bytes_total：gc晋升到下一代的内存大小jvm_gc_pause_seconds：gc等待的时间jvm_gc_pause_seconds_max：gc等待的最大时间#线程信息jvm_threads_daemon_threads：守护线程jvm_threads_live_threads：存活线程jvm_threads_peak_threads：线程峰值jvm_threads_states_thread：不同状态的线程tomcat_threads_busy_threads：繁忙的线程数tomcat_threads_config_max_threads：配置的最大线程数tomcat_threads_current_threads：当前线程数#进程信息process_cpu_usage：cpu使用率process_files_max_files：最大文件数process_files_open_files：打开文件数process_start_time_seconds：进程启动时刻process_uptime_seconds：进程运行时间#系统信息system_cpu_count：cpu个数system_cpu_usage：cpu使用情况system_load_average_1m：系统平均负载#tomcat信息tomcat_global_error_total：总体报错数tomcat_global_received_bytes_total：接收的字节总数tomcat_global_sent_bytes_total：发出的字节总数tomcat_global_request_max_seconds：每秒最大请求数tomcat_global_request_seconds：每秒请求数tomcat_sessions_active_current_sessions：目前活跃会话数tomcat_sessions_active_max_sessions：活跃最大会话数tomcat_sessions_alive_max_seconds：会话活跃的最长时间tomcat_sessions_created_sessions_total：累计创建的会话数tomcat_sessions_expired_sessions_total：累计失效的会话数tomcat_sessions_rejected_sessions_total：累计拒绝的会话数 10 自定义监控我们已经实现了对于操作系统、硬件、系统、应用程序(比如springboot程序)中间件等的监控。但如果想对一些业务指标做监控（比如想要知道一个订单系统的实时订单数、订单金额、一个用户系统的在线人数，应该怎么做呢？——需要进行自定义监控 10.1 业务监控&#x2F;自定义监控指标已有的exporter默认采集到的数据不能满足产品、数据需要，需要额外采集其他数据，就需要后端自行写代码。常用的业务指标有： 用户数据指标 新增用户 日新增用户数 活跃用户 活跃率 留存用户 留存率 活跃用户数：日活、周活、月活 活跃率&#x3D;活跃用户数&#x2F;总用户数 留存率&#x3D;（第一天新增用户中在第N天使用过产品的用户数）&#x2F;第一天新增用户数 通常需要计算：次日、7日、30日留存率） 行为数据指标 PV&#x3D;page view 访问次数 UV&#x3D;unique visitor 访问人数 转发率&#x3D;转发某功能用户数&#x2F;看到该功能的用户数 转化率：与具体业务相关 K因子（衡量推荐的效果）&#x3D;平均每个用户向多少人发邀请X接收到邀请的人转化为新用户的转化率 产品数据指标 总量：成交总额（GMV）、成交数量、访问时长 人均付费&#x3D;总收入&#x2F;总用户数（ARPV&#x2F;客单价） 付费用户人均付费&#x3D;总收入&#x2F;付费人数 人均访问时长&#x3D;总时长&#x2F;总用户数 付费率&#x3D;付费人数&#x2F;总用户数 复购率&#x3D;消费两次以上人数&#x2F;付费人数 产品：热销产品数、好评产品数、差评产品数 需要注意的是，每家公司都会根据的自身实际情况设立业务指标，因此不同公司的业务指标可能会有所不同。 同一家公司，在不同的阶段可能会侧重于不同的业务指标（比如，初创公司暂不关心收费、付费，优先关注访问人数；等稳定发展了再追求高付费率、高ARPU值）。 10.2 收集业务指标&#x2F;自定义指标的步骤 应用程序抛出一些数据指标 -&gt; 给prometheus Prometheus的服务器加工采集这些数据指标 -&gt; 给Grafana Grafana用于展现自定义监控指标","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"部署与配置","slug":"Monitor/prometheus/deploy","date":"2025-08-27T10:16:18.000Z","updated":"2025-09-12T16:01:32.681Z","comments":true,"path":"Monitor/prometheus/deploy/","permalink":"https://aquapluto.github.io/Monitor/prometheus/deploy/","excerpt":"","text":"1 使用包管理器Rocky&#x2F;CentOS上可基于yum repository安装Prometheus-Server 12345678910https://packagecloud.io/app/prometheus-rpm/release/search[prometheus]name=prometheusbaseurl=https://packagecloud.io/prometheus-rpm/release/el/$releasever/$basearchrepo_gpgcheck=1enabled=1gpgkey=https://packagecloud.io/prometheus-rpm/release/gpgkey https://raw.githubusercontent.com/lest/prometheus-rpm/master/RPM-GPG-KEY-prometheus-rpmgpgcheck=1metadata_expire=300 如无版本上的特殊要求，Ubuntu和Debian可直接使用apt命令安装 1~# apt-get install prometheus 2 二进制安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354root@ubuntu2004:~# wget https://github.com/prometheus/prometheus/releases/download/v2.53.3/prometheus-2.53.3.linux-amd64.tar.gzroot@ubuntu2004:~# tar xf prometheus-2.53.3.linux-amd64.tar.gz -C /usr/local/root@ubuntu2004:~# cd /usr/local/root@ubuntu2004:/usr/local# ln -sv prometheus-2.53.3.linux-amd64 prometheus&#x27;prometheus&#x27; -&gt; &#x27;prometheus-2.53.3.linux-amd64&#x27;root@ubuntu2004:/usr/local# cd prometheusroot@ubuntu2004:/usr/local/prometheus# lltotal 261348drwxr-xr-x 2 1001 127 4096 Nov 5 12:35 console_libraries/ # 扩展库drwxr-xr-x 2 1001 127 4096 Nov 5 12:35 consoles/ # 模版文件-rw-r--r-- 1 1001 127 11357 Nov 5 12:35 LICENSE-rw-r--r-- 1 1001 127 3773 Nov 5 12:35 NOTICE-rwxr-xr-x 1 1001 127 137839708 Nov 5 12:19 prometheus*-rw-r--r-- 1 1001 127 934 Nov 5 12:35 prometheus.yml # 配置文件-rwxr-xr-x 1 1001 127 129729365 Nov 5 12:19 promtool* # 测试工具# 启动服务，前台运行，不推荐root@ubuntu2004:~./prometheus#编写服务启动文件来启动服务root@ubuntu2004:~# useradd -r prometheusroot@ubuntu2004:~# mkdir /usr/local/prometheus/dataroot@ubuntu2004:~# chown -R prometheus.prometheus /usr/local/prometheus/dataroot@ubuntu2004:~# vim /usr/lib/systemd/system/prometheus.service[Unit]Description=Monitoring system and time series databaseDocumentation=https://prometheus.io/docs/introduction/overview/[Service]Restart=alwaysUser=prometheusEnvironmentFile=-/etc/default/prometheusExecStart=/usr/local/prometheus/prometheus \\ --config.file=/usr/local/prometheus/prometheus.yml \\ --storage.tsdb.path=/usr/local/prometheus/data \\ --web.console.libraries=/usr/share/prometheus/console_libraries \\ --web.enable-lifecycle \\ $ARGSExecReload=/bin/kill -HUP $MAINPIDTimeoutStopSec=20sSendSIGKILL=noLimitNOFILE=8192[Install]WantedBy=multi-user.targetroot@ubuntu2004:~# systemctl daemon-reloadroot@ubuntu2004:~# systemctl start prometheus.serviceroot@ubuntu2004:~# systemctl enable prometheus.serviceroot@ubuntu2004:~# ss -tnlp | grep &#x27;9090&#x27;LISTEN 0 4096 *:9090 *:* users:((&quot;prometheus&quot;,pid=5038,fd=7)) 服务启动参数介绍，使用 ./prometheus --help 查看更多选项帮助 官网说明：Prometheus-command-line 1234--config.file #指定配置文件--storage.tsdb.path #数据存储目录--web.console.libraries #指定控制台库，允许用户创建自定义的、交互式的监控仪表板--web.enable-lifecycle #开启配置文件热加载 3 容器化部署命令 1234~# docker run \\ -p 9090:9090 \\ -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml \\ prom/Prometheus:v2.44.0 docker-compose 1234567891011121314151617volumes: prometheus_data: &#123;&#125;services: prometheus: image: prom/prometheus:v2.44.0 volumes: # 通过volume，从节点上加载定制的配置文件 - ./prometheus/:/etc/prometheus/ - prometheus_data:/prometheus command: - &#x27;--config.file=/etc/prometheus/prometheus.yml&#x27; - &#x27;--storage.tsdb.path=/prometheus&#x27; - &#x27;--web.console.libraries=/usr/share/prometheus/console_libraries&#x27; - &#x27;--web.console.templates=/usr/share/prometheus/consoles&#x27; - &#x27;--web.enable-lifecycle&#x27; ports: - 9090:9090 restart: always 4 k8s部署12345678910111213141516# prometheus-cm.yamlapiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitordata: prometheus.yml: | global: scrape_interval: 15s # Prometheus每隔15s就会从所有配置的目标端点抓取最新的数据 scrape_timeout: 15s # 某个抓取操作在 15 秒内未完成，会被视为超时，不会包含在最新的数据中。 evaluation_interval: 15s # 每15s对告警规则进行计算 scrape_configs: - job_name: &quot;prometheus&quot; static_configs: - targets: [&quot;localhost:9090&quot;] 1234567891011121314151617181920212223242526272829303132333435363738394041# prometheus-pv-pvc.yaml# 要访问集群中的资源，就要有对应的权限apiVersion: v1kind: PersistentVolumemetadata: name: prometheus-local labels: app: prometheusspec: accessModes: - ReadWriteOnce capacity: storage: 20Gi storageClassName: local-storage local: path: /data/k8s/prometheus nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node-01 persistentVolumeReclaimPolicy: Retain---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: prometheus-data namespace: monitorspec: selector: matchLabels: app: prometheus accessModes: - ReadWriteOnce resources: requests: storage: 20Gi storageClassName: local-storage 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# prometheus-rbac.yamlapiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules: - apiGroups: - &#x27;&#x27; resources: - nodes - services - endpoints - pods - nodes/proxy verbs: - get - list - watch - apiGroups: - &#x27;extensions&#x27; resources: - ingresses verbs: - get - list - watch - apiGroups: - &#x27;&#x27; resources: - configmaps - nodes/metrics verbs: - get - nonResourceURLs: # 用来对非资源型 metrics 进行操作的权限声明 - /metrics verbs: - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole # 由于我们要获取的资源信息，在每一个 namespace 下面都有可能存在，所以我们这里使用的是 ClusterRole 的资源对象 name: prometheussubjects: - kind: ServiceAccount name: prometheus namespace: monitor 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950apiVersion: apps/v1kind: Deploymentmetadata: name: prometheus namespace: monitor labels: app: prometheusspec: selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: serviceAccountName: prometheus securityContext: runAsUser: 0 # 以root用户运行容器，prometheus镜像中使用的是nobody用户，会出现权限问题 containers: - image: prom/prometheus:v2.53.0 name: prometheus args: - &#x27;--config.file=/etc/prometheus/prometheus.yml&#x27; # 指定配置文件路径（挂载自 ConfigMap） - &#x27;--storage.tsdb.path=/prometheus&#x27; # TSDB 数据存储路径（挂载自 PVC） - &#x27;--storage.tsdb.retention.time=24h&#x27; # 数据保留时间（24小时，可根据需求调整） - &#x27;--web.enable-admin-api&#x27; # 启用 Admin API（支持删除时间序列等操作，生产环境需控制访问） - &#x27;--web.enable-lifecycle&#x27; # 启用热重载（无需重启容器，访问 /-/reload 即可加载新配置） ports: - containerPort: 9090 name: http volumeMounts: - mountPath: &#x27;/etc/prometheus&#x27; # 配置文件挂载路径（对应 ConfigMap） name: config-volume # 与 spec.volumes 中 configMap 卷的 name 一致 - mountPath: &#x27;/prometheus&#x27; # 数据存储挂载路径（对应 PVC） name: data # 与 spec.volumes 中 PVC 卷的 name 一致 resources: requests: cpu: 100m memory: 512Mi limits: cpu: 200m memory: 1Gi volumes: - name: data persistentVolumeClaim: claimName: prometheus-data - name: config-volume configMap: name: prometheus-config 1234567891011121314151617# prometheus-svc.yamlapiVersion: v1kind: Servicemetadata: name: prometheus namespace: monitor labels: app: prometheusspec: selector: app: prometheus type: NodePort ports: - name: web port: 9090 targetPort: 9090 #targetPort: http 1234567kubectl create namespace monitorkubectl apply -f prometheus-cm.yamlmkdir -p /data/k8s/prometheus # 在pv所亲和的节点上创建kubectl apply -f prometheus-pv-pvc.yamlkubectl apply -f prometheus-rbac.yaml kubectl apply -f prometheus-deploy.yaml kubectl apply -f prometheus-svc.yaml 添加监控项，然后 apply -f 123456789101112131415161718192021222324apiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitordata: prometheus.yml: | global: scrape_interval: 15s # Prometheus每隔15s就会从所有配置的目标端点抓取最新的数据 scrape_timeout: 15s # 某个抓取操作在 15 秒内未完成，会被视为超时，不会包含在最新的数据中。 evaluation_interval: 15s # 每15s对告警规则进行计算 scrape_configs: - job_name: &quot;prometheus&quot; static_configs: - targets: [&quot;localhost:9090&quot;] - job_name: &#x27;example-random&#x27; scrape_interval: 5s static_configs: - targets: [&#x27;192.168.71.101:8080&#x27;, &#x27;192.168.71.101:8081&#x27;] labels: group: &#x27;production&#x27; - targets: [&#x27;192.168.71.101:8082&#x27;] labels: group: &#x27;canary&#x27; 重载服务 12345[root@k8s-master-01 ~]# kubectl -n monitor get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESprometheus-779cfbf9d4-zsdms 1/1 Running 0 44m 10.244.1.5 k8s-node-01 &lt;none&gt; &lt;none&gt;[root@k8s-master-01 ~]# curl -X POST &quot;http://10.244.1.5:9090/-/reload&quot; 5 配置文件说明5.1 Global全局配置如果有内部单独设定，会覆盖全局配置 1234global: scrape_interval: 15s # 抓取数据间隔为每 15 秒一次。默认值为每 1 分钟一次。 evaluation_interval: 15s # 记录规则和报警规则拉取间隔为每 15 秒一次。默认值为每 1 分钟一次。 scrape_timeout: 15s # 单次数据拉取超时时间，全局默认值为10s。当报context deadline exceeded错误时需要在特定的job下配置该字段 5.2 Alertmanager配置12345alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 #指定prometheus将报警信息推送到指定的alertmanager实例地址 5.3 rule_files 规则文件配置用来设置告警规则，基于设定什么指标进行报警(类似触发器trigger)。这里设定好规则以后，prometheus会根据全局global设定的evaluation_interval参数进行扫描加载，规则改动后会自动加载。其报警媒介和 route 由alertmanager插件实现 12345# 规则文件，比如指定了报警规则所在的位置，prometheus可以根据这个配置加载规则，用于生成新的时间序列数据或者报警信息# 只加载规则一次，然后根据全局 &#x27;evaluation_interval&#x27; 定期拉取它们。rule_files: # - &quot;first_rules.yml&quot; # - &quot;second_rules.yml&quot; 5.4 scrape_configs 监控采集配置12345678910111213# 一个要抓取的端点的抓取配置，用于控制 prometheus 监控哪些资源，下例是Prometheus自身scrape_configs: # job名称会增加到拉取到的所有采样点上，同时还有一个instance目标服务的host:port标签也会增加到采样点上 - job_name: &quot;prometheus&quot; # 采集指标数据的url，也就是Prometheus需要从http://localhost:9090/metrics中去采集指标数据 # metrics_path defaults to &#x27;/metrics&#x27; # 请求方法，如果要加密改成https # scheme defaults to &#x27;http&#x27;. static_configs: - targets: [&quot;localhost:9090&quot;] scrapy_config 配置参数说明 参数 说明 scrape_interval 抓取间隔,默认继承global值 scrape_timeout 抓取超时时间,默认继承global值 metric_path 抓取路径， 默认是&#x2F;metrics scheme 指定采集使用的协议，http或者https params 指定url参数 basic_auth 指定认证信息 *_sd_configs 指定服务发现配置 static_configs 静态指定服务job relabel_config target relabel重新打标设置","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"监控概论","slug":"Monitor/conspect","date":"2025-08-27T10:15:50.000Z","updated":"2025-09-12T08:45:32.071Z","comments":true,"path":"Monitor/conspect/","permalink":"https://aquapluto.github.io/Monitor/conspect/","excerpt":"","text":"1 监控系统基础1.1 监控系统组件 指标数据采集（抓取） 指标数据存储 指标数据趋势分析及可视化 告警 1.2 监控体系（自底向上）系统层监控 系统监控：CPU、Load、Memory、Swap、Disk IO、Processes、Kernel Parameters、…… 网络监控：网络设备、工作负载、网络延迟、丢包率、…… 中间件及基础设施类系统监控 消息中间件：Kafka、RocketMQ和RabbitMQ等； Web服务容器：Tomcat和Jetty等； 数据库及缓存系统：MySQL、PostgreSQL、MogoDB、ElasticSearch和Redis等； 数据库连接池：ShardingSpere等； 存储系统：NFS和Ceph等 应用层监控 用于衡量应用程序代码的状态和性能 业务层监控 用于衡量应用程序的价值，例如电子商务网站上的销售量 QPS、DAU日活、转化率； 业务接口：登录数、注册数、订单量、搜索量和支付量等； 1.3 云原生时代的可观测性可观测性系统 指标监控（Metrics）：随时间推移产生的一些与监控相关的可聚合数据点； 日志监控（Logging）：离散式的日志或事件； 链路跟踪（Tracing）：分布式应用调用链跟踪； CNCF将可观测性和数据分析归类一个单独的类别，且划分成了4个子类 监控系统：以Prometheus等为代表； 日志系统：以ElasticStack和PLG Stack等为代表； 分布式调用链跟踪系统：以Zipkin、Jaeger、SkyWalking、Pinpoint等为代表； 混沌工程系统（稳定性测试）：以ChaosMonkey和ChaosBlade等为代表 1.4 监控的目的 长期趋势分析，预测未来的扩容 故障根因分析 –&gt; 查历史监控 对照分析，明确不同版本的运行情况 告警通知，超过阈值则报警，提前处理问题，防患于未然 系统状态时时了然于胸 1.5 监控指标的分类硬件监控 服务器：风扇转速，cpu温度，主板温度、内存状态、电源状态、RAID磁盘阵列状态，磁盘状态，网卡状态。 防火墙、交换机、路由器或UPS等。 系统监控 基础监控：cpu使用率，内存使用率，磁盘(和inode)使用量率，磁盘io使用率 其他监控(linux)：&#x2F;etc&#x2F;passwd 文件。打开文件描述符的最大数量(如:open too many files错误)，最大进程数，用户登录数。 网络监控 网络流量情况，丢包率，错包率，连接数等等 应用程序监控&#x2F;中间件监控 数据库监控：数据库连接数、当前运行的线程、QPS、TPS、并行处理的会话数、缓存命中率、主从延时、锁状态、慢查询 Nginx：活跃连接数、等待连接数、丢弃连接数、请求量、耗时、5XX错误率 消息队列：连接数、队列数、生产速率、消费速率、消息堆积量 缓存：成功连接数、阻塞连接数、已使用内存、内存碎片率、请求量、耗时、缓存命中率 Tomcat：最大线程数、当前线程数、请求量、耗时、错误量、堆内存使用情况、GC次数和耗时 业务监控 核心业务的监控：登录，注册，下单，支付等等。 日志监控：访问日志、错误日志 业务指标：比如PV、订单量等 客户端监控 用户行为信息，业务返回码，客户端性能，运营商，版本，操作系统等。 全国移动，联通，电信用户访问情况监控(第三方工具SmokePing，监控宝，阿里云站点监控等) iOS，安卓，微信小程序(阿里云的ARMS) 1.6 监控的常用衡量指标在软件系统的高可靠性（也称为可用性，英文描述为HA，High Available）里有个衡量其可靠性的标准——X个9，这个X是代表数字3~5。X个9表示在软件系统1年时间的使用过程中，系统可以正常使用时间与总时间（1年）之比，我们通过下面的计算来感受下X个9在不同级别的可靠性差异。 1234561个9：(1-90%)365=36.5天，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是36.5天2个9：(1-99%)365=3.65天 ，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是3.65天3个9：(1-99.9%)365*24=8.76小时，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是8.76小时。4个9：(1-99.99%)365*24=0.876小时=52.6分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是52.6分钟。5个9：(1-99.999%)365*24*60=5.26分钟，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是5.26分钟。6个9：(1-99.9999%)365*24*60*60=31秒，表示该软件系统在连续运行1年时间里最多可能的业务中断时间是31秒 2 著名的监控方法论2.1 Google SRE的四黄金指标Google的四个黄金指标常用于在服务级别帮助衡量终端用户体验、服务中断、业务影响等层面的问题，适用于应用及服务监控 延迟（Latency） 服务请求所需要的时长，例如HTTP请求平均延迟 应用程序响应时间会受到所有核心系统资源（包括网络、存储、CPU和内存）延迟的影响 需要区分失败请求和成功请求 流量（Traffic），有时也称为吞吐量 衡量服务的容量需求， 例如每秒处理的HTTP请求数或者数据库系统的事务数量 吞吐量指标包括每秒 Web 请求、API 调用等示例，并且被描述为通常表示为每秒请求数的需求 错误（Errors） 失败的请求（流量）的数量，通常以绝对数量或错误请求占请求总数的百分比表示 请求失败的速率，用于衡量错误发生的情况 例如，HTTP 500错误数等显式失败，返回错误内容或无效内容等隐式失败，以及由策略原因导致的失败 例如强制要求响应时间超过30毫秒的请求视为错误 饱和度（Saturation） 衡量资源的使用情况，用于表达应用程序有多“满” 资源的整体利用率，包括 CPU（容量、配额、节流）、内存（容量、分配）、存储（容量、分配和 I&#x2F;O 吞吐量）和网络 例如内存、CPU、I&#x2F;O、磁盘等资源的使用量 2.2 USE方法Netflix的USE（Utilization Saturation and Errors Method）方法主要用于分析系统性能问题，可以指导用户快速识别资源瓶颈以及错误的方法，应用于主机指标监控，分析系统性能问题 使用率(Utilization) 关注系统资源的使用情况。 这里的资源主要包括但不限于：CPU，内存，网络，磁盘等等 100%的使用率通常是系统性能瓶颈的标志 饱和度(Saturation) 例如CPU的平均运行排队长度，这里主要是针对资源的饱和度(注意，不同于4大黄金信号) 任何资源在某种程度上的饱和都可能导致系统性能的下降 错误(Errors) 错误计数 例如：“网卡在数据包传输过程中检测到的以太网网络冲突了14次” 2.3 RED方法RED方法是Weave Cloud在基于Google的4个黄金指标的原则下结合Prometheus以及Kubernetes容器实践，细化和总结的方法论，特别适合于云原生应用以及微服务架构应用的监控和度量，在四大黄金指标的原则下，RED方法可以有效地帮助用户衡量云原生以及微服务应用下的用户体验问题；RED方法主要关注以下3种关键指标 (Request)Rate：每秒钟接收的请求数； (Request)Errors：每秒失败的请求数； (Request)Duration：每个请求所花费的时长； 3 黑盒和白盒监控1、白盒监控 “白”指的是要了解系统内部的详细运行状态，是为了做好预防，防患于未然 要把系统的内部细节都监控起来，使用这些监控项做分析，预测可能出现的问题，对可能出现的问题设置好阈值，超过阈值则报警，报警后立即处理 例如监控cpu、内存、硬盘、nginx后端服务器的响应时长、磁盘的I&#x2F;O负载值等 2、黑盒监控 “黑”指的是不需要了解系统内部细节，而是站在用户的角度去测试服务的外部可见性，早于真正的用户找到问题 例如HTTP探针、TCP探针等用于检测站点或者服务的可访问性以及访问效率等 二者对比 白盒侧重于主动发现或者预测潜在问题，它监控的指标可能并没有当即引发问题，但是都属于会引发问题的高风险指标，通过白盒能够了解其内部的运行状态，以及对监控指标的观察能够预判可能出现的潜在问题，从而对潜在的不确定因素进行提前优化并避免问题的发生 黑盒以故障为导向，站在用户的角度用一下，以便早于真正的用户找到问题 总结 通过白盒监控能够提前预知业务瓶颈 通过黑盒监控能够第一时间发现业务故障并通过告警通告运维人员进行紧急恢复，从而将业务影响降到最低。 一个完善的监控目标是要能够从白盒的角度发现潜在问题，能够在黑盒的角度快速发现已经发生的问题 4 监控工具4.1 NagiosNagios 作为一款开源免费的监控工具，可有效监控 Windows、Linux、Unix 主机状态，以及交换机、路由器等网络设备和打印机等，能在系统或服务状态异常时通过邮件、短信等多种方式第一时间告警通知运维人员，状态恢复后也会发出正常通知，其核心特征在于强大的监控告警功能，尤其告警方式丰富。但它存在明显短板：缺乏强大的数据收集机制，数据出图简陋；监控主机增多时，添加操作繁琐，且依赖文本配置文件，不支持 Web 方式管理配置，易出错且难维护；同时，分层告警机制和自定义监控能力较弱，监控主机数量有限，承载能力较低。官网 4.2 猎鹰 Open-falconOpen-falcon（猎鹰）是由小米公司发起的一款监控系统，采用 golang 和 python 开发。相较于 Zabbix，它在性能、扩展性及用户使用效率上具有较明显的优势，但作为一款发布时间较短的监控工具，也存在不少短板：不仅缺乏对 Tomcat、Apache 等许多基础服务的监控插件支持，功能不够完善且更新速度较慢，同时没有专门的运维支撑，社区运营也存在欠缺。官网 GitHub 4.3 夜莺 Nightingale夜莺（Nightingale）是由滴滴基础平台联合滴滴云研发并开源的企业级监控解决方案，旨在满足云原生时代企业级监控需求。作为 Open-Falcon 原核心研发团队历时五年打磨的成果，它在 Open-Falcon 的基础上，结合滴滴内部的最佳实践，在性能、可维护性和易用性上进行了大量改进，最终在产品完成度、系统高可用性及用户体验上达到企业级标准，可灵活适配不同规模场景 —— 从小到几台机器的小型环境，到大至数十万节点的大规模集群，均能稳定支撑。夜莺兼顾云原生与裸金属环境，同时支持应用监控与系统监控，具备灵活的插件机制和丰富完善的插件生态，拥有高度的灵活性与可扩展性。在滴滴内部，它作为集团统一的监控解决方案，已支撑起数十亿监控指标的采集与分析，覆盖系统、容器、应用等全层面监控需求，服务着数千周活跃用户。此外，夜莺当前社区活跃度高，版本升级频繁，持续迭代优化。官网 4.4 ZabbixZabbix 作为一款成熟的企业级开源监控解决方案，凭借全面的监控能力、灵活的部署架构和强大的告警机制，在分布式系统与网络监控领域占据重要地位。它支持对服务器、网络设备、应用程序等多类对象的深度监控，通过主动 &#x2F; 被动采集、自动发现等方式适配复杂环境，同时提供丰富的可视化图表和多层次告警策略，满足从基础系统到业务指标的全链路监控需求。尽管在云原生场景下需结合插件增强适配性，但其稳定的性能、活跃的社区生态以及免费开源的特性，使其成为中大型企业混合 IT 环境监控的优选方案，广泛应用于金融、电商、制造业等对可靠性要求严苛的领域。官网 4.5 PrometheusPrometheus 是一款开源的服务监控系统，同时集成了时间序列数据库功能，尤其在容器监控领域表现突出。它专为云原生环境设计，能够通过动态服务发现机制自动识别并监控 Kubernetes 等容器编排平台中的容器实例，实时采集容器的资源使用率、健康状态等指标。其核心特点包括基于指标的多维数据模型、灵活的查询语言（PromQL）、强大的告警规则配置，以及与 Grafana 等工具的无缝集成，可实现监控数据的可视化与分析。凭借对动态环境的良好适配和开源社区的活跃支持，Prometheus 已成为容器化应用监控的主流方案，广泛应用于微服务、云平台等场景，助力用户实现从基础设施到应用层的全链路可观测性。官网 5 商业监控网站1234http://ping.chinaz.com/ #站长之家https://www.jiankongbao.com/ #监控宝https://www.toushibao.com/ #透视宝https://www.tingyun.com/ #听云 6 云监控云监控为云上用户提供开箱即用的企业级开放型一站式监控解决方案，涵盖 IT 设施基础监控、外网网络质量拨测监控，以及基于事件、自定义指标和日志的业务监控，全方位为用户提供更高效、全面、经济的监控服务 —— 既能帮助提升系统服务可用时长，又能降低企业 IT 运维监控成本。 该方案通过跨云服务、跨地域的应用分组管理模型及报警模板，助力用户快速构建可支持几十种云服务、管理数万实例的高效监控报警体系，实现对各云服务资源指标的实时监控，探测 ECS 等云服务及运营商站点的可用性，并可针对指定指标设置报警，让用户全面掌握阿里云上资源使用情况与业务运行状态，及时处理故障资源，保障业务正常运转。 https://help.aliyun.com/document_detail/35170.html https://cloud.tencent.com/document/product/248/13466","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[]},{"title":"prometheus概述","slug":"Monitor/prometheus/introduce","date":"2025-08-27T10:14:47.000Z","updated":"2025-09-12T09:02:39.178Z","comments":true,"path":"Monitor/prometheus/introduce/","permalink":"https://aquapluto.github.io/Monitor/prometheus/introduce/","excerpt":"","text":"1 Prometheus概述Prometheus是一款时序（time series）数据库；但它的功能却并非止步于TSDB，而是一款设计用于进行目标（Target）监控的关键组件；结合生态系统内的其它组件，例如Pushgateway、Altermanager和Grafana等，可构成一个完整的IT监控系统。 Exporters 收集目标系统的性能指标，并通过 HTTP API 暴露出来 Prometheus 根据配置定期从各个 Exporter 抓取最新指标数据，并存储这些数据 Grafana 连接到 Prometheus，使用 PromQL 查询数据，并生成直观的图表和仪表板供用户查看 Prometheus 内部的告警规则引擎监控数据变化，当满足设定条件时，向 Alertmanager 发送告警。Alertmanager 根据配置处理这些告警，并通过适当的方式通知相关人员 2 Prometheus的特性关键特性 多维护数据模型：以指标名称及附加的label标识时间序列 特有的数据查询语言：PromQL 单个服务器节点即可正常工作，不依赖分布式存储 基于HTTP协议，以Pull模式完成指标数据采集 借助于PushGateway，支持Push模式的指标数据采集 使用服务发现机制动态发现Target，或静态配置要监控的Target 支持多种Graph和Dashboard 不适用的场景 Prometheus是一款指标监控系统，不适合存储事件及日志； Prometheus认为只有最近的监控数据才有查询的需要，其本地存储的设计初衷只是保存短期数据，因而不支持针对大量的历史数据进行存储；若需要存储长期的历史数据，建议基于远端存储机制将数据保存于InfluxDB或OpenTSDB等系统中； 3 Prometheus与Zabbix的对比 特性 Zabbix Prometheus 开发语言及定制化 后端用 C 开发，界面用 PHP 开发，定制化难度很高。 后端用 golang 开发，前端是 Grafana，JSON 编辑即可解决。定制化难度较低。 集群规模 集群规模上限为 10000 个节点。 支持更大的集群规模，速度也更快。 监控环境适应性 更适合监控物理机环境。 更适合云环境的监控，对 OpenStack, Kubernetes 有更好的集成。 数据存储方式 监控数据存储在关系型数据库内，如 MySQL，很难从现有数据中扩展维度。 监控数据存储在基于时间序列的数据库内，便于对已有数据进行新的聚合。 安装复杂度 安装简单，zabbix-server 一个软件包中包括了所有的服务端功能。 安装相对复杂，监控、告警和界面都分属于不同的组件。 界面成熟度 图形化界面比较成熟，界面上基本上能完成全部的配置操作。 界面相对较弱，很多配置需要修改配置文件。 发展历程与解决方案成熟度 发展时间更长，对于很多监控场景，都有现成的解决方案。 2015 年后开始快速发展，但发展时间较短，成熟度不及 Zabbix。 4 Prometheus的架构上游：负责收集监控指标的组件 k8s（一些组件默认就暴漏接口给prometheus用） 各种exporter服务 Pushgateway：据收集代理服务器(类似于zabbix proxy)，push 的方式将指标数据推送到该网关 service discovery：动态发现服务 中间：负责保存数据，prometheus server Retrieval：对接上游，周期性去上游的监控目标target里拉取监控数据 TSDB：按照时序存储数据 HTTP server: 对下游暴漏访问TSDB的接口 下游：负责使用数据的组件 Altermanager： 接口prometheus推送过来的报警信息，然后发送给不同的报警媒介 Grafana：负责出图 5 Prometheus的生态组件 组件 说明 Prometheus Server 从各种数据源（如 Exporters 和 Pushgateway）去 pull 时序型指标数据，存储在TSDB中。TSDB不支持长期存储，如果想要长期存储，需要基于远端存储机制将数据保存于InfluxDB或OpenTSDB等系统中。提供 HTTP API 便于Alertmanager接收告警和Grafana查询数据。 Pushgateway 网关，适用于短期运行的作业或任务，它们把数据 push 给Pushgateway暂存，Prometheus再从Pushgateway pull 它们的数据，不然它们运行停止了，Prometheus就拉取不了了。 Exporters 指标暴露器，负责收集不支持内建Instrumentation的应用程序或服务的性能指标数据，并通过HTTP接口供Prometheus Server获取。用于暴露已有的第三方服务的 metrics 给 Prometheus。 Instrumentation 自带Prometheus兼容格式的测量系统的应用程序。Prometheus提供了支持多种语言的SDK，用于对接 Prometheus Server, 可以查询和上报数据，目的在于为那些期望原生提供Instrumentation功能的应用程序提供便捷的开发途径。 Service Discovery 服务发现，适用于容器环境，Prometheus通过服务发现动态发现待监控的Target。Prometheus支持多种服务发现机制，例如文件、DNS、Consul、Kubernetes等等。 Alertmanager 从Prometheus Server接收到“告警通知”后，通过去重、分组、路由等预处理功能后向用户完成告警信息发送。 6 Prometheus获取指标数据6.1 获取指标数据途径监控target分为三大类 服务&#x2F;应用软件（redis、mysql）—-&gt; redis_expoter、mysql_exporter 物理节点 —-&gt; node_exporter k8s集群 上述三类监控target的配置有两种配置 服务发现：本质就是通过标签或注解信息进行筛选，符合该规则都会被选出来作为prometheus周期性拉取的target 必须暴露一个 &#x2F;metrics 接口：prometheus server朝该接口发请求，则会返回固定格式的数据 有现成的 &#x2F;metrics 接口就可用，没有则部署exporter来采集指标并暴漏该接口 Prometheus支持通过三种类型的途径从目标上抓取（Scrape）指标数据 Exporters：Prometheus Server基于服务发现（Service Discovery）机制或静态配置获取要监视的目标（Target），并通过每个目标上的指标exporter来采集（Scrape）指标数据。 Instrumentation：有些自带Prometheus兼容格式的测量系统的应用程序，Prometheus提供了SDK Pushgateway：一些短期运行的作业的生命周期过短，难以有效地将必要的指标数据供给到Server端，它们一般会采用推送（Push）方式输出指标数据，Prometheus借助于Pushgateway接收这些推送的数据，进而由Server端进行抓取 6.2 获取指标数据方式基于HTTP call，从配置文件中指定的网络端点（endpoint）上周期性获取指标数据 TSDB：存储核心数据 scraper：刮擦器，对目标监控系统的各种指标进行刮擦并采集，采用的是pull模式和HTTP(S)协议 relabel：重新打标，和zabbix采集数据不同的是，zabbix可以在30个指标中只采集2个，而Prometheus是一次性刮擦所有指标的数据，除非使用其他规则过滤掉 Target：每一个被监控的系统，可以是一个主机，也可以是一个应用程序 6.3 Pull and PushPrometheus同其它TSDB相比有一个非常典型的特性：它主动从各Target上“拉取（pull)”数据，而非等待被监控端的“推送（push）”； 两个方式各有优劣，其中，Pull模型的优势在于： 集中控制：有利于将配置集在Prometheus Server上完成，包括指标及采取速率等； Promethes的根本目标在于收集在Target上预先完成聚合的聚合型数据，而非一款由事件驱动的存储系统； 6.4 作业（Job）和实例（Instance）Instance（实例） 指的是能够接收 Prometheus Server 数据采集（Scrape）操作的网络端点（endpoint）。无论是各类 exporter，还是自定义开发的服务，只要能提供符合 Prometheus 要求的数据格式，并允许通过 HTTP 请求获取信息，都可称为实例 Job（任务） 则是具有相同或类似功能的 Instance 的集合，例如一个 MySQL 主从复制集群中的所有 MySQL 进程。在数据采集层面，每个 Job 负责管理一类相同业务的实例，是 Prometheus 中组织和执行采集操作的基本单位。","categories":[{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"}]},{"title":"文本处理三剑客","slug":"Linux/basics/GSA","date":"2025-08-21T03:38:34.000Z","updated":"2025-09-06T11:22:46.475Z","comments":true,"path":"Linux/basics/GSA/","permalink":"https://aquapluto.github.io/Linux/basics/GSA/","excerpt":"","text":"一、grep作用：文本搜索工具，根据用户指定的“模式”对目标文本逐行进行匹配检查；打印匹配到的行，即过滤文本或者过滤命令的结果 模式：由正则表达式字符及文本字符所编写的过滤条件 123456789101112131415161718192021222324grep [OPTIONS] PATTERN [FILE...]--color=auto #对匹配到的文本着色显示-m N #匹配N次后停止-v #显示不被pattern匹配到的行,即取反-i #忽略字符大小写-n #显示匹配的行号-c #统计匹配的行数-o #仅显示匹配到的字符串-q #静默模式，不输出任何信息-A N #显示匹配到的字符串所在的行及其后n行-B N #显示匹配到的字符串所在的行及其前N行-C N #显示匹配到的字符串所在的行及其前后各N行-e #实现多个选项间的逻辑or关系-w #匹配整个单词-x #整行匹配，即仅选择完全匹配整行的模式。换句话说，只有当模式与整行内容完全匹配时，该行才会被选中-E #使用ERE，相当于egrep-F #不支持正则表达式，相当于fgrep，将模式视为固定字符串而不是正则表达式。通常用于搜索不包含正则表达式特殊字符的模式-P #支持Perl格式的正则表达式-f file #从文件中读取匹配规则，每行一条-r #递归目录，但不处理软链接-R #递归目录，但处理软链接-l #显示匹配上的文件名，只显示文件名-H #显示匹配行所在的文件名 范例 12345678910111213141516171819202122232425262728293031323334353637383940#取前三行[root@ubuntu2204 ~]# grep -m 3 bin /etc/passwd#显示匹配的行数[root@ubuntu2204 ~]# grep -c bash /etc/passwd#仅显示匹配到的字符串[root@ubuntu2204 ~]# grep -o root /etc/passwd#显示匹配root的行或匹配 bash 的行[root@ubuntu2204 ~]# grep -e root -e bash /etc/passwd#显示匹配root的行且匹配 bash 的行[root@ubuntu2204 ~]# grep root /etc/passwd | grep bash#从文件读取匹配规则[root@ubuntu2204 ~]# cat test.txtrootbash#匹配有root或者bash的行[root@ubuntu2204 ~]# grep -f test.txt /etc/passwd #只显示有匹配到的文件的文件名，不显示具体内容[root@ubuntu2204 ~]# grep root -l /etc/passwd /etc/sudoers /etc/my.cnf#显示内容来自于哪个文件[root@ubuntu2204 ~]# grep -H root /etc/passwd /etc/sudoers /etc/my.cnf#命令行展开[root@ubuntu2204 ~]# grep `whoami` /etc/passwdroot:x:0:0:root:/root:/bin/bash[root@ubuntu2204 ~]# echo Linux123 | grep $(uname)Linux123#变量展开[root@ubuntu2204 ~]# grep &quot;$USER&quot; /etc/passwd#取CPU核数[root@ubuntu2204 ~]# grep -c processor /proc/cpuinfo 过滤以#开头的注释行和空行123456[root@centos8 ~]#grep -v &quot;^#&quot; /etc/profile | grep -v &#x27;^$&#x27;[root@centos8 ~]#grep -v &quot;^#\\|^$&quot; /etc/profile[root@centos8 ~]#grep -v &quot;^\\(#\\|$\\)&quot; /etc/profile[root@centos8 ~]#grep -Ev &quot;^(#|$)&quot; /etc/profile[root@centos8 ~]#egrep -v &quot;^(#|$)&quot; /etc/profile[root@centos6 ~]#egrep -v &#x27;^(#|$)&#x27; /etc/httpd/conf/httpd.conf 取两个文件的相同行123456789101112131415[11:40:48 root@10 data[]#cat a.txtaefd gh[11:40:56 root@10 data[]#cat a.txt.orig abcd[11:41:16 root@10 data[]#grep -f a.txt a.txt.orig ad 分区利用率最大的值12345[root@centos8 ~]#df | grep &#x27;^/dev/sd&#x27; | tr -s &#x27; &#x27; % | cut -d% -f5 | sort -n | tail -1[root@centos8 ~]#df | grep &#x27;^/dev/sd&#x27; | grep -oE &#x27;\\&lt;[0-9]&#123;,3&#125;%&#x27; | tr -d &#x27;%&#x27;|sort -nr|head -n1[root@centos8 ~]#df | grep &#x27;^/dev/sd&#x27; | grep -oE &#x27;\\&lt;[0-9]&#123;,3&#125;%&#x27;|grep -Eo &#x27;[0-9]+&#x27;|sort -nr|head -n1[root@centos8 ~]#df | grep &#x27;^/dev/sd&#x27; | grep -oE [0-9]+% | tr -d % |sort -nr|head -n113 哪个IP和当前主机连接数最多的前三位1234[root@centos8 ~]#ss -nt | grep &quot;^ESTAB&quot; |tr -s &#x27; &#x27; : |cut -d: -f6|sort |uniq -c|sort -nr|head -n3 3 10.0.0.1 1 172.16.4.100 1 172.16.31.188 连接状态的统计12345678[root@wang-liyun-pc ~]# ss -nta | grep -v &#x27;^State&#x27; |cut -d&quot; &quot; -f1|sort |uniq -c 7 ESTAB 4 LISTEN 7 TIME-WAIT[root@wang-liyun-pc ~]# ss -nta | tail -n +2 |cut -d&quot; &quot; -f1|sort |uniq -c 3 ESTAB 4 LISTEN 12 TIME-WAIT 取IP地址1234567[root@centos8 ~]#ifconfig eth0 | grep -Eo &#x27;([0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]&#123;1,3&#125;&#x27;|head -110.0.0.8[root@ubuntu2204 ~]# cat reg.txt([0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]&#123;1,3&#125;[root@ubuntu2204 ~]# ifconfig | grep -oEf reg.txt | head -110.0.0.206 匹配字符前后一样的行12345678[root@centos8 ~]#grep &quot;^\\(.*\\)\\&gt;.*\\&lt;\\1$&quot; /etc/passwd[root@centos8 ~]#grep -E &quot;^(.*)\\&gt;.*\\&lt;\\1$&quot; /etc/passwd[root@centos8 ~]#egrep &quot;^(.*)\\&gt;.*\\&lt;\\1$&quot; /etc/passwdsync:x:5:0:sync:/sbin:/bin/syncshutdown:x:6:0:shutdown:/sbin:/sbin/shutdownhalt:x:7:0:halt:/sbin:/sbin/haltbash:x:1008:1008::/home/bash:/bin/bashnologin:x:1011:1011::/home/nologin:/sbin/nologin 二、sedSed是不会一下子把文件内容全部读入内容，而是读一行到内存处理一行，然后再读下一行，一次处理一行的设计模式使得sed性能很高，在读取大文件时不会出现卡顿的现象。支持管道符号 每当处理一行时，把当前处理的行存储在临时缓冲区 模式空间（Pattern Space） 中，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。 如果使用vi命令打开几十M上百M的文件，明显会出现有卡顿的现象，这是因为vi命令打开文件是一次性将文件加载到内存，然后再打开。Sed就避免了这种情况，一行一行的处理，打开速度非常快，执行速度也很快。 2.1 sed 基本用法1234567891011121314151617sed [option]... &#x27;script;script;...&#x27; [file...]-n #不输出模式空间内容到屏幕，即不自动打印-e #多点编辑，多个script，or 的关系-f FILE #从指定文件中读取编辑脚本-r, -E #使用扩展正则表达式-i.bak #-i 直接修改文件，-i.bak 以.bak后缀备份原文件-c #配合i一起使用，保留原文件-s #将多个文件视为独立文件，而不是单个连续的长文件流-e #多个script，or 的关系#说明:-ir #不支持-i -r #支持-ri #支持-ni #危险选项,会清空文件 script格式 1&#x27;定位+命令&#x27; #在哪些行，执行什么操作 定位格式 12345678910111213141516171. 不定位：对全文进行处理2. 行定位，指定行： N #指定的行 $ #最后一行 /pattern/ #被此处模式所能够匹配到的每一行 3. 范围定位： M,N #从M行到第N行，3，6 从第3行到第6行 M,+N #从M行到M+N行，3,+4 表示从3行到第7行 /pat1/,/pat2/ #从第一个匹配行开始，到第二个匹配行中间的行 M,/pat/ #行号开始，匹配结束 /pat/,N #匹配开始，行号结束 4. 步进：~ 1~2 #奇数行 2~2 #偶数行 命令格式 1234567891011121314151617181920212223p #打印当前模式空间内容，追加到默认输出之后Ip #忽略大小写输出d #删除模式空间匹配的行，并立即启用下一轮循环= #为模式空间中的行打印行号! #模式空间中匹配行取反处理q #结束或退出seda [\\]text #在指定行后面追加文本，支持使用\\n实现多行追加i [\\]text #在行前面插入文本c [\\]text #替换行为单行或多行文本w file #保存模式匹配的行至指定文件r file #读取指定文件的文本至模式空间中匹配到的行后s/pattern/string/修饰符 #查找替换,支持使用其它分隔符，可以是其它形式：s@@@，s### #修饰符 g #行内全局替换 p #显示替换成功的行 w /PATH/FILE #将替换成功的行保存至文件中 I,i #忽略大小写 #后向引用 \\1 #第一个分组 \\2 #第二个分组 \\N #第N个分组 &amp; #所有搜索内容 范例 12345678[root@centos7 ~]#cat &gt; test.logport=3306^C[root@centos7 ~]#sed &#x27;s/3306/3307/&#x27; test.log &gt; test2.log [root@centos7 ~]#cat test2.log port=3307[root@centos7 ~]#cat test.log port=3306 范例 123456789101112#script 中执行p命令，再加上默认输出，所有每行都显示了两次[root@ubuntu2204 ~]# sed &#x27;p&#x27; /etc/issueUbuntu 22.04 LTS \\n \\lUbuntu 22.04 LTS \\n \\l#关闭默认输出，script 为空，则无任何输出[root@ubuntu2204 ~]# sed -n &#x27;&#x27; /etc/issue[root@ubuntu2204 ~]##用 -n 选项关闭默认输出，script 中执行p命令[root@ubuntu2204 ~]# sed -n &#x27;p&#x27; /etc/issueUbuntu 22.04 LTS \\n \\l 范例：\\ 的作用 123456789101112[root@ubuntu2204 ~]# sed &#x27;2a *******&#x27; test.txtaaabbb*******cccbbb[root@ubuntu2204 ~]# sed &#x27;2a\\ *******&#x27; test.txtaaabbb *******cccbbb 范例：取第几行的问题 123456[15:36:26 root@10 ~[]#seq 10 | sed -n &#x27;3,5p&#x27;345[20:00:38 root@10 ~[]#seq 10 | sed -n &#x27;3p&#x27;3 范例：给特定的行过滤掉 1234567[20:03:22 root@10 ~[]#seq 10 | sed &#x27;3,6d&#x27;1278910 范例：追加内容 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[20:31:16 root@10 ~[]#seq 10 | sed &#x27;2~2ahello&#x27;12hello34hello56hello78hello910hello[20:31:23 root@10 ~[]#cat .bashrc# .bashrc# User specific aliases and functionsalias rm=&#x27;rm -i&#x27;alias cp=&#x27;cp -i&#x27;alias mv=&#x27;mv -i&#x27;# Source global definitionsif [ -f /etc/bashrc ]; then . /etc/bashrcfi[20:34:26 root@10 ~[]#sed &#x27;User specificaalias text=ls&#x27; .bashrcsed：-e 表达式 #1，字符 1：未知的命令：“U”[20:35:11 root@10 ~[]#sed &#x27;/User specific/aalias text=ls&#x27; .bashrc# .bashrc# User specific aliases and functionsalias text=lsalias rm=&#x27;rm -i&#x27;alias cp=&#x27;cp -i&#x27;alias mv=&#x27;mv -i&#x27;# Source global definitionsif [ -f /etc/bashrc ]; then . /etc/bashrcfi#加-i.bak可修改文件，改过的在.bashrc，原来的在.bashrc.bak 范例：将内容保存到指定文件 12345678910111213[21:01:35 root@10 ~[]#seq 10 | sed &#x27;3w c.txt&#x27;12345678910[21:02:02 root@10 ~[]#cat c.txt 3 范例：追加文件内容到指定的行后 12345678910111213141516171819202122232425[21:02:07 root@10 ~[]#seq 10 | sed &#x27;2~2r /etc/issue&#x27;12\\SKernel \\r on an \\m34\\SKernel \\r on an \\m56\\SKernel \\r on an \\m78\\SKernel \\r on an \\m910\\SKernel \\r on an \\m 命令行和变量展开123456789[root@centos7 ~]#sed &quot;s/3306/`id -u wu`/&quot; test.log[root@centos7 ~]#sed &#x27;s/3306/&#x27;`id -u wu`&#x27;/&#x27; test.logport=1000[root@centos7 ~]#echo $UID0[root@centos7 ~]#sed &quot;s/3306/$UID/&quot; test.log[root@centos7 ~]#sed &#x27;s/3306/&#x27;$UID&#x27;/&#x27; test.logport=0 只要#号开头的行12345678910111213[20:07:48 root@10 ~[]#sed -n &#x27;/^#/p&#x27; /etc/profile# /etc/profile# System wide environment and startup programs, for login setup# Functions and aliases go in /etc/bashrc# It&#x27;s NOT a good idea to change this file unless you know what you# are doing. It&#x27;s much better to create a custom.sh shell script in# /etc/profile.d/ to make custom changes to your environment, as this# will prevent the need for merging in future updates.# Path manipulation# By default, we want umask to get set. This sets it for login shell# Current threshold for system reserved uid/gids is 200# You could check uidgid reservation validity in# /usr/share/doc/setup-*/uidgid file 不要#号开头的行12345[20:08:15 root@10 ~[]#sed -n &#x27;/^[^#]/p&#x27; /etc/profile[root@centos8 ~]#sed -n &#x27;/^#/!p&#x27; fstab[20:11:18 root@10 ~[]#sed &#x27;/^#/d&#x27; /etc/profile 将#开头的行删除#1[root@centos8 ~]#sed -ri.bak &#x27;/^#/s/^#//&#x27; /etc/fstab #先读取#开头的行，然后将#替换成空 将非#开头的行加#123[root@centos8 ~]#sed -rn &quot;s/^[^#]/#&amp;/p&quot; /etc/fstab[root@centos8 ~]#sed -rn &#x27;s/^[^#](.*)/#\\1/p&#x27; /etc/fstab[root@centos8 ~]#sed -rn &#x27;/^#/!s@^@#@p&#x27; /etc/fstab 不显示注释行和空行12[root@centos6 ~]#sed &#x27;/^#/d;/^$/d&#x27; /etc/httpd/conf/httpd.conf[root@centos6 ~]#grep -Ev &#x27;^#|^$&#x27; /etc/httpd/conf/httpd.conf 在&#x2F;etc&#x2F;passwd需要b开头和s开头的行123456[20:15:39 root@10 ~[]#sed -n &#x27;/^b/,/^s/p&#x27; /etc/passwdbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/sync 获取分区利用率123456[root@centos8 ~]#df | sed -En &#x27;/^\\/dev\\/sd/s@.* ([0-9]+)%.*@\\1@p&#x27;3113[root@centos8 ~]#df | sed -rn &#x27;/^\\/dev\\/sd/ s#([^[:space:]]+[[:space:]]+)&#123;4&#125;(.*)%.*#\\2#p&#x27;[root@centos8 ~]#df | sed -rn &#x27;/^\\/dev\\/sd/ s#(\\S+\\s+)&#123;4&#125;(.*)%.*#\\2#p&#x27; 取 IP 地址123456789101112[root@centos8 ~]#ifconfig eth0 |sed -nr &quot;2s/[^0-9]+([0-9.]+).*/\\1/p&quot; 10.0.0.8[root@centos6 ~]#ifconfig eth0 | sed -En &#x27;2s/^[^0-9]+([0-9.]&#123;7,15&#125;).*/\\1/p&#x27;10.0.0.6[root@centos8 ~]#ifconfig eth0 | sed -rn &#x27;2s/^[^0-9]+([0-9.]+) .*$/\\1/p&#x27;10.0.0.8[root@centos8 ~]#ifconfig eth0 | sed -n &#x27;2s/^.*inet //p&#x27; | sed -n &#x27;s/netmask.*//p&#x27;10.0.0.8[root@centos8 ~]#ifconfig eth0 | sed -n &#x27;2s/^.*inet //;s/ netmask.*//p&#x27;10.0.0.8[root@centos8 ~]#ifconfig eth0 | sed -rn &#x27;2s/(.*inet )([0-9].*)(netmask.*)/\\2/p&#x27;10.0.0.8 取目录名和基名123456789echo &quot;/etc/sysconfig/network-scripts/&quot; |sed -r &#x27;s#(^/.*/)([^/]+/?)#\\2#&#x27; 取基名echo &quot;/etc/sysconfig/network-scripts/&quot; |sed -r &#x27;s#(^/.*/)([^/]+/?)#\\1#&#x27; 取目录#取目录名[root@centos8 ~]#echo /etc/sysconfig/ | sed -rn &#x27;s#(.*)/([^/]+)/?#\\1#p&#x27;/etc#取基名[root@centos8 ~]#echo /etc/sysconfig/ | sed -rn &#x27;s#(.*)/([^/]+)/?#\\2#p&#x27;sysconfig 取文件的前缀和后缀12345678910[root@centos8 data]#echo a.b.c.gz |sed -En &#x27;s/(.*)\\.([^.]+)$/\\1/p&#x27;a.b.c[root@centos8 data]#echo a.b.c.gz |sed -En &#x27;s/(.*)\\.([^.]+)$/\\2/p&#x27;gz[root@centos8 data]#echo a.b.c.gz |grep -Eo &#x27;.*\\.&#x27;a.b.c[root@centos8 data]#echo a.b.c.gz |grep -Eo &#x27;[^.]+$&#x27;gz[root@centos8 ~]#echo a.b.tar.gz | sed -rn &#x27;s@.*\\.([^.]+)\\.([^.]+)$@\\1.\\2@p&#x27;tar.gz 修改网卡配置1[root@centos8 ~]#sed -Ei.bak &#x27;/^GRUB_CMDLINE_LINUX/s/(.*)(&quot;)$/\\1net.ifnames=0\\2/&#x27; /etc/default/grub 修改内核参数12345[root@centos8 ~]#sed -nr &#x27;/^GRUB_CMDLINE_LINUX/s/&quot;$/ net.ifnames=0&quot;/p&#x27;/etc/default/grub[root@centos8 ~]#sed -rn &#x27;/^GRUB_CMDLINE_LINUX=/s@(.*)&quot;$@\\1 net.ifnames=0&quot;@p&#x27;/etc/default/grub[root@centos8 ~]#sed -rn &#x27;/^GRUB_CMDLINE_LINUX=/s@&quot;$@ net.ifnames=0&quot;@p&#x27;/etc/default/grub 修改网卡名称123456#centos7,8[root@centos8 ~]#sed -i &#x27;/GRUB_CMDLINE_LINUX=/s#quiet#&amp; net.ifnames=0#&#x27;/etc/default/grub[root@centos8 ~]#sed -ri &#x27;/^GRUB_CMDLINE_LINUX=/s@&quot;$@ net.ifnames=0&quot;@&#x27;/etc/default/grub[root@centos8 ~]#grub2-mkconfig -o /boot/grub2/grub.cfg#ubuntu[root@ubuntu ~]#grub-mkconfig -o /boot/grub/grub.cfg 后项引用将需要的内容用()分组，然后后续引用它，命令为 \\1;\\2;\\3 \\1 表示第一个分组 \\0 表示正则表达式匹配的所有字符 范例：要把123456789换成以下三种情况 12345678[10:58:07 root@10 ~[]#echo 123456789 | sed -n &#x27;s/\\(123\\)\\(456\\)\\(789\\)/\\2\\1\\3/p&#x27;456123789[10:58:19 root@10 ~[]#echo 123456789 | sed -n &#x27;s/\\(123\\)\\(456\\)\\(789\\)/\\1\\3/p&#x27;123789[10:59:20 root@10 ~[]#echo 123456789 | sed -n &#x27;s/\\(123\\)\\(456\\)\\(789\\)/\\1xyx\\3/p&#x27;123xyx789 范例：取ip地址 12[13:36:02 root@10 ~[]#ifconfig ens160 | sed -r -n &#x27;2s/^(.*inet +)([0-9.]+)( +netmask.*)$/\\2/p&#x27;10.0.0.131 2.2 sed 高级用法sed 中除了模式空间，还另外还支持保持空间（Hold Space）,利用此空间，可以将模式空间中的数据，临时保存至保持空间，从而后续接着处理，实现更为强大的功能 常见的高级命令 12345678910P #打印模式空间开端至\\n内容，并追加到默认输出之前h #把模式空间中的内容覆盖至保持空间中H #把模式空间中的内容追加至保持空间中g #从保持空间取出数据覆盖至模式空间G #从保持空间取出内容追加至模式空间x #把模式空间中的内容与保持空间中的内容进行互换n #读取匹配到的行的下一行覆盖至模式空间N #读取匹配到的行的下一行追加至模式空间d #删除模式空间中的行D #如果模式空间包含换行符，则删除直到第一个换行符的模式空间中的文本，并不会读取新的输入行，而使用合成的模式空间重新启动循环。如果模式空间不包含换行符，则会像发出d命令那样启动正常的新循环 范例 1234567891011121314sed -n &#x27;n;p&#x27; FILEseq 10 | sed &#x27;N;s/\\n//&#x27;sed &#x27;1!G;h;$!d&#x27; FILEseq 10 | sed -n &#x27;/3/&#123;g;1!p;&#125;;h&#x27; #前一行seq 10 | sed -nr &#x27;/3/&#123;n;p&#125;&#x27; #后一行sed &#x27;N;D&#x27;FILEseq 10 |sed &#x27;3h;9G;9!d&#x27;sed &#x27;$!N;$!D&#x27; FILEsed &#x27;$!d&#x27; FILEsed &#x27;G&#x27; FILEsed &#x27;g&#x27; FILEsed &#x27;/^$/d;G&#x27; FILEsed &#x27;n;d&#x27; FILEsed -n &#x27;1!G;h;$p&#x27; FILE 范例: 打印偶数行 123456789101112131415161718192021222324[root@centos8 ~]#seq 10 | sed -n &#x27;n;p&#x27;246810[root@centos8 ~]#seq 10 | sed -n &#x27;2~2p&#x27;246810[root@centos8 ~]#seq 10 | sed &#x27;1~2d&#x27;246810[root@centos8 ~]#seq 10 | sed -n &#x27;1~2!p&#x27;246810 三、awk3.1 awk工作原理和基本用法格式化输出，针对的是有规律的文本文件，支持管道符号 执行BEGIN&#123;action;… &#125;语句块中的语句 读入文本的一行内容，进行处理 先把这行内容整体赋值给一个变量$0 以冒号为分隔符对该行进行分割 第一段内容 –赋值给—》$1 第二段内容 –赋值给—》$2 把行号赋值NR变量 把这一行分的总段数赋值给变量NF 执行规则&#39;NR&gt;=1 &amp;&amp; NR&lt;=3&#123;print $1&#125;&#39; 后续就是重复2，3步直到处理完文件的所有行 读取完所有的行之后执行END&#123;action;… &#125;语句块中的语句 格式 1234567awk [options] &#x27;program&#x27; var=value file…awk [options] -f programfile var=value file…#常用选项-f progfile #从文件中读入program-F fs #指定分隔符，默认的分隔符是若干个连续空白符,可以指定多个-v var=val #设置变量 program格式 12345678910&#x27;pattern&#123;action statements;..&#125;&#x27;pattern：定位 行号：NR==5或NR&gt;=1 &amp;&amp; NR&lt;=3或NR&lt;=3 || NR &gt;= 5（可以运用各种操作符） 正则定位：/正则表达式/action statements：对数据进行处理 &#123;print $1&#125; 算术，比较表达式 if, while等 组合语句 说明：program通常是被放在单引号中，并可以由三种部分组成 BEGIN语句块，BEGIN&#123;&#125; 模式匹配的通用语句块，[pattern]&#123;COMMAND&#125; END语句块，END&#123;&#125; 分割符、域和记录 由分隔符分隔的字段（列column,域field）标记的$1,$2...$n称为域标识，$0为所有域 文件的每一行称为记录record 如果省略action，则默认执行print $0的操作 范例 1234567[root@centos7 ~]#awk &#x27;BEGIN&#123;print strftime(&quot;%Y-%m-%dT%H:%M&quot;,systime()-3600)&#125;&#x27;2023-10-04T15:10[root@centos7 ~]#cat a.awkBEGIN&#123;print strftime(&quot;%Y-%m-%dT%H:%M&quot;,systime()-3600)&#125;[root@centos7 ~]#awk -f a.awk2023-10-04T15:09 范例：awk打印列很简单，$1表示第一列 12345678910111213141516171819202122232425[root@centos7 ~]#df | awk &#x27;&#123;print $5&#125;&#x27;Use%0%0%3%0%13%15%0%0%#指定分隔符[root@centos7 ~]#df | awk &#x27;&#123;print $5&#125;&#x27; | awk -F&quot;%&quot; &#x27;&#123;print $1&#125;&#x27;Use0030131500[root@centos7 ~]#getent passwd | awk -F&quot;:&quot; &#x27;&#123;print $1,$3&#125;&#x27;[root@centos7 ~]#getent passwd | awk -F&quot;:&quot; &#x27;&#123;print $1&quot;=&quot;$3&#125;&#x27; 范例 123456789101112131415#指定多个分隔符[root@centos7 ~]#df | awk -F&quot; +|%&quot; &#x27;&#123;print $5&#125;&#x27; #多个空格和%作为分隔符Use0030131500#但是这种写法中，会多出一列空行，即第6列是空行，因为%这里分割的时候，会分割成两列，前面是数字，但是后面原来啥也没有，所以自动补充了一列空格#这种写法不会出现以上情况[root@centos8 ~]#df | awk -F&quot;[ %]+&quot; &#x27;&#123;print $5&#125;&#x27; #表示多个空格和多个%作为分隔符 原因的话，我们用下列例子来分析 123456789101112131415161718[root@ubuntu2004 ~]#cat a.txt1 2% 41 2%3 4Avail Use% MountedFilesystem Size Used Avail Use% Mounted on[root@ubuntu2004 ~]#awk -F&quot;[ %]+&quot; &#x27;&#123;print NF&#125;&#x27; a.txt3437[root@ubuntu2004 ~]#awk -F&quot; +|%&quot; &#x27;&#123;print NF&#125;&#x27; a.txt4448#解析如下 123456789总的来说，跟离散数学的逻辑相似p：使用空格作为分隔符q：使用%作为分隔符awk -F&quot;[ %]+&quot;的逻辑就是(p∧¬q)V(¬p∧q) #这表示使用空格作为分隔符或者使用%作为分隔符awk -F&quot; +|%&quot;的逻辑就是pVq #这表示使用空格作为分隔符或者使用%作为分隔符，或者两者都可能发生这里的“∨”是逻辑或的意思，通常对应于集合的并集。它涵盖了所有满足至少一个条件的情况。然而，在经典逻辑中，如果p和q不能同时为真，即它们是互斥的情况，那么“p ∨ q”实际上只会有两种可能性，即p为真或q为真，但不会同时为真。这是因为在经典逻辑中，“∨”通常被解释为包含性的或（inclusive or），意味着如果p和q都是真的，那么“p ∨ q”也是真的。 3.2 动作print和printf3.2.1 print123456print item1, item2, ...逗号分隔符输出item可以字符串，也可是数值；当前记录的字段、变量或awk的表达式如省略item，相当于print $0固定字符符需要用“ ” 引起来，而变量和数字不需要 范例 123456789101112131415161718192021222324252627282930313233[root@centos7 ~]#awk &#x27;&#123;print &quot;hello&quot;&#125;&#x27; 123helloabchello^C[root@centos7 ~]#awk &#x27;&#123;print &quot;hello&quot;&#125;&#x27; &lt; /etc/issuehellohellohello[root@centos7 ~]#cat /etc/issue | awk &#x27;&#123;print &quot;hello&quot;&#125;&#x27; &lt; /etc/issuehellohellohello[root@centos8 ~]#seq 10 | awk &#x27;&#123;print &quot;hello,awk&quot;&#125;&#x27;hello,awkhello,awkhello,awkhello,awkhello,awkhello,awkhello,awkhello,awkhello,awkhello,awk[root@centos8 ~]#seq 3 | awk &#x27;&#123;print 2*3&#125;&#x27;666[root@centos7 etc]#awk &#x27;&#123;print &quot;2*3&quot;&#125;&#x27; &lt; /etc/issue2*32*32*3 面试题：取出网站访问量最大的前3个IP1[root@VM_0_10_centos logs]# awk &#x27;&#123;print $1&#125;&#x27; nginx.access.log-20200428|sort |uniq -c |sort -nr|head -3 面试题：取出分区利用率123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@centos8 ~]#df | awk &#x27;&#123;print $1,$5&#125;&#x27;Filesystem Use%devtmpfs 0%tmpfs 0%tmpfs 2%tmpfs 0%/dev/sda2 3%/dev/sda3 1%/dev/sda1 15%tmpfs 0%#使用扩展的正则表达式[root@centos8 ~]#df | awk -F&quot;[[:space:]]+|%&quot; &#x27;&#123;print $5&#125;&#x27;Use001051921[root@centos8 ~]#df | awk -F&quot;[ %]+&quot; &#x27;&#123;print $5&#125;&#x27;Use001031190[root@rocky8 ~]#df | awk -F&quot; +|%&quot; &#x27;&#123;print $5&#125;&#x27;Use001031170[root@centos8 ~]#df | grep &quot;^/dev/sd&quot; | awk -F&quot;[[:space:]]+|%&quot; &#x27;&#123;print $5&#125;&#x27;5192[root@centos8 ~]#df | grep &#x27;^/dev/sd&#x27;| awk -F&#x27;[[:space:]]+|%&#x27; &#x27;&#123;print $1,$5&#125;&#x27; /dev/sda2 3/dev/sda3 1/dev/sda1 15[root@centos8 ~]#df | awk -F&quot;[[:space:]]+|%&quot; &#x27;/^\\/dev\\/sd/&#123;print $5&#125;&#x27;5192[root@centos8 ~]#df | awk -F&#x27;[[:space:]]+|%&#x27; &#x27;/^\\/dev\\/sd/&#123;print $1,$5&#125;&#x27; /dev/sda2 3/dev/sda3 1/dev/sda1 15[root@centos8 ~]#df|awk -F&#x27; +|%&#x27; &#x27;/^\\/dev\\/sd/&#123;print $1,$5&#125;&#x27;/dev/sda2 3/dev/sda3 2/dev/sda1 100 取nginx的访问日志的中IP和时间123456789101112[root@VM_0_10_centos ~]# head -n 3 /apps/nginx/logs/nginx.access.log58.87.87.99 - - [09/Jun/2020:03:42:43 +0800] &quot;POST /wp-cron.php?doing_wp_cron=1591645363.2316548824310302734375 HTTP/1.1&quot; &quot;&quot;sendfileon128.14.209.154 - - [09/Jun/2020:03:42:43 +0800] &quot;GET / HTTP/1.1&quot; &quot;&quot;sendfileon64.90.40.100 - - [09/Jun/2020:03:43:11 +0800] &quot;GET /wp-login.php HTTP/1.1&quot;&quot;&quot;sendfileon[root@VM_0_10_centos ~]# awk -F&#x27;[[ ]&#x27; &#x27;&#123;print $1,$5&#125;&#x27;/apps/nginx/logs/nginx.access.log|head -358.87.87.99 09/Jun/2020:03:42:43128.14.209.154 09/Jun/2020:03:42:4364.90.40.100 09/Jun/2020:03:43:11 面试题：取 ifconfig 输出结果中的IP地址123456789101112[root@centos8 ~]#ifconfig eth0|sed -n &#x27;2p&#x27; |awk &#x27;&#123;print $2&#125;&#x27;|cat -A10.0.0.8$[root@centos8 ~]#ifconfig eth0 | awk &#x27;/netmask/&#123;print $2&#125;&#x27;10.0.0.8[root@centos6 ~]#ifconfig eth0 |awk -F &quot; +|:&quot; &#x27;/Mask/&#123;print $4&#125;&#x27;10.0.0.6[root@centos6 ~]#ip a show eth0 |awk -F&#x27; +|\\/&#x27; &#x27;/\\&lt;inet\\&gt;/&#123;print $3&#125;&#x27; 2&gt;/dev/null10.0.0.6[root@centos8 ~]#ifconfig eth0| sed -rn &#x27;2s/^[^0-9]+([0-9.]+) .*$/\\1/p&#x27;10.0.0.8[root@centos6 ~]#ifconfig eth0| sed -rn &#x27;2s/^[^0-9]+([0-9.]+) .*$/\\1/p&#x27;10.0.0.6 面试题：文件host_list.log如下格式，请提取”.magedu.com”前面的主机名部分并写入到回到该文件中1234567891011121314151617181920212223242526[root@centos8 ~]#cat host_list.log1 www.magedu.com2 blog.magedu.com3 study.magedu.com4 linux.magedu.com5 python.magedu.com[root@centos8 ~]#awk -F&quot;[ .]&quot; &#x27;&#123;print $2&#125;&#x27; host_list.logwwwblogstudylinuxpython[root@centos8 ~]#awk -F&quot;[ .]&quot; &#x27;&#123;print $2&#125;&#x27; host_list.log &gt;&gt; host_list.log[root@centos8 ~]#cat host_list.log1 www.magedu.com2 blog.magedu.com3 study.magedu.com4 linux.magedu.com5 python.magedu.comwwwblogstudylinuxpython 3.2.2 printfprintf 可以实现格式化输出 12345printf “FORMAT”, item1, item2, ...必须指定FORMAT不会自动换行，需要显式给出换行控制符 \\nFORMAT中需要分别为后面每个item指定格式符 格式符：与item一一对应 12345678%s：显示字符串%d, %i：显示十进制整数%f：显示为浮点数%e, %E：显示科学计数法数值%c：显示字符的ASCII码%g, %G：以科学计数法或浮点形式显示数值%u：无符号整数%%：显示%自身 修饰符 123#[.#] 第一个数字控制显示的宽度；第二个#表示小数点后精度，如：%3.1f- 左对齐（默认右对齐） 如：%-15s+ 显示数值的正负符号 如：%+d 范例 123456789awk -F: &#x27;&#123;printf &quot;%s&quot;,$1&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;%s\\n&quot;,$1&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;%20s\\n&quot;,$1&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;%-20s\\n&quot;,$1&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;%-20s %10d\\n&quot;,$1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;Username: %s\\n&quot;,$1&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf “Username: %sUID:%d\\n&quot;,$1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;Username: %25sUID:%d\\n&quot;,$1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;&#123;printf &quot;Username: %-25sUID:%d\\n&quot;,$1,$3&#125;&#x27; /etc/passwd 3.3 awk 变量3.3.1 常见的内置变量 FS：输入字段分隔符，默认为空白字符，功能相当于 -F -F 和 FS 变量功能一样，同时使用，-F优先级高 12345678[root@centos8 ~]#awk -v FS=&quot;:&quot; &#x27;&#123;print $1FS$3&#125;&#x27; /etc/passwd |head -n3root:0bin:1daemon:2[root@centos8 ~]#S=:;awk -F$S &#x27;&#123;print $1,$3&#125;&#x27; /etc/passwd|head -n3root 0bin 1daemon 2 OFS：输出字段分隔符，默认为空白字符 1234[root@centos8 ~]#awk -v FS=&#x27;:&#x27; &#x27;&#123;print $1,$3,$7&#125;&#x27; /etc/passwd|head -n1root 0 /bin/bash[root@centos8 ~]#awk -v FS=&#x27;:&#x27; -v OFS=&#x27;:&#x27; &#x27;&#123;print $1,$3,$7&#125;&#x27; /etc/passwd|head -n1root:0:/bin/bash RS：换行符，用于分割指定文件的行，默认是换行符 12345678910[root@centos7 ~]#cat b.txt a,b,c;1,2,3;x,y,z[root@centos7 ~]#awk -v RS=&quot;;&quot; &#x27;&#123;print $1&#125;&#x27; b.txt a,b,c1,2,3x,y,z[root@centos7 ~]#awk -v RS=&quot;;&quot; -v FS=&quot;,&quot; &#x27;&#123;print $1&#125;&#x27; b.txt a1x ORS：输出换行符，输出时用指定符号代替换行符 12[root@centos7 ~]#awk -v RS=&quot;;&quot; -v FS=&quot;,&quot; -v ORS=&quot;-&quot; &#x27;&#123;print $1&#125;&#x27; b.txt a-1-x- NF：统计一条记录的字段数量 1234567#引用变量时，变量前不需加$[root@centos8 ~]#awk -F: &#x27;&#123;print NF&#125;&#x27; /etc/fstab[root@centos8 ~]#awk -F: &#x27;&#123;print $(NF-1)&#125;&#x27; /etc/passwd #打印倒数第二列[root@centos8 ~]#ls /misc/cd/BaseOS/Packages/*.rpm |awk -F&quot;.&quot; &#x27;&#123;print $(NF-1)&#125;&#x27;|sort |uniq -c 389 i686 208 noarch 1060 x86_64 面试题：连接数最多的前3个IP1234567891011121314151617[root@centos8 ~]#awk -F&quot; +|:&quot; &#x27;&#123;print $(NF-2)&#125;&#x27; ss.log |sort |uniq -c|sort -nr|head -n3 12 223.88.255.148 11 119.250.197.118 10 183.202.63.36[root@centos8 ~]#awk -F&quot; +|:&quot; &#x27;/^ESTAB/&#123;print $(NF-2)&#125;&#x27; ss.log |sort |uniq -c|sort -nr|head -n3 12 223.88.255.148 10 183.202.63.36 9 117.152.155.119[root@centos8 ~]#ss -nt |grep &quot;^ESTAB&quot; | awk -F&quot;[[:space:]]+|:&quot; &#x27;&#123;print $(NF-2)&#125;&#x27;10.0.0.110.0.0.710.0.0.1[root@centos8 ~]#ss -nt |awk -F&quot;[[:space:]]+|:&quot; &#x27;/^ESTAB/&#123;print $(NF-2)&#125;&#x27;[root@centos8 ~]#ss -nt|awk -F: &#x27;&#123;print $(NF-1)&#125;&#x27; |awk &#x27;/^[0-9]/&#123;print $NF&#125;&#x27;|sort |uniq -c |head -n 3[root@wang-liyun-pc ~]# awk -F&#x27; +|:&#x27; &#x27;NR!=1&#123;print $(NF-2)&#125;&#x27; ss.log|sort |uniq -c 1 100.100.30.25 86 39.164.140.134 每十分钟检查将连接数超过100个以上的IP放入黑名单拒绝访问12345678910111213[root@centos8 ~]#cat deny_dos.shLINK=100while true;doss -nt | awk -F&quot;[[:space:]]+|:&quot; &#x27;/^ESTAB/&#123;print $(NF-2)&#125;&#x27;|sort |uniq -c|while read count ip;doif [ $count -gt $LINK ];then iptables -A INPUT -s $ip -j REJECTfidonedone[root@centos8 ~]#chmod +x /root/deny_dos.sh[root@centos8 ~]#crontab -e[root@centos8 ~]#crontab -l*/10 * * * * /root/deny_dos.sh NR：记录的编号（行号） 1234567891011121314[root@centos7 ~]#awk &#x27;&#123;print NR&#125;&#x27; /etc/passwd1234.....129[root@centos7 ~]#awk &#x27;END&#123;print NR&#125;&#x27; /etc/passwd129[root@centos7 ~]#awk &#x27;BEGIN&#123;print NR&#125;&#x27; /etc/passwd0[root@centos8 ~]#ifconfig eth0 | awk &#x27;NR==2&#123;print $2&#125;&#x27; #根据行号筛选想要的行10.0.0.8 取ifconfig输出结果中的IP地址1234[root@centos8 ~]#ifconfig eth0 | awk &#x27;/netmask/&#123;print $2&#125;&#x27;10.0.0.8[root@centos8 ~]#ifconfig eth0 | awk &#x27;NR==2&#123;print $2&#125;&#x27;10.0.0.8 FNR：各文件分别计数，记录的编号 1[root@centos7 ~]#awk &#x27;&#123;print FNR&#125;&#x27; /etc/passwd /etc/issue FILENAME：当前文件名 123456789101112[root@centos7 ~]#awk &#x27;&#123;print FNR,FILENAME&#125;&#x27; /etc/issue /etc/hosts1 /etc/issue2 /etc/issue3 /etc/issue1 /etc/hosts2 /etc/hosts[root@centos7 ~]#awk &#x27;&#123;print FILENAME&#125;&#x27; /etc/issue /etc/hosts/etc/issue/etc/issue/etc/issue/etc/hosts/etc/hosts ARGC：命令行参数的个数 1234567root@centos8 ~]#awk &#x27;&#123;print ARGC&#125;&#x27; /etc/issue /etc/redhat-release3333[root@centos8 ~]#awk &#x27;BEGIN&#123;print ARGC&#125;&#x27; /etc/issue /etc/redhat-release3 ARGV：数组，保存的是命令行所给定的各参数，每一个参数：ARGV[0]，…… 123456789[root@centos8 ~]#awk &#x27;BEGIN&#123;print ARGV[0]&#125;&#x27; /etc/issue /etc/redhat-releaseawk[root@centos8 ~]#awk &#x27;BEGIN&#123;print ARGV[1]&#125;&#x27; /etc/issue /etc/redhat-release/etc/issue[root@centos8 ~]#awk &#x27;BEGIN&#123;print ARGV[2]&#125;&#x27; /etc/issue /etc/redhat-release/etc/redhat-release[root@centos8 ~]#awk &#x27;BEGIN&#123;print ARGV[3]&#125;&#x27; /etc/issue /etc/redhat-release[root@centos8 ~]# 3.3.2 自定义变量自定义变量是区分字符大小写的,使用下面方式进行赋值 -v var&#x3D;value 在program中直接定义（要加BEGIN，不然会让你在屏幕输入内容） awk和shell赋值的区别 awk 赋值，a&#x3D;b&#x3D;1，这个表示先赋值 b&#x3D;1，之后将 b&#x3D;1返回值赋值给a shell中 -v k1=k2=v 这个是赋值一个字符串 范例 12345678910[root@centos7 ~]#awk -v test1=&quot;hello,gawk&quot; &#x27;BEGIN&#123;print test1&#125;&#x27;hello,gawk[root@centos7 ~]#awk -v test1=&quot;hello,gawk&quot; &#x27;BEGIN&#123;print test1;test1=&quot;hello,awk&quot;;print test1&#125;&#x27;hello,gawkhello,awk[root@centos8 ~]#awk -v test1=test2=&quot;hello,gawk&quot; &#x27;BEGIN&#123;print test1,test2&#125;&#x27; #test1是后面字符串，test2为空test2=hello,gawk[root@centos8 ~]#awk -v test1=test2=&quot;hello1,gawk&quot;&#x27;BEGIN&#123;test1=test2=&quot;hello2,gawk&quot;;print test1,test2&#125;&#x27; hello2,gawk hello2,gawk 3.4 操作符算术操作符：x+y, x-y, x*y, x/y, x^y, x%y -x：转换为负数 +x：将字符串转换为数值 字符串操作符：没有符号的操作符，字符串连接 赋值操作符：=, +=, -=, *=, /=, %=, ^=，++, -- 1234[root@centos8 ~]#awk &#x27;BEGIN&#123;i=0;print i++,i&#125;&#x27;0 1[root@centos8 ~]#awk &#x27;BEGIN&#123;i=0;print ++i,i&#125;&#x27;1 1 比较操作符：==, !=, &gt;, &gt;=, &lt;, &lt;= 1234[root@centos8 ~]#awk -F: &#x27;$3&gt;=1000&#x27; /etc/passwdnobody:x:65534:65534:Kernel Overflow User:/:/sbin/nologinwang:x:1000:1000:wang:/home/wang:/bin/bashmage:x:1001:1001::/home/mage:/bin/bash 范例：取奇，偶数行 123456789101112131415161718[root@centos8 ~]#seq 10 | awk &#x27;NR%2==0&#x27;246810[root@centos8 ~]#seq 10 | awk &#x27;NR%2==1&#x27;13579[root@centos8 ~]#seq 10 | awk &#x27;NR%2!=0&#x27;13579 模式匹配符： ~ 左边是否和右边匹配，包含关系 !~ 是否不匹配 12345678[root@centos8 ~]#awk -F: &#x27;$0 ~ /root/&#123;print $1&#125;&#x27; /etc/passwd[root@centos8 ~]#awk -F: &#x27;$0 ~ &quot;^root&quot;&#123;print $1&#125;&#x27; /etc/passwd[root@centos8 ~]#awk &#x27;$0 !~ /root/&#x27; /etc/passwd[root@centos8 ~]#df | awk -F&quot;[[:space:]]+|%&quot; &#x27;$0 ~ /^\\/dev\\/sd/&#123;print $5&#125;&#x27;5192 逻辑操作符： 与：&amp;&amp;，并且关系 或：||，或者关系 非：!，取反 1234awk -F: &#x27;$3&gt;=0 &amp;&amp; $3&lt;=1000 &#123;print $1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;$3==0 || $3&gt;=1000 &#123;print $1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;!($3==0) &#123;print $1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;!($3&gt;=500) &#123;print $1,$3&#125;&#x27; /etc/passwd 条件表达式（三目表达式） 1234567selector?if-true-expression:if-false-expression[root@centos8 ~]#awk -F: &#x27;&#123;$3&gt;=1000?usertype=&quot;Common User&quot;:usertype=&quot;SysUser&quot;;printf&quot;%-20s:%12s\\n&quot;,$1,usertype&#125;&#x27; /etc/passwd[root@centos8 ~]#df | awk -F&quot;[ %]+&quot; &#x27;/^\\/dev\\/sd/&#123;$(NF-1)&gt;10?disk=&quot;full&quot;:disk=&quot;OK&quot;;print $(NF-1),disk&#125;&#x27;3 OK1 OK13 full 范例 123456789101112[root@rocky8 ~]#cat scores.txtwang 100li 90zhang 50zhao 80han 70[root@rocky8 ~]#awk &#x27;$2&gt;=60?type=&quot;pass&quot;:type=&quot;nopass&quot;&#123;print $1,type&#125;&#x27; scores.txtwang passli passzhang nopasszhao passhan pass 3.5 模式PATTERNPATTERN：根据pattern条件，过滤匹配的行，再做处理 如果未指定：空模式，匹配每一行 1[root@centos8 ~]#awk -F: &#x27;&#123;print $1,$3&#125;&#x27; /etc/passwd &#x2F;regular expression&#x2F;：仅处理能够模式匹配到的行，需要用&#x2F; &#x2F;括起来 12345678910111213141516[root@centos8 ~]#df | awk &#x27;/^\\/dev\\/sd/&#x27;/dev/sda2 104806400 4935924 99870476 5% //dev/sda3 52403200 398876 52004324 1% /data/dev/sda1 999320 848572 81936 92% /boot[root@centos7 ~]#df | awk -F&quot;[ %]&quot; &#x27;/^\\/dev\\//&#123;print $(NF-2)&#125;&#x27;1315[root@centos7 ~]#ifconfig eth0 | awk &#x27;/netmask/&#123;print $2&#125;&#x27;10.0.0.183[root@centos7 ~]#awk &#x27;!/^#|^$/&#x27; /etc/fstab/dev/mapper/centos_10-root / xfs defaults 0 0UUID=263ab425-76cc-48bf-952f-b5958e7d325d /boot xfs defaults 0 0/dev/mapper/centos_10-swap swap swap defaults 0 0 relational expression: 关系表达式，结果为“真”才会被处理 真：结果为非0值，非空字符串 假：结果为空字符串或0值 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@centos8 ~]#seq 10 | awk &#x27;i=0&#x27; #0为假，不输出任何东西[root@centos8 ~]#seq 10 | awk &#x27;i=1&#x27;12345678910[root@centos8 ~]#seq 10 | awk &#x27;i=!i&#x27; #首先i没赋值为假，取反为真，打印1，接着取反为假，2不打印，接着取反为真，打印3...13579[root@centos8 ~]#seq 10 | awk &#x27;!(i=!i)&#x27;246810[root@centos8 ~]#seq 10 | awk -v i=1 &#x27;i=!i&#x27;246810[root@centos8 ~]#seq 10 | awk -v i=0 &#x27;i=!i&#x27;13579[root@centos8 ~]#seq 10 | awk &#x27;&#123;i=!i;print i&#125;&#x27;1010101010 范例 1234567891011121314awk -F: &#x27;i=1;j=1&#123;print i,j&#125;&#x27; /etc/passwdAwk -F: &#x27;$3&gt;=1000&#123;print $1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;$3&lt;1000&#123;print $1,$3&#125;&#x27; /etc/passwdawk -F: &#x27;$NF==&quot;/bin/bash&quot;&#123;print $1,$NF&#125;&#x27; /etc/passwd[root@centos8 ~]#awk -F: &#x27;$NF==&quot;/bin/bash&quot;&#123;print $1,$NF&#125;&#x27; /etc/passwdroot /bin/bashwang /bin/bashmage /bin/bash[root@centos8 ~]#awk -F: &#x27;$NF ~ /bash$/&#123;print $1,$NF&#125;&#x27; /etc/passwdroot /bin/bashwang /bin/bashmage /bin/bash line ranges：行范围 不支持直接用行号，但可以使用变量NR间接指定行号 &#x2F;pat1&#x2F;,&#x2F;pat2&#x2F; 不支持直接给出数字格式 12345678910111213141516171819202122232425[root@centos8 ~]#seq 10 | awk &#x27;NR&gt;=3 &amp;&amp; NR&lt;=6&#x27;3456[root@centos8 ~]#awk &#x27;NR&gt;=3 &amp;&amp; NR&lt;=6&#123;print NR,$0&#125;&#x27; /etc/passwd3 daemon:x:2:2:daemon:/sbin:/sbin/nologin4 adm:x:3:4:adm:/var/adm:/sbin/nologin5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin6 sync:x:5:0:sync:/sbin:/bin/sync[root@centos8 ~]#sed -n &#x27;3,6p&#x27; /etc/passwddaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologinsync:x:5:0:sync:/sbin:/bin/sync[root@centos8 ~]#awk &#x27;/^bin/,/^adm/&#x27; /etc/passwdbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin[root@centos8 ~]#sed -n &#x27;/^bin/,/^adm/p&#x27; /etc/passwdbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologin BEGIN&#x2F;END模式 BEGIN&#123;&#125;：仅在开始处理文件中的文本之前执行一次 END&#123;&#125;：仅在文本处理完成之后执行一次 范例：打印表格 123456789[root@centos8 ~]#awk -F: &#x27;BEGIN&#123;printf &quot;--------------------------------\\n%-20s|%10s|\\n--------------------------------\\n&quot;,&quot;username&quot;,&quot;uid&quot;&#125;&#123;printf&quot;%-20s|%10d|\\n--------------------------------\\n&quot;,$1,$3&#125;&#x27; /etc/passwd--------------------------------username | uid |--------------------------------root | 0 |bin | 1 |daemon | 2 |adm | 3 |lp | 4 | 3.6 条件判断 if-else12if(condition)&#123;statement;…&#125;[else statement]if(condition1)&#123;statement1&#125;else if(condition2)&#123;statement2&#125;else if(condition3)&#123;statement3&#125;...... else &#123;statementN&#125; 使用场景：对awk取得的整行或某个字段做条件判断 123456789101112[root@centos7 ~]#cat score.txtname scorehuang 90wu 80 zhao 70opan 50[root@centos7 ~]#awk &#x27;NR!=1&#123;score=$2;if(score&gt;=80)&#123;print $1,&quot;good&quot;&#125;else if(score&gt;=60)&#123;print $1,&quot;pass&quot;&#125;else &#123;print $1,&quot;no pass&quot;&#125;&#125;&#x27; score.txthuang goodwu goodzhao passopan no pass 3.7 条件判断 switchawk 中的switch分支语句功能较弱，只能进行等值比较或正则匹配 各分支结尾需使用break来终止 12switch(expression) &#123;case VALUE1 or /REGEXP/: statement1; case VALUE2 or/REGEXP2/: statement2; ...; default: statementn&#125; 3.8 循环 while语法：while (condition) &#123;statement;…&#125; 使用场景： 对一行内的多个字段逐一类似处理时使用 对数组中的各元素逐一处理时使用 12root@ubuntu2004:~# awk -v i=1 -v sum=0 &#x27;BEGIN&#123;while(i&lt;=100)&#123;sum+=i;i++&#125;;print sum&#125;&#x27;5050 3.9 循环 do-while语法：do &#123;statement;…&#125;while(condition) 12[root@centos8 ~]#awk &#x27;BEGIN&#123; total=0;i=1;do&#123; total+=i;i++;&#125;while(i&lt;=100);print total&#125;&#x27;5050 3.10 循环 for语法：for(expr1;expr2;expr3) &#123;statement;…&#125; 常见用法：for(variable assignment;condition;iteration process) &#123;for-body&#125; 特殊用法：能够遍历数组中的元素，for(var in array) &#123;for-body&#125; 范例 123456root@ubuntu2004:~# awk &#x27;BEGIN&#123;sum=0;for(i=1;i&lt;=100;i++)&#123;sum+=i&#125;;print sum&#125;&#x27;5050#shell实现root@ubuntu2004:~# for((i=1,sum=0;i&lt;=100;i++));do let sum+=i;done;echo $sum5050 面试题：文件abc.txt只有一行数字，计算其总和12345678[root@centos8 ~]#cat abc.txt1 2 3 4 5[root@centos8 ~]#cat abc.txt |awk &#x27;&#123;for(i=1;i&lt;=NF;i++)&#123;sum+=i&#125;;print sum&#125;&#x27;15[root@centos8 ~]#cat abc.txt|tr &#x27; &#x27; + |bc15[root@centos8 ~]#sum=0;for i in `cat abc.txt`;do let sum+=i;done;echo $sum15 性能比较 1234time (awk &#x27;BEGIN&#123; total=0;for(i=0;i&lt;=10000;i++)&#123;total+=i;&#125;;print total;&#125;&#x27;)time (total=0;for i in &#123;1..10000&#125;;do total=$(($total+i));done;echo $total)time (for ((i=0;i&lt;=10000;i++));do let total+=i;done;echo $total)time (seq –s ”+” 10000|bc) 取出字符串中的数字123456789101112[root@ubuntu2204 ~]# echo &#x27;dsFUs34tg*fs5a%8ar%$#@&#x27; |awk -F &quot;&quot; &#x27;&gt; &#123;&gt; for(i=1;i&lt;=NF;i++)&gt; &#123; &gt; if ($i ~ /[0-9]/) &gt; &#123;&gt; str=(str $i) #连字符，字符串拼接&gt; &#125; &gt; &#125;&gt; print str&gt; &#125;&#x27;3458 3.11 continue 和 breakcontinue 中断本次循环 break 中断整个循环 123456continue [n]break [n][root@centos8 ~]#awk &#x27;BEGIN&#123;for(i=1;i&lt;=100;i++)&#123;if(i==50)continue;sum+=i&#125;;print sum&#125;&#x27;5000[root@centos8 ~]#awk &#x27;BEGIN&#123;for(i=1;i&lt;=100;i++)&#123;if(i==50)break;sum+=i&#125;;print sum&#125;&#x27;1225 3.12 nextnext 可以提前结束对本行处理而直接进入下一行处理（awk自身循环） 1234567891011121314151617181920212223242526#奇数行不处理[root@centos8 ~]#awk -F: &#x27;&#123;if($3%2!=0) next; print $1,$3&#125;&#x27; /etc/passwd root 0daemon 2lp 4shutdown 6mail 8games 12ftp 14nobody 65534polkitd 998gluster 996rtkit 172rpc 32chrony 994saslauth 992clevis 984pegasus 66colord 982setroubleshoot 980gdm 42gnome-initial-setup 978sshd 74avahi 70tcpdump 72wang 1000 3.13 数组awk的数组为关联数组 123array_name[index-expression]weekdays[&quot;mon&quot;]=&quot;Monday&quot; index-expression 利用数组，实现 k&#x2F;v 功能 可使用任意字符串；字符串要使用双引号括起来 如果某数组元素事先不存在，在引用时，awk会自动创建此元素，并将其值初始化为“空串” 若要判断数组中是否存在某元素，要使用“index in array”格式进行遍历 范例 12[root@centos8 ~]#awk &#x27;BEGIN&#123;weekdays[&quot;mon&quot;]=&quot;Monday&quot;;weekdays[&quot;tue&quot;]=&quot;Tuesday&quot;;print weekdays[&quot;mon&quot;]&#125;&#x27;Monday 范例：去重 123456789101112131415[root@centos7 ~]#cat c.txtabcabcd[root@centos7 ~]#awk &#x27;!line[$0]++&#x27; c.txt abcd首先是line[‘a’]=“”，a空值，为假，取反后为真，++后a=1；同理b和c一样，接着又是a，line[&#x27;a&#x27;]=1，取反后为假，b和c同理 范例：判断数组索引是否存在 123456[root@centos8 ~]# awk &#x27;BEGIN&#123;array[&quot;i&quot;]=&quot;x&quot;; array[&quot;j&quot;]=&quot;y&quot; ; print &quot;i&quot; in array, &quot;y&quot; in array &#125;&#x27;1 0[root@centos8 ~]#awk &#x27;BEGIN&#123;array[&quot;i&quot;]=&quot;x&quot;; array[&quot;j&quot;]=&quot;y&quot; ;if (&quot;i&quot; in array )&#123;print &quot;存在&quot;&#125;else&#123;print &quot;不存在&quot;&#125;&#125;&#x27;存在[root@centos8 ~]#awk &#x27;BEGIN&#123;array[&quot;i&quot;]=&quot;x&quot;; array[&quot;j&quot;]=&quot;y&quot; ;if (&quot;abc&quot; in array )&#123;print &quot;存在&quot;&#125;else&#123;print &quot;不存在&quot;&#125;&#125;&#x27;不存在 若要遍历数组中的每个元素，要使用 for 循环 1for(var in array) &#123;for-body&#125; #var 会遍历array的每个索引 范例：遍历数组 12345678910111213141516171819202122232425[root@ubuntu2204 ~]# awk &#x27;BEGIN&#123;weekdays[&quot;mon&quot;]=&quot;Monday&quot;;weekdays[&quot;tue&quot;]=&quot;Tuesday&quot;;for(i in weekdays)&#123;print i,weekdays[i]&#125;&#125;&#x27;tue Tuesdaymon Monday[root@ubuntu2204 ~]# awk &#x27;BEGIN&#123;students[1]=&quot;user1&quot;;students[2]=&quot;user2&quot;;students[3]=&quot;user3&quot;;for(x in students)&#123;print x&quot;:&quot;students[x]&#125;&#125;&#x27;1:user12:user23:user3[root@centos8 ~]#awk &#x27;BEGIN &#123;a[&quot;x&quot;] = &quot;welcome&quot;a[&quot;y&quot;] = &quot;to&quot;a[&quot;z&quot;] = &quot;Magedu&quot;for (i in a) &#123; print i,a[i]&#125;&#125;&#x27;x welcomey toz Magedu[root@ubuntu2204 ~]#awk -F: &#x27;&#123;user[$1]=$3&#125;END&#123;for(i in user)&#123;print &quot;username:&quot;i,&quot;uid: &quot;user[i]&#125;&#125;&#x27; /etc/passwdusername: adm uid: 3username: rpc uid: 32username: dnsmasq uid: 985 范例：显示主机的连接状态出现的次数 1234567[root@centos7 ~]#ss -ant | awk &#x27;NR&gt;=2&#123;print $1&#125;&#x27; | sort | uniq -c 1 ESTAB 4 LISTEN[root@centos7 ~]#ss -ant | awk &#x27;NR&gt;=2&#123;state[$1]++&#125;END&#123;for(i in state)&#123;print state[i],i&#125;&#125;&#x27;4 LISTEN1 ESTAB 范例 1234567891011121314151617181920[root@ubuntu2204 ~]# awk &#x27;&#123;ip[$1]++&#125;END&#123;for(i in ip)&#123;print i,ip[i]&#125;&#125;&#x27; /var/log/httpd/access_log172.20.0.200 1482172.20.21.121 2172.20.30.91 29172.16.102.29 864172.20.0.76 1565172.20.9.9 15172.20.1.125 463172.20.61.11 2172.20.73.73 198[root@ubuntu2204 ~]# awk &#x27;&#123;ip[$1]++&#125;END&#123;for(i in ip)&#123;print ip[i],i&#125;&#125;&#x27; access_log|sort -nr| head -34870 172.20.116.2283429 172.20.116.2082834 172.20.0.222[root@ubuntu2204 ~]# wk &#x27;&#123;ip[$1]++&#125;END&#123;for(i in ip)&#123;print i,ip[i]&#125;&#125;&#x27; access_log|sort -k2 -nr|head -3172.20.116.228 4870172.20.116.208 3429172.20.0.222 2834 范例：封掉查看访问日志中连接次数超过1000次的IP 1[root@centos8 ~]#awk &#x27;&#123;ip[$1]++&#125;END&#123;for(i in ip)&#123;if(ip[i]&gt;=1000)&#123;system(&quot;iptables -A INPUT -s &quot;i&quot; -j REJECT&quot;)&#125;&#125;&#125;&#x27; nginx.access.log-20200428 范例：多维数组 1234567891011121314151617[root@centos8 ~]#awk &#x27;BEGIN&#123;&gt; array[1][1]=11&gt; array[1][2]=12&gt; array[1][3]=13&gt; array[2][1]=21&gt; array[2][2]=22&gt; array[2][3]=23&gt; for (i in array)&gt; for (j in array[i])&gt; print array[i][j]&gt; &#125;&#x27;111213212223 3.14 awk函数官方文档：https://www.gnu.org/software/gawk/manual/gawk.html 3.14.1 常见内置函数数值处理： 123rand()：返回0和1之间一个随机数srand()：配合rand() 函数,生成随机数的种子int()：返回整数 范例 1234567891011121314151617[root@centos8 ~]#awk &#x27;BEGIN&#123;srand();print rand()&#125;&#x27;0.790437[root@centos8 ~]#awk &#x27;BEGIN&#123;srand();print rand()&#125;&#x27;0.283736[root@centos8 ~]#awk &#x27;BEGIN&#123;srand(); for (i=1;i&lt;=10;i++)print int(rand()*100)&#125;&#x27;35173595191570544693 字符串处理： 12345str1=(str2,str3)：连字符，字符串拼接length([s])：返回指定字符串的长度sub(r,s,[t])：对t字符串搜索r表示模式匹配的内容，并将第一个匹配内容替换为s（懒惰模式）gsub(r,s,[t])：对t字符串进行搜索r表示的模式匹配的内容，并全部替换为s所表示的内容（贪婪模式）split(s,array,[r])：以r为分隔符，切割字符串s，并将切割后的结果保存至array所表示的数组中，第一个索引值为1,第二个索引值为2,… 范例: 统计用户名的长度 12root@ubuntu2004:~# cut -d: -f1 /etc/passwd | awk &#x27;&#123;print length()&#125;&#x27;root@ubuntu2004:~# awk -F: &#x27;&#123;print length($1)&#125;&#x27; /etc/passwd 范例 12345#内置函数length()返回字符数，而非字节数[root@centos8 ~]#awk &#x27;BEGIN&#123;print length(&quot;hello&quot;)&#125;&#x27;5[root@centos8 ~]#awk &#x27;BEGIN&#123;print length(&quot;你好&quot;)&#125;&#x27;2 范例 123456[root@centos8 ~]#echo &quot;2008:08:08 08:08:08&quot; | awk &#x27;sub(/:/,&quot;-&quot;,$1)&#x27;2008-08:08 08:08:08[root@centos8 ~]#echo &quot;2008:08:08 08:08:08&quot; | awk &#x27;gsub(/:/,&quot;-&quot;,$1)&#x27;2008-08-08 08:08:08[root@centos8 ~]#echo &quot;2008:08:08 08:08:08&quot; | awk &#x27;gsub(/:/,&quot;-&quot;,$0)&#x27;2008-08-08 08-08-08 范例 12345678[root@centos7 ~]#head -n1 /etc/passwd | awk &#x27;&#123;split($0,array,&quot;:&quot;)&#125;END&#123;print array[1]&#125;&#x27;root[root@centos7 ~]#head -n1 /etc/passwd | awk &#x27;&#123;split($0,array,&quot;:&quot;)&#125;END&#123;print array[4]&#125;&#x27;0[root@centos8 ~]#netstat -tn | awk &#x27;/^tcp/&#123;split($5,ip,&quot;:&quot;);count[ip[1]]++&#125;END&#123;for(i in count)&#123;print i,count[i]&#125;&#125;&#x27;10.0.0.1 110.0.0.6 110.0.0.7 673 调用shell命令 123system(&#x27;cmd&#x27;)空格是awk中的字符串连接符，如果system中需要使用awk中的变量可以使用空格分隔，或者说除了awk的变量外其他一律用&quot;&quot;引用起来 范例 12awk &#x27;BEGIN&#123;system(&quot;hostname&quot;)&#125;&#x27;awk &#x27;BEGIN&#123;score=100; system(&quot;echo your score is &quot; score) &#125;&#x27; 时间函数 12systime() 当前时间到1970年1月1日的秒数strftime() 指定时间格式 范例 1234[root@centos8 ~]#awk &#x27;BEGIN&#123;print systime()&#125;&#x27;1609917829[root@centos8 ~]#awk &#x27;BEGIN&#123;print strftime(&quot;%Y-%m-%dT%H:%M&quot;,systime()-3600)&#125;&#x27;2021-01-06T14:24 3.14.2 自定义函数自定义函数格式： 1234function name ( parameter, parameter, ... ) &#123; statements return expression&#125; 范例 1234567[root@ubuntu2204 ~]# awk &#x27;function test()&#123;print &quot;hello test func&quot;&#125;BEGIN&#123;test()&#125;&#x27;hello test func[root@ubuntu2204 ~]# awk -F: &#x27;function test(uname,uid)&#123;print uname&quot; id is &quot;uid&#125;&#123;test($1,$3)&#125;&#x27; /etc/passwdroot id is 0bin id is 1daemon id is 2 范例 123456789[root@centos8 ~]#cat func.awkfunction max(x,y) &#123;x&gt;y?var=x:var=yreturn var&#125;BEGIN&#123;print max(a,b)&#125;[root@centos8 ~]#awk -v a=30 -v b=20 -f func.awk30 3.15 awk 脚本将awk程序写成脚本，直接调用或执行 12345678910[root@centos8 ~]#cat test.awk#!/bin/awk -f#this is a awk script&#123;if($3&gt;=1000)print $1,$3&#125;[root@centos8 ~]#chmod +x test.awk[root@centos8 ~]#./test.awk -F: /etc/passwdnobody 65534wang 1000mage 1001 向awk脚本传递参数 格式：awkfile var=value var2=value2... Inputfile 注意： 上面格式变量在BEGIN过程中不可用。直到首行输入完成以后，变量才可用 可以通过 -v 参数，让awk在执行BEGIN之前得到变量的值 命令行中每一个指定的变量都需要一个 -v 参数 范例 123456789101112131415161718192021[root@ubuntu2204 ~]# awk -v x=100 &#x27;BEGIN&#123;print x&#125;&#123;print x+100&#125;&#x27; /etc/hosts100200200[root@ubuntu2204 ~]# awk &#x27;BEGIN&#123;print x&#125;&#123;print x+100&#125;&#x27; x=200 /etc/hosts300300[root@centos8 ~]#cat test2.awk#!/bin/awk -f&#123;if($3 &gt;=min &amp;&amp; $3&lt;=max)print $1,$3&#125;[root@centos8 ~]#chmod +x test2.awk[root@centos8 ~]#./test2.awk -F: min=100 max=200 /etc/passwdsystemd-resolve 193rtkit 172pulse 171qemu 107usbmuxd 113abrt 173 范例: 检查出最近一小时内访问nginx服务次数超过3次的客户端IP 12345678910111213141516171819202122232425262728[root@VM_0_10_centos ~]# cat check_nginx_log.awk#!/usr/bin/awk -fBEGIN &#123; beg=strftime(&quot;%Y-%m-%dT%H:%M&quot;,systime()-3600) ; #定义一个小时前的时间，并格式化日期格式 end=strftime( &quot;%Y-%m-%dT%H:%M&quot;,systime()-60) ; #定义结束时间 #print beg; #print end;&#125;$4 &gt; beg &amp;&amp; $4 &lt; end &#123;#定义取这个时间段内的日志 count[$12]+=1;#利用ip当做数组下标，次数当做数组内容&#125;END &#123; for(i in count)&#123;#结束从数组取数据代表数组的下标，也就是ip if(count[i]&gt;3) &#123; #如果次数大于3次，做操作 print count [i]&quot; &quot;i; #system(&quot;iptables -I INPUT -S”i”j DROP&quot; ) &#125; &#125;&#125;#awk -F&#x27;&quot;&#x27; -f check_nginx_log.awk /apps/nginx/logs/access.log[root@VM_0_10_centos ~]# awk -F&#x27;&quot;&#x27; -f check_nginx_log.awk /apps/nginx/logs/access_json.log4 127.0.0.156 172.105.120.925 58.87.87.9911 111.199.184.16","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"PAM认证机制","slug":"Linux/encryption-security/PAM","date":"2025-08-21T03:03:21.000Z","updated":"2025-09-07T06:10:49.154Z","comments":true,"path":"Linux/encryption-security/PAM/","permalink":"https://aquapluto.github.io/Linux/encryption-security/PAM/","excerpt":"","text":"1 PAM介绍PAM：Pluggable Authentication Modules，插件式的验证模块，Sun公司于1995 年开发的一种与认证相关的通用框架机制。PAM 只关注如何为服务验证用户的 API，通过提供一些动态链接库和一套统一的API，将系统提供的服务和该服务的认证方式分开，使得系统管理员可以灵活地根据需要给不同的服务配置不同的认证 方式而无需更改服务程序一种认证框架，自身不做认证 官网：http://www.linux-pam.org/ 2 PAM架构 PAM提供了对所有服务进行认证的中央机制，适用于本地登录，远程登录，如：telnet,rlogin,fsh,ftp,点对点协议PPP，su等应用程序中，系统管理员通过PAM配置文件来制定不同应用程序的不同认证策略；应用程序开发者通过在服务程序中使用PAM API(pam_xxxx( ))来实现对认证方法的调用；而PAM服务模块的开发者则利用PAM SPI来编写模块（主要调用函数pam_sm_xxxx( )供PAM接口库调用，将不同的认证机制加入到系统中；PAM接口库（libpam）则读取配置文件，将应用程序和相应的PAM服务模块联系起来 3 PAM相关文件包名: pam 模块文件目录：&#x2F;lib64&#x2F;security&#x2F;*.so（放各种模块） 特定模块相关的设置文件：&#x2F;etc&#x2F;security&#x2F;（复杂模块的专有配置文件） 应用程序调用PAM模块的配置文件 主配置文件：&#x2F;etc&#x2F;pam.conf，默认不存在，一般不使用主配置 为每种应用模块提供一个专用的配置文件：&#x2F;etc&#x2F;pam.d&#x2F;APP_NAME（哪些服务调用&#x2F;lib64&#x2F;security&#x2F;里哪些模块的配置文件） 注意：如&#x2F;etc&#x2F;pam.d存在，&#x2F;etc&#x2F;pam.conf将失效 范例：查看程序是否支持PAM 123456789[root@centos8 ~]#ldd `which sshd` |grep libpamlibpam.so.0 =&gt; /lib64/libpam.so.0 (0x00007fea8e70d000)[root@centos8 ~]#ldd `which passwd` |grep pamlibpam.so.0 =&gt; /lib64/libpam.so.0 (0x00007f045b805000)libpam_misc.so.0 =&gt; /lib64/libpam_misc.so.0 (0x00007f045b601000)#不支持PAM[root@centos6 ~]#ldd /usr/sbin/httpd |grep pam[root@centos6 ~]# 4 PAM工作原理PAM认证一般遵循这样的顺序：Service(服务)→PAM(配置文件)→pam_*.so PAM认证首先要确定那一项服务，然后加载相应的PAM的配置文件(位于&#x2F;etc&#x2F;pam.d下)，最后调用认证文件(位于&#x2F;lib64&#x2F;security下)进行安全认证 PAM认证过程示例： 使用者执行&#x2F;usr&#x2F;bin&#x2F;passwd 程序，并输入密码 passwd开始调用PAM模块，PAM模块会搜寻passwd程序的PAM相关设置文件，这个设置文件一般是在&#x2F;etc&#x2F;pam.d&#x2F;里边的与程序同名的文件，即PAM会搜寻&#x2F;etc&#x2F;pam.d&#x2F;passwd此设置文件 经由&#x2F;etc&#x2F;pam.d&#x2F;passwd设定文件的数据，取用PAM所提供的相关模块来进行验证 将验证结果回传给passwd这个程序，而passwd这个程序会根据PAM回传的结果决定下一个动作（重新输入密码或者通过验证） 5 PAM配置文件格式说明通用配置文件&#x2F;etc&#x2F;pam.conf格式,此格式不使用 1application type control module-path arguments 专用配置文件&#x2F;etc&#x2F;pam.d&#x2F; 格式 1234567type control module-path argumentsapplication #指服务名，如：telnet、login、ftp等，服务名字“OTHER”代表所有没有在该文件中明确配置的其它服务type #指模块类型，即功能control #PAM库该如何处理与该服务相关的PAM模块的成功或失败情况，一个关健词实现module-path #用来指明本模块对应的程序文件的路径名Arguments #用来传递给该模块的参数 模块类型（module-type） Auth：账号的认证和授权 Account：帐户的有效性，与账号管理相关的非认证类的功能，如：用来限制&#x2F;允许用户对某个服务的访问时间，限制用户的位置(例如：root用户只能从控制台登录) Password：用户修改密码时密码复杂度检查机制等功能 Session：用户会话期间的控制，如：最多打开的文件数，最多的进程数等 -type：表示因为缺失而不能加载的模块将不记录到系统日志,对于那些不总是安装在系统上的模块有用 Control required ：一票否决，表示本模块必须返回成功才能通过认证，但是如果该模块返回失败，失败结果也不会立即通知用户，而是要等到同一type中的所有模块全部执行完毕，再将失败结果返回给应用程序，即为必要条件 requisite ：一票否决，该模块必须返回成功才能通过认证，但是一旦该模块返回失败，将不再执行同一type内的任何模块，而是直接将控制权返回给应用程序。是一个必要条件 sufficient ：一票通过，表明本模块返回成功则通过身份认证的要求，不必再执行同一type内的其它模块，但如果本模块返回失败可忽略，即为充分条件，优先于前面的required和requisite optional ：表明本模块是可选的，它的成功与否不会对身份认证起关键作用，其返回值一般被忽略 include： 调用其他的配置文件中定义的配置信息 在 control 列中，对于更复杂的语法，可以写成如下格式 1234[value1=action1 value2=action2 ...]valueN对应于在为其定义行的模块中调用的函数的返回代码,取值如下success, open_err, symbol_err, service_err, system_err, buf_err, perm_denied,auth_err, cred_insufficient, authinfo_unavail, user_unknown,maxtries,new_authtok_reqd, acct_expired, session_err, cred_unavail, cred_expired,cred_err, no_module_data, conv_err, authtok_err,authtok_recover_err,authtok_lock_busy, authtok_disable_aging, try_again, ignore, abort,authtok_expired, module_unknown, bad_item, conv_again,incomplete, and default module-path: 模块文件所在绝对路径：&#x2F;path&#x2F;XYZ.so 模块文件所在相对路径：&#x2F;lib64&#x2F;security目录下的模块可使用相对路径，如：pam_shells.so、pam_limits.so，表示 so 文件在 &#x2F;lib64&#x2F;security&#x2F; 目录下 有些模块有自已的专有配置文件，在&#x2F;etc&#x2F;security&#x2F;*.conf目 录下，这种写法表示直接使用另一个配置，例如postlogin Arguments debug ：该模块应当用syslog( )将调试信息写入到系统日志文件中 no_warn ：表明该模块不应把警告信息发送给应用程序 use_first_pass ：该模块不能提示用户输入密码，只能从前一个模块得到输入密码 try_first_pass ：该模块首先用前一个模块从用户得到密码，如果该密码验证不通过，再提示用户输入新密码 use_mapped_pass 该模块不能提示用户输入密码，而是使用映射过的密码 expose_account 允许该模块显示用户的帐号名等信息，一般只能在安全的环境下使用，因为泄漏用户名会对安全造成一定程度的威胁 注意：修改PAM配置文件将马上生效 建议：编辑pam规则时，保持至少打开一个root会话，以防止root身份验证错误 6 PAM模块帮助在线文档：http://www.linux-pam.org/Linux-PAM-html/ 离线文档：http://www.linux-pam.org/documentation/ 12345678910man pam#查询模块在哪一个man 章节man -k 模块名[root@ubuntu ~]# man -k pam_nologinpam_nologin (8) - Prevent non-root users from login#在指定章节查询模块man N 模块名[root@ubuntu ~]# man 8 pam_nologin 7 常用PAM模块7.1 pam_nologin.so模块功能：如果&#x2F;etc&#x2F;nologin文件存在，将导致非root用户不能登陆,当该用户登陆时，会显示&#x2F;etc&#x2F;nologin文件内容，并拒绝登陆，前提是相应的服务使用了该模块，此规则才会生效 查询有哪些服务使用了该模块 1234[root@ubuntu ~]# grep pam_nologin /etc/pam.d/*/etc/pam.d/login:auth requisite pam_nologin.so/etc/pam.d/ppp:auth required pam_nologin.so/etc/pam.d/sshd:account required pam_nologin.so 默认此模块可以对ssh等登录有效，由于 su 服务没有使用该模块，所以使用 su 切换账号，不受影响 1234[root@centos8 pam.d]#grep pam_nologin *login:account required pam_nologin.soremote:account required pam_nologin.sosshd:account required pam_nologin.so 范例 123456789101112131415161718#创建/etc/nologin文件，普通用户立即无法远程登录[root@ubuntu ~]# touch /etc/nologin[root@ubuntu ~]# ll /etc/nologin-rw-r--r-- 1 root root 0 May 28 22:01 /etc/nologin#普通用户远程登录的拒绝日志，可以在 /var/log/secure 文件中查询，明确说明是pam account 配置拒绝登录[root@ubuntu ~]# tail -2 /var/log/secureMay 28 22:03:15 ubuntu sshd[25125]: Failed password for mage from 10.0.0.1 port52961 ssh2May 28 22:03:15 ubuntu sshd[25125]: fatal: Access denied for user mage by PAM account configuration [preauth]#写入内容到文件[root@ubuntu ~]# echo &quot;pam deny user login&quot; &gt; /etc/nologin#普通用户远程登录，被拒绝时能看到 &quot;pam deny user login&quot; 的提示C:\\Users\\44301&gt;ssh mage@10.0.0.206mage@10.0.0.206&#x27;s password:pam deny user loginConnection closed by 10.0.0.206 port 22 7.2 pam_limits.so模块功能：在用户级别实现对其可使用的资源的限制，例如：可打开的文件数量，可运行的进程数量，可用内存空间 查看有哪些服务使用了该模块 123456789[root@ubuntu ~]# grep pam_limits /etc/pam.d/*/etc/pam.d/atd:session required pam_limits.so/etc/pam.d/cron:session required pam_limits.so/etc/pam.d/login:session required pam_limits.so/etc/pam.d/runuser:session required pam_limits.so/etc/pam.d/sshd:session required pam_limits.so/etc/pam.d/su:session required pam_limits.so/etc/pam.d/sudo:session required pam_limits.so/etc/pam.d/sudo-i:session required pam_limits.so 修改限制的实现方式： （1）ulimit命令 用于对shell进程及其子进程进行资源限制，使用ulimit进行修改，立即生效，limit只影响shell进程及其子进程，用户登出后失效 可以在profile中加入ulimit的设置，变相的做到永久生效 ulimit的设定值是 per-process 的，也就是说，每个进程有自己的limits值 使用ulimit进行修改，立即生效 12345678910111213141516171819-H 设置硬件资源限制.-S 设置软件资源限制.-a 显示当前所有的资源限制.-c size:设置core文件的最大值.单位:blocks-d size:设置数据段的最大值.单位:kbytes-f size:设置创建文件的最大值.单位:blocks-l size:设置在内存中锁定进程的最大值.单位:kbytes-m size:设置可以使用的常驻内存的最大值.单位:kbytes-n size:设置内核可以同时打开的文件描述符的最大值.单位:n，最大是1048576-p size:设置管道缓冲区的最大值.单位:kbytes-s size:设置堆栈的最大值.单位:kbytes-t size:设置CPU使用时间的最大上限.单位:seconds-u size:最大用户进程数-v size:设置虚拟内存的最大值.单位:kbytesunlimited 是一个特殊值，用于表示不限制#说明查询时，若不加H或S参数，默认显示的是软限制修改时，若不加H或S参数，两个参数一起改变 案例：查看各种默认资源限制 1[root@centos8 ~]#ulimit -a 案例： 查看指定进程的资源限制 12#cat /proc/PID/limits[root@wang-liyun-pc ~]# cat /proc/`pidof nginx | xargs -n1 | sort -n|head -1`/limits 案例：ulimit 命令修改用户可同时打开的最大文件个数 123456789[root@centos8 ~]#ulimit -n1024[root@centos8 ~]#ulimit -n 1048577-bash: ulimit: open files: cannot modify limit: Operation not permitted[root@centos8 ~]#ulimit -n 1048576[root@centos8 ~]#ulimit -a#测试[root@ubuntu ~]# ab -c 1100 -n 10000 http://www.magedu.com/ (2) 使用 pam_limits 模块来配置相关参数 配置文件：pam_limits的设定值是基于 per-process 的 12/etc/security/limits.conf #需要重启机器才能生效/etc/security/limits.d/*.conf #无需重启系统，退出当前终端重新进入即可生效 配置文件格式： 123#每行一个定义&lt;domain&gt; &lt;type&gt; &lt;item&gt; &lt;value&gt;应用对象 限制类型 限制的资源 指定具体值 格式说明： 应用于哪些对象 1234Username #单个用户@group #组内所有用户* #所有用户% #仅用于限制 maxlogins limit , 可以使用 %group 语法. 只用 % 相当于 * ，表示对所有用户的maxsyslogins limit限制. %group 表示限制此组中的所有用户总的最大登录数 限制的类型 123Soft #软限制,普通用户自己可以修改Hard #硬限制,由root用户设定，且通过kernel强制生效- #二者同时限定 软限制（soft limit）： 这是用户可以自行调整的最大限制值（只要不超过硬限制）。当软限制被达到时，进程或用户可能会收到警告消息，但允许继续使用资源，除非资源的使用量达到了硬限制。 硬限制（hard limit）： 这是由管理员设定的最大限制值，普通用户不能更改硬限制，超过硬限制后，系统将采取强制措施，通常是拒绝进一步的资源分配或者终止进程，防止用户过度消耗系统资源。 限制的资源 123456789101112131415161718nofile #所能够同时打开的最大文件数量,默认为1024nproc #所能够同时运行的进程的最大数量,默认为1024core #控制进程生成核心转储文件的大小限制。data #控制进程数据段（data segment）的大小限制。fsize #控制进程可以创建的文件的最大大小限制。memlock #控制进程可以锁定在内存中的物理内存量的限制。rss #控制进程的常驻集大小（resident set size）的限制。stack #控制进程堆栈大小的限制。cpu #控制进程可以使用的 CPU 时间量的限制。as #控制进程的地址空间大小的限制。maxlogins #控制用户可以同时拥有的最大登录会话数量的限制。maxsyslogins #控制系统可以同时拥有的最大登录会话数量的限制。priority #控制进程的优先级限制。locks #控制进程可以锁定的文件数量的限制。sigpending #控制进程可以排队等待处理的挂起信号的数量限制。msgqueue #控制进程可以拥有的 POSIX 消息队列的最大数量限制。nice #控制进程可以调整的优先级范围的限制。rtprio #控制进程可以设置的实时优先级的范围限制。 案例：限制用户最多打开的文件数和运行进程数，并持久保存 1234567891011121314151617181920cat /etc/pam.d/system-authsession required pam_limits.sovim /etc/security/limits.conf #用户apache可打开10240个文件apache - nofile 10240#用户student不能运行超过20个进程student hard nproc 10#用student登录多次运行bash，观察结果[root@centos8 ~]#vim /etc/security/limits.confwang - nofile 66666wang - nproc 5mage - nofile 88888[root@centos8 ~]#su - wangLast login: Mon May 25 14:40:38 CST 2020 on pts/0[wang@centos8 ~]$ulimit -n66666 案例：限制mage用户最大的同时登录次数 1234567891011121314[root@centos8 ~]#tail -n1 /etc/security/limits.confmage - maxlogins 2[root@centos8 ~]#whomage tty1 2021-04-28 09:27root pts/0 2021-04-28 08:50 (10.0.0.1)root pts/1 2021-04-28 09:09 (10.0.0.1)root pts/2 2021-04-28 09:19 (10.0.0.1)mage tty3 2021-04-28 09:27[root@centos8 ~]#tail /var/log/secure -fApr 28 09:28:06 centos8 login[23278]: pam_limits(login:session): Too many logins (max 2) for mageApr 28 09:28:06 centos8 login[23278]: pam_unix(login:session): session opened for user mage by LOGIN(uid=0)Apr 28 09:28:06 centos8 login[23278]: Permission denied 生产环境下常用的调优选项 123456vim /etc/security/limits.conf * - core unlimited* - nproc 1000000* - nofile 1000000* - memlock 32000* - msgqueue 8192000 注意：systemd 的service 资源设置需要单独配置 在Centos7以上版本中，使用Systemd替代了之前的SysV。&#x2F;etc&#x2F;security&#x2F;limits.conf文件的配置作用域缩小了，其作用范围是针对普通用户和用户组，而不是针对系统服务或进程。 /etc/security/limits.conf的配置，只适用于通过PAM认证登录用户的资源限制，它对systemd的service的资源限制不生效。因此登录用户的限制，通过/etc/security/limits.conf与/etc/security/limits.d下的文件设置即可 对于systemd service的资源设置，则需修改全局配置，全局配置文件放在/etc/systemd/system.conf和/etc/systemd/user.conf，同时也会加载两个对应目录中的所有.conf文件/etc/systemd/system.conf.d/*.conf和/etc/systemd/user.conf.d/*.conf。 system.conf是系统实例使用的，user.conf是用户实例使用的。 123456789vim /etc/systemd/system.confDefaultLimitNOFILE=100000DefaultLimitNPROC=65535#或者针对指定的service添加下面行[root@ubuntu ~]## vim /usr/lib/systemd/system/nginx.service[Service]LimitNOFILE=100000 #设置服务进程可以打开的文件描述符数量限制LimitNPROC=65535 #设置服务进程可以创建的子进程数量限制","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"加密和安全","slug":"加密和安全","permalink":"https://aquapluto.github.io/tags/%E5%8A%A0%E5%AF%86%E5%92%8C%E5%AE%89%E5%85%A8/"}]},{"title":"sudo实现授权","slug":"Linux/encryption-security/sudo","date":"2025-08-21T03:03:05.000Z","updated":"2025-09-07T06:13:26.040Z","comments":true,"path":"Linux/encryption-security/sudo/","permalink":"https://aquapluto.github.io/Linux/encryption-security/sudo/","excerpt":"","text":"1 sudo介绍允许系统管理员让普通用户执行一些或者全部的root命令的一个工具，如halt，reboot，su等等。这样不仅减少了root用户的登录 和管理时间，同样也提高了安全性 一般用户管理系统的方式是利用su切换为超级用户。但是使用su的缺点之一在于必须要先告知超级用户的密码。sudo使一般用户不需要知道超级用户的密码即可获得权限。首先超级用户将普通用户的名字、可以执行的特定命令、按照哪种用户或用户组的身份执行等信息，登记在特殊的文件中（通常是&#x2F;etc&#x2F;sudoers），即完成对该用户的授权（此时该用户称为“sudoer”）；在一般用户需要取得特殊权限时，其可在命令前加上“sudo”，此时sudo将会询问该用户自己的密码（以确认终端机前的是该用户本人），回答后系统即会将该命令的进程以超级用户的权限运行。之后的一段时间内（默认为5分钟，可在&#x2F;etc&#x2F;sudoers自定义），使用sudo不需要再次输入密码 由于不需要超级用户的密码，部分Unix系统甚至利用sudo使一般用户取代超级用户作为管理帐号 sudo特性 sudo能够授权指定用户在指定主机上运行某些命令。如果未授权用户尝试使用 sudo，会提示联系管理员 sudo提供了丰富的日志，详细地记录了每个用户干了什么。它能够将日志传到中心主机或者日志服务器 sudo使用时间戳文件来执行类似的“检票”系统。当用户调用sudo并且输入它的密码时，用户获得了一张存活期为5分钟的票 sudo的配置文件是sudoers文件，它允许系统管理员集中的管理用户的使用权限和使用的主机。它所存放的位置默认是在&#x2F;etc&#x2F;sudoers，属性必须为0440 2 sudo组成包：sudo 主配置文件： 1/etc/sudo.conf 授权规则配置文件： 12/etc/sudoers #重点/etc/sudoers.d 工具命令 12345678#安全编辑授权规则文件和语法检查工具/usr/sbin/visudo#授权编辑规则文件的工具/usr/bin/sudoedit#执行授权命令/usr/bin/sudo 范例：由于&#x2F;etc&#x2F;sudoers不建议用vim直接改，所以用下面方式来修改文件 123456#检查语法visudo -c#检查指定配置文件语法visudo -f /etc/sudoers.d/test#visudo缺点是没有带颜色 范例：修改visudo的默认编辑器 12root@ubuntu1804:~# export EDITOR=vimroot@ubuntu1804:~# visudo 授权编辑规则文件的工具： 1/usr/bin/sudoedit 时间戳文件： 1/var/db/sudo 日志文件： 1/var/log/secure 3 sudo命令切换身份功能和 su 相似，但不一样，sudo必须提前授权，而且要输入自已的密码 12345678910sudo [-u user] COMMAND-V #显示版本信息等配置信息-u user #指定代表的用户，默认为root-l,ll #列出用户在主机上可用的和被禁止的命令-v #再延长密码有效期限5分钟,更新时间戳-k #清除时间戳（1970-01-01），下次需要重新输密码-K #与-k类似，还要删除时间戳文件-b #在后台执行指令-p #改变询问密码的提示符号，示例：-p &quot;password on %h for user %p: &quot; 4 sudo 授权规则配置配置文件：/etc/sudoers，/etc/sudoers.d/ 配置文件中支持使用通配符 glob 123456? #任意单一字符* #匹配任意长度字符[wxc] #匹配其中一个字符[!wxc] #除了这三个字符的其它字符\\x #转义[[alpha]] #字母 配置文件规则有两类 1、别名定义：不是必须的 2、授权规则：必须的 sudoers 授权规则格式： 12345678910用户 登入主机=(代表用户) 命令user host=(runas) command #授权user用户可以在host主机上以runas的身份执行commanduser: 运行命令者的身份host: 登入哪些主机(runas)：以哪个用户的身份command: 运行哪些命令，要带路径#范例root ALL=(ALL) ALL sudoers的别名 123456789101112131415User和runas: username #uid %group_name %#gid user_alias|runas_aliashost: ip或hostname #ip地址,主机名 network(/netmask) #网段/子网掩码 host_alias #别名command: command name directory/* #某个目录下所有命令 sudoedit #授权其可以修改sudo文件 Cmnd_Alias #命令别名 sudo别名有四种类型： User_Alias Runas_Alias Host_Alias Cmnd_Alias 别名格式： 12#由大写字母开始，后面接大写字母，数字，下划线[A-Z]([A-Z][0-9]_)* 别名定义： 1Alias_Type NAME1 = item1,item2,item3 : NAME2 = item4, item5 配置范例 1234#指定IP或网段的写法，要求在对应主机上也有该配置jose 10.0.0.158=(root) /bin/ls /root/jose 10.0.0.0/24=(root) /bin/touch /root/from-josejose 10.0.0.157=(root) /sbin/shutdown -h now 范例：授权wu身份去运行mount和umount命令 123456789101112[root@centos7 ~]#su wu[wu@centos7 root]$mount /dev/cdrom /mnt/mount: only root can do that[root@centos7 ~]#visudowu ALL=(root) /usr/bin/mount /dev/cdrom /mnt/,/usr/bin/umount /mnt/[wu@centos7 root]$sudo mount /dev/cdrom /mnt/[sudo] password for wu: mount: /dev/sr0 is write-protected, mounting read-only[wu@centos7 root]$sudo umount /mnt/ 5 实战案例案例：授权写法 1234567Student ALL=(ALL) ALL%wheel ALL=(ALL) ALLstudent ALL=(root) /sbin/pidof,/sbin/ifconfig%wheel ALL=(ALL) NOPASSWD: ALL #NOPASSWD可以让授权账号不需要输密码执行操作wang 192.168.1.6,192.168.1.8=(root) /usr/sbin/,!/usr/sbin/useradd 案例：别名授权 12345678#授权netuser1和netuser2可以执行ip和ifconfig命令User_Alias NETADMIN= netuser1,netuser2Cmnd_Alias NETCMD = /usr/sbin/ip,/usr/sbin/ifconfigNETADMIN ALL=（root） NETCMD#授权adminuser1和adminuser2可以执行useradd,usermod,passwd命令，但是passwd命令只可以修改别人密码，不可以修改root密码User_Alias ADMINUSER = adminuser1,adminuser2Cmnd_Alias ADMINCMD = /usr/sbin/useradd，/usr/sbin/usermod, /usr/bin/passwd [a-zA-Z]*, !/usr/bin/passwd rootADMINUSER ALL=(root) NOPASSWD:ADMINCMD，PASSWD:/usr/sbin/userdel 案例：默认指定授权用户 12345Defaults:wang runas_default=tom #指定了用户 wang 在使用 sudo 执行命令时，默认以 tom 用户的身份来执行wang ALL=(tom,jerry) ALLwang$ sudo cmd #默认以tom的身份执行cmdwang$ sudo -u jerry cmd #以 jerry 用户的身份来执行命令 cmd 案例：解决通配符带来的安全风险 1234#这样子写，可以让wang访问/var/log/messages.1，也可以访问/var/log/messages /etc/shadow两文件wang ALL=(ALL) /bin/cat /var/log/messages*#解决方法wang ALL=(ALL) /bin/cat /var/log/messages*,!/bin/cat /var/log/messages* * 案例：授权可以放在新建在sudoers.d目录下的文件（生产中配置如果比较复杂，此方法可以让每一个项目写一个单独的文件，互相不干扰） 123456[root@centos8 ~]#vim /etc/sudoers.d/testWang ALL=(ALL) sudoedit#wang 可以执行下面命令[root@centos8 ~]#sudoedit /etc/sudoers[root@centos8 ~]#sudoedit /etc/sudoers.d/test 案例：修改验证密码间隔为2分钟 1234[root@centos8 ~]#vim /etc/sudoersDefaults env_reset , timestamp_timeout=2[root@centos8 ~]#sudo -V 范例：ubuntu 默认用户具有sudo权限 1234567root@ubuntu1804:~# grep %sudo /etc/sudoers%sudo ALL=(ALL:ALL) ALLroot@ubuntu1804:~# id wanguid=1000(wang) gid=1000(wang)groups=1000(wang),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),108(lxd),113(lpadmin),114(sambashare)#默认的用户wang 属于此sudo组，所以wang有所有权限 范例：修改sudo 提示符格式 1234[wang@centos8 ~]$sudo cat /var/log/messages[sudo] password for wang:[wang@centos8 ~]$sudo -p &quot;password on %h for user %p: &quot; cat /var/log/messagespassword on centos8 for user wang: 范例：删除时间戳文件 12345678910[root@centos8 ~]#su - wangLast login: Mon May 25 10:28:14 CST 2020 on pts/1[wang@centos8 ~]$sudo -K[wang@centos8 ~]$exitlogout[root@centos8 ~]#ll /run/sudo/tstotal 4-rw------- 1 root mage 112 May 25 10:11 mage[root@centos8 ~]#file /run/sudo/ts/mage/run/sudo/ts/mage: data 案例：禁止git用户执行wget命令 1234[root@centos8 ~]#vim /etc/sudoers.d/testgit ALL=(ALL) /usr/bin/wget,!/usr/bin/wget[root@centos8 ~]#alias wget=&#x27;echo &quot;wget is disabled for this user&quot;&#x27;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"加密和安全","slug":"加密和安全","permalink":"https://aquapluto.github.io/tags/%E5%8A%A0%E5%AF%86%E5%92%8C%E5%AE%89%E5%85%A8/"}]},{"title":"OpenSSL","slug":"Linux/encryption-security/OpenSSL","date":"2025-08-21T03:02:59.000Z","updated":"2025-09-07T06:10:04.683Z","comments":true,"path":"Linux/encryption-security/OpenSSL/","permalink":"https://aquapluto.github.io/Linux/encryption-security/OpenSSL/","excerpt":"","text":"1 Base64 编码Base64就是一种基于64个可打印字符来表示二进制数据的方法（不是加密，只是系统数据由base64的方式保存） 1234[root@centos8 ~]#echo -n ab | base64YWI=[root@centos8 ~]#echo -n ab | base64 | base64 -dab[root@centos8 ~]# 2 openssl 命令OpenSSL是一个开放源代码的软件库包，应用程序可以使用这个包来进行安全通信，避免窃听，同时确认另一端连线者的身份。这个包广泛被应用在互联网的网页服务器上，实现了基本的加密功能，实现了SSL与TLS协议 包括三个组件： libcrypto：用于实现加密和解密的库 libssl：用于实现ssl通信协议的安全库 openssl：多用途命令行工具 openssl两种运行模式： 交互模式 批处理模式 openssl三种子命令： 标准命令 消息摘要命令（配合不同算法） 加密命令（配合不同算法） 范例: openssl的交互和非交互式查看版本 12345678910111213root@ubuntu2004:~# openssl versionOpenSSL 1.1.1f 31 Mar 2020[root@centos8 ~]#openssl versionOpenSSL 1.1.1c FIPS 28 May 2019[root@ubuntu1804 ~]#opensslOpenSSL&gt; versionOpenSSL 1.1.1 11 Sep 2018[root@centos7 ~]#opensslOpenSSL&gt; versionOpenSSL 1.0.2k-fips 26 Jan 2017 分类列出所有命令与算法 1234567891011121314#列出所有标准命令[root@ubuntu ~]# openssl list --commands#列出所有摘要命令[root@ubuntu ~]# openssl list -digest-commands#列出所有摘要算法[root@ubuntu ~]# openssl list -digest-algorithms#列出所有加密命令[root@ubuntu ~]# openssl list -cipher-commands#列出所有加密算法[root@ubuntu ~]# openssl list -cipher-algorithms 2.1 单向哈希加密工具：openssl dgst 算法：md5sum, sha1sum, sha224sum,sha256sum… 12345[root@centos8 data]#openssl md5 fstabMD5(fstab)= 8f8e3b0d0c17f1b29d404544c7b310da[root@centos8 data]#openssl sha512 fstabSHA512(fstab)=81f67107026a43bf60fff2cdd6ebe93f49ad3bf48e3645912aa0e8d27eec8d9647121f608c7b6ad194856318f0381db21f6961db862e99644126b64c38a5eeb6 补充知识 12MAC: Message Authentication Code，单向加密的一种延伸应用，用于实现网络通信中保证所传输数据的完整性机制HMAC：hash-based MAC，使用哈希算法 2.2 生成用户密码1234567891011121314151617openssl passwd [options...] STRING-help #获取帮助信息-in infile #从文件读取要加密的内容-noverify #从标准输入接收密码时，不用输两次-quiet #不输出告警信息-table #以表格形式输出-salt val #手动指定盐值，默认每次自动随机生成-stdin #从标准输入接收要加密的内容-6 #使用SHA512 算法加密-5 #使用SHA256 算法加密-apr1 #使用 apache 特有的MD5算法加密-1 #使用MD5 加密算法-aixmd5 #使用AIX MD5 加密算法-rand val #将文件加到随机数生成器-writerand outfile #将此过程中产生的随机数写到指定文件-crypt #标准unix密码加密算法，旧版中默认项 范例：从标准输入读取 12[root@ubuntu ~]# echo 123456 | openssl passwd -stdin$1$2b626GRH$mY5SAZYhOMadVjJyY52.n. 两个$之间的是盐，加盐的目的就是为了让你不能根据这个加密的结果来判断俩密码一样不一样，就算是相同的密码，盐不一样，加密后的结果会不同，只有盐一样，加密的结果才会相同 指定盐值 123456#-1和$1相对应[root@ubuntu ~]# openssl passwd -1 -salt abcd1234 123456$1$abcd1234$flW8OGJjRMMEgtyb4lbLN0[root@ubuntu ~]# openssl passwd -1 -salt abcd1234 123456$1$abcd1234$flW8OGJjRMMEgtyb4lbLN0 &#x2F;etc&#x2F;shadow 文件中的密码字段说明 123456[root@Rocky8 ~]#getent shadow wuwu:$6$8fvMk.Ge4CHdKb0l$a/MiQroCDB7sHYYGqnpQUk6xzNiohkdQAfXGBHBlu7zk7qeMT4AcJ8xyQhnsFPM6IhEgXZLRlqcMrTxlwGikB0::0:99999:7:::$6：使用SHA512 算法加密8fvMk.Ge4CHdKb0l：盐值$a/MiQroCDB7sHYYGqnpQUk6xzNiohkdQAfXGBHBlu7zk7qeMT4AcJ8xyQhnsFPM6IhEgXZLRlqcMrTxlwGikB：真正的加密后密文 相同加密算法，相同内容相同盐值的情况下，加密后的内容也相同 12345[root@Rocky8 ~]#getent shadow wuwu:$6$8fvMk.Ge4CHdKb0l$a/MiQroCDB7sHYYGqnpQUk6xzNiohkdQAfXGBHBlu7zk7qeMT4AcJ8xyQhnsFPM6IhEgXZLRlqcMrTxlwGikB0::0:99999:7:::[root@Rocky8 ~]#openssl passwd -6 -salt &quot;8fvMk.Ge4CHdKb0l&quot; zjwjl2004$6$8fvMk.Ge4CHdKb0l$a/MiQroCDB7sHYYGqnpQUk6xzNiohkdQAfXGBHBlu7zk7qeMT4AcJ8xyQhnsFPM6IhEgXZLRlqcMrTxlwGikB0 不指定盐值，每次都会随机生成 12345[root@Rocky8 ~]#openssl passwd -6 zjwjl2004$6$t9hIpHGtEOs8mjHs$XV5/TTsbdxxyMCnbo5NpxSu6MoQMr1MZx.XKbv.ckqQXv1HImi/H7W7g00yXrGZH.QdjjeaFGVKldAxz8cVNr/[root@rocky8 ~]#openssl passwd -6 zjwjl2004$6$nhk.wwXsRt1JviHf$TeS9MFeW6VzZR0SICF0fqca140pcgVXGjNz2uH/sU6Sm6KdjrM9E079fPi/YNc9S7CyiicPrshIBNoyrjQ7TY. 范例: 利用Python程序在CentOS7 生成sha512加密密码 1234[root@centos7 ~]#python -c &#x27;importcrypt,getpass;pw=&quot;magedu&quot;;print(crypt.crypt(pw))&#x27;$6$pt0SFMf6YqKea3mh$.7Hkslg17uI.Wu7BcMJStVVtkzrwktXrOC8DxcMFC4JO1igrqR7VAi87H5PHOuLTUEjl7eJqKUhMT1e9ixojn1 范例：创建新用户同时指定密码，在CentOS8和Ubuntu都通用 123456789101112[root@centos8 ~]#useradd -p `echo magedu | openssl passwd -6 -saltY16DiwuVQtL6XCQK -stdin` zhang[root@centos8 ~]#getent shadow zhangzhang:$6$Y16DiwuVQtL6XCQK$DAQO4BhVbfQmaUMFWKR61hVwFvxk7J9U4pZaFcwf6nBwERUN6bL3wALPonDRebk3CgooupeXHfRuFKRciUe6q.:18402:0:99999:7:::[root@centos8 ~]#getent shadow zhang wangzhang:$6$Y16DiwuVQtL6XCQK$DAQO4BhVbfQmaUMFWKR61hVwFvxk7J9U4pZaFcwf6nBwERUN6bL3wALPonDRebk3CgooupeXHfRuFKRciUe6q.:18402:0:99999:7:::wang:$6$Y16DiwuVQtL6XCQK$DAQO4BhVbfQmaUMFWKR61hVwFvxk7J9U4pZaFcwf6nBwERUN6bL3wALPonDRebk3CgooupeXHfRuFKRciUe6q.:18373:0:99999:7::: 2.3 生成随机数随机数生成器：伪随机数字，利用键盘和鼠标，块设备中断生成随机数 12345678910/dev/random #仅从熵池返回随机数；随机数用尽，阻塞/dev/urandom #从熵池返回随机数；随机数用尽，会利用软件生成伪随机数，非阻塞openssl rand [options...] NUMNUM #字符个数-out outfile #输出到指定文件-rand val #将文件加到随机数生成器-writerand outfile #将此过程中产生的随机数写到指定文件-base64 #base64编码后显示-hex #16进制显示，每个字符为十六进制，相当于4位二进制，出现的字符数为NUM*2，一个字节占8位 范例：生成随机10位长度密码 123456789[root@centos8 ~]#openssl rand -base64 9 |head -c10 #9*8=72,72/6=12位；除不尽时用=代替（不是3的整数倍）ip97t6qQes[root@centos8 ~]#[root@centos8 ~]#tr -dc &#x27;[:alnum:]&#x27; &lt; /dev/urandom |head -c10DO2mDp3eZu[root@centos8 ~]##16进制显示，一个字节可表示8个二进制，一个16进制可表示4个二进制，所以一个字节可以表示2个16进制[root@ubuntu ~]# openssl rand -hex 1015e0b3882859424f6d4f 2.4 实现密钥对PKI生成私钥，再从私钥中提取公钥 1234567891011#私钥文件-----BEGIN RSA PRIVATE KEY----------END RSA PRIVATE KEY-----#公钥文件-----BEGIN PUBLIC KEY----------END PUBLIC KEY-----#RSA公钥文件-----BEGIN RSA PUBLIC KEY----------END RSA PUBLIC KEY----- 格式 123456789101112131415161718192021222324252627282930313233#生成私钥，私钥包含公钥信息openssl genrsa [options...] [NUM]NUM #指定密钥长度,单位bit,默认2048-help #获取帮助信息-out outfile #输出到指定文件-rand val #以文件作随机数种子-writerand outfile #将此过程中产生的随机数写到指定文件-passout val #输出文件的保护口令-* #加密算法#从私钥提取公钥openssl rsa [options...]-help #获取帮助信息 -inform format #显示指定输入文件格式DEM|PEM,默认PEM-outform format #指定输出文件模式DER|PEM，默认PEM-in val #指定输入的文件，通常是私钥-out outfile #指定要输出的文件,不指定就是标准输出-pubin #从输入文件中读取公钥值，默认是读取私钥值-pubout #指定导出公钥，默认输出私钥-passout val #输出文件的保护口令-passin val #输入文件的保护口令-RSAPublicKey_in #输入文件格式为RSAPublicKey-RSAPublicKey_out #输出文件格式为RSAPublicKey-noout #不输出任何内容-text #输出所有信息-modulus #输出公钥信息-check #检查公钥是否匹配-* #指定私钥的保护加密算法#加密算法aes128|aes192|aes256|aria128|aria192|aria256|camellia128|camellia192|camellia256|des|des3|idea 公钥加密： 算法：RSA, ELGamal 工具：gpg, openssl rsautl（man rsautl） 数字签名： 算法：RSA, DSA, ELGamal 密钥交换： 算法：dh，DSA，DSS，RSA openssl命令生成密钥对儿：man genrsa 生成私钥 1openssl genrsa -out /PATH/TO/PRIVATEKEY.FILE [-aes128] [-aes192] [-aes256] [-des3] [NUM_BITS,默认2048] 解密加密的私钥 1openssl rsa -in /PATH/TO/PRIVATEKEY.FILE -out /PATH/TO/PRIVATEKEY2.FILE 范例 12345678910#生成私钥及其存放文件[root@ubuntu ~]# openssl genrsa -out test.key#从指定私钥提取出公钥[root@ubuntu ~]# openssl rsa -in test.key -pubout -out test.pubkeywriting RSA key[root@ubuntu ~]# ls -al test*-rw------- 1 root root 1704 May 21 16:03 test.key-rw-r--r-- 1 root root 451 May 21 16:07 test.pubkey 范例：生成私钥时加密 1234567891011121314151617#指定加密算法，指定口令[root@ubuntu ~]# openssl genrsa -out test2.key -des3 -passout pass:&quot;123456&quot;#解密加密的私钥[root@ubuntu ~]# openssl rsa -in test2.key -out test2.key2Enter pass phrase for test2.key:writing RSA key#提取公钥要求输入密码[root@ubuntu ~]# openssl rsa -in test2.key -pubout -out test2.pubkeyEnter pass phrase for test2.key:writing RSA key[root@ubuntu ~]# ll test2*-rw------- 1 root root 1854 May 21 16:09 test2.key-rw------- 1 root root 1704 May 21 16:09 test2.key2-rw-r--r-- 1 root root 451 May 21 16:12 test2.pubkey 范例: 密钥文件要保证权限，生成的私钥设置权限保证安装 1234567#对私钥通过设置严格的权限实现安全，应用更广泛[root@centos8 ~]#(umask 077; openssl genrsa -out /data/app1.key 2048)[root@centos8 ~]#cat /data/app1.key#用加密对称密钥加密私钥,此方式更安全，但是不方便[root@centos8 ~]#openssl genrsa -out /data/app2.key -des3 2048[root@centos8 ~]#cat /data/app2.key 从私钥中提取出公钥 1234openssl rsa -in PRIVATEKEYFILE -pubout -out PUBLICKEYFILE#范例openssl rsa -in test.key -pubout -out test.pubkey 范例 1234567891011121314151617[root@centos7 ~]#(umask 066;openssl genrsa -out /data/app.key)Generating RSA private key, 2048 bit long modulus........................+++.+++e is 65537 (0x10001)[root@centos7 ~]#ls -l /data/total 4-rw------- 1 root root 1679 Feb 3 15:26 app.key[root@centos8 ~]#openssl genrsa -out /data/app.key 1024Generating RSA private key, 1024 bit long modulus (2 primes).............................................................................+++++..........+++++e is 65537 (0x010001)[root@centos8 ~]#ll /data/app.key-rw------- 1 root root 891 Feb 3 14:52 /data/app.key 范例：利用私钥提取公钥 1234567891011121314151617[root@centos8 ~]#(umask 066;openssl genrsa -out /data/app.key)[root@centos8 ~]#openssl rsa -in /data/app.key -pubout -out /data/app.key.pubwriting RSA key[root@centos8 ~]#ls -l /data/total 8-rw------- 1 root root 887 Feb 3 15:28 app.key-rw-r--r-- 1 root root 272 Feb 3 15:32 app.key.pub[root@centos8 ~]#cat /data/app.key.pub-----BEGIN PUBLIC KEY-----MIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCvkS+Z4NWAMoXEwNUyn58J0oI+ZjXotZUJLfbVHvGd3Ug6Rk52imHp1J629edUn0Cw7KoPfQLegmWsldG4v931HCdlELT2vj+QE7KJhc1tGFomzCnX8Q41tRrVVbHPxQYvNmMRXRqIdqXGxFpR758EngxFzAGcnLTrDz/I2GocrQIDAQAB-----END PUBLIC KEY----- 范例：利用加密的私钥提取公钥 12345678910111213141516[root@centos8 ~]#openssl genrsa -out /data/app2.key -des3 2048[root@centos8 ~]#openssl rsa -in /data/app2.key -pubout -out /data/app2.pubkeyEnter pass phrase for /data/app2.key:writing RSA key[root@centos8 ~]#cat /data/app2.pubkey-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2VXLcmClBWiqL8u3f1vOmjCEV+9c6S0qDXNiZrCRiYUyIMKLhyXnVLw+k6uGmC4bdATFgxDU2zjdJF3bptS6dNZzMQJ5uAQOxQ1KHKm3O+s+Isg+H/LTHUDyc4szQZ3gjJCTKculS60qsWV7lcGPPNSzXr3/F/TlLMRxv/9GrEjYXDgCAJt2lxWgvgXqX8Y1mc1FFkBRXVZr/CnXaij5JIA89/OHIJoX+mQIuQEjmwFMCX/6cm64iks2obgmzluvm6fM6dkvlHDGpZicNZI15vaQcO7sJ4YTUGwJrDShC9R++vrAvfahvTDV3n/MLmfwS+8nhUA0Dr7M7I0GOYMpEQIDAQAB-----END PUBLIC KEY----- 范例：生成加密的私钥，并解密 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@centos8 ~]#openssl genrsa -out /data/app.key -des3 1024Generating RSA private key, 1024 bit long modulus (2 primes)......+++++...........+++++e is 65537 (0x010001)Enter pass phrase for /data/app.key:Verifying - Enter pass phrase for /data/app.key:[root@centos8 ~]#ls -l /datatotal 4-rw------- 1 root root 963 Feb 3 15:27 app.key[root@centos8 ~]#cat /data/app.key-----BEGIN RSA PRIVATE KEY-----Proc-Type: 4,ENCRYPTEDDEK-Info: DES-EDE3-CBC,577C3B861BAD86B6VM8P7vx1UUcSJyXCB0pDO9xgmdNgsMOcl6NitdUvBA9Jx2oLyxsT6TYbbvZvlF55aQB0bq43atECDBz2+v1ghacPp78S2wuGuTR1hdWwfFKJNr6d/5yXO4y1ZOt3RLvRE4K6TCeSwZTIUNeQyuh+vstarQmaLQmdObb3lsMG+WipQj3hb0oGdZcWjuQ0gi1BRKN1duhsWFQbdXZamBqWQqCbvigmqRwjk7S6GE3YwVhys1T4N0BFX/edNCMnzb796/mR+LJ2Wz/ecJXB5250rVby3h88ZNsgARg7jUM9zI6jf7G4t1etRlCJ8A9TvDe8J/5lkDUSWEh1dnB+xw5uamDY7f3GanuKTEe54DxuBwmbBpphV1QTTefSJ01Q6l9KwS0zV6WE+vCt99dE9J8+GXGD77twRcbmjDWfaoibvwMu00crB9K5dbxdSX50jlD9Mj+bVr9tcwQW/WzA+V05Ndb74e8OE97pEFjTX8DeIxcZomDUcpNGpQ0eWvyE+A2xSrux9nN8z9dUF963V4NjQGUg1owQPAlfO6zBGObXnynOqKDmBj+8FfWrnHnZUVt53HTV+uSkLuA+8lGoNoxH4/6ZLfvY0Y5+WSg3st2EvwGT74SNNrsNYD0qGt1LujQxIiwfCI0uv8rqgtLtsYmJmYI0t7hWUVmb6QgX1Qh0Kvzc0A34IMDjY6dhXTKnxeF3LGkrFAgl3+6tKXxMuQDLB6Jy9m3SOwW/JoXMVVcYHrSPzTgAl2sgAkgEq8nf4yfmZP9WHrDe10yXY+5K2h8UiFhvrnQ+YnH4BcTrKuEa9T7pxToo0cTdqg==-----END RSA PRIVATE KEY-----[root@centos8 ~]#openssl rsa -in /data/app.key -out /data/app.keyEnter pass phrase for /data/app.key:writing RSA key[root@centos8 ~]#ls -l /datatotal 4-rw------- 1 root root 887 Feb 3 15:28 app.key[root@centos8 ~]#cat /data/app.key-----BEGIN RSA PRIVATE KEY-----MIICXQIBAAKBgQCvkS+Z4NWAMoXEwNUyn58J0oI+ZjXotZUJLfbVHvGd3Ug6Rk52imHp1J629edUn0Cw7KoPfQLegmWsldG4v931HCdlELT2vj+QE7KJhc1tGFomzCnX8Q41tRrVVbHPxQYvNmMRXRqIdqXGxFpR758EngxFzAGcnLTrDz/I2GocrQIDAQABAoGAQ/uDJCGkanSlya8lnumiKqqB1mm7nDWb1ScgOhw2UPubeT06Krqg+WtkXdJQVjsoUJoDq+WrU7/IYRDOWayp5Be3EXCdyldSrWu1+wqJ1Vnpk2oUAEyr+lzcHhW1FNQ/5rb8kIUjR7DZpwnsYJxDygnaKaNKiUiF2FsMX8JcS8ECQQDoZt3zSsXYeR4tY9kPPA19npQXx9K4Wv2wsCR904pznzoaJ9Kj+6E/3AdxtXcTD0GiZe8vW+H6WCmWgB1NpGiRAkEAwWTwO9ZncQnA+X2PYTkizBp/JdEdRjcL/D2g+g3rpL2nLChI56C5zA4NsJFmblE2uY1OLIJBGExiZP/XS74gXQJBAISTOgYyH48P+OEX1plUPrXsorq2KUU10wbaVNbauF6g9Lo7AXS+dQxC7pQ1Wsoqp9yGnd28Yrs3U/Ig/5ZtNaECQG+/kKUy3bDOjwhbCjeGmVnQ0bmbXMwO0MkfH15+HrShtfBpEr9s+w8y66wkSEjkere7M/m6Bj0xHgX4Y4JryS0CQQChBua8JXCCUGLle7+IEEcgQZSF4PdLrmnhrRG7Qrrgyd6pPuvd2jAGv5fMhjROmf9MWc4DFiRK0B6dz7OyF9j/-----END RSA PRIVATE KEY----- 3 建立私有CA实现证书申请颁发3.1 证书规范和配置命令建立私有CA： OpenCA：OpenCA开源组织使用Perl对OpenSSL进行二次开发而成的一套完善的PKI免费软件 openssl：相关包 openssl 和 openssl-libs 证书申请及签署步骤： 生成证书申请请求 RA核验 CA签署 获取证书 123456789#安装包[root@rocky86 ~]# yum install openssl-libs#查看配置文件[root@rocky86 ~]# cat /etc/pki/tls/openssl.cnf#安装包[root@ubuntu ~]# apt install libssl-dev#查看配置文件[root@ubuntu ~]# cat /etc/ssl/openssl.cnf 三种策略：match匹配、optional可选、supplied提供 123match：要求用户申请填写的信息跟CA设置信息必须一致optional：可有可无，跟CA设置信息可不一致supplied：必须填写这项申请信息 CA证书配置规范 1234567891011121314151617181920212223242526272829303132[root@centos8 ~]#cat /etc/pki/tls/openssl.cnf[ ca ]default_ca = CA_default # The default ca section[ CA_default ]dir = /etc/pki/CA #存放CA数据的目录，即所有与证书相关的文件目录（centos7默认有，8没有） certs = $dir/certs #颁发的证书文件crl_dir = $dir/crl #吊销的证书文件database = $dir/index.txt #证书索引文件new_certs_dir = $dir/newcerts #新颁发的证书存放路径certificate = $dir/cacert.pem #CA机构自己的证书serial = $dir/serial #证书编号文件，里面的编号给下一个颁发的证书使用crlnumber = $dir/crlnumber #证书吊销列表编号，即存放当前CRL编号的文件 crl = $dir/crl.pem #证书吊销列表文件private_key = $dir/private/cakey.pem #CA证书自己的私钥RANDFILE = $dir/private/.rand #私有随机数文件x509_extensions = usr_cert #要添加到证书的扩展name_opt = ca_default #“使用者名称”选项cert_opt = ca_default #证书字段选项default_days = 365 #认证多长时间default_crl_days= 30 #距离下一次 CRL 还有多久default_md = sha256 #默认使用 SHA-256preserve = no #保持传递DN排序policy = policy_match[ policy_match ]countryName = match #国家stateOrProvinceName = match #省organizationName = match #组织（公司名）organizationalUnitName = optional #部门commonName = supplied #通用名（哪个服务使用这个证书）emailAddress = optional #邮箱bash openssl req 命令：生成证书请求或自签名证书 选项 说明 -new 生成新的证书签署请求（CSR） -x509 专用于 CA 生成自签证书（不生成请求，直接输出证书） -key 指定已有的私钥文件用于生成请求 -days n 指定证书的有效期限（单位：天） -out /PATH/TO/SOMECERTFILE 指定生成的证书或请求文件的保存路径 -keyout 指定生成新私钥时的文件名（与 -newkey 配合使用） -nodes 如果使用 -newkey 自动生成密钥，则该密钥不加密（无需输入密码） -newkey rsa:bits 自动生成 RSA 私钥，位数由 bits 指定（如 rsa:2048），并基于它生成请求 -subj &quot;/C=.../ST=.../L=.../O=.../OU=.../CN=...&quot; 自定义证书请求中的主题信息，避免交互式输入常见字段：C&#x3D;国家, ST&#x3D;省份, L&#x3D;城市, O&#x3D;组织, OU&#x3D;部门, CN&#x3D;通用名（域名或 IP） -set_serial n 在生成自签名证书时指定序列号（仅与 -x509 一起使用） openssl ca 命令：指定CA（证书颁发机构）签发证书 选项 说明 -cert ca_cert.pem 指定 CA 的证书文件（PEM 格式） -keyfile ca_key.pem 指定 CA 的私钥文件（PEM 格式） -in file 输入待签署的证书请求文件（CSR） -out file 指定输出的已签署证书文件路径 -key file 签署证书时使用的私钥文件（通常为 CA 私钥） -revoke file 吊销指定的已签发证书（需配合 CRL 使用） -days arg 设置签发证书的有效期天数 -status serial 检查指定序列号的证书状态（有效&#x2F;已吊销） -config ca_config.cnf 指定 OpenSSL 配置文件路径 -outdir dir 指定证书输出目录 -infiles file ... 批量输入多个证书请求文件 -spkac file 输入 Netscape SPKAC 文件（较少使用） -extensions section 指定配置文件中定义的证书扩展段（如 v3_ca、v3_req） -extfile file 从指定文件读取证书扩展配置 -subj arg 覆盖请求中的主题信息（用于自定义签发主题） -utf8 启用 UTF-8 编码解析主题信息 -nameopt option 控制主题名称的显示格式 -enddate YYMMDDHHMMSSZ 手动设置证书的过期时间戳 -md arg 指定摘要算法（如 sha256、sha1） -batch 批量模式，自动完成签发，无需手动确认 -preserveDN 保留原始请求中的 DN（Distinguished Name）顺序 -include file 包含其他配置文件内容 -rand file(s) 指定随机数种子文件 -engine id 指定加密引擎（如硬件加速） -updatedb 更新 CA 的证书数据库 -crldays arg 设置 CRL（证书吊销列表）的有效期天数 -crlhours arg 设置 CRL 有效期小时数 -crlsec arg 设置 CRL 有效期秒数 -crlexts section 指定 CRL 扩展字段配置段 -createdb 创建新的证书数据库（首次使用 CA 时） -msie_hack 兼容旧版 IE 浏览器的证书格式 -noemailDN 不在主题中包含电子邮件字段 -selfsign 使用当前私钥自签名 CA 证书 -separate 对每个输入文件单独生成输出证书 openssl x509 命令：查看、转换和处理 X.509 证书 选项 说明 -in 指定输入的证书文件（默认为标准输入） -out 指定输出的证书文件（默认为标准输出） -noout 不输出证书编码内容（仅用于查看文本信息） -days 设置证书有效期（默认 30 天，常用于测试） -req 表示输入是一个证书请求（CSR），需进行签名输出 -CA ca.pem 指定用于签名的 CA 证书文件（PEM 格式） -CAkey key.pem 指定 CA 的私钥文件，用于签名 -set_serial n 设置证书的序列号（必须与 -CA 和 -CAkey 一起使用） -text 以可读文本格式输出证书详细信息（如版本、序列号、有效期、公钥等） -in cert.der 输入 DER 格式的证书文件 -outform PEM 输出为 PEM 格式（Base64 编码） -out cert.pem 输出为 PEM 格式的证书文件 -in cert.pem 输入 PEM 格式的证书文件 -outform DER 输出为 DER 格式（二进制） -out cert.der 输出为 DER 格式的证书文件 3.2 创建私有CA1、创建私有CA所需要的文件 12345678#创建相关目录mkdir -pv /etc/pki/CA/&#123;certs,crl,newcerts,private&#125; #生成证书索引数据库文件touch /etc/pki/CA/index.txt#指定第一个颁发证书的序列号echo 01 &gt; /etc/pki/CA/serial 2、 生成私有CA的私钥 12cd /etc/pki/CA/(umask 066; openssl genrsa -out private/cakey.pem 2048) 3、生成私有CA的自签名证书 1234567891011121314151617openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -days 3650 -out /etc/pki/CA/cacert.pem....Country Name (2 letter code) [AU]:CN #国家代码State or Province Name (full name) [Some-State]:beijing #省/州Locality Name (eg, city) []:beijing #城市Organization Name (eg, company) [Internet Widgits Pty Ltd]:magedu #公司/单位Organizational Unit Name (eg, section) []:m54 #部门Common Name (e.g. server FQDN or YOUR name) []:www.magedu.org #域名Email Address []: #邮箱选项说明：openssl req：#申请-new：#生成新证书签署请求-x509：#专用于CA生成自签证书标准规范-key：#生成请求时用到的私钥文件-days n：#证书的有效期限-out /PATH/TO/SOMECERTFILE: #证书的保存路径 国家代码： https://country-code.cl/ 范例：查看证书信息 12345[root@centos8 ~]#openssl x509 -in /etc/pki/CA/cacert.pem -noout -text-text：以文本方式展示如果传到Windows，把文件后缀改成crt就能看了 范例：生成自签名证书 1234567891011121314[root@centos8 ~]#openssl req -utf8 -newkey rsa:1024 -subj &quot;/CN=www.magedu.org&quot; -keyout app.key -nodes -x509 -out app.crtGenerating a RSA private key...........................+++++...+++++writing new private key to &#x27;app.key&#x27;-----选项说明：-newkey：#指在生成证书请求或者自签名证书的时候自动生成密钥，然后生成的密钥名称由-keyout参数指定。当指定newkey选项时，后面指定rsa:bits说明产生rsa密钥，位数由bits指定。 如果没有指定选项-key和-newkey，默认自动生成秘钥-keyout：#指明创建的新的私有密钥文件的文件名-subj args ：#替换或自定义证书请求时需要输入的信息，并输出修改后的请求信息。args的格式为&quot;/type0=value0/type1=value1...&quot;，如果value为空，则表示使用配置文件中指定的默认值，如果value值为&quot;.&quot;，则表示该项留空。其中可识别type有：C是Country、ST是state、L是localcity、O是Organization、OU是Organization Unit、CN是common name等-nodes：#如果指定-newkey自动生成秘钥，那么-nodes选项说明生成的秘钥不需要加密，即不需要输入passphase[root@centos8 ~]#openssl x509 -in app.crt -noout -text 3.3 申请证书并颁发证书1、为需要使用证书的用户生成他自己的私钥 12#/data/test.key可以改，将来给网站或者服务申请证书可换路径(umask 066; openssl genrsa -out /data/test.key 2048) 2、为需要使用证书的主机利用他的私钥生成证书申请文件 12#/data/test.csr是证书申请文件openssl req -new -key /data/test.key -out /data/test.csr 3、在CA签署证书并将证书颁发给请求者 12#/etc/pki/CA/certs/test.crt为证书文件openssl ca -in /data/test.csr -out /etc/pki/CA/certs/test.crt -days 100 注意：默认要求 国家，省，公司名称三项必须和CA一致，如果证书申请文件中的配置项与CA机构的匹配规则不一致，将无法签发证书，要是想不一致，就去OpenSSL配置文件将match改成optional 4、查看证书中的信息 123456789101112openssl x509 -in /PATH/FROM/CERT_FILE -noout -text|issuer|subject|serial|dates#查看指定编号的证书状态openssl ca -status 0F#原来是0F，加1后变成10[root@ubuntu CA]# cat /etc/pki/CA/serial10#V 表示有效，230830 表示2023年8月30日过期，0F 表示证书编号[root@ubuntu CA]# cat /etc/pki/CA/index.txtV 230830135623Z 0F unknown /C=CN/ST=beijing/O=magedu/OU=m54-class/CN=www.m54.magedu.com 3.4 吊销证书在客户端获取要吊销的证书的serial 1openssl x509 -in /PATH/FROM/CERT_FILE -noout -serial -subject 在CA上，根据客户提交的serial与subject信息，对比检验是否与index.txt文件中的信息一致，吊销证书： 1openssl ca -revoke /etc/pki/CA/newcerts/SERIAL.pem 指定第一个吊销证书的编号,注意：第一次更新证书吊销列表前，才需要执行 1echo 01 &gt; /etc/pki/CA/crlnumber 生成证书吊销列表 1openssl ca -gencrl -out /etc/pki/CA/crl.pem 查看crl文件： 123openssl crl -in /etc/pki/CA/crl.pem -noout -text如果传到Windows，把文件后缀改成crl就能看了 3.5 CentOS 7创建自签名证书因为centos7上有Makefile文件，可以方便快捷生成证书，不用敲上面很长的命令，要是想centos8上也可以这样，将centos7的Makefile文件传到centos8上 123456789101112131415[root@centos7 ~]#cd /etc/pki/tls/certs[root@centos7 certs]#make[root@centos7 certs]#make /data/test.key #生成私钥文件，需要输入口令，可以编辑Makefile文件删除[root@centos7 certs]#lsca-bundle.crt ca-bundle.trust.crt make-dummy-cert Makefile renew-dummy-cert[root@centos7 certs]#cat Makefile.....%.key:umask 77 ; \\/usr/bin/openssl genrsa -aes128 $(KEYLEN) &gt; $@ #将-aes12删除......[root@centos7 certs]#make /data mysql.crt #生成自签名证书 3.6 实战案例：在CentOS 8上实现私有CA和证书申请3.6.1 创建CA相关目录和文件1234[root@centos8 ~]#mkdir -pv /etc/pki/CA/&#123;certs,crl,newcerts,private&#125;[root@centos8 ~]#tree /etc/pki/CA/[root@centos8 ~]#touch /etc/pki/CA/index.txt[root@centos8 ~]#echo 0F &gt; /etc/pki/CA/serial index.txt和serial文件在颁发证书时需要使用，如果不存在，会出现以下错误提示 1234567891011121314151617[root@centos8 ~]#openssl ca -in /data/app1/app1.csr -out/etc/pki/CA/certs/app1.crt -days 1000Using configuration from /etc/pki/tls/openssl.cnf140040142845760:error:02001002:system library:fopen:No such file ordirectory:crypto/bio/bss_file.c:72:fopen(&#x27;/etc/pki/CA/index.txt&#x27;,&#x27;r&#x27;)140040142845760:error:2006D080:BIO routines:BIO_new_file:no suchfile:crypto/bio/bss_file.c:79:[root@centos8 ~]#openssl ca -in /data/app1/app1.csr -out/etc/pki/CA/certs/app1.crt -days 1000Using configuration from /etc/pki/tls/openssl.cnf/etc/pki/CA/serial: No such file or directoryerror while loading serial number140240559408960:error:02001002:system library:fopen:No such file ordirectory:crypto/bio/bss_file.c:72:fopen(&#x27;/etc/pki/CA/serial&#x27;,&#x27;r&#x27;)140240559408960:error:2006D080:BIO routines:BIO_new_file:no suchfile:crypto/bio/bss_file.c:79: 3.6.2 创建CA的私钥12345678910111213141516[root@centos8 ~]#cd /etc/pki/CA/[root@centos8 CA]#(umask 066; openssl genrsa -out private/cakey.pem 2048)Generating RSA private key, 2048 bit long modulus (2 primes)..........................................................................+++++.............................+++++e is 65537 (0x010001)[root@centos8 CA]#tree...[root@centos8 CA]#ll private/total 4-rw------- 1 root root 1679 May 20 11:55 cakey.pem[root@centos8 CA]#cat private/cakey.pem... 3.6.3 给CA颁发自签名证书123456789[root@centos8 ~]#openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -days 3650 -out /etc/pki/CA/cacert.pem[root@centos8 ~]#tree /etc/pki/CA[root@centos8 ~]#cat /etc/pki/CA/cacert.pem[root@centos8 ~]#openssl x509 -in /etc/pki/CA/cacert.pem -noout -text[root@centos8 ~]#sz /etc/pki/CA/cacert.pem#将文件cacert.pem传到windows上，修改文件名为cacert.pem.crt，双击可以看到下面显示 3.6.4 用户生成私钥和证书申请1234567[root@centos8 ~]#mkdir /data/app1#生成私钥文件[root@centos8 ~]#(umask 066; openssl genrsa -out /data/app1/app1.key 2048)[root@centos8 ~]#cat /data/app1/app1.key#生成证书申请文件[root@centos8 ~]#openssl req -new -key /data/app1/app1.key -out /data/app1/app1.csr 3.6.5 CA颁发证书12[root@centos8 ~]#openssl ca -in /data/app1/app1.csr -out /etc/pki/CA/certs/app1.crt -days 1000[root@centos8 ~]#tree /etc/pki/CA 默认有三项内容必须和CA一致：国家，省份，组织，如果不同，会出现下面的提示 123456[root@centos8 ~]#openssl ca -in /data/app2/app2.csr -out /etc/pki/CA/certs/app2.crtUsing configuration from /etc/pki/tls/openssl.cnfCheck that the request matches the signatureSignature okThe stateOrProvinceName field is different betweenCA certificate (beijing) and the request (hubei) 3.6.6 查看证书1234567891011121314[root@centos8 ~]#cat /etc/pki/CA/certs/app1.crt[root@centos8 ~]#openssl x509 -in /etc/pki/CA/certs/app1.crt -noout -text[root@centos8 ~]#openssl x509 -in /etc/pki/CA/certs/app1.crt -noout -issuer[root@centos8 ~]#openssl x509 -in /etc/pki/CA/certs/app1.crt -noout -subject[root@centos8 ~]#openssl x509 -in /etc/pki/CA/certs/app1.crt -noout -dates[root@centos8 ~]#openssl x509 -in /etc/pki/CA/certs/app1.crt -noout -serial#验证指定编号对应证书的有效性[root@centos8 ~]#openssl ca -status 0F[root@centos8 ~]#cat /etc/pki/CA/index.txt[root@centos8 ~]#cat /etc/pki/CA/index.txt.old[root@centos8 ~]#cat /etc/pki/CA/serial[root@centos8 ~]#cat /etc/pki/CA/serial.old 3.6.7 将证书相关文件发送到用户端使用12[root@centos8 ~]#cp /etc/pki/CA/certs/app1.crt /data/app1/[root@centos8 ~]#tree /data/app1/ 3.6.8 证书的信任默认生成的证书，在windows上是不被信任的，可以通过下面的操作实现信任 打开internet属性 3.6.9 证书的吊销123[root@centos8 ~]#openssl ca -revoke /etc/pki/CA/newcerts/11.pem[root@centos8 ~]#openssl ca -status 11[root@centos8 ~]#cat /etc/pki/CA/index.txt 3.6.10 生成证书吊销列表文件123456789101112131415161718192021[root@centos8 ~]#openssl ca -gencrl -out /etc/pki/CA/crl.pemUsing configuration from /etc/pki/tls/openssl.cnf/etc/pki/CA/crlnumber: No such file or directoryerror while loading CRL number140511895181120:error:02001002:system library:fopen:No such file ordirectory:crypto/bio/bss_file.c:72:fopen(&#x27;/etc/pki/CA/crlnumber&#x27;,&#x27;r&#x27;)140511895181120:error:2006D080:BIO routines:BIO_new_file:no suchfile:crypto/bio/bss_file.c:79:[root@centos8 ~]#echo 01 &gt; /etc/pki/CA/crlnumber[root@centos8 ~]#openssl ca -gencrl -out /etc/pki/CA/crl.pemUsing configuration from /etc/pki/tls/openssl.cnf[root@centos8 ~]#cat /etc/pki/CA/crlnumber02[root@centos8 ~]#cat /etc/pki/CA/crl.pem[root@centos8 ~]#openssl crl -in /etc/pki/CA/crl.pem -noout -text[root@centos8 ~]#sz /etc/pki/CA/crl.pem#将此文件crl.pem传到windows上并改后缀为crl.pem.crl，双击可以查看以下显示 4 gpg 命令GPG是一个完全免费的开源实现，用于OpenPGP标准的数据加密和解密。这种加密方式可以用于保护敏感数据，确保其在传输过程中不被截获或篡改 安装gpg 1sudo apt-get install gnupg 语法 1234567891011gpg [参数] 文件名--import #导入或合并密钥--verify #验证签名--output #输出信息到文件--decrypt #解密数据-c #设置加密文件 -o #设置解密文件--export #导出密钥信息--gen-key #生成密钥对--encrypt #加密数据 范例 1234567891011#基于对称加密方式，加密指定文件gpg -c File #基于对称加密方式，解密指定文件gpg -o mydecrypt -d File.gpg #生成密钥对文件gpg --gen-key #查看已有密钥列表gpg --list-keys 范例 12345678#导入公钥gpg --import public.key#验证签名gpg --verify zzyenv-5.3.1-x64.tar.gz.asc#解密文件并输出到指定文件gpg --output zzyenv-5.3.1-x64.tar.gz --decrypt zzyenv-5.3.1-x64.tar.gz.asc","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"加密和安全","slug":"加密和安全","permalink":"https://aquapluto.github.io/tags/%E5%8A%A0%E5%AF%86%E5%92%8C%E5%AE%89%E5%85%A8/"}]},{"title":"安全机制","slug":"Linux/encryption-security/security-mechanisms","date":"2025-08-21T03:02:54.000Z","updated":"2025-09-07T06:16:51.094Z","comments":true,"path":"Linux/encryption-security/security-mechanisms/","permalink":"https://aquapluto.github.io/Linux/encryption-security/security-mechanisms/","excerpt":"","text":"1 常见的安全攻击 STRIDE Spoofing 假冒，与真实网站界面相同，欺骗浏览者提交敏感信息 123456789101112131415161718192021发送邮件服务yum info postfix25端口发送邮件telnet 127.0.0.1 25内容+域名hello magedu.com指定发送人和收件人mail from:jack.ma@alibaba.comrcpt to:wang@xxx.com（有邮箱写邮箱）内容datasubject:hellowelcome to alibaba.（以.结束内容）quit切换用户查看邮件su -wangmail Tampering 篡改，修改网络中的数据包以达到某种目的 Repudiation 否认 Information Disclosure 信息泄漏 12345678910yum -y install telnet.serversystemctl enable --now telnet.socket端口号是23远程登陆telnet 10.0.0.8输用户，密码敲命令wireshark抓包 TCP追踪流 Denial of Service 拒绝服务，使计算机或网络无法提供正常的服务 Elevation of Privilege 提升权限，将用户权限通过非法手段提高，以到达执行敏感操作的目的 2 加密算法和协议对称加密 非对称（公钥）加密 单向加密 认证协议 2.1 对称加密算法 对称加密：加密和解密使用同一个密钥 特性： 加密、解密使用同一个密钥，效率高 将原始数据分割成固定大小的块，逐个进行加密 缺陷： 密钥过多 密钥分发 数据来源无法确认 常见对称加密算法: DES：Data Encryption Standard，56bits 3DES AES：Advanced (128, 192, 256bits) Blowfish，Twofish IDEA，RC6，CAST5 123Alice ---Bob 对称算法 key1=key2data -- 加密(key1) -- data&#x27; ---解密（key2）---data 2.2 非对称加密算法2.2.1 非对称加密算法介绍非对称加密：密钥是成对出现 公钥：public key，公开给所有人，主要给别人加密使用 私钥：secret key，private key 自己留存，必须保证其私密性，用于自已加密签名 特点：用公钥加密数据，只能使用与之配对的私钥解密；反之亦然 功能： 数据加密：适合加密较小数据,比如: 加密对称密钥 数字签名：主要在于让接收方确认发送方身份 缺点： 密钥长,算法复杂 加密解密效率低下 常见算法： RSA：由 RSA 公司发明，是一个支持变长密钥的公共密钥算法，需要加密的文件块的长度也是可变的,可实现加密和数字签名 DSA（Digital Signature Algorithm）：数字签名算法，是一种标准的 DSS（数字签名标准） ECC（Elliptic Curves Cryptography）：椭圆曲线密码编码学，比RSA加密算法使用更小的密钥，提供相当的或更高等级的安全 2.2.2 非对称加密实现加密 接收者公钥加密，接收者私钥解密 数据加密是为了保证信息的机密性，任何知道接收方的公钥的都可以向接收方发送信息，但是只有拥有私钥的才能解密出来 接收者 生成公钥&#x2F;密钥对：P和S 公开公钥P，保密密钥S 发送者 使用接收者的公钥来加密消息M 将P(M)发送给接收者 接收者 使用密钥S来解密：M&#x3D;S(P(M)) 12345678910111213141516171819Alice ---Bob 非对称算法 key1&lt;&gt;key2data -- 加密(key1) -- data&#x27; ---解密（key2）---dataAlicePublic key 公开 PaSecret private key 不公开 SaBobPublic key PbSecret private key Sb 用任何一个密钥加密，只能用对应另一把密钥解密Alice ---&gt; Bob 加密data -- 加密(key1=Pb) -- data&#x27; ---解密（key2=Sb）---dataAlice ---&gt; Bob 来源确认 ---&gt; jack data -- 加密(key1=Sa) -- data&#x27; ---解密（key2=Pa）---data 2.2.3 非对称加密实现数字签名 发送者私钥加密，发送者公钥加密 数字签名是为了保证信息的完整性、真实性、不可否认性，任何接收方都可以用公钥解密，验证数据的正确性 得到文件之后，用相同的算法获取文件摘要，再跟官方提供的摘要值对比，就能判断文件是否被篡改过 发送者 生成公钥&#x2F;密钥对：P和S 公开公钥P，保密密钥S 使用密钥S来加密消息M 发送给接收者S(M) 接收者 使用发送者的公钥来解密M&#x3D;P(S(M)) 2.2.4 RSA和DSARSA：RSA是目前最有影响力的公钥加密算法，它能够抵抗到目前为止已知的所有密码攻击，已被ISO推荐为公钥数据加密标准。RSA算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但那时想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥 DSA：DSA是基于整数有限域离散对数难题的，其安全性与RSA相比差不多。DSA只是一种算法，和RSA不同之处在于它不能用作加密和解密，也不能进行密钥交换，只用于签名,它比RSA要快很多 2.3 单向哈希算法哈希算法：也称为散列算法，将任意数据缩小成固定大小的“指纹”，称为digest，即摘要 特性： 任意长度输入，固定长度输出 若修改数据，指纹也会改变，且有雪崩效应，数据的一点微小改变，生成的指纹值变化非常大。 无法从指纹中重新生成数据，即不要逆，具有单向性 功能：数据完整性 常见算法：md5: 128bits、sha1: 160bits、sha224 、sha256、sha384、sha512 常用工具 md5sum | sha1sum [ –check ] file openssl、gpg rpm -V 123456hash(data) --&gt; digest摘要,相当于指纹1 单向2 data 相同，digest必相同3 data不相同，digest必不相同4 hash算法固定，则digest的长度固定5 data发生轻微变化，而digest会发生巨大的变化 查看文件的哈希值 123#md5 以16进制显示, 32*4=128[root@centos7 ~]#md5sum /etc/hosts54fb6627dbaa37721048e4549db3224d /etc/hosts 2.4 综合应用多种加密算法2.4.1 实现数据加密对称加密和非对称加密 无法验证数据完整性和来源 123A--&gt;B加密：Key(data)+Pb(key) 解密：Sb(key)+Key(data) 2.4.2 实现数字签名不加密数据，可以保证数据来源的可靠性、数据的完整性和一致性 1data+Sa(hash(data)) #先哈希加密数据，再用A的私钥接着加密 2.4.3 综合加密和签名即实现数据加密，又可以保证数据来源的可靠性、数据的完整性和一致性 12#方法一Pb&#123;Sa[hash(data)]+data&#125; 12#方法二对称key&#123;Sa[hash(data)]+data&#125;+Pb(对称key) 2.5 密码交换密钥交换：IKE（ Internet Key Exchange ） 公钥加密：用目标的公钥加密对称密钥 DH (Deffie-Hellman)：生成对称（会话）密钥 DH 介绍 这个密钥交换方法，由惠特菲尔德·迪菲（Bailey Whitfield Diffie）和马丁·赫尔曼（MartinEdward Hellman）在1976年发表 它是一种安全协议，让双方在完全没有对方任何预先信息的条件下通过不安全信道建立起一个密钥，这个密钥一般作为“对称加密”的密钥而被双方在后续数据传输中使用。 DH数学原理是base离散对数问题。做类似事情的还有非对称加密类算法，如：RSA。 其应用非常广泛，在SSH、VPN、Https…都有应用，勘称现代密码基石 DH 实现过程： 123456A: g,p 协商生成公开的整数g, 大素数pB: g,pA:生成隐私数据:a (a&lt;p)，计算得出 g^a%p，发送给BB:生成隐私数据:b,(b&lt;p)，计算得出 g^b%p，发送给AA:计算得出 [(g^b%p)^a]%p = g^ab%p，生成为密钥B:计算得出 [(g^a%p)^b]%p = g^ab%p，生成为密钥 DH 特点 泄密风险：私密数据a，b在生成K后将被丢弃，因此不存在a，b过长时间存在导致增加泄密风险。 中间人攻击：由于DH在传输p，g时并无身份验证，所以有机会被实施中间人攻击，替换双方传输时的数据 范例 12345678910111213141516171819202122g=23p=5A:a=6g^a%p=23^6%5=4[(g^b%p)^a]%p=2^6%5=4B:b=15g^b%p=23^15%5=2[(g^a%p)^b]%p=4^15%5=4[root@centos8 ~]#echo 23^15%5|bc2[root@centos8 ~]#echo 23^6%5|bc4[root@centos8 ~]#echo 2^6%5|bc4[root@centos8 ~]#echo 4^15%5|bc4 3 CA和证书3.1 中间人攻击中间人截获服务器向客户端发送的真正公钥，将假公钥发送给客户端，让客户端用假公钥加密数据，从而中间人可以利用自己的私钥解密并查看加密的数据，并篡改数据利用真公钥发送给服务器 3.2 CA和证书CA证书代表信息的真伪，通过CA证书，可以确定客户端收到的公钥是否是真的公钥，从而防止中间人攻击，实现公钥的安全交换 子CA证书由根CA证书颁发，根CA证书由自己给自己颁发 PKI：Public Key Infrastructure 公共密钥加密体系 签证机构：CA（Certificate Authority） 注册机构：RA 证书吊销列表：CRL 证书存取库： X.509：定义了证书的结构以及认证协议标准 版本号 序列号 签名算法 颁发者 有效期限 主体名称 证书类型： 证书授权机构的证书 服务器证书 用户证书 获取证书两种方法： 自签名的证书： 自已签发自己的公钥 使用证书授权机构：生成证书请求（csr）将证书请求csr发送给CACA签名颁发证书 自签名证书和根证书的区别 根证书（Root Certificate）就是自签名证书，不过签发机构不同，他有就有区别，它们之间有以下区别： 自签名证书（Self-Signed Certificate） 颁发者和使用者是同一个实体： 自签名证书是由实际使用该证书的实体（通常是个人、组织或设备）自行创建和签名的证书。颁发者和使用者是同一个实体，没有第三方 CA 参与。 信任度较低： 自签名证书在公共网络中信任度较低，因为它们没有受到受信任的第三方 CA 的验证。当客户端连接到使用自签名证书的服务时，通常会收到安全警告，用户需要手动确认是否信任该证书。 用途有限： 自签名证书通常用于内部测试、开发环境或局域网中，而不是在公共网络中使用。它们提供了加密通信的能力，但没有得到广泛信任。 根证书（Root Certificate） 由受信任的 CA 颁发： 根证书是由受信任的证书颁发机构（CA）签名并颁发的证书。CA 是一个受信任的实体，它通过验证证书请求者的身份，并签发数字证书。根证书是 CA 的根证书，它本身是自签名证书。 信任度高： 根证书由操作系统、浏览器等软件内置，因此被广泛信任。当使用者收到由受信任 CA 签发的证书时，不会收到安全警告，因为这些证书是受信任的。 广泛用于公共网络： 根证书和由它签发的中间证书（Intermediate Certificate）通常用于公共网络中，例如用于加密网站的 HTTPS 通信。这些证书能够建立安全的、受信任的通信连接。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"加密和安全","slug":"加密和安全","permalink":"https://aquapluto.github.io/tags/%E5%8A%A0%E5%AF%86%E5%92%8C%E5%AE%89%E5%85%A8/"}]},{"title":"nft","slug":"Linux/firewall/nft","date":"2025-08-21T03:02:22.000Z","updated":"2025-09-07T06:28:30.637Z","comments":true,"path":"Linux/firewall/nft/","permalink":"https://aquapluto.github.io/Linux/firewall/nft/","excerpt":"","text":"1 nft介绍nftables 是一个 netfilter 项目，旨在替换现有的 {ip,ip6,arp,eb}tables 框架，为 {ip,ip6}tables 提供一个新的包过滤框架、一个新的用户空间实用程序（nft）和一个兼容层。它使用现有的钩子、链接跟踪系统、用户空间排队组件和 netfilter 日志子系统。 nftables 主要由三个组件组成：内核实现、libnl netlink 通信和 nftables 用户空间。 其中内核提供了一个 netlink 配置接口以及运行时规则集，libnl 包含了与内核通信的基本函数，用户空间可以通过 nft 和用户进行交互。 2 nft相关概念nftables 和 iptables 一样，由表（table）、链（chain）和规则（rule）组成，其中表包含链，链包含规则，规则是真正的 action，规则由地址，接口，端口或包含当前处理数据包中的其他数据等表达式以及诸如drop, queue, continue等声明组成。 与 iptables 相比，nftables 主要有以下几个变化： iptables 规则的布局是基于连续的大块内存的，即数组式布局；而 nftables 的规则采用链式布局，即数组和链表的区别 iptables 大部分工作在内核态完成，如果要添加新功能，只能重新编译内核；而 nftables 的大部分工作是在用户态完成的，添加新功能更加容易，不需要改内核 nftables不包含任何内置表和链 拥有使用额外脚本的能力, 拥有一些高级的类似编程语言的能力，例如定义变量和包含外部文件 iptables 有内置的链，即使只需要一条链，其他的链也会跟着注册；而 nftables 不存在内置的链，可以按需注册。由于 iptables 内置了一个数据包计数器，所以即使这些内置的链是空的，也会带来性能损耗 简化了 IPv4&#x2F;IPv6 双栈管理 原生支持集合、字典和映射 nftables 的每个表只有一个地址簇，并且只适用于该簇的数据包。表可以指定五个簇中的一个： nftables簇 iptables命令行工具 ip IPv4 地址 iptables ip6 IPv6 地址 ip6tables inet IPv4 和 IPv6 地址 iptables和ip6tables arp 地址解析协议(ARP)地址 arptables bridge 处理桥接数据包 ebtables inet 同时适用于 IPv4 和 IPv6 的数据包，即统一了 ip 和 ip6 簇，可以更容易地定义规则，注：当没有指定地址簇时，默认为ip 链是用来保存规则的，和表一样，链也需要被显示创建，因为 nftables 没有内置的链。链有以下两种类型： 基本链 : 数据包的入口点，需要指定钩子类型和优先级，相当于内置链 常规链 : 不需要指定钩子类型和优先级，可以用来做跳转，从逻辑上对规则进行分类，类似于自定义链 3 nft常见用法3.1 nft命令格式1234567891011121314151617[root@centos8 ~]#nft --helpUsage: nft [ options ] [ cmds... ]选项说明-h, --help 显示帮书-v, --version 显示版本信息-c, --check 检查命令的有效性，而不实际应用更改。-f, --file &lt;filename&gt; 包含文件内容&lt;filename&gt;-i, --interactive 从命令行读取输入-j, --json 以JSON格式化输出-n, --numeric 指定一次后，以数字方式显示网络地址（默认行为）。指定两次以数字方式显示Internet服务（端口号）。指定三次以数字方式显示协议，用户ID和组ID。-s, --stateless 省略规则集的有状态信息-N 将IP地址转换为名称。-a, --handle 显示规则句柄handle-e, --echo Echo what has been added, inserted or replaced.-I, --includepath &lt;directory&gt; 添加&lt;directory&gt;目录到包含文件的搜索路径中。默认为: /etc--debug &lt;level [,level...]&gt; 添加调试,在level处(scanner, parser, eval, netlink, mnl,proto-ctx, segtree, all) nft 命令基本格式：nft 操作符 操作目标 操作内容 123456789101 操作符: 增,删,改,查,清除,插入,创建表操作：add,delete,list,flush链操作：add,delete,rename,list,flush,create规则：add,delete,insert2 操作目标: 簇,表,链,规则链类型：filter,route,nat链钩子：hook3 操作内容：... 规则选项 动作（Action） 中文 含义 是否停止处理 说明 accept 接受 接受数据包，允许通过 ✅ 停止处理 包被放行，后续规则不再处理 drop 丢弃 丢弃数据包，不响应 ✅ 停止处理 静默丢弃，不返回任何错误信息（如 TCP RST） reject 拒绝 拒绝数据包，并返回错误响应 ✅ 停止处理 通常返回 ICMP 不可达或 TCP RST 包，通知发送方被拒 queue 队列 将数据包发送到用户空间程序处理 ✅ 停止处理 供如 nfqueue 等机制进一步处理（如深度包检测） continue 继续 继续执行后续规则 ❌ 继续处理 不终止规则链，匹配后仍继续处理下一条规则 return 返回 返回到调用它的规则链 ✅ 停止当前链处理 用于自定义链，执行完后返回原链继续处理 jump 跳跃 跳转到指定规则链执行 ⬇️ 执行完目标链后返回 执行目标链，完成后回到原链下一条规则 goto 转到 转到指定规则链执行 ⬇️ 执行完目标链后不返回 跳过当前链后续规则，不返回原链 limit 限制 限制匹配频率（防日志刷屏） ❌ 继续处理（条件性匹配） 常与 log 配合使用，控制单位时间内匹配次数 log 日志 记录数据包信息到系统日志 ❌ 继续处理 记录日志后，继续匹配后续规则 3.2 查看1234567nft list ruleset # 列出所有规则nft list tables # 列出所有表nft list table filter # 列出ip簇的filter表nft list table inet filter # 列出inet簇的filter表nft list chain filter INPUT # 列出filter表input链以上命令后面也可以加 -nn 用于不解析ip地址和端口加 -a 用于显示 handles 范例：CentOS8.1版才支持以下操作,CentOS8.2以后版本默认无任何规则 1234567891011121314151617181920212223242526272829303132333435363738394041[root@centos8 ~]#nft list tables[root@centos8 ~]#vim /etc/sysconfig/nftables.conf#删除此行前的注释include &quot;/etc/nftables/inet-filter.nft&quot; [root@centos8 ~]#systemctl restart nftables.service[root@centos8 ~]#nft list tablestable inet filter#默认为ip簇，无规则[root@centos8 ~]#nft list table filterError: Could not process rule: No such file or directorylist table filter ^^^^^^ #指定inet簇才有规则[root@centos8 ~]#nft list table inet filtertable inet filter &#123; chain input &#123; type filter hook input priority 0; policy accept; &#125; chain forward &#123; type filter hook forward priority 0; policy accept; &#125; chain output &#123; type filter hook output priority 0; policy accept; &#125;&#125;[root@centos8 ~]#nft list rulesettable inet filter &#123; chain input &#123; type filter hook input priority 0; policy accept; &#125; chain forward &#123; type filter hook forward priority 0; policy accept; &#125; chain output &#123; type filter hook output priority 0; policy accept; &#125;&#125; 3.3 增加 增加表：nft add table fillter 增加链：nft add chain filter input &#123; type filter hook input priority 0 ; &#125; # 要和hook（钩子）相关连 增加规则：nft add rule filter input tcp dport 22 accept 3.4 删 只需要把上面的 add 改为 delete 即可 3.5 改 更改链名用rename 更改规则用replace 4 nft实战案例4.1 创建表和删除表12345678[root@centos8 ~]#nft add table inet test_table[root@centos8 ~]#nft list tablestable inet filtertable inet test_table[root@centos8 ~]#nft delete table inet test_table[root@centos8 ~]#nft list tablestable inet filter[root@centos8 ~]#nft add table inet test_table 列出所有的规则 1[root@centos8 ~]#nft list ruleset 4.2 创建链现在表中还没有任何规则，需要创建一个链来保存规则 4.2.1 创建基本链1[root@centos8 ~]#nft add chain inet test_table test_filter_input_chain &#123; type filter hook input priority 0 \\; &#125; 反斜线（ \\ ）用来转义，这样 shell 就不会将分号解释为命令的结尾。 priority 采用整数值，可以是负数，值较小的链优先处理 4.2.2 创建常规链1[root@centos8 ~]#nft add chain inet test_table test_chain 4.2.3 列出链1234567[root@centos8 ~]#nft list table inet test_table[root@centos8 ~]#nft list chain inet test_table test_filter_input_chain[root@centos8 ~]#nft list chain inet test_table test_chain[root@centos8 ~]#nft list ruleset 4.3 创建规则4.3.1 创建常规链规则有了表和链之后，就可以创建规则了，规则由语句或表达式构成，包含在链中 123456789101112#创建常规链的规则[root@centos8 ~]#nft add rule inet test_table test_chain tcp dport http reject[root@centos8 ~]#nft list chain inet test_table test_chaintable inet test_table &#123; chain test_chain &#123; tcp dport http reject &#125;&#125;#常规链规则默认不生效[root@centos7 ~]#curl 10.0.0.8rft Http Server add 表示将规则添加到链的末尾，如果想将规则添加到链的开头，可以使用 insert 1[root@centos8 ~]# nft insert rule inet test_table test_chain tcp dport mysql reject 列出规则 123[root@centos8 ~]#nft list chain inet test_table test_chain[root@centos8 ~]#nft list table inet test_table 4.3.2 创建基本链规则12345678910111213141516[root@centos8 ~]#nft add rule inet test_table test_filter_input_chain tcp dporthttp reject[root@centos8 ~]#nft add rule inet test_table test_filter_input_chain ip saddr10.0.0.6 reject[root@centos8 ~]#nft list chain inet test_table test_filter_input_chaintable inet test_table &#123; chain test_filter_input_chain &#123; type filter hook input priority 0; policy accept; tcp dport http reject ip saddr 10.0.0.6 reject &#125;&#125;[root@centos7 ~]#curl 10.0.0.8curl: (7) Failed connect to 10.0.0.8:80; Connection refused 4.4 插入链的指定位置将规则插入到链的指定位置，有两种方法： 4.4.1 使用 index 来指定规则的索引index 类似于 iptables 的 -I 选项， index 的值是从 0 开始表示第一条rule index 必须指向一个存在的规则，比如 nft insert rule … index 0 就是非法的。 add 表示新规则添加在索引位置的规则后面 insert 表示新规则添加在索引位置的规则前面 12345[root@centos8 ~]#nft insert rule inet test_table test_filter_input_chain index 0 tcp dport mysql reject[root@centos8 ~]#nft list chain inet test_table test_filter_input_chain[root@centos8 ~]#nft insert rule inet test_table test_filter_input_chain index 1 udp dport 123 accept[root@centos8 ~]#nft list chain inet test_table test_filter_input_chain 4.4.2 使用 handle 来指定规则的句柄在 nftables 中，句柄值是固定不变的，除非规则被删除，这就为规则提供了稳定的索引。而 index 的值是可变的，只要有新规则插入，就有可能发生变化。一般建议使用 handle 来插入新规则。 add 表示新规则添加在索引位置的规则后面， insert 表示新规则添加在索引位置的规则前面。 handle 的值可以通过参数 –handle 或者 -a 获取 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@centos8 ~]#nft -a list chain inet test_table test_filter_input_chaintable inet test_table &#123; chain test_filter_input_chain &#123; # handle 2 type filter hook input priority 0; policy accept; tcp dport mysql reject # handle 7 udp dport ntp accept # handle 8 tcp dport http reject # handle 6 &#125;&#125;[root@centos8 ~]#nft add rule inet test_table test_filter_input_chain handle 7 tcp dport ftp reject[root@centos8 ~]#nft -a list chain inet test_table test_filter_input_chaintable inet test_table &#123; chain test_filter_input_chain &#123; # handle 2 type filter hook input priority 0; policy accept; tcp dport mysql reject # handle 7 tcp dport ftp reject # handle 9 udp dport ntp accept # handle 8 tcp dport http reject # handle 6 &#125;&#125;[root@centos8 ~]#nft insert rule inet test_table test_filter_input_chain handle 8 udp dport 8080 reject[root@centos8 ~]#nft -a -nn list rulesettable inet filter &#123; # handle 4 chain input &#123; # handle 1 type filter hook input priority 0; policy accept; &#125; chain forward &#123; # handle 2 type filter hook forward priority 0; policy accept; &#125; chain output &#123; # handle 3 type filter hook output priority 0; policy accept; &#125;&#125;table inet test_table &#123; # handle 7 chain test_chain &#123; # handle 1 tcp dport 3306 reject # handle 5 tcp dport 80 reject # handle 4 &#125; chain test_filter_input_chain &#123; # handle 2 type filter hook input priority 0; policy accept; tcp dport 3306 reject # handle 7 tcp dport 21 reject # handle 9 udp dport 8080 reject # handle 10 udp dport 123 accept # handle 8 tcp dport 80 reject # handle 6 &#125;&#125; 可以在创建规则时就获取到规则的句柄值，在创建规则时同时加上参数 –echo或者-e 和 –handle 123456789101112131415[root@centos8 ~]#nft -a -e add rule inet test_table test_filter_input_chain tcp dport 6379 rejectadd rule inet test_table test_filter_input_chain tcp dport 6379 reject # handle11[root@centos8 ~]#nft -a list chain inet test_table test_filter_input_chaintable inet test_table &#123; chain test_filter_input_chain &#123; # handle 2 type filter hook input priority 0; policy accept; tcp dport mysql reject # handle 7 tcp dport ftp reject # handle 9 udp dport http-alt reject # handle 10 udp dport ntp accept # handle 8 tcp dport http reject # handle 6 tcp dport 6379 reject # handle 11 &#125;&#125; 4.5 删除规则4.5.1 删除单个规则单个规则只能通过其句柄删除，首先需要找到想删除的规则句柄 1[root@centos8 ~]#nft --handle list ruleset 然后使用句柄值来删除该规则 1[root@centos8 ~]#nft delete rule inet test_table test_filter_input_chain handle8 4.5.2 删除所有规则12[root@centos8 ~]#nft flush ruleset[root@centos8 ~]#nft list ruleset 4.5 列出规则可以列出所有规则，也可以列出规则的一部分 4.5.1 列出所有规则1[root@centos8 ~]#nft list ruleset 4.5.2 列出指定表中的所有规则1[root@centos8 ~]#nft list table inet test_table 4.5.3 列出指定链中的所有规则1[root@centos8 ~]#nft list chain inet test_table test_filter_input_chain 5 备份还原规则都是临时的，要想永久生效，可以将规则备份，重启后自动加载恢复 查看service文件 1[root@centos8 ~]#cat /lib/systemd/system/nftables.service 备份配置并还原 1234567891011#备份至文件中[root@centos8 ~]#nft list ruleset[root@centos8 ~]#nft list ruleset &gt; /etc/sysconfig/nftables.conf#删除所有规则[root@centos8 ~]#nft flush ruleset[root@centos8 ~]#nft list ruleset#重新启动后全部还原[root@centos8 ~]#systemctl restart nftables.service[root@centos8 ~]#nft list ruleset 启用指定的配置文件 12345[root@centos8 ~]#cat nftables2.conf#-f 指定规则配置文件，如果已经有规则，是追加至现有规则后[root@centos8 ~]#nft -f nftables2.conf[root@centos8 ~]#nft list ruleset 6 迁移iptables规则到nft1.若要将现有规则保存到文件，请运行以下命令 1# iptables-save &gt; rules.iptables 2.通过 scp 或 ftp 将 step1 文件移动到 CentOS&#x2F;RHEL 8 服务器。也可以使用 vi 编辑器来从 CentOS&#x2F;RHEL 6 或 7 计算机复制内容。 3.运行以下命令，在 CentOS&#x2F;RHEL 8 上使用 iptables 规则生成 nft rules 文件文件 1# iptables-restore-translate -f rules.iptables &gt; rules.nft 4.在 CentOS&#x2F;RHEL 8 机器中加载规则，确保 nftables 服务在系统 1# nft -f rules.nft # load the rule via nft to nftables. 5.在 CentOS&#x2F;RHEL 8 Server 中显示规则 1# nft list ruleset 也可以看到规则已从 CentOS&#x2F;RHEL 6 或 7 迁移到 CentOS&#x2F;RHEL 8 服务器并且也可以测试它们","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"防火墙","slug":"防火墙","permalink":"https://aquapluto.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"}]},{"title":"firewalld","slug":"Linux/firewall/firewalld","date":"2025-08-21T03:02:14.000Z","updated":"2025-09-07T06:24:21.710Z","comments":true,"path":"Linux/firewall/firewalld/","permalink":"https://aquapluto.github.io/Linux/firewall/firewalld/","excerpt":"","text":"1 firewalld介绍firewalld是CentOS 7.0新推出的管理netfilter的用户空间软件工具，由firewalld包提供，用于配置和监控防火墙规则的系统守护进程。可以实现iptables ,ip6tables, ebtables的功能，也被ubuntu18.04版以上所支持(apt install firewalld安装即可) firewalld支持划分区域zone，每个zone可以设置独立的防火墙规则 归入zone顺序： 先根据数据包中源地址，将其纳为某个zone 纳为网络接口所属zone 纳入默认zone，默认为public zone,管理员可以改为其它zone 网卡默认属于public zone,lo网络接口属于trusted zone firewalld zone 分类 zone名称 默认配置 trusted 允许所有流量 home 拒绝除和传出流量相关的，以及ssh,mdsn,ipp-client,samba-client,dhcpv6-client预定义服务之外其它所有传入流量 internal 和home相同 work 拒绝除和传出流量相关的，以及ssh,ipp-client,dhcpv6-client预定义服务之外的其它所有传入流量 public 拒绝除和传出流量相关的，以及ssh,dhcpv6-client预定义服务之外的其它所有传入流量，新加的网卡默认属于public zone external 拒绝除和传出流量相关的，以及ssh预定义服务之外的其它所有传入流量，属于external zone的传出ipv4流量的源地址将被伪装为传出网卡的地址。 dmz 拒绝除和传出流量相关的，以及ssh预定义服务之外的其它所有传入流量 block 拒绝除和传出流量相关的所有传入流量 drop 拒绝除和传出流量相关的所有传入流量（甚至不以ICMP错误进行回应） 预定义服务 服务名称 配置 ssh Local SSH server. Traffic to 22&#x2F;tcp dhcpv6-client Local DHCPv6 client. Traffic to 546&#x2F;udp on the fe80::&#x2F;64 IPv6 network ipp-client Local IPP printing. Traffic to 631&#x2F;udp. samba-client Local Windows file and print sharing client. Traffic to 137&#x2F;udp and 138&#x2F;udp. mdns Multicast DNS (mDNS) local-link name resolution. Traffic to 5353&#x2F;udp to the224.0.0.251 (IPv4) or ff02::fb (IPv6) multicast addresses. firewalld预定义服务配置 firewall-cmd –get-services 查看预定义服务列表 &#x2F;usr&#x2F;lib&#x2F;firewalld&#x2F;services&#x2F;*.xml预定义服务的配置 firewalld 三种配置方法 firewall-config 图形工具: 需安装 firewall-config包 firewall-cmd 命令行工具: firewalld包,默认安装 &#x2F;etc&#x2F;firewalld&#x2F; 配置文件，一般不建议,如:&#x2F;etc&#x2F;firewalld&#x2F;zones&#x2F;public.xml 2 firewall-cmd命令firewall-cmd 格式 1firewall-cmd [OPTIONS...] 常见选项 123456789101112131415161718192021222324252627282930313233343536373839404142434445--state #查看服务运行状态 --reload #重载，修改后只是在当前运行环境下生效，如果想还原回去之前的配置，可以使用此选项；删除当前运行时配置，应用加载永久配置--complete-reload #完全重载--runtime-to-permanent #将当前运行状态的配置永久保存--check-config #检查规则配置是否出错--get-log-denied #显示日记记录规则--set-log-denied= #设置日志记录规则 all|unicast|broadcast|multicast|off--permanent #设置永久生效规则时加上此项，如果没有此项，则更改的都是临时生效规则--get-zones #列出所有可用区域--get-default-zone #查询默认区域--set-default-zone=&lt;ZONE&gt; #设置默认区域--get-active-zones #列出当前正使用的区域--get-services #显示所有己定义的 services,每一个 services 对应一个或多个端口规则，多个 service 组成 zone--get-icmptypes #显示icmp协议类型--get-zone-of-interface= #显示指定设备的 zone--list-all-zones #列出每个zone中的所有规则--new-zone= #添加一个新的zone--new-zone-from-file= #从指定文件中读取规则，添加新zone--delete-zone= #删除指定zone--zone= #指定zone,配合其它选项--info-zone= #查看指定zone运行情况--add-service=&lt;SERVICE&gt; [--zone=&lt;ZONE&gt;] #允许服务的流量通过，如果无--zone= 选项，使用默认区域--remove-service=&lt;SERVICE&gt; [--zone=&lt;ZONE&gt;] #从区域中删除指定服务，禁止该服务流量，如果无--zone= 选项，使用默认区域--new-service= #添加一个新的 service--new-service-from-file= #从文件中添加一个新的 serivce--list-services #查看开放的服务--delete-service= #删除 service--info-service= #输出 info 相关信息--add-source=&lt;CIDR&gt;[--zone=&lt;ZONE&gt;] #添加源地址的流量到指定区域，如果无--zone= 选项，使用默认区域--remove-source=&lt;CIDR&gt; [--zone=&lt;ZONE&gt;] #从指定区域删除源地址的流量，如无--zone= 选项，使用默认区域--add-interface=&lt;INTERFACE&gt;[--zone=&lt;ZONE&gt;] #添加来自于指定接口的流量到特定区域，如果无--zone= 选项，使用默认区域--change-interface=&lt;INTERFACE&gt;[--zone=&lt;ZONE&gt;] #改变指定接口至新的区域，如果无--zone=选项，使用默认区域--add-port=&lt;PORT/PROTOCOL&gt;[--zone=&lt;ZONE&gt;] #允许指定端口和协议的流量，如果无--zone= 选项，使用默认区域--remove-port=&lt;PORT/PROTOCOL&gt;[--zone=&lt;ZONE&gt;] #从区域中删除指定端口和协议，禁止该端口的流量，如果无 --zone= 选项，使用默认区域--list-ports #查看开放的端口--list-all [--zone=&lt;ZONE&gt;] #列出指定区域的所有配置信息，包括接口，源地址，端口，服务等，如果无 --zone= 选项，使用默认区域 范例 1234567891011121314#查看默认zonefirewall-cmd --get-default-zone#默认zone设为dmzfirewall-cmd --set-default-zone=dmz#在internal zone中增加源地址192.168.0.0/24的永久规则firewall-cmd --permanent --zone=internal --add-source=192.168.0.0/24#在internal zone中增加协议mysql的永久规则firewall-cmd --permanent --zone=internal --add-service=mysql#加载新规则以生效firewall-cmd --reload 范例：查看 12345678910111213141516171819202122232425262728293031323334353637383940#查看服务当前运行状态[root@rocky86 ~]# firewall-cmd --staterunning#检查配置[root@rocky86 ~]# firewall-cmd --check-configsuccess#查看默认 Zone[root@rocky86 ~]# firewall-cmd --get-default-zonepublic#显示当前默认的zone中的 service[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client ssh#结果同上一条[root@rocky86 ~]# firewall-cmd --zone=public --list-servicescockpit dhcpv6-client ssh#显示指定zone中的 service[root@rocky86 ~]# firewall-cmd --zone=home --list-servicescockpit dhcpv6-client mdns samba-client ssh#显示所有 zones[root@rocky86 ~]# firewall-cmd --get-zonesblock dmz drop external home internal libvirt nm-shared public trusted work#显示当前正在使用的 zone[root@rocky86 ~]# firewall-cmd --get-active-zonesblock interfaces: eth0 eth1 #生效的网络设备 #显示zone中的自定义规则[root@rocky86 ~]# firewall-cmd --zone=block --list-ports8080/tcp#显示工作在指定设备上的 zone[root@rocky86 ~]# firewall-cmd --get-zone-of-interface=eth0block 范例：查看 services 12345678910111213141516#查看所有可用 services[root@rocky86 ~]# firewall-cmd --get-services#查看 service 详细信息[root@rocky86 ~]# firewall-cmd --info-service=ssh#查看 service 描述信息 [root@rocky86 ~]# firewall-cmd --permanent --service=ssh --get-description#查看 service 简略描述[root@rocky86 ~]# firewall-cmd --permanent --service=ssh --get-short SSH#查看 service 端口[root@rocky86 ~]# firewall-cmd --permanent --service=ssh --get-ports 22/tcp 范例：查看详细信息 12345678910111213141516171819202122#查看默认Zone中的所有内容[root@rocky86 ~]# firewall-cmd --list-allblock (active) target: %%REJECT%% #目标 icmp-block-inversion: no #决定 icmp-blocks interfaces: eth0 eth1 #生效的网络设备 sources: #来源，IP或MAC services: http ssh #放行的服务 ports: #允许的目标端口，即本地开放的端口 protocols: #允许通过的协议 forward: no #允许转发的端口 masquerade: no #是否允许伪装（yes/no），可改写来源IP地址及mac地址 forward-ports: #允许转发的端口 source-ports: #允许的源端口 icmp-blocks: #ICMP类型，配合 icmp-block-inversion=no/yes一起使用 rich rules: #富规则 #查看指定zone中的所有内容 [root@rocky86 ~]# firewall-cmd --list-all --zone=block#查看所有 zone 的详细规则[root@rocky86 ~]# firewall-cmd --list-all-zones 范例：修改默认 Zone 123456789101112131415161718192021222324#查看默认 zone[root@rocky86 ~]# firewall-cmd --get-default-zonepublic#修改默认zone[root@rocky86 ~]# firewall-cmd --set-default-zone=blocksuccess#再次查看，修改成功[root@rocky86 ~]# firewall-cmd --get-default-zoneblock#block Zone 中没有任何开启的服务[root@rocky86 ~]# firewall-cmd --list-services[root@rocky86 ~]##在Windows下测试，SSH无法连接，但己连接的不受影响[C:\\~]$ ssh root@10.0.0.157Connecting to 10.0.0.157:22...Could not connect to &#x27;10.0.0.157&#x27; (port 22): Connection failed.#WEB服务不可用C:\\Users\\44301&gt;curl 10.0.0.157curl: (28) Failed to connect to 10.0.0.157 port 80 after 21022 ms: Timed out 范例：修改指定设备的zone 12345678[root@rocky86 ~]# firewall-cmd --get-zone-of-interface=eth0public[root@rocky86 ~]# firewall-cmd --change-interface=eth0 --zone=blocksuccess[root@rocky86 ~]# firewall-cmd --get-zone-of-interface=eth0block 范例：修改 Zone 中的 Service 12345678910111213141516171819202122232425#查看默认 zone[root@rocky86 ~]# firewall-cmd --get-default-zoneblock#添加 ssh 服务，不指定 zone，表示加到默认zone中[root@rocky86 ~]# firewall-cmd --add-service=sshsuccess#查看zone中的 service[root@rocky86 ~]# firewall-cmd --list-servicessh#再次测试，SSH服务可用，但WEB服务不通#添加 web 服务放行规则[root@rocky86 ~]# firewall-cmd --zone=block --add-service=httpsuccess#再次查看[root@rocky86 ~]# firewall-cmd --list-servicehttp ssh#测试，WEB服务可用C:\\Users\\44301&gt;curl 10.0.0.157&lt;h1&gt;test page from 10.0.0.157&lt;/h1&gt; 范例：增加非 service 规则 1234[root@rocky86 ~]# firewall-cmd --zone=block --add-port=8080/tcpsuccess[root@rocky86 ~]# firewall-cmd --zone=block --list-ports 8080/tcp 范例：自定义 zones 12345678910111213141516171819#自定义zone必须要加 --permanent 选项[root@rocky86 ~]# firewall-cmd --permanent --new-zone=test-zonesuccess#往zone 中添加 service[root@rocky86 ~]# firewall-cmd --zone=test-zone --add-service=sshError: INVALID_ZONE: test-zone#需要先 reload[root@rocky86 ~]# firewall-cmd --reloadsuccess#往 zone 中增加 service[root@rocky86 ~]# firewall-cmd --zone=test-zone --add-service=sshsuccess#查看[root@rocky86 ~]# firewall-cmd --zone=test-zone --list-servicesssh 范例：自定义 service 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@rocky86 ~]# firewall-cmd --new-service=test-serviceusage: see firewall-cmd man pageOption can be used only with --permanent.#添加 service[root@rocky86 ~]# firewall-cmd --permanent --new-service=test-servicesuccess#查看[root@rocky86 ~]# firewall-cmd --get-services | grep test-service[root@rocky86 ~]# firewall-cmd --permanent --get-services | grep test-servicetest-service#重载[root@rocky86 ~]# firewall-cmd --reloadsuccess#设置service的desc[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --set-description=&quot;test desc&quot;success#添加port[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --add-port=8080/tcpsuccess[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --add-port=8081/tcpsuccess#查看 port[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-ports8080/tcp 8081/tcp#查看 ports[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-port8080/tcp 8081/tcp#添加 protocol[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --add-protocol=stpsuccess#查看[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-protocolstp 范例：删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#删除protocol[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-protocolstp[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --remove-protocol=stpsuccess[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-protocol#删除ports[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-ports8080/tcp 8081/tcp[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --remove-port=8080/tcpsuccess[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --remove-port=8081/tcpsuccess[root@rocky86 ~]# firewall-cmd --permanent --service=test-service --get-ports#删除 services[root@rocky86 ~]# firewall-cmd --permanent --delete-service=test-servicesuccess[root@rocky86 ~]# firewall-cmd --permanent --info-service=test-serviceError: INVALID_SERVICE: test-service[root@rocky86 ~]# firewall-cmd --info-service=test-servicetest-serviceports: 8080/tcp 8081/tcpprotocols:source-ports:modules:destination:includes:helpers:#reload[root@rocky86 ~]# firewall-cmd --reloadsuccess[root@rocky86 ~]# firewall-cmd --info-service=test-serviceError: INVALID_SERVICE: test-service#删除zone[root@rocky86 ~]# firewall-cmd --permanent --delete-zone=test-zonesuccess[root@rocky86 ~]# firewall-cmd --permanent --get-zonesblock dmz drop external home internal libvirt nm-shared public trusted work[root@rocky86 ~]# firewall-cmd --get-zonesblock dmz drop external home internal libvirt nm-shared public test-zone trusted work#需要 reload[root@rocky86 ~]# firewall-cmd --get-zonesblock dmz drop external home internal libvirt nm-shared public trusted work#己删除[root@rocky86 ~]# firewall-cmd --get-zonesblock dmz drop external home internal libvirt nm-shared public trusted work 范例：持久化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@rocky86 ~]# firewall-cmd --add-service=httpsuccess[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client http ssh#重启服务[root@rocky86 ~]# systemctl restart firewalld.service#http service 丢失[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client ssh#添加的时候使用 --permanent 选项，只保存，但当前不生效[root@rocky86 ~]# firewall-cmd --permanent --add-service=httpsuccess#添加成功[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client ssh#但当前状态看不到[root@rocky86 ~]# firewall-cmd --permanent --list-servicescockpit dhcpv6-client http ssh#reload 或 重启服务[root@rocky86 ~]# systemctl restart firewalld.service[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client http ssh#添加 https[root@rocky86 ~]# firewall-cmd --add-service=httpssuccess#当前可见[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client http https ssh#永久状态不可见[root@rocky86 ~]# firewall-cmd --permanent --list-servicescockpit dhcpv6-client http ssh#将当前状态永久保存[root@rocky86 ~]# firewall-cmd --runtime-to-permanentsuccess#查看[root@rocky86 ~]# firewall-cmd --permanent --list-servicescockpit dhcpv6-client http https ssh#重启[root@rocky86 ~]# systemctl restart firewalld.service#查看[root@rocky86 ~]# firewall-cmd --list-servicescockpit dhcpv6-client http https ssh 范例：配置firewalld 12345678910systemctl mask iptablessystemctl mask ip6tablessystemctl status firewalldsystemctl enable firewalldsystemctl start firewalldfirewall-cmd --get-default-zonefirewall-cmd --set-default-zone=publicfirewall-cmd --permanent --zone=public --list-allfirewall-cmd --permanent --zone=public --add-port 8080/tcpfirewall-cmd ---reload 3 其它规则当基本firewalld语法规则不能满足要求时，可以使用以下更复杂的规则 rich-rules 富规则，功能强,表达性语言 Direct configuration rules 直接规则，灵活性差, 帮助：man 5 firewalld.direct 3.1 管理rich规则rich规则比基本的firewalld语法实现更强的功能，不仅实现允许&#x2F;拒绝，还可以实现日志syslog和auditd，也可以实现端口转发，伪装和限制速率 规则实施顺序： 该区域的端口转发，伪装规则 该区域的日志规则 该区域的允许规则 该区域的拒绝规则 每个匹配的规则生效，所有规则都不匹配，该区域默认规则生效 rich语法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556rule [source] &lt;!-- 源地址匹配 --&gt; [not] 取反匹配 address=&quot;IP地址[/子网掩码]&quot; | mac=&quot;MAC地址&quot; | ipset=&quot;ipset名称&quot; [destination] &lt;!-- 目标地址匹配 --&gt; [not] address=&quot;IP地址[/子网掩码]&quot; [service|port|protocol|icmp|masquerade|forward-port|source-port] &lt;!-- 过滤条件 --&gt; # service name=&quot;服务名&quot; # 示例: service name=&quot;http&quot; # port port=&quot;端口号&quot; protocol=&quot;tcp|udp&quot; # 示例: port port=&quot;8080&quot; protocol=&quot;tcp&quot; # protocol value=&quot;协议名&quot; # 示例: protocol value=&quot;icmp&quot; # 支持协议见: /etc/protocols # icmp-block name=&quot;ICMP类型&quot; # 示例: icmp-block name=&quot;echo-request&quot; # 查看支持类型: firewall-cmd --get-icmptypes # icmp-type name=&quot;ICMP类型&quot; # 示例: icmp-type name=&quot;echo-reply&quot; # 用于允许特定 ICMP 类型 # masquerade # 启用 IP 伪装（NAT），常用于私网访问外网 # forward-port port=&quot;端口&quot; protocol=&quot;tcp|udp&quot; # to-port=&quot;目标端口&quot; to-addr=&quot;目标IP&quot; # 示例: forward-port port=&quot;22&quot; protocol=&quot;tcp&quot; to-port=&quot;2222&quot; to-addr=&quot;192.168.1.10&quot; # source-port port=&quot;端口&quot; protocol=&quot;tcp|udp&quot; # 匹配源端口 # 示例: source-port port=&quot;1024-65535&quot; protocol=&quot;udp&quot; [log] &lt;!-- 日志记录 --&gt; log [prefix=&quot;日志前缀文本&quot;] [level=&quot;日志级别&quot;] [limit value=&quot;速率/持续时间&quot;] # 示例: log prefix=&quot;SSH-access&quot; level=&quot;info&quot; limit value=&quot;5/m&quot; [audit] &lt;!-- 审计记录（需 auditd 服务） --&gt; audit [limit value=&quot;速率/持续时间&quot;] # 示例: audit limit value=&quot;1/s&quot; [accept|reject|drop] &lt;!-- 最终动作 --&gt; # 三者选其一： # accept - 允许通过 # reject - 拒绝，并返回拒绝响应（如 TCP RST） # drop - 丢弃，不返回任何响应（静默丢弃） 格式 123456firewall-cmd [OPTIONS...]--list-rich-rules #列出 rich rule--add-rich-rule= #添加 rich rule--remove-rich-rule= #移除 rich rule--query-rich-rule= #查询 rich rule 3.2 rich规则实现拒绝从192.168.0.100的所有流量，当address 选项使用source 或 destination时，必须用family&#x3D; ipv4|ipv6 1firewall-cmd --permanent --zone=public --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.0.100/32 reject&#x27; 限制每分钟只有两个连接到ftp服务 1firewall-cmd --add-rich-rule=‘rule service name=ftp limit value=2/m accept’ 抛弃esp（ IPsec 体系中的一种主要协议）协议的所有数据包 1firewall-cmd --permanent --add-rich-rule=&#x27;rule protocol value=esp drop&#x27; 接受所有192.168.1.0&#x2F;24子网端口5900-5905范围的TCP流量 1firewall-cmd --permanent --zone=vnc --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.1.0/24 port port=5900-5905 protocol=tcp accept&#x27; 限制来自10.0.0.150 的 PING 1[root@rocky86 ~]# firewall-cmd --add-rich-rule=&#x27;rule family=ipv4 source address=10.0.0.150/32 protocol value=icmp limit value=3/m accept&#x27; rich日志规则 12345log [prefix=&quot;&lt;PREFIX TEXT&gt;&quot; [level=&lt;LOGLEVEL&gt;] [limit value=&quot;&lt;RATE/DURATION&gt;&quot;]&lt;LOGLEVEL&gt; 可以是emerg,alert, crit, error, warning, notice, info, debug.&lt;DURATION&gt; s：秒, m：分钟, h：小时, d：天audit [limit value=&quot;&lt;RATE/DURATION&gt;&quot;] 范例 1234567891011#接受ssh新连接，记录日志到syslog的notice级别，每分钟最多三条信息firewall-cmd --permanent --zone=work --add-rich-rule=&#x27;rule service name=&quot;ssh&quot;log prefix=&quot;ssh &quot; level=&quot;notice&quot; limit value=&quot;3/m&quot; accept#从2001:db8::/64子网的DNS连接在5分钟内被拒绝，并记录到日志到audit,每小时最大记录一条信息firewall-cmd --add-rich-rule=&#x27;rule family=ipv6 source address=&quot;2001:db8::/64&quot;service name=&quot;dns&quot; audit limit value=&quot;1/h&quot; reject&#x27; --timeout=300firewall-cmd --permanent --add-rich-rule=&#x27;rule family=ipv4 source address=172.25.X.10/32 service name=&quot;http&quot; log level=notice prefix=&quot;NEW HTTP &quot; limit value=&quot;3/s&quot; accept&#x27;firewall-cmd --reloadtail -f /var/log/messagescurl http://serverX.example.com 3.3 伪装和端口转发NAT网络地址转换，firewalld支持伪装和端口转发两种NAT方式 伪装NAT 1234firewall-cmd --permanent --zone=&lt;ZONE&gt; --add-masqueradefirewall-cmd --query-masquerade #检查是否允许伪装firewall-cmd --add-masquerade #允许防火墙伪装IPfirewall-cmd --remove-masquerade #禁止防火墙伪装IP 范例 123firewall-cmd --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.0.0/24 masquerade&#x27;等价于iptables -t nat -A POSTROUTING -s LocalNET ! -d LocalNet -j MASQUERADE 端口转发：将发往本机的特定端口的流量转发到本机或不同机器的另一个端口。通常要配合地址伪装才能实现 1firewall-cmd --permanent --zone=&lt;ZONE&gt; --add-forward-port=port=&lt;PORTNUMBER&gt;:proto=&lt;PROTOCOL&gt;[:toport=&lt;PORTNUMBER&gt;][:toaddr=] 说明：toport= 和 &#96;toaddr&#x3D; 至少要指定一个 范例： 1234#转发传入的连接9527/TCP，到防火墙的80/TCP到public zone 的192.168.0.254firewall-cmd --add-masquerade 启用伪装firewall-cmd --zone=public --add-forward-port=port=9527:proto=tcp:toport=80:toaddr=192.168.0.254 rich规则的port转发语法 1forward-port port=&lt;PORTNUM&gt; protocol=tcp|udp [to-port=&lt;PORTNUM&gt;] [to-addr=&lt;ADDRESS&gt;] 范例 1234567#转发从192.168.0.0/24来的，发往80/TCP的流量到防火墙的端口8080/TCPfirewall-cmd --zone=work --add-rich-rule=&#x27;rule family=ipv4 source address=192.168.0.0/24 forward-port port=80 protocol=tcp to-port=8080&#x27;firewall-cmd --permanent --add-rich-rule &#x27;rule family=ipv4 source address=172.25.X.10/32 forward-port port=443 protocol=tcp to-port=22&#x27;firewall-cmd --reloadssh -p 443 serverX.example.com 范例：限制ssh服务非标准端口访问 1234567891011121314151617cp /usr/lib/firewalld/services/ssh.xml /etc/firewalld/services/ssh.xmlvim /etc/firewalld/services/ssh.xml&lt;port protocol=&quot;tcp&quot; port=&quot;999&quot;/&gt;systemctl restart sshd.servicesystemctl status -l sshd.servicesealert -a /var/log/audit/audit.logsemanage port -a -t ssh_port_t -p tcp 999systemctl restart sshd.servicess -tulpn | grep sshdfirewall-cmd --permanent --zone=work --add-source=172.25.X.0/24firewall-cmd --permanent --zone=work --add-port=999/tcpfirewall-cmd --reload","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"防火墙","slug":"防火墙","permalink":"https://aquapluto.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"}]},{"title":"iptables","slug":"Linux/firewall/iptables","date":"2025-08-21T03:02:08.000Z","updated":"2025-08-28T12:33:05.228Z","comments":true,"path":"Linux/firewall/iptables/","permalink":"https://aquapluto.github.io/Linux/firewall/iptables/","excerpt":"","text":"1 iptables的组成iptables由五个表table和五个链chain以及一些规则组成 链 chain： 内置链：对应内核中的每一个勾子函数 （INPUT，OUTPUT，FORWARD，PREROUTING，POSTROUTING） 自定义链：用于对内置链进行扩展或补充，可实现更灵活的规则组织管理机制；只有Hook钩子调用自定义链时，才生效 五个表table：filter、nat、mangle、raw、security filter：过滤规则表，根据预定义的规则过滤符合条件的数据包，默认表，实现防火墙功能 nat：network address translation 地址转换规则表，实现共享上网，端口和IP映射 mangle：修改数据标记位规则表，即对数据包进行重构或修改，比如服务类型、TTL（生存时间）等字段 raw：关闭启用的数据连接跟踪机制，加快封包穿越防火墙速度，决定了数据包是否应被状态跟踪机制（如连接跟踪）处理 security：用于强制访问控制（MAC）网络规则，由Linux安全模块（如SELinux）实现 表的优先级由高到低的顺序为 1security --&gt;raw--&gt;mangle--&gt;nat--&gt;filter 表和链对应关系 filter表 INPUT：负责过滤所有目标地址是本机地址的数据包，通俗来讲，就是过滤进入主机的数据包（能否让数据包进入服务器） FORWARD：路过，负责转发流经主机的数据包，起转发的作用，和NAT关系大，在LVS NAT模式中，net.ipv4.ip_forward&#x3D;0 OUTPUT：处理所有源地址是本机地址的数据包，通俗来讲，就是处理从主机发出去的数据包，当数据包从本地主机发送至外部网络时，确定哪些本地系统发出的数据包可以被发送到外部网络 nat表 OUTPUT：和主机放出去的数据包有关，改变主机发出数据包的目的地址 PREROUTING：在数据包到达防火墙时，进行路由判断之前执行的规则，作用是改变数据包的目的地址、目的端口等 POSTROUTING：在数据包离开防火墙时进行路由判断之后执行的规则，作用是改变数据包的源地址、源端口等 数据包过滤匹配流程 内核中数据包的传输过程 当一个数据包进入网卡时，数据包首先进入PREROUTING链，内核根据数据包目的IP判断是否需要转送出去 如果数据包是进入本机的，数据包就会沿着图向下移动，到达INPUT链。数据包到达INPUT链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包经过OUTPUT链，然后到达POSTROUTING链输出 如果数据包是要转发出去的，且内核允许转发，数据包就会向右移动，经过FORWARD链，然后到达POSTROUTING链输出 范例：查看表匹配哪些链 12345678[root@centos8 ~]#iptables -vnL -t filter[root@centos8 ~]#iptables -vnL -t nat[root@centos8 ~]#iptables -vnL -t mangle[root@centos8 ~]#iptables -vnL -t raw[root@centos8 ~]#iptables -vnL -t security#CentOS 6 nat表不支持INPUT链[root@centos6 ~]#iptables -vnL -t nat 2 iptables规则说明2.1 iptables规则组成规则rule：根据规则的匹配条件尝试匹配报文，对匹配成功的报文根据规则定义的处理动作作出处理，规则在链接上的次序即为其检查时的生效次序 匹配条件：默认为与条件，同时满足 基本匹配：IP，端口，TCP的Flags（SYN,ACK等） 扩展匹配：通过复杂高级功能匹配 处理动作：称为target，跳转目标 内建处理动作：ACCEPT,DROP,REJECT,SNAT,DNAT,MASQUERADE,MARK,LOG… 自定义处理动作：自定义chain，利用分类管理复杂情形 规则要添加在链上，才生效；添加在自定义链上不会自动生效 白名单:只有指定的特定主机可以访问,其它全拒绝 黑名单:只有指定的特定主机拒绝访问,其它全允许，默认方式 2.2 iptables规则添加时考量点 要实现哪种功能：判断添加在哪张表上 报文流经的路径：判断添加在哪个链上 报文的流向：判断源和目的 匹配规则：业务需要 2.3 本章学习环境准备CentOS 7，8： 12345systemctl stop firewalld.servicesystemctl disable firewalld. service#或者systemctl disable --now firewalld. service CentOS 6： 12service iptables stopchkconfig iptables off 注意：如果不是最小化安装，在禁用 frewalld 之后，再次重启还是能看到防火墙规则，这是由KVM启动的 12#卸载kvm功能[root@rocky86 ~]# yum remove qemu-kvm Ubuntu 1systemctl disable --now ufw 2.4 规则说明iptables 防火墙中的规则，在生效时会按照顺序，从上往下生效，当前一条规则命中后，不再继续往下匹配。 如果多条规则里面，匹配条件中有交集，或者有包含关系，则这些规则，要注意前后顺序，范围小的，需要精确匹配的，要往前放，范围大的，往后放，负责兜底的，放在最后。 如果多条规则里面，匹配条件没有交集，彼此不会互相影响，则无所谓前后顺序，但是从效率上来讲，更容易命中，范围大的放在前面。 3 iptables用法说明 格式 1234567891011121314iptables [-t table] &#123;-A|-C|-D&#125; chain rule-specificationiptables [-t table] -I chain [rulenum] rule-specificationiptables [-t table] -R chain rulenum rule-specificationiptables [-t table] -D chain rulenumiptables [-t table] -S [chain [rulenum]]iptables [-t table] &#123;-F|-L|-Z&#125; [chain [rulenum]] [options...]iptables [-t table] -N chainiptables [-t table] -X [chain]iptables [-t table] -P chain targetiptables [-t table] -E old-chain-name new-chain-namerule-specification = [matches...] [target]match = -m matchname [per-match-options]target = -j targetname [per-target-options] iptables命令格式详解 1iptables [-t table] SUBCOMMAND chain [-m matchname [per-match-options]] -j targetname [per-target-options] 3.1 table指定表：raw, mangle, nat, [filter]默认 3.2 COMMAND3.2.1 链管理类12345-N：new, 自定义一条新的规则链-E：重命名自定义链；引用计数不为0的自定义链不能够被重命名，也不能被删除-X：delete，删除自定义的空的规则链-P：Policy，设置默认策略；对filter表中的链而言，其默认策略有：ACCEPT：接受, DROP：丢弃-C：检查链上的规则是否正确 3.2.2 查看类1234567891011-L：list, 列出指定鏈上的所有规则，本选项须置后-n：numberic，以数字格式显示地址和端口号-v：verbose，详细信息-vv 更详细-x：exactly，显示计数器结果的精确值,而非单位转换后的易读值--line-numbers：显示规则的序号-S selected,以iptables-save命令格式显示链上规则常用组合-vnL-vvnxL --line-numbers 范例：查看表匹配哪些链 在使用 -L 选项查看规则时，如果规则不为空，单独 -L 选项显示时有可能会很慢，这是因为需要对主机名和服务名进行反解导致的，可以加 -n 选项来规避 默认 filter 表，可以不指定 12345678910111213141516171819[root@centos8 ~]#iptables -vnL -t filterChain INPUT (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destinationpkts：在当前规则中，被命中的数据包数量bytes：在当前规则中，被命中的总流量大小target：动作，目标，DROP 表示丢弃prot：具体协议opt：选项in：数据从哪个网络设备进来out：数据从哪个网络设备上出去source：源，可以是主机IP，网段，anywhere 表示不受限 destination：目标，可以是主机IP，网段，anywherer 表示不受限 范例：指定表和链 1[root@ubuntu ~]# iptables -t filter -vnL INPUT 3.2.3 规则管理类1234567891011-A：append，往链上追加规则-I：insert, 插入，要指明插入至的规则编号，默认为第一条-D：delete，删除 (1) 指明规则序号 (2) 指明规则本身-R：replace，替换指定链上的指定规则编号-F：flush，清空指定的规则链-Z：zero，置零 iptables的每条规则都有两个计数器 (1) 匹配到的报文的个数 (2) 匹配到的所有报文的大小之和 范例 1234567#这条命令指定了来自源地址为 10.0.0.1 的数据包将被接受（ACCEPT）。换句话说，它允许了从地址为 10.0.0.1 的主机发送到你的主机的数据包通过防火墙。iptables -t filter -A INPUT -s 10.0.0.1 -j ACCEPT#这条命令指定了目标地址为 10.0.0.1 的数据包将被接受（ACCEPT）。换句话说，它允许了发送到地址为 10.0.0.1 的主机的数据包通过防火墙。iptables -t filter -A INPUT -d 10.0.0.1 -j ACCEPT#因此，这两条命令的区别在于它们是针对数据包的不同方向进行匹配的。第一条命令是匹配从指定源地址发送到你的主机的数据包，而第二条命令是匹配发送到指定目标地址的数据包。 范例 1234567891011121314151617181920212223242526#在INPUT 链的 filter 表上设置过滤规则，将来自 10.0.0.150 的数据包丢弃掉[root@ubuntu ~]# iptables -t filter -A INPUT -s 10.0.0.150 -j DROP#将来自10网段的数据包丢弃掉[root@ubuntu ~]# iptables -t filter -A INPUT -s 10.0.0.0/24 -j DROP#删除第3条规则，根据规则编号删除[root@ubuntu ~] iptables -t filter -D INPUT 3#在最前面插入 127.1 的ACCEPT 规则[root@ubuntu ~]# iptables -t filter -I INPUT -s 127.0.0.1 -j ACCEPT#指定入口设备是本地回环网卡[root@ubuntu ~]# iptables -t filter -I INPUT -i lo -j ACCEPT#替换第一条规则[root@ubuntu ~]# iptables -t filter -R INPUT 1 -s 127.0.0.1 -j ACCEPT#清空INPUT链上的第一条规则统计数据 [root@ubuntu ~]# iptables -t filter -Z INPUT 1#清空表上的所有统计数据[root@ubuntu ~]# iptables -t filter -Z#修改默认规则[root@ubuntu ~]# iptables -P FORWARD DROP 范例：黑名单和白名单 12345678910111213141516#只允许 10.0.0.1 连接[root@ubuntu ~]# iptables -t filter -A INPUT -s 10.0.0.1 -j ACCEPT#其它机器数据全部拒绝，不指定匹配条件，则全部拒绝[root@ubuntu ~]# iptables -t filter -A INPUT -j REJECT#这种情况下，ACCEPT规则一定要写在前面[root@ubuntu ~]# iptables -t filter -nL INPUT --line-numberChain INPUT (policy ACCEPT)num target prot opt source destination 1 ACCEPT all -- 10.0.0.1 0.0.0.0/0 2 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachable#由于拒绝了10.0.0.1 之外的主机，本机也无法使用[root@ubuntu ~]# ping 127.1PING 127.1 (127.0.0.1) 56(84) bytes of data. 3.3 chain指定链：PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING 3.4 parameter匹配条件 基本：通用的，PARAMETERS 扩展：需加载模块，MATCH EXTENTIONS 3.4.1 基本匹配条件基本匹配条件：无需加载模块，由iptables&#x2F;netfilter自行提供 1234567[!] -s, --source address[/mask][,...]：源IP地址或者不连续的IP地址[!] -d, --destination address[/mask][,...]：目标IP地址或者不连续的IP地址[!] -p, --protocol protocol：指定协议，可使用数字如0（all） protocol: tcp, udp, icmp, icmpv6, udplite,esp, ah, sctp, mh or“all“ 参看：/etc/protocols[!] -i, --in-interface name：报文流入的接口；只能应用于数据报文流入环节，只应用于INPUT、FORWARD、PREROUTING链[!] -o, --out-interface name：报文流出的接口；只能应用于数据报文流出的环节，只应用于FORWARD、OUTPUT、POSTROUTING链 范例 123456[root@centos8 ~]#iptables -A INPUT -s 10.0.0.6,10.0.0.10 -j REJECT[root@centos8 ~]#iptables -I INPUT -i lo -j ACCEPT[root@centos8 ~]#curl 127.0.0.110.0.0.8[root@centos8 ~]#curl 10.0.0.810.0.0.8 范例：根据协议过滤 12#接受10.0.0.6上的除icmp协议的其他协议[root@centos8 ~]#iptables -I INPUT 2 -s 10.0.0.6 ! -p icmp -j ACCEPT 范例：规则取反 12#设置取反规则，除了 10.0.0.150 来的数据，其它都拒绝[root@ubuntu ~]# iptables -t filter -R INPUT 2 ! -s 10.0.0.150 -j REJECT 范例：根据目标地址匹配（目标地址是机器上有多网卡或者一网卡有多个地址时才使用） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@rocky8 ~]#ip a a 10.0.0.10/24 dev eth0 label eth0:1[root@rocky8 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:d2:a3:ac brd ff:ff:ff:ff:ff:ff inet 10.0.0.179/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet 10.0.0.10/24 scope global secondary eth0:1 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fed2:a3ac/64 scope link valid_lft forever preferred_lft forever#现在能访问10[root@centos8 ~]#ping 10.0.0.10PING 10.0.0.10 (10.0.0.10) 56(84) bytes of data.64 bytes from 10.0.0.10: icmp_seq=1 ttl=64 time=1.23 ms64 bytes from 10.0.0.10: icmp_seq=2 ttl=64 time=10.8 ms#任何来访问10的IP地址都拒绝，不管什么协议[root@rocky8 ~]#iptables -A INPUT -d 10.0.0.10 -j REJECT[root@rocky8 ~]#iptables -vnLChain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 0.0.0.0/0 10.0.0.10 reject-with i #现在不能ping了[root@centos8 ~]#ping 10.0.0.10PING 10.0.0.10 (10.0.0.10) 56(84) bytes of data.From 10.0.0.10 icmp_seq=1 Destination Port UnreachableFrom 10.0.0.10 icmp_seq=2 Destination Port Unreachable#远程也不能链接[root@centos8 ~]#ssh 10.0.0.10^C#来自icmp协议的拒绝。其他协议可以访问[root@rocky8 ~]#iptables -R INPUT 1 -d 10.0.0.10 -p icmp -j REJECT#还是不能ping[root@centos8 ~]#ping 10.0.0.10[root@centos8 ~]#ping 10.0.0.10PING 10.0.0.10 (10.0.0.10) 56(84) bytes of data.From 10.0.0.10 icmp_seq=1 Destination Port UnreachableFrom 10.0.0.10 icmp_seq=2 Destination Port Unreachable#可以远程连接[root@centos8 ~]#ssh 10.0.0.10The authenticity of host &#x27;10.0.0.10 (10.0.0.10)&#x27; can&#x27;t be established.ECDSA key fingerprint is SHA256:LjI4Fn8mPxXYJjqouGvxROkMgF9Nv5fDZpHKfzM8hRs.Are you sure you want to continue connecting (yes/no/[fingerprint])? 3.4.2 扩展匹配条件扩展匹配条件：需要加载扩展模块（&#x2F;usr&#x2F;lib64&#x2F;xtables&#x2F;*.so），方可生效 扩展模块的查看帮助 ：man iptables-extensions 扩展匹配条件： 隐式扩展 显式扩展 3.4.2.1 隐式扩展iptables 在使用 -p 选项指明了特定的协议时，无需再用 -m 选项指明扩展模块的扩展机制，不需要手动加载扩展模块 tcp，upd，icmp 这三个协议是可以用 -m 指定的模块，但同时，也可以在基本匹配里面用 -p 来指定这几个协议 3.4.2.1.1 tcp协议1234567891011[!] --source-port, --sport port[:port]：匹配报文源端口,可为端口连续范围[!] --destination-port,--dport port[:port]：匹配报文目标端口,可为连续范围[!] --tcp-flags mask comp mask 需检查的标志位列表，用,分隔 , 例如 SYN,ACK,FIN,RST comp 在mask列表中必须为1的标志位列表，无指定则必须为0，用,分隔tcp协议的扩展选项--tcp-flags SYN,ACK,FIN,RST SYN #检查SYN,ACK,FIN,RST四个标志位，其中SYN值为1，其它值为0，表示第一次握手--tcp-flags SYN,ACK,FIN,RST SYN,ACK #第二次握手[!] --syn：用于匹配第一次握手, 相当于：--tcp-flags SYN,ACK,FIN,RST SYN#错误包--tcp-flags ALL ALL --tcp_flags ALL NONE 范例：用tcp 协议和目标端口拒绝ssh服务 12#拒绝来自于150 的，其目标IP是110，目标端口是 21 到 23 的 tcp协议数据包[root@ubuntu ~]# iptables -t filter -A INPUT -s 10.0.0.150 -d 10.0.0.110 -p tcp --dport 21:23 -j REJECT 范例：拒绝ssh服务，接受http服务 12345678910111213141516171819202122232425[root@rocky8 ~]#yum -y install nginx[root@rocky8 ~]#systemctl start nginx[root@rocky8 ~]#ss -ntlState Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 128 0.0.0.0:80 0.0.0.0:* LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 128 [::]:80 [::]:* LISTEN 0 128 [::]:22 [::]:*[root@rocky8 ~]#iptables -A INPUT -s 10.0.0.1 -j ACCEPT[root@rocky8 ~]#iptables -A INPUT -p tcp --dport 80 -j ACCEPT[root@rocky8 ~]#iptables -A INPUT -p tcp -d 10.0.0.179 -j REJECT(iptables -A INPUT -j REJECT)[root@rocky8 ~]#iptables -vnLChain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 288 17760 ACCEPT all -- * * 10.0.0.1 0.0.0.0/0 0 0 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 0 0 REJECT tcp -- * * 0.0.0.0/0 10.0.0.179 reject-with icmp-port-unreachable( 0 0 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachable )[root@centos8 ~]#ssh 10.0.0.179ssh: connect to host 10.0.0.179 port 22: Connection refused[root@centos8 ~]#curl 10.0.0.179&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.1//EN&quot; &quot;http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd&quot;&gt;.......... 范例：根据TCP标志位过滤 12#拒绝第一次握手[root@ubuntu ~]# iptables -t filter -A INPUT -s 10.0.0.150 -d 10.0.0.110 -p tcp --tcp-flags SYN,ACK,FIN,RST SYN -j REJECT 范例：TCP隐式扩展 1iptables -A INPUT -p tcp --dport 80 -j ACCEPT 3.4.2.1.2 udp协议12[!] --source-port, --sport port[:port]：匹配报文的源端口或端口范围[!] --destination-port,--dport port[:port]：匹配报文的目标端口或端口范围 3.4.2.1.3 icmp协议1234[!] --icmp-type &#123;type[/code]|typename&#125; type/code 0/0 echo-reply #icmp应答 8/0 echo-request #icmp请求 范例 123[root@centos8 ~]#iptables -A INPUT -s 10.0.0.6 -p tcp --dport 21:23 -j REJECT[root@centos8 ~]#iptables -A INPUT -p tcp --syn -j REJECT[root@centos8 ~]#iptables -A INPUT -s 10.0.0.6 -p icmp --icmp-type 8 -j REJECT 范例：我能ping对方，对方不能ping我 1234iptables -A INPUT -s 10.0.0.0/24 -j REJECTiptables -A INPUT -p icmp --icmp-type 0 -j ACCEP#对方主机发出的 ICMP Echo Request 报文将被第一条规则拒绝，而你的本地主机发出的 ICMP Echo Reply 报文（响应对方的 ping 请求）将被第二条规则接受。 范例：拒绝ICMP的请求包 1[root@ubuntu ~]# iptables -t filter -A INPUT -s 10.0.0.150 -p icmp --icmp-type 8 -j REJECT 3.4.2.2 显式扩展及相关模块显示扩展即必须使用-m选项指明要调用的扩展模块名称，需要手动加载扩展模块 1[-m matchname [per-match-options]] 扩展模块的使用帮助： CentOS 7,8: man iptables-extensions CentOS 6: man iptables 3.4.2.2.1 multiport扩展以离散方式定义多端口匹配,最多指定15个端口 12345678#指定多个源端口[!] --source-ports,--sports port[,port|,port:port]...# 指定多个目标端口[!] --destination-ports,--dports port[,port|,port:port]...#多个源或目标端[!] --ports port[,port|,port:port]... 范例 12[root@centos8 ~]#iptables -A INPUT -s 172.16.0.0/16 -d 172.16.100.10 -p tcp -m multiport --dports 20:22,80 -j ACCEPT[root@centos8 ~]#iptables -A INPUT -s 10.0.0.6 -p tcp -m multiport --dports 445,139 -j REJECT 3.4.2.2.2 iprange扩展指明连续的（但一般不是整个网络）ip地址范围 12[!] --src-range from[-to] 源IP地址范围[!] --dst-range from[-to] 目标IP地址范围 范例： 1iptables -A INPUT -d 172.16.1.100 -p tcp --dport 80 -m iprange --src-range 172.16.1.5-172.16.1.10 -j DROP 3.4.2.2.3 mac扩展mac 模块可以指明源MAC地址,，适用于：PREROUTING, FORWARD，INPUT chains 1[!] --mac-source XX:XX:XX:XX:XX:XX 范例 1234#仅有 mac 地址是 00:0c:29:f3:44:9a 的主机才能访问[root@ubuntu ~]# iptables -t filter -A INPUT -d 10.0.0.110 -m mac --mac-source 00:0c:29:f3:44:9a -j ACCEPT#其它主机拒绝[root@ubuntu ~]# iptables -t filter -A INPUT -d 10.0.0.110 -j REJECT 3.4.2.2.4 string扩展对报文中的应用层数据做字符串模式匹配检测 1234567--algo &#123;bm|kmp&#125; 字符串匹配检测算法 bm：Boyer-Moore kmp：Knuth-Pratt-Morris--from offset 开始偏移--to offset 结束偏移[!] --string pattern 要检测的字符串模式[!] --hex-string pattern 要检测字符串模式，16进制格式 范例：数据报文结构中，前62位都是报文头(22位），IP（10位)和TCP(20位)，不会有敏感词汇，所以从第62位开始检查 请求报文中（INPUT）访问的形式是index.html，不会带有敏感词汇，是回来的报文带有（OUTPUT） 12345678910111213141516[root@rocky ~]# curl 10.0.0.206/bd.htmlbaidu[root@rocky ~]# curl 10.0.0.206/gg.htmlgoogle#设置出口规则，在返回的数据包中，跳过前62字节的报文头，如果内容中出现 google，则拒绝返回[root@ubuntu ~]# iptables -t filter -A OUTPUT -m string --algo kmp --from 62 --string &quot;google&quot; -j REJECT#再次测试[root@rocky ~]# curl 10.0.0.110/bd.htmlbaidu#google 无法返回[root@rocky ~]# curl 10.0.0.110/gg.html#区分大小写，大写不会被命中[root@rocky ~]# curl 10.0.0.110/GG.htmlGOOGLE 3.4.2.2.5 time扩展注意：CentOS 8 此模块有问题 根据将报文到达的时间与指定的时间范围进行匹配 123456789--datestart YYYY[-MM[-DD[Thh[:mm[:ss]]]]] 日期--datestop YYYY[-MM[-DD[Thh[:mm[:ss]]]]]--timestart hh:mm[:ss] 时间--timestop hh:mm[:ss][!] --monthdays day[,day...] 每个月的几号[!] --weekdays day[,day...] 星期几，1 – 7 分别表示星期一到星期日--kerneltz：内核时区（当地时间），不建议使用，CentOS 7版本以上系统默认为 UTC注意： centos6 不支持kerneltz ，--localtz指定本地时区(默认)说明：UTC时间在原来时间上减8 范例: CentOS 8 的 time模块问题 1234[root@centos8 ~]#rpm -ql iptables |grep time/usr/lib64/xtables/libxt_time.so[root@centos8 ~]#iptables -A INPUT -m time --timestart 12:30 --timestop 13:30 -j ACCEPTiptables v1.8.4 (nf_tables): Couldn&#x27;t load match `time&#x27;:No such file or directory 范例 123456789101112131415161718#在ubuntu中设置[root@ubuntu ~]# iptables -t filter -A INPUT -m time --timestart 01:00 --timestop 14:00 -s 10.0.0.150 -j REJECT#在150上PING 主机，被拒绝[root@rocky ~]# ping 10.0.0.206 -c1PING 10.0.0.206 (10.0.0.206) 56(84) bytes of data.From 10.0.0.206 icmp_seq=1 Destination Port Unreachable#修改主机上的时间，让当前时间不在iptables 规则范围内[root@ubuntu ~]# date +&quot;%F %T&quot;2023-06-10 21:02:46[root@ubuntu ~]# date -s &quot;+1 hour&quot;Sat Jun 10 10:03:09 PM CST 2023[root@ubuntu ~]# date +&quot;%F %T&quot;2023-06-10 22:03:11[root@rocky ~]# ping 10.0.0.206 -c1PING 10.0.0.206 (10.0.0.206) 56(84) bytes of data.64 bytes from 10.0.0.206: icmp_seq=1 ttl=64 time=0.573 ms 3.4.2.2.6 connlimit扩展根据每客户端IP做并发连接数数量匹配 可防止Dos(Denial of Service，拒绝服务)攻击 12--connlimit-upto N #连接的数量小于等于N时匹配--connlimit-above N #连接的数量大于N时匹配 范例 12#如果并发连接数超过2个，则拒绝连接iptables -A INPUT -d 172.16.100.10 -p tcp --dport 22 -m connlimit --connlimit-above 2 -j REJECT 3.4.2.2.7 limit扩展基于收发报文的速率做匹配 , 令牌桶过滤器（限制流量） connlimit 扩展是限制单个客户端对服务器的并发连接数，limit 扩展是限制服务器上所有的连接数 12--limit-burst number #前多少个包不限制--limit #[/second|/minute|/hour|/day] #在一个时间区间内能接收的连接数# 范例 1234567891011121314151617181920212223242526272829#添加 icmp 放行规则，前10个不处理，后面每分钟放行20个[root@centos8 ~]#iptables -A INPUT -p icmp -m limit --limit-burst 10 --limit 20/minute -j ACCEPT#兜底的拒绝规则[root@centos8 ~]#iptables -A INPUT -p icmp -j REJECT[root@centos6 ~]#ping 10.0.0.8PING 192.168.39.8 (192.168.39.8) 56(84) bytes of data.64 bytes from 192.168.39.8: icmp_seq=1 ttl=64 time=0.779 ms64 bytes from 192.168.39.8: icmp_seq=2 ttl=64 time=0.436 ms64 bytes from 192.168.39.8: icmp_seq=3 ttl=64 time=0.774 ms64 bytes from 192.168.39.8: icmp_seq=4 ttl=64 time=0.391 ms64 bytes from 192.168.39.8: icmp_seq=5 ttl=64 time=0.441 ms64 bytes from 192.168.39.8: icmp_seq=6 ttl=64 time=0.356 ms64 bytes from 192.168.39.8: icmp_seq=7 ttl=64 time=0.553 ms64 bytes from 192.168.39.8: icmp_seq=8 ttl=64 time=0.458 ms64 bytes from 192.168.39.8: icmp_seq=9 ttl=64 time=0.459 ms64 bytes from 192.168.39.8: icmp_seq=10 ttl=64 time=0.479 ms64 bytes from 192.168.39.8: icmp_seq=11 ttl=64 time=0.450 ms64 bytes from 192.168.39.8: icmp_seq=12 ttl=64 time=0.471 ms64 bytes from 192.168.39.8: icmp_seq=13 ttl=64 time=0.531 ms64 bytes from 192.168.39.8: icmp_seq=14 ttl=64 time=0.444 msFrom 192.168.39.8 icmp_seq=15 Destination Port Unreachable64 bytes from 192.168.39.8: icmp_seq=16 ttl=64 time=0.668 msFrom 192.168.39.8 icmp_seq=17 Destination Port UnreachableFrom 192.168.39.8 icmp_seq=18 Destination Port Unreachable64 bytes from 192.168.39.8: icmp_seq=19 ttl=64 time=0.692 msFrom 192.168.39.8 icmp_seq=20 Destination Port UnreachableFrom 192.168.39.8 icmp_seq=21 Destination Port Unreachable64 bytes from 192.168.39.8: icmp_seq=22 ttl=64 time=0.651 ms 3.4.2.2.8 state扩展state 扩展模块，可以根据”连接追踪机制“去检查连接的状态，较耗资源 conntrack机制：追踪本机上的请求和响应之间的关系 格式 1[!] --state state state状态类型： NEW：新发出请求；连接追踪信息库中不存在此连接的相关信息条目，因此，将其识别为第一次发出的请求 ESTABLISHED：NEW状态之后，连接追踪信息库中为其建立的条目失效之前期间内所进行的通信状态 RELATED：新发起的但与已有连接相关联的连接，如：ftp协议中的数据连接与命令连接之间的关系 INVALID：无效的连接，如flag标记不正确 UNTRACKED：未进行追踪的连接，如：raw表中关闭追踪 查看是否启用了连接追踪机制 1234567#当前还没有启用[root@rocky ~]# lsmod | grep nf_conntrack#启用连接跟踪机制后，会自动生成此文件，[root@rocky ~]# cat /proc/net/nf_conntrack#只要系统中某处要使用到连接追踪机制，连接追踪机制相关模块会自动加载 已经追踪到的并记录下来的连接信息库 1234567891011[root@centos8 ~]#cat /proc/net/nf_conntrackipv4 2 icmp 1 29 src=10.0.0.206 dst=10.0.0.150 type=8 code=0 id=1 src=10.0.0.150 dst=10.0.0.206 type=0 code=0 id=1 mark=0 zone=0 use=2ipv4 2 tcp 6 299 ESTABLISHED src=10.0.0.150 dst=10.0.0.1 sport=22 dport=51848 src=10.0.0.1 dst=10.0.0.150 sport=51848 dport=22 [ASSURED] mark=0 zone=0 use=2ipv4 #第一列 网络层协议名称2 #第二列 网络层协议号icmp #第三列 传输层协议名称1 #第四列 传输层协议号29 #第五列 无后续包进入时无效的秒数，即老化时间ESTABLISHED #第六列 该连接的连接状态，不是所有协议都有此列值 #后续都是k=v 格式的连接参数和选项 连接追踪功能所能够容纳的最大连接数量 12345#两文件，不管修改哪个，没改的会随之改变[root@centos8 ~]#cat /proc/sys/net/netfilter/nf_conntrack_max26624[root@centos8 ~]#cat /proc/sys/net/nf_conntrack_max26624 己追踪的连接数量 12[root@centos8 ~]#cat /proc/sys/net/netfilter/nf_conntrack_count10 不同的协议的连接追踪时长，单位为秒，超时后会踢出己监控的队列 1[root@centos8 ~]#ll /proc/sys/net/netfilter/*_timeout_* 说明： 连接跟踪，需要加载模块： modprobe nf_conntrack_ipv4 当服务器连接多于最大连接数时dmesg 可以观察到 ：kernel: ip_conntrack: table full, dropping packet错误,并且导致建立TCP连接很慢。 各种状态的超时后，链接会从表中删除 范例: 不允许远程主机 10.0.0.7 访问本机,但本机可以访问10.0.0.7 12345678[root@centos8 ~]#iptables -S-P INPUT ACCEPT-P FORWARD ACCEPT-P OUTPUT ACCEPT-A INPUT -s 10.0.0.1/32 -j ACCEPT-A INPUT -m state --state ESTABLISHED -j ACCEPT-A INPUT ! -s 10.0.0.7/32 -m state --state NEW -j ACCEPT-A INPUT -j REJECT --reject-with icmp-port-unreachable 范例 12iptables -A INPUT -d 172.16.1.10 -p tcp -m multiport --dports 22,80 -m state --state NEW,ESTABLISHED -j ACCEPTiptables -A OUTPUT -s 172.16.1.10 -p tcp -m multiport --sports 22,80 -m state --state ESTABLISHED -j ACCEPT 范例：新用户不能连，老用户可以连 123456789101112#老用户（不能断开，断开就不是已经建立联机的用户了）[root@centos7 ~]#ping 10.0.0.176PING 10.0.0.176 (10.0.0.176) 56(84) bytes of data.64 bytes from 10.0.0.176: icmp_seq=1 ttl=64 time=0.830 ms[root@centos8 ~]#iptables -A INPUT -m state --state ESTABLISHED -j ACCEPT[root@centos8 ~]#iptables -A INPUT -m state --state NEW -j REJECT#新用户[root@rocky8 ~]#ping 10.0.0.176PING 10.0.0.176 (10.0.0.176) 56(84) bytes of data.From 10.0.0.176 icmp_seq=1 Destination Port Unreachable 案例：开放被动模式的ftp服务 CentOS 8 此模块有bug (1) 装载ftp连接追踪的专用模块： 跟踪模块路径： &#x2F;lib&#x2F;modules&#x2F;kernelversion&#x2F;kernel&#x2F;net&#x2F;netfilter 123vim /etc/sysconfig/iptables-configIPTABLES_MODULES=“nf_conntrack_ftp&quot;modprobe nf_conntrack_ftp (2) 放行请求报文： 命令连接：NEW, ESTABLISHED 数据连接：RELATED, ESTABLISHED 12iptables -I INPUT -d LocalIP -p tcp -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -A INPUT -d LocalIP -p tcp --dport 21 -m state --state NEW -j ACCEPT (3) 放行响应报文： 范例：开放被动模式的ftp服务示例 1iptables -I OUTPUT -s LocalIP -p tcp -m state --state ESTABLISHED -j ACCEPT 范例：开放被动模式的ftp服务示例 12345678910yum install vsftpdsystemctl start vsftpdmodprobe nf_conntrack_ftpiptables -Fiptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPTiptables -A INPUT -p tcp --dport 21 -m state --state NEW -j ACCEPTiptables -A OUTPUT -m state --state ESTABLISHED -j ACCEPTiptables -P INPUT DROPiptables -P OUTPUT DROPiptables -vnL 3.5 Target处理动作1-j targetname [per-target-options] （1）简单动作 Target 说明 ACCEPT 接受，命中此规则后，不再对比当前链的其它规则，进入下一个链 DROP 抛弃，不回应，命中此规则后，该连接直接中断，发送方收不到任何回应 REJECT 拒绝，有回应，命中此规则后，该连接直接中断，发送方能收到拒绝回应 （2）扩展动作 Target 说明 REJECT --reject-with：icmp-port-unreachable默认 RETURN 返回调用链，在当前链中不再继续匹配，返回到主链，一般用于自定义规则链中，假如input链定义了第一条和第二条和第四条规则 第三条是自定义链 自定义链有三条规则 如果自定义链中第二条规则加了return 数据报文碰到return并满足 就不会执行自定义链的第三条规则 而是执行input链的第四条规则 REDIRECT 端口重定向 LOG 记录日志，dmesg，非中断target，本身不拒绝和允许，放在拒绝和允许规则前，将被命中的数据的日志记录在&#x2F;var&#x2F;log&#x2F;messages系统日志中- --log-level level：debug, info, notice, warning, error, crit, alert, emerg- --log-prefix prefix：日志前缀，用于区别不同的日志，最多29个字符 MARK 做防火墙标记 DNAT 目标IP地址转换 SNAT 源IP地址转换 MASQUERADE 地址伪装（源IP地址动态转换） 自定义链 见第4节 制定规则时先把本机访问和自身访问开了，免得后续制定其他规则时本机不能访问和自己不能访问自己 12iptables -A INPUT -s windowsip -j ACCEPTiptables -A INPUT -i lo -j ACCEPT 范例：记录日志 1234567[root@centos8 ~]#iptables -I INPUT -s 10.0.0.0/24 -p tcp -m multiport --dports 80,21,22,23 -m state --state NEW -j LOG --log-prefix &quot;new connections: &quot;[root@centos8 ~]#tail -f /var/log/messagesMar 19 18:41:07 centos8 kernel: iptables tcp connection: IN=eth0 OUT=MAC=00:0c:29:f8:5d:b7:00:50:56:c0:00:08:08:00 SRC=10.0.0.1 DST=10.0.0.8 LEN=40 TOS=0x00 PREC=0x00 TTL=128 ID=43974 DF PROTO=TCP SPT=9844 DPT=22 WINDOW=4102 RES=0x00 ACK URGP=0Mar 19 18:41:07 centos8 kernel: new connections: IN=eth0 OUT=MAC=00:0c:29:f8:5d:b7:00:50:56:c0:00:08:08:00 SRC=10.0.0.1 DST=10.0.0.8 LEN=40 TOS=0x00 PREC=0x00 TTL=128 ID=43975 DF PROTO=TCP SPT=9844 DPT=22 WINDOW=4102 RES=0x00 ACK URGP=0 范例：记录日志 12345[root@centos8 ~]#iptables -R INPUT 2 -p tcp --dport 21 -m state --state NEW -j LOG --log-prefix &quot;ftp new link: &quot;[root@centos8 ~]#tail -f /var/log/messagesDec 21 10:02:31 centos8 kernel: ftp new link: IN=eth0 OUT=MAC=00:0c:29:f9:8d:90:00:0c:29:10:8a:b1:08:00 SRC=192.168.39.6 DST=192.168.39.8LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=15556 DF PROTO=TCP SPT=53706 DPT=21 WINDOW=14600 RES=0x00 SYN URGP=0 4 iptables自定义链iptables 中除了系统自带的五个链之外，还可以自定义链，来实现将规则进行分组，重复调用的目的。自定义链添加规则之后，要作为系统链的 target 与之关联，才能起到作用 生产中频繁要改的规则可以放在自定义链中，不需要放在主链中，防止修改时出错误，修改时直接修改自定义链，不用修改主链 项目 说明 作用 组织和复用规则，提高可读性和管理效率 创建 iptables -N &lt;chain-name&gt; 删除 iptables -X &lt;chain-name&gt; 调用 在内置链中使用 -j &lt;chain-name&gt; 跳转 RETURN 行为 在自定义链中遇到 RETURN，将返回主链的下一条规则，不再执行该自定义链后续规则 范例：拒绝http和https服务 123iptables -N WEB_CHAINiptables -A WEB_CHAIN -s 10.0.0.6 -p tcp -m multiport --dports80,443 -j REJECTiptables -A INPUT -j WEB_CHAIN 范例: 创建自定义链实现WEB的访问控制 12345678910111213141516171819202122232425262728293031323334[root@centos8 ~]#iptables -N web_chain#改名[root@centos8 ~]#iptables -E web_chain WEB_CHAIN[root@centos8 ~]#iptables -A WEB_CHAIN -p tcp -m multiport --dports 80,443,8080 -j ACCEPT[root@centos8 ~]#iptables -I INPUT 3 -s 10.0.0.0/24 -j WEB_CHAIN[root@centos8 ~]#iptables -A WEB_CHAIN -p icmp -j ACCEPT[root@centos8 ~]#iptables -I WEB_CHAIN 2 -s 10.0.0.6 -j RETURN[root@centos8 ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 10 867 ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 2 5637 423K ACCEPT all -- * * 10.0.0.1 0.0.0.0/0 3 248 20427 WEB_CHAIN all -- * * 10.0.0.0/24 0.0.0.0/0 4 4278 248K REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain WEB_CHAIN (1 references)num pkts bytes target prot opt in out source destination 1 36 2619 ACCEPT tcp -- * * 0.0.0.0/0 0.0.0.0/0 multiport dports 80,443,80802 16 1344 RETURN all -- * * 10.0.0.6 0.0.0.0/0 3 184 15456 ACCEPT icmp -- * * 0.0.0.0/0 0.0.0.0/0 [root@centos6 ~]#curl 10.0.0.8centos8 website[root@centos6 ~]#curl 10.0.0.8centos8 website[root@centos6 ~]#ping -c1 10.0.0.8 范例: 删除自定义链 123456789101112131415161718192021222324252627#无法直接删除自定义链,删除自定义链和创建的顺序相反[root@centos8 ~]#iptables -X WEB_CHAINiptables v1.8.4 (nf_tables): CHAIN_USER_DEL failed (Device or resource busy):chain WEB_CHAIN[root@centos8 ~]#iptables -D INPUT 3[root@centos8 ~]#iptables -X WEB_CHAINiptables v1.8.4 (nf_tables): CHAIN_USER_DEL failed (Device or resource busy):chain WEB_CHAIN#先清除规则再删[root@centos8 ~]#iptables -F WEB_CHAIN[root@centos8 ~]#iptables -L WEB_CHAINChain WEB_CHAIN (0 references)target prot opt source destination [root@centos8 ~]#iptables -X WEB_CHAIN[root@centos8 ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 10 867 ACCEPT all -- lo * 0.0.0.0/0 0.0.0.0/0 2 5824 437K ACCEPT all -- * * 10.0.0.1 0.0.0.0/0 3 4279 248K REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 5 规则优化最佳实践 安全放行所有入站和出站的状态为ESTABLISHED状态连接,建议放在第一条，效率更高 谨慎放行入站的新请求 有特殊目的限制访问功能，要在放行规则之前加以拒绝 同类规则（访问同一应用，比如：http ），匹配范围小的放在前面，用于特殊处理 不同类的规则（访问不同应用，一个是http，另一个是mysql ），匹配范围大的放在前面，效率更高 12-s 10.0.0.6 -p tcp --dport 3306 -j REJECT-s 172.16.0.0/16 -p tcp --dport 80 -j REJECT 应该将那些可由一条规则能够描述的多个规则合并为一条,减少规则数量,提高检查效率 设置默认策略，建议白名单（只放行特定连接）iptables -P，不建议，容易出现“自杀现象”规则的最后定义规则做为默认策略，推荐使用，放在最后一条 6 iptables规则保存使用iptables命令定义的规则，手动删除之前，其生效期限为kernel存活期限，当系统重启之后，定义的规则会消失。 6.1 持久保存规则121 iptables-save &gt; /PATH/TO/SOME_RULES_FILE2 rc.local 1234567891011[root@centos8 ~]#iptables -A INPUT -s 10.0.0.1 -j ACCEPT[root@centos8 ~]#iptables-save &gt; iptables.rule[root@centos8 ~]#cat iptables.rule# Generated by iptables-save v1.8.4 on Tue Nov 21 15:47:34 2023*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT [0:0]:OUTPUT ACCEPT [0:0]-A INPUT -s 10.0.0.1/32 -j ACCEPTCOMMIT# Completed on Tue Nov 21 15:47:34 2023 6.2 加载规则CentOS 7,8 重新载入预存规则文件中规则： 1iptables-restore &lt; /PATH/FROM/SOME_RULES_FILE iptables-restore选项 12-n, --noflush：不清除原有规则-t, --test：仅分析生成规则集，但不提交 范例 12345678910111213141516171819202122[root@centos8 ~]#iptables -F[root@centos8 ~]#iptables -vnLChain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination [root@centos8 ~]#iptables-restore &lt; iptables.rule [root@centos8 ~]#iptables -vnLChain INPUT (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 6 364 ACCEPT all -- * * 10.0.0.1 0.0.0.0/0 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 4 packets, 416 bytes) pkts bytes target prot opt in out source destination 6.3 开机自动重载规则 用脚本保存各个iptables命令；让此脚本开机后自动运行&#x2F;etc&#x2F;rc.d&#x2F;rc.local文件中添加脚本路径 &#x2F;PATH&#x2F;TO&#x2F;SOME_SCRIPT_FILE 用规则文件保存各个规则，开机时自动载入此规则文件中的规则在&#x2F;etc&#x2F;rc.d&#x2F;rc.local文件添加 12345iptables-restore &lt; /PATH/FROM/IPTABLES_RULES_FILE#ubuntu中没有该文件，要自行创建[root@ubuntu ~]# vim /etc/rc.local#!/bin/bash 定义Unit File, CentOS 7，8 可以安装 iptables-services 实现iptables.service 范例: CentOS 7，8 使用 iptables-services 123456789101112131415[root@centos8 ~]#yum -y install iptables-services[root@rocky ~]# systemctl start iptables#这些规则是启动iptables 服务后加载的规则，其保存文件为 /etc/sysconfig/iptables[root@centos8 ~]#cp /etc/sysconfig/iptables&#123;,.bak&#125;#保存现在的规则到文件中方法1[root@centos8 ~]#/usr/libexec/iptables/iptables.init save#保存现在的规则到文件中方法2[root@centos8 ~]#iptables-save &gt; /etc/sysconfig/iptables#开机启动[root@centos8 ~]#systemctl enable iptables.service [root@centos8 ~]#systemctl mask firewalld.service nftables.service 7 网络防火墙iptables&#x2F;netfilter 利用filter表的FORWARD链,可以充当网络防火墙： 注意的问题： (1) 请求-响应报文均会经由FORWARD链，要注意规则的方向性 (2) 如果要启用conntrack机制，建议将双方向的状态为ESTABLISHED的报文直接放行 7.1 NAT表NAT: 网络地址转换，支持PREROUTING，INPUT，OUTPUT，POSTROUTING四个链 局域网中的主机都是分配的私有IP地址，这些IP地址在互联网上是不可达的，局域网中的主机，在与互联网通讯时，要经过网络地址转换，去到互联网时，变成公网IP地址对外发送数据。服务器返回数据时，也是返回到这个公网地址，再经由网络地址转换返回给局域网中的主机 一个局域网中的主机，想要访问互联网，在出口处，应该有一个公网可达的IP地址，应该能将局域网中的IP地址通过NAT转换成公网IP NAT的实现分为下面类型： SNAT：source NAT ，支持POSTROUTING, INPUT，让本地网络中的主机通过某一特定地址访问外部网络，实现地址伪装（源地址转换，将请求报文中的源IP地址转换为公网地址） DNAT：destination NAT ，支持PREROUTING , OUTPUT，把本地网络中的主机上的某服务开放给外部网络访问(发布服务和端口映射)，但隐藏真实IP（目标地址转换，将响应报文中的目标IP地址转换为私网地址） PNAT: port nat，端口转换，IP地址和端口都进行转换 NAT 网络转换原理 当我们从运营商接入宽带之后，经过路由器，防火墙，交换机等各种设备，后面再接入终端机，包括打印机，手机，电脑等，在大多数情况下，我们在接入运营商的宽带后，会在使用（公司，学校，工厂，家庭）范围内组建一个局域网。 以我现在的工作机为例。 物理机的IP地址是 192.168.1.101，在物理机上运行着两台虚拟机，它们的IP 分别是 10.0.0.150 和10.0.0.151。 但是这三台主机，在访问互联网时，互联网上得到的这三台主机的IP都是 219.143.130.7 我们知道，两台主机之间通讯，是需要经过路由表的，路由表中保存了去到下一个IP地址的路由记录，但是 192.168.0.101 和 10.0.0.150 这两个IP地址都是私有地址，在网路上是不可达的，也就是说，在互联网上，是没有一个去到 192.168.0.101 这个网段的路由记录的(10.0.0.150 同理)，那么，我的物理机和虚拟机，是怎么访问互联网的，互联网上的主机，又是怎么回传数据给我的物理机和虚拟机的呢 根据我们前面学过的识知，两台主机之间通讯，是客户端主机启一个随机端口，去连接服务器的IP地址和固定端口，连接成功后，客户端发送数据，服务端根据客户端发送的数据，做出响应，再返回给客户端。 思考题 12在单位内部使用未经申请的公网地址,如:6.0.0.0/8网段,进行内部网络通讯,并利用SNAT连接Internet,是否可以?答：不可以，万一使用的恰好是公网中有的地址，就会连不上 7.2 SNATSNAT：源地址转换，基于nat表的target，适用于固定的公网IP，工作在 POSTROUTING 链上，支持专线 具体是指将经过当前主机转发的请求报文的源IP地址转换成根据防火墙规则指定的IP地址，解决私网访问公网的问题 局域网内所有设备要上外网，如果每个设备都分配一个公网ip成本太大，可以在路由器出口分配一个公网ip，局域网内的设备访问外网时统一走路由器出口，路由器此时需要做两件事： 数据包从出口出去之前，将数据包的源地址和源端口改成公网ip和随机端口 同时将转换关系记录保存，响应数据包返回时根据记录转发给局域网内的设备 SNAT选项： 12--to-source [ipaddr[-ipaddr]][:port[-port]] #转换成指定IP，或指定范围内的IP，端口可选--random #端口映射基于hash算法随机化 范例 1iptables -t nat -A POSTROUTING -s LocalNET ! -d LocalNet -j SNAT --to-source ExtIP 注意: 需要开启 ip_forward 12[root@ubuntu ~]# sysctl -a | grep ip_forwardnet.ipv4.ip_forward = 1 范例 12345678#在防火墙主机上添加规则，如果源IP是 10.0.0.0/24网段的IP，则出去的时候，替换成192.168.10.123[root@ubuntu ~]# iptables -t nat -A POSTROUTING -s 10.0.0.0/24 ! -d 10.0.0.0/24 -j SNAT --to-source 192.168.10.123#替换成172.18.1.6-172.18.1.9范围内的地址[root@ubuntu ~]#iptables -t nat -A POSTROUTING -s 10.0.0.0/24 ! –d 10.0.0.0/24 -j SNAT --to-source 172.18.1.6-172.18.1.9#只转换tcp协议 目标端口为80的报文，且通过12345 端口向外访问[root@ubuntu ~]# iptables -t nat -R POSTROUTING 1 -s 10.0.0.0/24 ! -d 10.0.0.0/24 -o ens37 -p tcp --dport 80 -j SNAT --to-source 192.168.10.123:12345 MASQUERADE：基于nat表的target，适用于动态的公网IP，既支持拨号网络，也支持专线 如果我们内网的出口设备上有固定IP，则直接指定 –to-source IP 没有任何问题，但是如果是使用拨号上网，出口网络设备上的IP地址会发生变化，这种情况下，我们的出口IP不能写成固定的。 这种场景下，我们需要使用 MASQUERADE 进行地址转换，MASOUERADE 可以从主机网卡上自动获取IP地址当作出口IP地址 MASQUERADE选项： 12--to-ports port[-port] #指定端口--random #端口映射基于hash算法随机化 范例 12345#从本地内网地址发出，只要不是到达本地地址的都转换为公网地址iptables -t nat -A POSTROUTING -s LocalNET ! -d LocalNet -j MASQUERADE#从10网段发出，只要不是到达10网段的都转换为公网地址iptables -t nat -A POSTROUTING -s 10.0.0.0/24 ! -d 10.0.0.0/24 -j MASQUERADE 范例：查看本地主机访问公网时使用的IP 123456789101112131415161718192021222324252627[root@centos8 ~]#curl http://ip.sb111.199.191.204#Windows10 支持curlC:\\Users\\Wang&gt;curl ip.sb111.199.184.218[root@centos8 ~]#curl http://ipinfo.io/ip/111.199.191.204[root@centos8 ~]#curl http://ifconfig.me111.199.191.204[root@centos8 ~]#curl -L http://tool.lu/ip当前IP: 111.199.191.204归属地: 中国 北京 北京[root@centos8 ~]#curl -sS --connect-timeout 10 -m 60https://www.bt.cn/Api/getIpAddress111.199.189.164[root@centos8 ~]#curl &quot;https://api.ipify.org?format=string&quot;111.199.184.218[root@firewall ~]#curl cip.ccIP : 39.164.140.134地址 : 中国 河南 鹤壁运营商 : 移动数据二 : 河南省郑州市 | 移动数据三 :URL : http://www.cip.cc/39.164.140.134 范例: SNAT 客户端端口随机，http的端口为80 由于10.0.0.8占用了12345端口，所以假如10.0.0.18同时向外网连接的话，端口号就要转换为192.168.10.7这个公网地址没人用的端口号 序号 源地址 目的地址 1 10.0.0.8:12345 192.168.10.100:80 2 192.168.10.7:12345 192.168.0.100:80 3 192.168.0.100:80 192.168.10.7:12345 4 192.168.0.100:80 10.0.0.:12345 1 10.0.0.18:12345 192.168.10.100:80 2 192.168.10.7:12346 192.168.10.100:80 3 192.168.10.100:80 192.168.0.7:12346 4 192.168.10.100:80 10.0.0.1:12345 实现内网两机器可以访问外网 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#做好测试服务，注意先做好再去配置地址和网关和网络模式[root@lanserver1 ~]#yum -y install httpd[root@lanserver2 ~]#yum -y install httpd[root@internet ~]#yum -y install httpd[root@lanserver1 ~]#hostname -I &gt; /var/www/html/index.html[root@lanserver2 ~]#hostname -I &gt; /var/www/html/index.html[root@Internet ~]#hostname -I &gt; /var/www/html/index.html[root@lanserver1 ~]#systemctl start httpd[root@lanserver2 ~]#systemctl start httpd[root@internet ~]#systemctl start httpd#配置地址和网关#lanserver1DEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.179PREFIX=24GATEWAY=10.0.0.176ONBOOT=yes#lanserver2DEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.180PREFIX=24GATEWAY=10.0.0.176ONBOOT=yes #firewallDEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.176PREFIX=24ONBOOT=yesDEVICE=eth1NAME=eth1BOOTPROTO=noneIPADDR=192.168.10.8PREFIX=24#internetDEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=192.168.10.6PREFIX=24ONBOOT=yes#测试连接状态[root@firewall ~]#curl 192.168.10.6192.168.10.6#启用路由转发（只要数据包经过forward链就要开启）[root@firewall ~]#vim /etc/sysctl.confnet.ipv4.ip_forward=1[root@firewall ~]#sysctl -p#配置规则（两种方法，因为现在192.168.10.8假设是专线IP）1 针对专线静态公共IP#对于来自 10.0.0.0/24 子网的数据包，只有当它们不要访问 10.0.0.0/24 子网时才进行 SNAT 转换[root@firewall ~]#iptables -t nat -A POSTROUTING -s 10.0.0.0/24 ! -d 10.0.0.0/24 -j SNAT --to-source 192.168.10.82 针对拨号网络和专线静态公共IP[root@firewall ~]#iptables -t nat -A POSTROUTING -s 10.0.0.0/24 ! -d 10.0.0.0/24 -j MASQUERADE#10.0.0.179转换为192.168.10.8去ping192.168.10.6[root@lanserver1 ~]#ping 192.168.10.6PING 192.168.10.6 (192.168.10.6) 56(84) bytes of data.64 bytes from 192.168.10.6: icmp_seq=9 ttl=63 time=6.18 ms64 bytes from 192.168.10.6: icmp_seq=10 ttl=63 time=6.12 ms[root@internet ~]#tcpdump -i eth0 -nn icmpdropped privs to tcpdumptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes21:14:43.283872 IP 192.168.10.8 &gt; 192.168.10.6: ICMP echo request, id 3, seq 28, length 6421:14:43.283948 IP 192.168.10.6 &gt; 192.168.10.8: ICMP echo reply, id 3, seq 28, length 6421:14:44.294611 IP 192.168.10.8 &gt; 192.168.10.6: ICMP echo request, id 3, seq 29, length 6421:14:44.294695 IP 192.168.10.6 &gt; 192.168.10.8: ICMP echo reply, id 3, seq 29, length 64#10.0.0.179转换为192.168.10.8去访问192.168.10.6[root@lanserver1 ~]#curl 192.168.10.6192.168.10.6 [root@internet ~]#tail -f /var/log/httpd/access_log 192.168.10.8 - - [21/Nov/2023:21:16:14 +0800] &quot;GET / HTTP/1.1&quot; 200 14 &quot;-&quot; &quot;curl/7.61.1&quot;#查看转换状态信息[root@firewall ~]#cat /proc/net/nf_conntrackipv4 2 tcp 6 117 TIME_WAIT src=10.0.0.179 dst=192.168.10.6 sport=47724 dport=80 src=192.168.10.6 dst=192.168.10.8 sport=80 dport=47724 [ASSURED] mark=0 zone=0 use=2#查看监听端口[root@firewall ~]#ss -ntlState Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 128 [::]:22 [::]:* #会发现监听不到80端口，原因是ss命令监听的是应用程序打开的端口，而iptables是属于内核参数的，内核不属于应用程序空间#外网不可以访问内网[root@internet ~]#curl 10.0.0.179curl: (7) Couldn&#x27;t connect to server#解决办法是使用DNAT技术 7.3 DNATDNAT：目标地址转换，nat表的target，适用于端口映射，即可重定向到本机，也可以支持重定向至不同主机的不同端口，但不支持多目标，即不支持负载均衡功能，工作在 PREROUTING 链上 在内网环境中，使用私有IP地址的设备要与互联网进行通讯时，需要借助出口设备将源内网IP地址转换成公网可达的IP地址再进行通讯。 在内网环境中，只有在出口设备上才有一个(或数个)公网可达的IP，所以在互联网上，是不能路由至内网主机的。如果要让内网主机上的服务在公网上可见，我们需要使用 DNAT 实现目标IP地址转换，解决公网访问私网的问题 修改端口号：重定向端口 DNAT选项 12--to-destination [ipaddr[-ipaddr]][:port[-port]] #转换成指定IP，或指定范围内的IP，端口可选-d ExtIP #指公网可达的IP地址，必须是固定IP，不可以是拨号网络 DNAT 格式 1iptables -t nat -A PREROUTING -d ExtIP -p tcp|udp --dport PORT -j DNAT --to-destination InterSeverIP[:PORT] 范例 123456#在防火墙主机上设置DNAT转发规则，在访问 192.168.10.123 的 web 服务时，转发到 10.0.0.150上[root@ubuntu ~]# iptables -t nat -A PREROUTING -d 192.168.10.123 -p tcp --dport 80 -j DNAT --to-destination 10.0.0.150:80[root@ubuntu ~]#iptables -t nat -A PREROUTING -s 0/0 -d 172.18.100.6 -p tcp --dport 22 -j DNAT --to-destination 10.0.1.22[root@ubuntu ~]#iptables -t nat -A PREROUTING -s 0/0 -d 172.18.100.6 -p tcp --dport 80 -j DNAT --to-destination 10.0.1.22:8080 注意: 需要开启 ip_forward 范例: DNAT 序号 源地址 目的地址 1 192.168.10.100:12345 192.168.10.7:80 2 192.168.10.100:12345 10.0.0.8:80 3 10.0.0.8:80 192.168.10.100:12345 4 192.168.10.7:80 192.168.10.100:12345 实现外网访问内网 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#配置规则[root@firewall ~]#iptables -t nat -A PREROUTING -d 192.168.10.8 -p tcp --dport 80 -j DNAT --to-destination 10.0.0.179:80#外网通过将防火墙IP地址转换为内网地址去访问内网10.0.0.179[root@internet ~]#curl 192.168.10.810.0.0.179 [root@lanserver1 ~]#tail /var/log/httpd/access_log 192.168.10.6 - - [21/Nov/2023:21:46:22 +0800] &quot;GET / HTTP/1.1&quot; 200 12 &quot;-&quot; &quot;curl/7.61.1&quot;[root@firewall ~]#cat /proc/net/nf_conntrackipv4 2 tcp 6 28 TIME_WAIT src=192.168.10.6 dst=192.168.10.8 sport=53096 dport=80 src=10.0.0.179 dst=192.168.10.6 sport=80 dport=53096 [ASSURED] mark=0 zone=0 use=2#注意不能直接访问内网地址，因为没有设置路由表[root@internet ~]#curl 10.0.0.179curl: (7) Couldn&#x27;t connect to server#外网不能通过将防火墙IP地址转换为内网地址去访问内网10.0.0.180[root@firewall ~]#iptables -t nat -A PREROUTING -d 192.168.10.8 -p tcp --dport 80 -j DNAT --to-destination 10.0.0.180:80[root@firewall ~]#iptables -t nat -vnLChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 2 120 DNAT tcp -- * * 0.0.0.0/0 192.168.10.8 tcp dpt:80 to:10.0.0.179:80 0 0 DNAT tcp -- * * 0.0.0.0/0 192.168.10.8 tcp dpt:80 to:10.0.0.180:80 [root@internet ~]#curl 192.168.10.810.0.0.179 #原因是iptables规则是有先后次序的，匹配了第一条，也就是访问192.168.10.8是去访问10.0.0.179:80的，就不会去匹配第二条，解决方法是重新指定端口，不能是80端口了，因为已经被10.0.0.179占了[root@firewall ~]#iptables -t nat -R PREROUTING 2 -d 192.168.10.8 -p tcp --dport 81 -j DNAT --to-destination 10.0.0.180:80[root@internet ~]#curl 192.168.10.8:8110.0.0.180[root@internet ~]#curl 192.168.10.8:8010.0.0.179#修改内网httpd服务自身的端口号[root@lanserver2 ~]#vim /etc/httpd/conf/httpd.confListen 8080[root@lanserver2 ~]#systemctl restart httpd#现在拒绝连接了root@internet ~]#curl 192.168.10.8:81curl: (7) Failed to connect to 192.168.10.8 port 81: 拒绝连接#解决办法是使用REDIRECT来重定向端口 7.4 REDIRECT转发REDIRECT，是NAT表的 target，通过改变目标IP和端口，将接受的包转发至同一个主机的不同端口，可用于PREROUTING，OUTPUT链 REDIRECT 功能无需开启内核 ip_forward 转发 REDIRECT选项 1--to-ports port[-port] 范例：继续根据上面案例 1234567891011[root@lanserver2 ~]#ss -ntlState Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 128 *:8080 *:* LISTEN 0 128 [::]:22 [::]:* #当我本机（10.0.0.180）80端口收到请求后通过REDIRECT转发到我本机的8080端口[root@lanserver2 ~]#iptables -t nat -A PREROUTING -d 10.0.0.180 -p tcp --dport 80 -j REDIRECT --to-ports 8080[root@internet ~]#curl 192.168.10.8:8110.0.0.180 7.5 FORWARD链实现内外网络的流量控制范例: 实现内网访问可以访问外网,反之禁止 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168#环境准备[root@internet ~]#hostname -I192.168.0.6[root@internet ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 1002 0 0 eth00.0.0.0 192.168.0.8 0.0.0.0 UG 0 0 0 eth0[root@firewall ~]#hostname -I10.0.0.8 192.168.0.8[root@firewall ~]#vim /etc/sysctl.confnet.ipv4.ip_forward=1[root@firewall ~]#sysctl -p[root@lanserver1 ~]#hostname -I10.0.0.7[root@lanserver1 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.8 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0[root@lanserver2 ~]#hostname -I10.0.0.17[root@lanserver2 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.8 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0#方法1 通过标准模块实现内网访问外网特定服务http和icmp,反之禁止[root@firewall ~]#iptables -A FORWARD -j REJECT[root@firewall ~]#iptables -I FORWARD -s 10.0.0.0/24 -p tcp --dport 80 -j ACCEPT[root@firewall ~]#iptables -I FORWARD -d 10.0.0.0/24 -p tcp --sport 80 -j ACCEPT[root@firewall ~]#iptables -I FORWARD -s 10.0.0.0/24 -p icmp --icmp-type 8 -j ACCEPT[root@firewall ~]#iptables -I FORWARD -d 10.0.0.0/24 -p icmp --icmp-type 0 -j ACCEPT[root@firewall ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 174 14616 ACCEPT icmp -- * * 0.0.0.0/0 10.0.0.0/24 icmptype 02 218 18312 ACCEPT icmp -- * * 10.0.0.0/24 0.0.0.0/0 icmptype 83 10 1084 ACCEPT tcp -- * * 0.0.0.0/0 10.0.0.0/24 tcp spt:804 31 1938 ACCEPT tcp -- * * 10.0.0.0/24 0.0.0.0/0 tcp dpt:805 312 25632 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination #方法2 利用state模块实现内网访问可以访问外网,反之禁止[root@firewall ~]#iptables -D FORWARD 1[root@firewall ~]#iptables -D FORWARD 2[root@firewall ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 342 28728 ACCEPT icmp -- * * 10.0.0.0/24 0.0.0.0/0 icmptype 82 47 2898 ACCEPT tcp -- * * 10.0.0.0/24 0.0.0.0/0 tcp dpt:803 462 37608 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination [root@firewall ~]#iptables -I FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT[root@firewall ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 40 3429 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED2 443 37212 ACCEPT icmp -- * * 10.0.0.0/24 0.0.0.0/0 icmptype 83 49 3018 ACCEPT tcp -- * * 10.0.0.0/24 0.0.0.0/0 tcp dpt:804 563 46068 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination [root@lanserver1 ~]#ping 192.168.0.6 -c1PING 192.168.0.6 (192.168.0.6) 56(84) bytes of data.64 bytes from 192.168.0.6: icmp_seq=1 ttl=63 time=2.20 ms[root@lanserver2 ~]#curl 192.168.0.6internet[root@internet ~]#ping 10.0.0.7 -c1PING 10.0.0.7 (10.0.0.7) 56(84) bytes of data.From 192.168.0.8 icmp_seq=1 Destination Port Unreachable[root@internet ~]#curl 10.0.0.7curl: (7) couldn&#x27;t connect to host#利用state模块实现允许内网可以访问外网所有资源[root@firewall ~]#iptables -D FORWARD 2[root@firewall ~]#iptables -D FORWARD 2[root@firewall ~]#iptables -I FORWARD 2 -s 10.0.0.0/24 -m state --state NEW -j ACCEPT[root@firewall ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 134 15209 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED2 3 204 ACCEPT all -- * * 10.0.0.0/24 0.0.0.0/0 state NEW 3 572 46680 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination [root@lanserver1 ~]#ping 192.168.0.6 -c1PING 192.168.0.6 (192.168.0.6) 56(84) bytes of data.64 bytes from 192.168.0.6: icmp_seq=1 ttl=63 time=2.26 ms[root@lanserver2 ~]#curl 192.168.0.6internet[root@lanserver2 ~]#ssh 192.168.0.6The authenticity of host &#x27;192.168.0.6 (192.168.0.6)&#x27; can&#x27;t be established.RSA key fingerprint is SHA256:ldHMw3UFehPuE3bgtMHIX5IxRRTM7fwC4iZ0Qqglcys.RSA key fingerprint is MD5:8c:44:d9:3d:22:54:62:d8:27:77:d5:06:09:58:76:92.Are you sure you want to continue connecting (yes/no)?[root@internet ~]#curl 10.0.0.7curl: (7) couldn&#x27;t connect to host[root@internet ~]#ping 10.0.0.7 -c1PING 10.0.0.7 (10.0.0.7) 56(84) bytes of data.From 192.168.0.8 icmp_seq=1 Destination Port Unreachable[root@internet ~]#ssh 10.0.0.7ssh: connect to host 10.0.0.7 port 22: Connection refused#允许内网指定主机被外网访问[root@firewall ~]#iptables -I FORWARD 3 -d 10.0.0.7 -p tcp --dport 80 -j ACCEPT[root@firewall ~]#iptables -vnL --line-numbersChain INPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 63 12862 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED2 9 612 ACCEPT all -- * * 10.0.0.0/24 0.0.0.0/0 state NEW3 1 60 ACCEPT tcp -- * * 0.0.0.0/0 10.0.0.7 tcp dpt:804 586 47464 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 reject-with icmp-port-unreachableChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination [root@internet ~]#curl 10.0.0.7lanserver1[root@internet ~]#ping 10.0.0.7 -c1PING 10.0.0.7 (10.0.0.7) 56(84) bytes of data.From 192.168.0.8 icmp_seq=1 Destination Port Unreachable[root@internet ~]#curl 10.0.0.17curl: (7) couldn&#x27;t connect to host 范例：内部可以访问外部，外部禁止访问内部 12345678910111213141516171819202122[root@internet-host ~]#hostname -I10.0.0.6[root@internet-host ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface10.0.0.0 0.0.0.0 255.255.255.0 U 1 0 0 eth00.0.0.0 10.0.0.8 0.0.0.0 UG 0 0 0 eth0[root@firewall-host ~]#hostname -I10.0.0.8 192.168.100.8[root@lan-host ~]#hostname -I192.168.100.7[root@lan-host ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.100.8 0.0.0.0 UG 100 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0[root@firewall-host ~]#vim /etc/sysctl.confnet.ipv4.ip_forward=1[root@firewall-host ~]#sysctl -p[root@firewall-host ~]#iptables -A FORWARD -d 192.168.100.0/24 -m state --state NEW -j REJECT 范例：针对内部的特定服务可以允许外部访问，其它服务禁止访问 12345678[root@firewall-host ~]#iptables -I FORWARD -d 192.168.100.0/24 -p tcp --dport 80 -j ACCEPT[root@firewall-host ~]#iptables -vnL FORWARD --line-numbersChain FORWARD (policy ACCEPT 0 packets, 0 bytes)num pkts bytes target prot opt in out source destination 1 6 486 ACCEPT tcp -- * * 0.0.0.0/0 192.168.100.0/24 tcp dpt:802 3 228 REJECT all -- * * 0.0.0.0/0 192.168.100.0/24 state NEW reject-with icmp-port-unreachable 范例：针对内网不能访问外网的某一地址 123456789101112131415161718192021[root@internet ~]#ip a a 192.168.10.200 dev eth0 label eth0:1[root@internet ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:9c:0d:3c brd ff:ff:ff:ff:ff:ff inet 192.168.10.6/24 brd 192.168.10.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet 192.168.10.200/32 scope global eth0:1 valid_lft forever preferred_lft forever [root@firewall ~]#iptables -A FORWARD -s 10.0.0.0/24 -d 192.168.10.200 -j REJECT[root@lanserver1 ~]#curl 192.168.10.6192.168.10.6 [root@lanserver1 ~]#curl 192.168.10.200curl: (7) Failed to connect to 192.168.10.200 port 80: Connection refused 8 综合案例: 两个私有网络的互相通迅 12345#192.168.10.6访问172.16.0.7192.168.10.6（SNAT）-10.0.0.18（DNAT）-172.16.0.7#172.16.0.7访问192.168.10.6172.16.0.7（SNAT）-10.0.0.8（DNAT）-192.168.10.6 9 iptables常见操作替换系统防火墙在Centos7系统中默认防火墙管理工具不是iptables,当需要使用时则需要自己安装替换. 1234567[root@localhost ~]# systemctl stop firewalld[root@localhost ~]# systemctl disable firewalld[root@localhost ~]# yum install -y iptables iptables-services[root@localhost ~]# systemctl restart iptables[root@localhost ~]# systemctl enable iptables 查询完整防火墙规则使用 -L -n –line-numbers 参数查看防火墙默认配置规则. 12345678910111213141516[root@localhost ~]# iptables -L -n --line-numbers[root@localhost ~]# iptables -F # 临时清空规则Chain INPUT (policy ACCEPT)num target prot opt source destination 1 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 state RELATED,ESTABLISHED2 ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 3 ACCEPT all -- 0.0.0.0/0 0.0.0.0/0 4 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 state NEW tcp dpt:225 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibitedChain FORWARD (policy ACCEPT)num target prot opt source destination 1 REJECT all -- 0.0.0.0/0 0.0.0.0/0 reject-with icmp-host-prohibitedChain OUTPUT (policy ACCEPT)num target prot opt source destination 设置防火墙默认拒绝设置默认拒绝规则，把 INPUT 链设置为默认拒绝,也就是拒绝所有连接请求. 1234567891011[root@localhost ~]# iptables -P INPUT DROP[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP) #这里可以看出INPUT链已变成DROPnum target prot opt source destination Chain FORWARD (policy ACCEPT)num target prot opt source destination Chain OUTPUT (policy ACCEPT)num target prot opt source destination 开启防火墙ICMP回显在默认规则拒绝的情况下,设置开启ICMP测试,允许主机ping通. 123456789101112[root@localhost ~]# iptables -I INPUT -p icmp -j ACCEPT[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 Chain FORWARD (policy ACCEPT)num target prot opt source destination Chain OUTPUT (policy ACCEPT)num target prot opt source destination 允许客户SSH远程连接在默认拒绝的情况下,设置开启22号端口,允许远程ssh连接到本机. 12345678910[root@localhost ~]# iptables -I INPUT -p tcp --dport 22 -j ACCEPT[root@localhost ~]# iptables -I OUTPUT -p tcp --sport 22 -j ACCEPT[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy ACCEPT)num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:22Chain OUTPUT (policy ACCEPT)num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp spt:22 删除指定规则在默认拒绝的情况下,删除INPUT链,第2条数据,删除ICMP规则. 1234567891011[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:222 ACCEPT icmp -- 0.0.0.0/0 0.0.0.0/0 [root@localhost ~]# iptables -D INPUT 2[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 ACCEPT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 指定允许网段访问在默认拒绝的情况下,设置只允许192.168.1.0&#x2F;24网段的主机访问本机的22号端口. 1234567891011[root@localhost ~]# iptables -I INPUT -s 192.168.1.0/24 -p tcp --dport 22 -j ACCEPT[root@localhost ~]# iptables -I OUTPUT -s 192.168.1.0/24 -p tcp --sport 22 -j ACCEPT[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 ACCEPT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:22Chain OUTPUT (policy ACCEPT)num target prot opt source destination 1 ACCEPT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp spt:22 拒绝访问指定端口在INPUT规则链中,添加拒绝所有人访问本机的8888号端口. 123456[root@localhost ~]# iptables -I INPUT -p tcp --dport 8888 -j REJECT[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8888 reject-with icmp-port-unreachable2 ACCEPT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:22 拒绝访问指定主机网段的端口在INPUT规则链中,添加拒绝192.168.1.20主机访问本机的80端口. 12345678[root@localhost ~]# iptables -I INPUT -p tcp -s 192.168.1.20 --dport 80 -j REJECT[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 REJECT tcp -- 192.168.1.20 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachable2 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8888 reject-with icmp-port-unreachable3 ACCEPT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:22 拒绝访问指定端口范围在INPUT规则链中,添加拒绝所有主机访问本机1000-2000端口. 1234567891011[root@localhost ~]# iptables -A INPUT -p tcp --dport 1000:2000 -j REJECT[root@localhost ~]# iptables -A INPUT -p udp --dport 1000:2000 -j REJECT[root@localhost ~]# iptables -L -n --line-numbersChain INPUT (policy DROP)num target prot opt source destination 1 REJECT tcp -- 192.168.1.20 0.0.0.0/0 tcp dpt:80 reject-with icmp-port-unreachable2 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8888 reject-with icmp-port-unreachable3 ACCEPT tcp -- 192.168.1.0/24 0.0.0.0/0 tcp dpt:224 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpts:1000:2000 reject-with icmp-port-unreachable5 REJECT udp -- 0.0.0.0/0 0.0.0.0/0 udp dpts:1000:2000 reject-with icmp-port-unreachable SNAT-源地址转换&lt;内网映射到公网&gt;从本地发出的数据包,经过SNAT后,会自动伪装成公网的IP,并以公网IP访问指定服务. 123456#例：将本地 192.168.1.1 的请求自动伪装成外网地址 59.110.167.234[root@localhost ~]# iptables -t nat -A POSTROUTING -o ens32 -s 192.168.1.1 -j SNAT --to-source 59.110.167.234 -o #指定外网接口,此处为ens32 -s #指定内网口地址,此处为192.168.1.1 --to-source #外网口的地址 DNAT-目标地址转换&lt;公网映射到内网&gt;从公网接收的数据包,经过DNAT后,会自动将数据包转到指定的内网主机. 1234567#例：将请求 59.110.167.234 且端口为 80 的数据包,自动映射到内网 192.168.1.10[root@localhost ~]# iptables -t nat -A PREROUTING -i ens32 -d 59.110.167.234 -p tcp --dport 80 -j DNAT --to-destination 192.168.1.10 --to-destination #内网口地址,此处为192.168.1.1 -i #绑定外网接口,此处为ens32 -d #外网地址,此处为8.8.8.8 -dport #内网端口,此处为80 限制物理请求连接数iptables可以利用connlimit模块实现限制同一IP针对某个端口的连接数. 允许限制每个客户端IP的并发连接数,即每个IP同时连接到一个服务器个数,还可以限制内网用户的网络使用,对服务器而言则可以限制每个IP发起的连接数. 12345678910111213141516# 限制同一IP同时最多100个http连接[root@localhost ~]# iptables -I INPUT -p tcp --syn --dport 80 -m connlimit --connlimit-above 100 -j REJECT[root@localhost ~]# iptables -I INPUT -p tcp --syn --dport 80 -m connlimit ! --connlimit-above 100 -j ACCEPT# 只允许每组C类IP同时100个http连接[root@localhost ~]# iptables -p tcp --syn --dport 80 -m connlimit --connlimit-above 100 --connlimit-mask 24 -j REJECT# 只允许每个IP同时5个80端口转发,超过的丢弃[root@localhost ~]# iptables -I FORWARD -p tcp --syn --dport 80 -m connlimit --connlimit-above 5 -j DROP# 限制某IP最多同时100个http连接[root@localhost ~]# iptables -A INPUT -s 192.168.1.100 -p tcp --syn --dport 80 -m connlimit --connlimit-above 100 -j REJECT# 限制每IP在一定的时间(比如60秒)内允许新建立最多100个http连接数[root@localhost ~]# iptables -A INPUT -p tcp --dport 80 -m recent --name BAD_HTTP_ACCESS --update --seconds 60 --hitcount 100 -j REJECT[root@localhost ~]# iptables -A INPUT -p tcp --dport 80 -m recent --name BAD_HTTP_ACCESS --set -j ACCEPT 配置基本防火墙规则我们可以在新安装的系统中依次执行下方代码,来配置一个基本的防火墙规则. 12345678910111213141516171819202122232425262728293031323334353637383940# 删除已有规则iptables --delete-chainiptables --flush# 默认禁止进,允许出,允许回环网卡iptables -P INPUT DROPiptables -P FORWARD DROPiptables -P OUTPUT ACCEPTiptables -A INPUT -i lo -j ACCEPT# 允许已建立的或相关连接的通行iptables -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPTiptables -A OUTPUT -m state --state RELATED,ESTABLISHED -j ACCEPT# 限制80端口443端口的单个IP的最大连接数为10iptables -I INPUT -p tcp --dport 80 -m connlimit --connlimit-above 10 -j DROPiptables -I INPUT -p tcp --dport 443 -m connlimit --connlimit-above 10 -j DROP# 允许80(HTTP)/873(RSYNC)/443(HTTPS)/20,21(FTP)/25(SMTP)端口的连接iptables -A INPUT -p tcp -m tcp --dport 80 -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 20 -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 21 -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 25 -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 443 -j ACCEPTiptables -A INPUT -p tcp -m tcp --dport 873 -j ACCEPT# 允许SSH端口的连接,放行SSH端口iptables -A INPUT -p tcp -m tcp --dport 22 -j ACCEPTiptables -A OUTPUT -p tcp -m tcp --dport 22 -j ACCEPT# 允许pingiptables -A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT iptables -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT# 放行允许DNS解析端口iptables -A OUTPUT -p udp -m udp -d 8.8.8.8 --dport 53 -j ACCEPTiptables -A OUTPUT -p udp -m udp -d 114.114.114.114 --dport 53 -j ACCEPT# 保存规则iptables-save 生产常用配置规则下面是收藏的一些生成环境下常用规则的配置,一般情况下配置这些规则足够使用. 1234567891011121314151617181920iptables -t filter -P INPUT DROP #设置默认规则,拒绝所有iptables -t filter -A INPUT -p tcp --dport 80 -j ACCEPT #放行80口iptables -t filter -A INPUT -p tcp --dport 443 -j ACCEPT #放行22口iptables -t filter -A INPUT -p tcp --dport 22 -j ACCEPT #放行22口iptables -t filter -A INPUT -p tcp --dport 80 -j ACCEPT #放行80端口iptables -t filter -I INPUT -p tcp --dport 443-j ACCEPT #插入在顶端一条放行443端口的规则iptables -t filter -I INPUT 2 -p tcp --dport 443 -j ACCEPT #在第二列插入一条443放行规则iptables -t filter -A INPUT -p tcp --dport 80 -j DROP #丢弃80端口的请求iptables -I INPUT 2 -p icmp -j DROP #丢弃ICMP请求iptables -t filter -D INPUT 3 #删除第三条规则iptables -A FORWARD -s 192.168.1.10 -j REJECT #拒绝IP的转发请求iptables -I INPUT -s 10.20.30.0/24 -j DROP #丢弃IP网段的入站请求iptables -A INPUT -i eth1 -s 192.168.0.0/16 -j DROP #丢弃从eth1网卡流入,且地址匹配的数据包iptables -A INPUT -o eth0 -s 192.168.1.0/24 -j DROP #丢弃从eth0网卡流出,且地址匹配的数据包iptables -A INPUT -p tcp --dport 20:21 -j ACCEPT #放行20-21端口的数据包iptables -I INPUT -p tcp -m multiport --dport 80-90,85 -j ACCEPTiptables -A FORWARD -p tcp -m iprange --src-range 192.168.1.10-192.168.1.100 -j ACCEPT 防止常见网络攻击什么是syn，ddos，ping 12345SYN (Synchronize)：在 TCP（传输控制协议）中，SYN 是握手过程的一部分。当客户端尝试与服务器建立连接时，它发送一个带有 SYN 标志的数据包。服务器收到 SYN 数据包后，通常会回复一个带有 SYN 和 ACK（确认）标志的数据包，表示接受连接。最后，客户端再发送一个带有 ACK 标志的数据包，表示握手完成。这个过程通常称为三次握手。DDoS (Distributed Denial of Service)：DDoS 攻击是一种网络攻击，通过在短时间内向目标服务器发送大量的请求，使其超负荷，无法正常响应合法用户的请求。这些请求可以是来自多个分布式计算机的，使得攻击者能够利用分布式网络资源来发动攻击，使攻击更难以阻止。 配置防火墙防止syn，ddos攻击 123456# vim /etc/sysconfig/iptables在iptables中加入下面几行#anti syn，ddos-A FORWARD -p tcp --syn -m limit --limit 1/s --limit-burst 5 -j ACCEPT-A FORWARD -p tcp --tcp-flags SYN,ACK,FIN,RST RST -m limit --limit 1/s -j ACCEPT-A FORWARD -p icmp --icmp-type echo-request -m limit --limit 1/s -j ACCEPT 说明：第一行：每秒中最多允许5个新连接 第二行：防止各种端口扫描 第三行：Ping洪水攻击（Ping of Death），可以根据需要调整或关闭 重启防火墙 1# /etc/init.d/iptables restart 屏蔽一个IP 1# iptables -I INPUT -s 192.168.0.1 -j DROP 怎么防止别人ping我 1# iptables -A INPUT -p icmp -j DROP 防止同步包洪水（Sync Flood） 1# iptables -A FORWARD -p tcp --syn -m limit --limit 1/s -j ACCEPT 防止各种端口扫描 1# iptables -A FORWARD -p tcp --tcp-flags SYN,ACK,FIN,RST RST -m limit --limit 1/s -j ACCEPT Ping洪水攻击（Ping of Death） 123456789101112131415161718# iptables -A FORWARD -p icmp --icmp-type echo-request -m limit --limit 1/s -j ACCEPTNMAP FIN/URG/PSH# iptables -A INPUT -i eth0 -p tcp --tcp-flags ALL FIN,URG,PSH -j DROPXmas Tree# iptables -A INPUT -i eth0 -p tcp --tcp-flags ALL ALL -j DROPAnother Xmas Tree# iptables -A INPUT -i eth0 -p tcp --tcp-flags ALL SYN,RST,ACK,FIN,URG -j DROPNull Scan(possibly)iptables -A INPUT -i eth0 -p tcp --tcp-flags ALL NONE -j DROPSYN/RST# iptables -A INPUT -i eth0 -p tcp --tcp-flags SYN,RST SYN,RST -j DROPSYN/FIN -- Scan(possibly)# iptables -A INPUT -i eth0 -p tcp --tcp-flags SYN,FIN SYN,FIN -j DROP 限制对内部封包的发送速度 1# iptables -A INPUT -f -m limit --limit 100/s --limit-burst 100 -j ACCEPT 限制建立联机的转发 1# iptables -A FORWARD -f -m limit --limit 100/s --limit-burst 100 -j ACCEPT","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"防火墙","slug":"防火墙","permalink":"https://aquapluto.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"}]},{"title":"安全技术和防火墙","slug":"Linux/firewall/introduce","date":"2025-08-21T03:02:00.000Z","updated":"2025-09-07T06:25:51.445Z","comments":true,"path":"Linux/firewall/introduce/","permalink":"https://aquapluto.github.io/Linux/firewall/introduce/","excerpt":"","text":"1 安全技术和防火墙1.1 安全技术 入侵检测系统（Intrusion Detection Systems）：特点是不阻断任何网络访问，量化、定位来自内外网络的威胁情况，主要以提供报警和事后监督为主，提供有针对性的指导措施和安全决策依据,类似于监控系统一般采用旁路部署方式 入侵防御系统（Intrusion Prevention System）：以透明模式工作，分析数据包的内容如：溢出攻击、拒绝服务攻击、木马、蠕虫、系统漏洞等进行准确的分析判断，在判定为攻击行为后立即予以阻断，主动而有效的保护网络的安全，一般采用在线部署方式 防火墙（ FireWall ）：隔离功能，工作在网络或主机边缘，对进出网络或主机的数据包基于一定的规则检查，并在匹配某规则时由规则定义的行为进行处理的一组功能的组件，基本上的实现都是默认情况下关闭所有的通过型访问，只开放允许访问的策略,会将希望外网访问的主机放在DMZ(demilitarized zone)网络中 防水墙（Waterwall）：与防火墙相对，防水墙是一种防止内部信息泄漏的安全产品。它利用透明加解密，身份认证，访问控制和审计跟踪等技术手段，对涉密信息，重要业务数据和技术专利等敏感信息的存储，传播和处理过程，实施安全保护；最大限度地防止敏感信息泄漏、被破坏和违规外传，并完整记录涉及敏感信息的操作日志，以便日后审计。 1.2 防火墙的分类按保护范围划分： 主机防火墙：服务范围为当前一台主机 网络防火墙：服务范围为防火墙一侧的局域网 按实现方式划分: 硬件防火墙：在专用硬件级别实现部分功能的防火墙；另一个部分功能基于软件实现 软件防火墙：运行于通用硬件平台之上的防火墙的应用软件，Windows 防火墙 ISA –&gt; ForefrontTMG 按网络协议划分： 网络层防火墙：OSI模型下四层，又称为包过滤防火墙 应用层防火墙&#x2F;代理服务器：proxy 代理网关，OSI模型七层 包过滤防火墙 网络层对数据包进行选择，选择的依据是系统内设置的过滤逻辑，被称为访问控制列表（ACL），通过检查数据流中每个数据的源地址，目的地址，所用端口号和协议状态等因素，或他们的组合来确定是否允许该数据包通过优点：对用户来说透明，处理速度快且易于维护缺点：无法检查应用层数据，如病毒等 应用层防火墙 应用层防火墙&#x2F;代理服务型防火墙，也称为代理服务器（Proxy Server)将所有跨越防火墙的网络通信链路分为两段内外网用户的访问都是通过代理服务器上的“链接”来实现优点：在应用层对数据进行检查，比较安全缺点：增加防火墙的负载 提示：现实生产环境中所使用的防火墙一般都是二者结合体，即先检查网络数据，通过之后再送到应用层去检查 1.3 防火墙的作用防火墙的基本功能是监控和控制进出网络的数据流。它根据预先定义的安全规则来审查每一个数据包，这些规则可能基于IP地址、端口号、协议类型甚至是具体的内容。任何不符合规则的数据包都会被防火墙拦截，从而防止潜在的攻击和未经授权的访问。 除了基本的包过滤功能，现代防火墙还具备更高级的安全特性，如入侵检测系统（IDS）、入侵防御系统（IPS）和虚拟专用网络（VPN）等。这些功能使得防火墙成为了网络安全的第一道防线。 2 Linux防火墙的基本认识2.1 NetfilterLinux防火墙是由Netfilter组件提供的，Netfilter工作在内核空间，集成在linux内核中 Netfilter 是Linux 2.4.x之后新一代的Linux防火墙机制，是linux内核的一个子系统。Netfilter采用模块化设计，具有良好的可扩充性，提供扩展各种网络服务的结构化底层框架。Netfilter与IP协议栈是无缝契合，并允许对数据报进行过滤、地址转换、处理等操作 内核中netfilter相关的模块 12345[root@centos8 ~]#grep -m 10 NETFILTER /boot/config-4.18.0-193.el8.x86_64[root@centos7 ~]#grep -m 10 NETFILTER /boot/config-3.10.0-1127.el7.x86_64[root@centos6 ~]#grep -m 10 NETFILTER /boot/config-2.6.32-754.el6.x86_64[root@ubuntu2004 ~]#grep -m 10 NETFILTER /boot/config-5.4.0-33-generic[root@ubuntu1804 ~]#grep -m 10 NETFILTER /boot/config-4.15.0-29-generic 2.2 防火墙工具介绍2.2.1 iptables由软件包iptables提供的命令行工具，工作在用户空间，用来编写规则，写好的规则被送往netfilter，告诉内核如何去处理信息包 范例：安装iptables的service包 123456789101112131415[root@centos8 ~]#dnf -y install iptables-services#rocky中己默认安装[root@rocky86 ~]# rpm -q iptablesiptables-1.8.4-22.el8.x86_64#ubuntu中己默认安装root@ubuntu22:~# dpkg -l iptablesDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name Version Architecture Description+++-==============-==============-============-=================================================ii iptables 1.8.7-1ubuntu5 amd64 administration tools for packet filtering and NAT 2.2.2 firewalld从CentOS 7 版开始引入了新的前端管理工具 软件包： firewalld firewalld-config 管理工具： firewall-cmd 命令行工具 firewall-config 图形工作 123456789101112#rocky默认安装[root@rocky86 ~]# rpm -q firewalldfirewalld-0.9.3-13.el8.noarch#ubuntu中没有安装root@ubuntu22:~# dpkg -l firewalldDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name Version Architecture Description+++-==============-============-============-=================================un firewalld &lt;none&gt; &lt;none&gt; (no description available) 2.2.3 nftables此软件是CentOS 8 新特性,Nftables最初在法国巴黎的Netfilter Workshop 2008上发表，然后由长期的netfilter核心团队成员和项目负责人Patrick McHardy于2009年3月发布。它在2013年末合并到Linux内核中，自2014年以来已在内核3.13中可用。 它重用了netfilter框架的许多部分，例如连接跟踪和NAT功能。它还保留了命名法和基本iptables设计的几个部分，例如表，链和规则。就像iptables一样，表充当链的容器，并且链包含单独的规则，这些规则可以执行操作，例如丢弃数据包，移至下一个规则或跳至新链。 从用户的角度来看，nftables添加了一个名为nft的新工具，该工具替代了iptables，arptables和ebtables中的所有其他工具。从体系结构的角度来看，它还替换了内核中处理数据包过滤规则集运行时评估的那些部分 1234567891011[root@rocky86 ~]# rpm -q nftablesnftables-0.9.3-25.el8.x86_64[root@ubuntu ~]# dpkg -l nftablesDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name Version Architecture Description+++-==============-==============-============-==============================================================ii nftables 1.0.2-1ubuntu2 amd64 Program to control packet filtering rules by Netfilter project CentOS 8 支持三种防火墙服务 123[root@centos8 ~]#systemctl status iptables.service[root@centos8 ~]#systemctl status firewalld.service[root@centos8 ~]#systemctl status nftables.service ubuntu 中的防火墙工具 12345678910111213141516171819202122232425262728#系统层面状态[root@ubuntu ~]# systemctl status ufw● ufw.service - Uncomplicated firewall Loaded: loaded (/lib/systemd/system/ufw.service; enabled; vendor preset:enabled) Active: active (exited) since Thu 2023-06-08 08:51:58 CST; 5h 6min ago Docs: man:ufw(8) Process: 734 ExecStart=/lib/ufw/ufw-init start quiet (code=exited,status=0/SUCCESS) Main PID: 734 (code=exited, status=0/SUCCESS) CPU: 2msJun 08 08:51:58 ubuntu systemd[1]: Starting Uncomplicated firewall...Jun 08 08:51:58 ubuntu systemd[1]: Finished Uncomplicated firewall.#应用层面状态[root@ubuntu ~]# ufw statusStatus: inactive[root@ubuntu ~]# ufw enableCommand may disrupt existing ssh connections. Proceed with operation (y|n)? nAborted[root@ubuntu ~]# systemctl status nftables.service○ nftables.service - nftables Loaded: loaded (/lib/systemd/system/nftables.service; disabled; vendorpreset: enabled) Active: inactive (dead) Docs: man:nft(8) http://wiki.nftables.org 2.3 netfilter 中五个勾子函数和报文流向Netfilter在内核中选取五个位置放了五个hook(勾子) function(INPUT、OUTPUT、FORWARD、PREROUTING、POSTROUTING)，而这五个hook function向用户开放，用户可以通过一个命令工具（iptables）向其写入规则，内核中的勾子函数根据预设的规则进行工作，达到对流经的数据包进行过滤，拒绝，转发等功能。 由信息过滤表（table）组成，包含控制IP包处理的规则集（rules），规则被分组放在链（chain）上 特别说明 从 Linux kernel 4.2 版以后，netfilter 在prerouting 前加了一个 ingress 勾子函数。可以使用这个新的入口挂钩来过滤来自第2层的流量，这个新挂钩比预路由要早，基本上是 tc 命令（流量控制工具）的替代品。 三种报文流向 流入本机：PREROUTING –&gt; INPUT–&gt;用户空间进程 流出本机：用户空间进程 –&gt;OUTPUT–&gt; POSTROUTING 转发：PREROUTING –&gt; FORWARD –&gt; POSTROUTING INPUT 链 该链用于处理进入本地系统的流量（即目标是本机的流量）。 所有到达本机的流量都会经过 INPUT 链进行检查 适用于例如 SSH、HTTP 等服务的连接请求 OUTPUT 链 该链用于处理从本地系统发出的流量（即源是本机的流量）。 所有由本机发出的流量都会经过 OUTPUT 链 适用于例如从本机访问外部网站的请求 FORWARD 链 该链用于处理被路由通过本机的流量（即流量不会到达本机，而是转发到其他网络）。 此链仅适用于做路由的机器 适用于例如路由器转发的流量 PREROUTING 链 该链用于在路由决策之前处理流量。 通常用于目标地址转换（DNAT），在数据包到达路由前修改其目标地址 适用于例如网络地址转换（NAT）时，修改目标地址的规则 POSTROUTING 链 该链用于在路由决策之后处理流量。 通常用于源地址转换（SNAT），在数据包离开本机前修改源地址 例如 NAT 中修改源地址的规则","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"防火墙","slug":"防火墙","permalink":"https://aquapluto.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"}]},{"title":"ssh远程连接服务","slug":"Linux/service-manage/ssh","date":"2025-08-21T03:01:35.000Z","updated":"2025-09-09T05:38:44.859Z","comments":true,"path":"Linux/service-manage/ssh/","permalink":"https://aquapluto.github.io/Linux/service-manage/ssh/","excerpt":"","text":"1 ssh服务介绍ssh: secure shell protocol, 22&#x2F;tcp, 安全的远程登录，实现加密通信，代替传统的 telnet 协议，生产中要改端口号 具体的软件实现： OpenSSH：ssh协议的开源实现，CentOS 默认安装 dropbear：另一个ssh协议的开源项目的实现 ssh和 telnet 的区别 telnet不支持root用户登录，只允许普通用户登录；ssh支持root用户登录 telnet数据传输过程中明文的；ssh数据传输过程中时加密码 1.1 公钥交换原理 客户端发起链接请求 服务端返回自己的公钥，以及一个会话ID（这一步客户端得到服务端公钥）客户端生成密钥对 客户端用自己的公钥异或会话ID，计算出一个值Res，并用服务端的公钥加密 客户端发送加密后的值到服务端，服务端用私钥解密，得到Res 服务端用解密后的值Res异或会话ID，计算出客户端的公钥（这一步服务端得到客户端公钥） 最终：双方各自持有三个秘钥，分别为自己的一对公、私钥，以及对方的公钥，之后的所有通讯都会被加密 1.2 ssh加密通讯原理 2 openssh软件Openssh软件相关包： openssh openssh-clients openssh-server 服务器端程序：&#x2F;usr&#x2F;sbin&#x2F;sshd Unit 文件：&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;sshd.service 2.1 ssh命令ssh命令是ssh客户端，允许实现对远程系统经验证地加密安全访问 当用户远程连接ssh服务器时，会复制ssh服务器 /etc/ssh/ssh_host*key.pub 文件中的公钥到客户机的~/.ssh/know_hosts 中。下次连接时，会自动匹配相对应的私钥，不能匹配，将拒绝连接 ssh客户端配置文件： /etc/ssh/ssh_config，主要配置如下 12345678#StrictHostKeyChecking ask#首次登录不显示检查提示StrictHostKeyChecking no# IdentityFile ~/.ssh/id_rsa# IdentityFile ~/.ssh/id_dsa# IdentityFile ~/.ssh/id_ecdsa# IdentityFile ~/.ssh/id_ed25519# Port 22 命令格式 1234567891011ssh [user@]host [COMMAND]ssh [-l user] host [COMMAND]-p port #远程服务器监听的端口-b #指定连接的源IP-v #调试模式-C #压缩方式-X #支持x11转发-t #强制伪tty分配，如：ssh -t remoteserver1 ssh -t remoteserver2 ssh remoteserver3-o option #如：-o StrictHostKeyChecking=no-i &lt;file&gt; #指定私钥文件路径，实现基于key验证，默认使用文件： ~/.ssh/id_dsa,~/.ssh/id_ecdsa, ~/.ssh/id_ed25519，~/.ssh/id_rsa等 首次连接 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#首次连接，会显示目标主机的指纹信息（唯一标识），并提示是否继续#敲yes后会将目标主机的公钥保存在当前用户的~/.ssh/know_hosts 文件中root@ubuntu2204:~# ssh root@10.0.0.206The authenticity of host &#x27;10.0.0.206 (10.0.0.206)&#x27; can&#x27;t be established.ED25519 key fingerprint is SHA256:+CC49Sra7GzLnNYn4QqcT0QyDEwS+osEpcbQ5e1zCc8.This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added &#x27;10.0.0.206&#x27; (ED25519) to the list of known hosts.root@10.0.0.206&#x27;s password: #在客户端机器上可以看到远端主机的公钥root@ubuntu2204:~# ls -l .ssh/total 8-rw------- 1 root root 978 May 23 02:08 known_hosts-rw-r--r-- 1 root root 142 May 23 02:08 known_hosts.old#该文件中保存的内容，在远程主机上都能找到对应的root@ubuntu2204:~# cat .ssh/known_hosts#在10.0.0.206上查看[root@ubuntu ~]# ls -l /etc/ssh/*pub-rw-r--r-- 1 root root 605 May 4 17:02 /etc/ssh/ssh_host_dsa_key.pub-rw-r--r-- 1 root root 177 May 4 17:02 /etc/ssh/ssh_host_ecdsa_key.pub #公钥-rw-r--r-- 1 root root 97 May 4 17:02 /etc/ssh/ssh_host_ed25519_key.pub-rw-r--r-- 1 root root 569 May 4 17:02 /etc/ssh/ssh_host_rsa_key.pub#首次连接时并不能确定远端主机公钥的真伪,为了确定真伪，可以进行以下操作#在连接前，在准备要连接的机器上自己连自己，查看一下指纹，比对一下指纹[root@ubuntu ~]# ssh 127.1The authenticity of host &#x27;127.0.0.1 (127.0.0.1)&#x27; can&#x27;t be established.ED25519 key fingerprint is SHA256:+CC49Sra7GzLnNYn4QqcT0QyDEwS+osEpcbQ5e1zCc8.This key is not known by any other namesAre you sure you want to continue connecting (yes/no/[fingerprint])? #首次连接之后，保存了远程主机的公钥，后续如果有冒充这个IP地址的主机，连接到这个假冒的主机，发现公钥不一样，则会提示#如果远程主机确实发生了改变，则可以删除本地~.ssh/know_host中的对应的公钥root@ubuntu2204:~# ssh root@10.0.0.206@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the ED25519 key sent by the remote host isSHA256:1eOK9Aygq1kbA+smlYjwRwRsx+ZdSRkDnNy5AFp3aPE.Please contact your system administrator.Add correct host key in /root/.ssh/known_hosts to get rid of this message.Offending ECDSA key in /root/.ssh/known_hosts:3remove with:ssh-keygen -f &quot;/root/.ssh/known_hosts&quot; -R &quot;10.0.0.206&quot;Host key for 10.0.0.206 has changed and you have requested strict checking.Host key verification failed. 范例：修改客户端配置，首次连接自动回答为yes 1[root@centos7 ~]#sed -i.bak &#x27;/StrictHostKeyChecking/s/.*/StrictHostKeyChecking no/&#x27; /etc/ssh/ssh_config 范例：通过多个跳板登录远程主机10.0.0.6 123456[root@centos8 ~]#ssh -t 10.0.0.8 ssh -t 10.0.0.7 ssh 10.0.0.6root@10.0.0.8&#x27;s password:root@10.0.0.7&#x27;s password:root@10.0.0.6&#x27;s password:Last login: Fri May 22 09:10:28 2020 from 10.0.0.7[root@centos6 ~]# 范例：远程执行命令 1234[root@centos6 ~]#ssh 10.0.0.8 &quot;sed -i.bak&#x27;/StrictHostKeyChecking/s/.*/StrictHostKeyChecking no/&#x27; /etc/ssh/ssh_config&quot;root@10.0.0.8&#x27;s password:[root@centos6 ~]# 范例：在远程主机运行本地shell脚本 12345678[root@centos8 ~]#hostname -I10.0.0.8[root@centos8 ~]#cat test.sh#!/bin/bashhostname -I[root@centos8 ~]#ssh 10.0.0.18 /bin/bash &lt; test.shroot@10.0.0.18&#x27;s password:10.0.0.18 范例：加选项，首次连接时自动回答yes 123[root@ubuntu ~]# ssh -o StrictHostKeyChecking=no 10.0.0.210Warning: Permanently added &#x27;10.0.0.210&#x27; (ED25519) to the list of known hosts.root@10.0.0.210&#x27;s password: 2.2 scp命令123456789scp [options] [user@]host:/sourcefile /destpathscp [options] /sourcefile [user@]host:/destpathscp [options] [user@]host1:/sourcetpath [user@]host2:/destpath-C #压缩数据流-r #递归复制-p #保持原文件的属性信息-q #静默模式-P PORT #指定远程服务器的端口,默认22 范例：将本地文件复制到远程 123456789#将当前主机/root/test.sh 复制到远程主机 mage 的家目录下[root@ubuntu ~]# scp /root/test.sh mage@10.0.0.210:mage@10.0.0.210&#x27;s password:test.sh 100% 25 2.6KB/s 00:00#远程主机上查看root@ubuntu2204:~# ls -l /home/mage/total 4-rwxr-xr-x 1 mage mage 25 May 23 07:17 test.sh 范例：将远程主机文件复制到本地 1234[root@rocky86 ~]# scp root@10.0.0.157:/home/jose/test.sh .root@10.0.0.157&#x27;s password:test.sh 100% 25 10.0KB/s 00:00 范例：源和目标都不是本机 123456[root@rocky86 ~]# scp jose@10.0.0.157:test.sh root@10.0.0.154:jose@10.0.0.157&#x27;s password:root@10.0.0.154&#x27;s password:test.sh 100% 25 12.9KB/s 00:00 Connection to 10.0.0.157 closed. 范例：复制目录 12345678910[root@ubuntu ~]# scp -r /var/log/ root@10.0.0.161:/tmp/root@10.0.0.161&#x27;s password:syslog 100% 1575KB 15.4MB/s 00:00 sysinfo.log 100% 0 0.0KB/s 00:00 kern.log.2.gz 100% 281KB 45.9MB/s 00:00 ......scp -r dir dest/ #复制整个目录scp -r dir/ dest/ #复制整个目录scp -r dir/* dest/ #复制目录下所有文件 2.3 rsync命令rsync工具可以基于ssh和rsync协议实现高效率的远程系统之间复制文件，使用安全的shell连接做为传输方式，比scp更快，基于增量数据同步，即只复制两方不同的文件，此工具来自于rsync包 注意：通信两端主机都需要安装 rsync 软件 12rsync -av /etc server1:/tmp/ #复制目录和目录下文件rsync -av /etc/ server1:/tmp/ #只复制目录下文件 常用选项 123456789101112131415-n 模拟复制过程-v 显示详细过程-r 递归复制目录树-p 保留权限-t 保留修改时间戳-g 保留组信息-o 保留所有者信息-l 将软链接文件本身进行复制（默认）-L 将软链接文件指向的文件复制-u 如果接收者的文件比发送者的文件较新，将忽略同步-z 压缩，节约网络带宽-a 保留属性，相当于-rlptgoD，但不保留ACL（-A）和SELinux属性（-X）--delete 源数据删除，目标数据也自动同步删除--progress 显示进度--bwlimit=5120 #限速以KB为单位,5120表示5MB 范例 12345678910[root@centos8 ~]#rsync -auv --delete /data/test 10.0.0.7:/data[root@centos8 ~]#rsync -auv --progress --bwlimit=5120 --delete /data/test 10.0.0.7:/datasending incremental file listtest/test/a.img 104,857,600 100% 5.01MB/s 0:00:19 (xfr#1, to-chk=0/2)sent 104,883,319 bytes received 39 bytes 4,463,121.62 bytes/sectotal size is 104,857,600 speedup is 1.00[root@rocky8 ~]# 范例：增量复制 1234567891011121314151617181920212223[root@ubuntu ~]# cp /var/log/syslog ./0525[root@ubuntu ~]# ls -l ./0525total 1584-rw-r--r-- 1 123 456 657 May 25 15:46 fstab-rw-r--r-- 1 123 456 24 May 25 15:46 issue-rw-r----- 1 root root 1612718 May 25 15:55 syslog#这次只复制了新增的syslog 文件[root@ubuntu ~]# rsync -av /root/0525 root@10.0.0.161:/tmp/root@10.0.0.161&#x27;s password:sending incremental file list0525/0525/syslogsent 1,613,291 bytes received 39 bytes 460,951.43 bytes/sectotal size is 1,613,399 speedup is 1.00#查看目标机，0918这个目录属性更新了root@ubuntu:~# ls -l /tmp/0525/total 1584-rw-r--r-- 1 123 456 657 May 25 15:46 fstab-rw-r--r-- 1 123 456 24 May 25 15:46 issue-rw-r----- 1 root root 1612718 May 25 15:55 syslog 2.4 sftp命令交互式文件传输工具，用法和传统的ftp工具相似，利用ssh服务实现安全的文件上传和下载 使用 ls cd mkdir rmdir pwd get put 等指令，可用 ？或 help 获取帮助信息 12sftp [user@]hostsftp&gt; help 2.5 自动登录ssh工具sshpass由EPEL源提供，ssh登陆不能在命令行中指定密码。sshpass的出现，解决了这一问题。sshpass用于非交互SSH的密码验证，一般用在sh脚本中，无须再次输入密码（本机known_hosts文件中有的主机才能生效）。它允许你用 -p 参数指定明文密码，然后直接登录远程服务器，它支持密码从命令行、文件、环境变量中读取。 12345sshpass [option] command parameters-p password #后跟密码它允许你用 -p 参数指定明文密码，然后直接登录远程服务器-f filename #后跟保存密码的文件名，密码是文件内容的第一行-e #将环境变量SSHPASS作为密码 范例 12345[root@centos7 ~]#sshpass -p zjwjl2004 ssh -o StrictHostKeyChecking=no 10.0.0.129 hostname -I10.0.0.129 [root@ubuntu ~]# sshpass -p 123456 ssh 10.0.0.161 hostname -I10.0.0.161 范例 1234[root@ubuntu ~]# cat pwd.txt123456[root@ubuntu ~]# sshpass -f pwd.txt ssh 10.0.0.161 hostname -I10.0.0.161 范例 123[root@ubuntu ~]# export SSHPASS=123456[root@ubuntu ~]# sshpass -e ssh 10.0.0.161 hostname -I10.0.0.161 范例：批量修改主机密码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@ubuntu ~]# cat pwd.sh#!/bin/bashIPLIST=&quot;10.0.0.16110.0.0.210&quot;PASS=123456. /etc/os-releasepre_os () &#123; if [[ $ID =~ ubuntu ]];then dpkg -l sshpass &amp;&gt; /dev/null || &#123; apt update; apt -y install sshpass; &#125; elif [[ $ID =~ rocky|centos|rhel ]];then rpm -q sshpass &amp;&gt;/dev/null || yum -y install sshpass else echo &quot;不支持当前操作系统&quot; exit fi&#125;change_root_pass () &#123; [ -f ip_pass.txt ] &amp;&amp; mv ip_pass.txt ip_pass.txt.bak for ip in $IPLIST;do pass=`openssl rand -base64 9` sshpass -p $PASS ssh -o StrictHostKeyChecking=no $ip &quot;echo root:$pass | chpasswd&quot; if [ $? -eq 0 ];then echo &quot;$ip:root:$pass&quot; &gt;&gt; ip_pass.txt echo &quot;change root password is successfull on $ip&quot; else echo &quot;change root password is failed on $ip&quot; fi done&#125;pre_oschange_root_pass[root@ubuntu ~]# chmod a+x pwd.sh[root@ubuntu ~]# ./pwd.sh[root@ubuntu ~]# ./pwd.shchange root password is successfull on 10.0.0.161change root password is successfull on 10.0.0.210#记录执行结果[root@ubuntu ~]# cat ip_pass.txt10.0.0.161:root:uOihK+VaW54C10.0.0.210:root:aSQdPL57FHaX 2.6 lastb命令lastb 命令用于列出登入系统失败的用户相关信息。 单独执行 lastb 指令，它会读取位于 /var/log 目录下，名称为 btmp 的文件，并把该文件记录登入失败的用户名，全部显示出来 12345678910lastb [-adRx][-f &lt;记录文件&gt;][-n &lt;显示行数&gt;][帐号名称...][终端机编号...]参数说明：-R #省略主机名 hostname 的列-a #把从何处登入系统的主机名称或IP地址显示在最后一行。-d #将IP地址转换成主机名称。-f&lt;记录文件&gt; #指定记录文件。-n&lt;显示行数&gt;或-&lt;显示行数&gt; #显示名单的行数。-R #不显示登入系统的主机名称或IP地址。-x #显示系统关机，重新开机，以及执行等级的改变等信息。 范例：统计ssh登录失败次数最多的前十个远程IP 1234567891011121314151617181920212223[root@centos8 ~]#lastb -f btmp-test | awk &#x27;&#123;print $3&#125;&#x27;|sort |uniq -c|sort -nr|head 86294 58.218.92.37 43148 58.218.92.26 18036 112.85.42.201 10501 111.26.195.101 10501 111.231.235.49 10501 111.204.186.207 10501 111.11.29.199 10499 118.26.23.225 6288 42.7.26.142 4236 58.218.92.30 [root@centos8 ~]#lastb -f btmp-test | awk &#x27;&#123;ip[$3]++&#125;END&#123;for(i in ip)&#123;print ip[i],i&#125;&#125;&#x27;|sort -nr|head86294 58.218.92.3743148 58.218.92.2618036 112.85.42.20110501 111.26.195.10110501 111.231.235.4910501 111.204.186.20710501 111.11.29.19910499 118.26.23.2256288 42.7.26.1424236 58.218.92.30 3 ssh登录验证方式介绍基于用户和口令登录验证 客户端发起ssh请求，服务器会把自己的公钥发送给用户 用户会根据服务器发来的公钥对密码进行加密 加密后的信息回传给服务器，服务器用自己的私钥解密，如果密码正确，则用户登录成功 基于密钥的登录方式 首先在客户端生成一对密钥（ssh-keygen） 并将客户端的公钥ssh-copy-id 拷贝到服务端 当客户端再次发送一个连接请求，包括ip、用户名 服务端得到客户端的请求后，会到authorized_keys中查找，如果有响应的IP和用户，就会随机生成一个字符串，例如：magedu 服务端将使用客户端拷贝过来的公钥进行加密，然后发送给客户端 得到服务端发来的消息后，客户端会使用私钥进行解密，然后将解密后的字符串发送给服务端 服务端接受到客户端发来的字符串后，跟之前的字符串进行对比，如果一致，就允许免密码登录 3.1 实现基于密钥的登录方式在客户端生成密钥对 1ssh-keygen -t rsa [-P &#x27;password&#x27;] [-f “~/.ssh/id_rsa&quot;] [-C &quot;your_email@example.com&quot;] 命令释义说明： 使用 ssh-keygen 用于生成 RSA 密钥和公钥，-t 指定密钥类型，就是生成 RSA 加密的钥匙，-p 指定密码，-C 添加注释，一般用邮箱地址，-f 指定密钥对存放路径 RSA 也是默认的加密类型，所以可以只输入 ssh-keygen，默认的 RSA 长度是 2048 位，如果你非常注重安全，那么可以指定 4096 位的长度，指令如下： 1ssh-keygen -b 4096 -t rsa 生成密钥对后，你可以选择将其存储在默认位置（~/.ssh/ 目录下）或选择其他位置。 生成的密钥对包括两个文件：私钥文件（id_rsa）和公钥文件（id_rsa.pub）。私钥文件存储在本地，而公钥文件则需要被复制到远程服务器上。 把公钥文件传输至要免密登录的远程服务器对应用户的家目录 123456ssh-copy-id [-i [identity_file]] [user@]host选项-p：指定ssh端口号-i &lt;identity_file&gt;：指定复制到远程主机的公钥认证文件路径-o &lt;ssh_option&gt;：指定其他ssh参数 重设私钥口令： 1ssh-keygen -p 范例：10.0.0.183（centos7）远程免密登陆10.0.0.176（rocky8）和10.0.0.129（Ubuntu2004） 12345678910111213141516171819202122232425262728293031323334353637[root@centos7 ~]#ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): #回车，接受默认值Enter passphrase (empty for no passphrase): #回车，接受默认值，空密码Enter same passphrase again: #回车，接受默认值Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): #生成私钥存放路径，默认/root/.ssh/id_rsaEnter passphrase (empty for no passphrase): #私钥文件要不要加密Enter same passphrase again: root@centos7 ~]#ls .sshid_rsa id_rsa.pub[root@centos7 ~]#ssh-copy-id 10.0.0.129 #默认是root用户，如果要指定，wu@10.0.0.129[root@ubuntu2023 ~]#cat .ssh/authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCwUJnGIjiRoZkFN+RLZdS0/td6mYu9ANZJlNrTp/AbN73JXuMx9CAjhNPfS1NzeD8U0HCDyMBDZPjgiTtwB8rqHIL2TiKdf6gnXjiLbQ15v5fk7ZW2Na7Nh4x3sUY3ZpuZt/w2SEG37yHxwAKRCBBccafEbbo/GW8GOhTdbHEUlONSv5jDLVzQyxNzjAgpBX7Y/LYbtrY3QG6UJOw61gknI8rPWlBN+VuPo11NXqQ5F/mVCy/QgzZIkD5N/MBCBO2jMcLkXD0+kgrOZxx1DY7lC2jyGt27zu9OS/OLPR7ZNmLKKnCMJp+Yp+DHcelVrDEMD8SoAXh1gMAb7Jux04i/ root@centos7[root@centos7 ~]#cat .ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCwUJnGIjiRoZkFN+RLZdS0/td6mYu9ANZJlNrTp/AbN73JXuMx9CAjhNPfS1NzeD8U0HCDyMBDZPjgiTtwB8rqHIL2TiKdf6gnXjiLbQ15v5fk7ZW2Na7Nh4x3sUY3ZpuZt/w2SEG37yHxwAKRCBBccafEbbo/GW8GOhTdbHEUlONSv5jDLVzQyxNzjAgpBX7Y/LYbtrY3QG6UJOw61gknI8rPWlBN+VuPo11NXqQ5F/mVCy/QgzZIkD5N/MBCBO2jMcLkXD0+kgrOZxx1DY7lC2jyGt27zu9OS/OLPR7ZNmLKKnCMJp+Yp+DHcelVrDEMD8SoAXh1gMAb7Jux04i/ root@centos7[root@centos7 ~]#ssh 10.0.0.129 #免密登陆[root@ubuntu2023 ~]#[root@centos7 ~]#ssh 10.0.0.129 hostname #也可远程执行命令ubuntu2023[root@centos7 ~]#ssh-copy-id 10.0.0.176[root@Rocky8 ~]#ls .sshauthorized_keys[root@centos7 ~]#ssh 10.0.0.176[root@Rocky8 ~]##如果不想敲yesno[root@centos7 ~]#ssh -o StrictHostKeyChecking=no 10.0.0.176 范例：10.0.0.183（centos7，以该机为主要机器）10.0.0.176（rocky8）和10.0.0.129（Ubuntu2004）互相免密远程登陆 123456789101112[root@centos7 ~]#ssh-keygen [root@centos7 ~]#ssh-copy-id 10.0.0.183[root@centos7 ~]#ls .sshauthorized_keys id_rsa id_rsa.pub known_hosts[root@centos7 ~]#scp -r .ssh 10.0.0.176:/root/[root@Rocky8 ~]#ls .sshauthorized_keys id_rsa id_rsa.pub known_hosts[root@centos7 ~]#scp -r .ssh 10.0.0.129:/root/[root@ubuntu2023 ~]#ls .sshauthorized_keys id_rsa id_rsa.pub known_hosts authorized_keys known_hosts 位置 存在于服务器端（目标主机）。 存在于客户端（发起连接的主机）。 作用 存储允许登录的客户端公钥，用于身份验证。 存储远程主机的公钥，用于验证主机身份。 文件内容 每行一个公钥，对应一个允许登录的用户。 每行一个条目，包含主机名&#x2F;IP 和公钥。 权限要求 必须限制为仅用户可读写（600）。 必须限制为仅用户可读写（600）。 首次连接行为 不涉及。 客户端会提示确认主机公钥，并记录到 known_hosts。 范例：实现基于 key 验证 123456789101112131415161718192021222324252627282930313233343536373839[root@centos8 ~]#ssh-keygen[root@centos8 ~]#ll .ssh/total 8-rw------- 1 root root 2622 May 22 09:51 id_rsa-rw-r--r-- 1 root root 583 May 22 09:51 id_rsa.pub[root@centos8 ~]#ssh-copy-id root@10.0.0.7[root@centos7 ~]#ll .sshtotal 4-rw------- 1 root root 583 May 22 09:52 authorized_keys[root@centos8 ~]#ssh 10.0.0.7Last login: Fri May 22 08:43:50 2020 from 10.0.0.1[root@centos7 ~]#exitlogoutConnection to 10.0.0.7 closed.[root@centos8 ~]#scp /etc/fstab 10.0.0.7:/datafstab#对私钥加密[root@centos8 ~]#ssh-keygen -p[root@centos8 ~]#ssh 10.0.0.7Enter passphrase for key &#x27;/root/.ssh/id_rsa&#x27;: #输入私钥的密码Last login: Fri May 22 08:47:50 2020 from 10.0.0.8[root@centos7 ~]#exitlogoutConnection to 10.0.0.7 closed.#启用ssh代理[root@centos8 ~]#ssh-agent bash[root@centos8 ~]#ps aux|grep agentroot 1972 0.0 0.0 29440 548 ? Ss 10:18 0:00 ssh-agent bashroot 1992 0.0 0.1 12108 964 pts/0 S+ 10:18 0:00 grep --color=auto agent[root@centos8 ~]#ssh-addEnter passphrase for /root/.ssh/id_rsa:Identity added: /root/.ssh/id_rsa (root@centos8.wangxiaochun.com)[root@centos8 ~]#ssh 10.0.0.7Last login: Fri May 22 08:48:55 2020 from 10.0.0.8 脚本：向目标主机批量分发 SSH 公钥，实现免密码登录 第一种：使用expect工具处理交互，目标 IP 写在IPLIST变量中 12345678910111213141516171819202122232425262728#!/bin/bashCOLOR=&quot;echo -e \\E[1;32m&quot;END=&quot;\\E[0m&quot;PASSWORD=123456IPLIST=&quot;10.0.0.710.0.0.17&quot;[ ! -f ~/.ssh/id_rsa ] &amp;&amp; ssh-keygen -P &quot;&quot; -f ~/.ssh/id_rsa &amp;&gt;/dev/nullrpm -q expect &amp;&gt; /dev/null || yum -y -q install expect &amp;&gt;/dev/nullfor ip in $IPLIST ;do&#123; expect &lt;&lt;EOF set timeout 60 spawn ssh-copy-id $ip expect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\r&quot;;exp_continue &#125; &quot;password:&quot; &#123; send &quot;$PASSWORD\\r&quot; &#125; &#125; expect eof EOF $COLOR&quot;$ip is ready&quot;$END&#125;&amp;donewait$COLOR&quot;Push ssh key is finished!&quot;$END 第二种：使用expect工具处理交互，目标 IP 通过hosts.txt文件传入 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@centos8 ~]#cat push_ssh_key.sh#!/bin/bashPASS=123456rpm -q expect &amp;&gt; /dev/null || yum -y install expect &amp;&gt; /dev/nullif [ ! -e /root/.ssh/id_rsa ];then ssh-keygen -t rsa -P &quot;&quot; -f /root/.ssh/id_rsa &amp;&gt; /dev/null echo &quot;ssh key is created&quot;fiwhile read IP ;do expect &amp;&gt; /dev/null &lt;&lt;EOF #或者expect &lt;&lt;EOF &amp;&gt; /dev/null set timeout 20 spawn ssh-copy-id -i /root/.ssh/id_rsa.pub root@$IP expect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;$PASS\\n&quot; &#125; &#125; expect eof EOF echo $IP is readydone &lt; hosts.txt[root@centos8 ~]#cat hosts.txt10.0.0.710.0.0.6[root@centos8 ~]#bash push_ssh_key.shssh key is created10.0.0.7 is ready10.0.0.6 is ready[root@centos8 ~]#ssh 10.0.0.7Last login: Fri May 22 10:19:35 2020 from 10.0.0.8[root@centos7 ~]#exitlogoutConnection to 10.0.0.7 closed.[root@centos8 ~]#ssh 10.0.0.6Last login: Fri May 22 10:19:39 2020 from 10.0.0.8[root@centos6 ~]#exitlogoutConnection to 10.0.0.6 closed.[root@centos8 ~]#for i in `cat hosts.txt`;do ssh $i hostname -I ;done10.0.0.710.0.0.6 第三种：使用sshpass工具自动输入密码（替代expect），目标 IP 是连续网段（通过循环&#123;1..100&#125;生成） 1234567891011121314#!/bin/bashNET=10.0.0PASS=123456ssh-keygen -P &quot;&quot; -f /root/.ssh/id_rsa &amp;&gt; /dev/nullrpm -q sshpass &amp;&gt; /dev/null || yum -y install sshpass &amp;&gt; /dev/nullfor i in &#123;1..100&#125;;do&#123; sshpass -p $PASS ssh-copy-id -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa.pub $NET.$i &amp;&gt; /dev/null&#125;&amp;donewait 第四种：使用sshpass工具自动输入密码（替代expect），目标 IP 写在HOSTS变量中 12345678910111213141516171819#!/bin/bashHOSTS=&quot;10.0.0.610.0.0.710.0.0.1810.0.0.28&quot;PASS=magedussh-keygen -P &quot;&quot; -f /root/.ssh/id_rsa &amp;&gt; /dev/nullrpm -q sshpass &amp;&gt; /dev/null || yum -y install sshpass &amp;&gt; /dev/nullfor i in $HOSTS;do&#123; sshpass -p $PASS ssh-copy-id -o StrictHostKeyChecking=no -i /root/.ssh/id_rsa.pub $i &amp;&gt; /dev/null&#125;&amp;donewait 3.2 SSH密钥代理验证代理（authentication agent）保密解密后的密钥，口令就只需要输入一次，在GNOME中，代理被自动提供给root用户 SSH密钥代理是一个可以管理 SSH 私钥的程序，可以在一次登录后将私钥的解密密码缓存起来，以便后续的 SSH 操作无需再次输入密码 这样你就不需要每次 SSH 登录都输入私钥密码了，提高了使用的便利性和安全性。在实际使用中，你可能会有多个密钥对，用于不同的服务器或用途。为了更好地管理这些密钥对，可以使用 SSH 配置文件或密钥文件的别名。 12345#启用代理ssh-agent bash#添加私钥到代理ssh-add ~/.ssh/id_rsa 在SecureCRT或Xshell实现基于key验证 在SecureCRT工具 —&gt; 创建公钥 —&gt; 生成 Identity.pub 文件 转化为openssh兼容格式（适合SecureCRT，Xshell不需要转化格式），并复制到需登录主机上相应文件authorized_keys中，注意权限必须为600，在需登录的ssh主机上执行： 1ssh-keygen -i -f Identity.pub &gt;&gt; .ssh/authorized_keys 4 ssh服务器配置服务器端：sshd 服务器端的配置文件: &#x2F;etc&#x2F;ssh&#x2F;sshd_config 服务器端的配置文件帮助：man 5 sshd_config 常用参数： 1234567891011121314151617181920212223242526272829Port 22 #生产建议修改，不然容易被黑客攻击ListenAddress ip #监听的ip地址LoginGraceTime 2mPermitRootLogin yes #默认ubuntu不允许root远程ssh登录StrictModes yes #检查.ssh/文件的所有者，权限等MaxAuthTries 6 #一次连接，最多可以输错6次密码MaxSessions 10 #同一个连接最大会话PubkeyAuthentication yes #基于key验证PermitEmptyPasswords no #空密码连接PasswordAuthentication yes #基于用户名和密码连接PrintMotd no #是否输出motd信息，改成yes 则motd 会输出两次ChallengeResponseAuthentication yes #改成no，提高登陆速度PrintLastLog yes #是否输出上次登录信息UseDNS yes #是否需要解析主机名，no可加快连接速度GSSAPIAuthentication yes #是否开启客户端对IP反解析，提高速度可改为noBanner /path/file #远程连接时的登录前提示GatewayPorts noClientAliveInterval 10 #单位:秒ClientAliveCountMax 3 #默认3MaxStartups #未认证连接最大值，默认值10#以下可以限制可登录用户的办法：AllowUsers user1 user2 user3 #用户名白名单DenyUsers user1 user2 user3 #用户名黑名单AllowGroups g1 g2 #用户组白名单DenyGroups g1 g2 #用户组黑名单 范例：设置ssh服务超时断开连接 12#10s如果没有互动，则断开链接，永久生效可以写配置文件,例如可以写在 /etc/profile 里面[root@ubuntu ~]# export TMOUT=10 范例：设置ssh空闲60s自动注销 123456Vim /etc/ssh/sshd_configClientAliveInterval 60ClientAliveCountMax 0Service sshd restart#注意：新开一个连接才有效 范例：解决ssh登录缓慢的问题 12345vim /etc/ssh/sshd_configUseDNS no #关闭dns解析选项GSSAPIAuthentication nosystemctl restart sshd 范例：在 ubuntu 上启用 root 远程ssh登录 123456#修改sshd服务配置文件vim /etc/ssh/sshd_config#PermitRootLogin prohibit-password 注释掉此行，修改为下面形式PermitRootLogin yes systemctl restart sshd 4.1 优化ssh服务12UseDNS no # 禁止ssh进行dns反向解析，影响ssh连接效率参数GSSAPIAuthentication no # 禁止GSS认证，减少连接时产生的延迟 4.2 ssh服务的最佳实践123456789101112建议使用非默认端口禁止使用protocol version 1限制可登录用户设定空闲会话超时时长利用防火墙设置ssh访问策略仅监听特定的IP地址基于口令认证时，使用强密码策略，比如：tr -dc A-Za-z0-9_ &lt; /dev/urandom | head -c 12| xargs使用基于密钥的认证禁止使用空密码禁止root用户直接登录限制ssh的访问频度和并发在线数经常分析日志 5 ssh服务加固安全5.1 限制和加固 SSH 访问禁用密码身份验证是提高 SSH 安全性的重要步骤之一。这样，用户只能通过密钥身份验证进行访问，而不再依赖弱密码。 在 sshd_config 中禁用密码身份验证 12345$ sudo vim /etc/ssh/sshd_config# 找到并修改以下行：PasswordAuthentication no$ sudo service ssh restart 在 sshd_config 中禁用root身份验证 1PermitRootLogin no 5.2 使用 sshd_config 文件设置访问限制sshd_config 文件包含了 SSH 服务器的配置选项。通过修改这个文件，你可以设置一些限制，例如限制用户和 IP 地址的访问。 一些常用的 sshd_config 选项： AllowUsers：指定允许登录的用户列表。 DenyUsers：指定禁止登录的用户列表。 AllowGroups：指定允许登录的用户组列表。 DenyGroups：指定禁止登录的用户组列表。 PermitRootLogin：禁用或限制 root 用户的远程登录。 12345678910111213$ sudo vim /etc/ssh/sshd_config# 找到并修改以下行#限制用户访问，将username替换为允许访问的用户名。你还可以使用逗号分隔的列表允许多个用户。AllowUsers username# 限制IP地址访问，将username替换为允许访问的用户名，your_ip替换为允许访问的IP地址。AllowUsers username@your_ip# 允许特定IP地址段，下列设置表示将允许来自192.168.1网段的所有IP地址的用户名为username的访问。AllowUsers username@192.168.1.*$ sudo service ssh restart 配置范例 12345AllowUsers alice bobDenyUsers malloryAllowGroups sshusersDenyGroups badusersPermitRootLogin no 以上配置将只允许用户 alice 和 bob 以及属于 sshusers 组的用户登录，同时拒绝用户 mallory 和属于 badusers 组的用户登录。此外，禁止 root 用户通过 SSH 远程登录。 5.3 使用 TCP Wrappers 进行进一步访问控制在 /etc/hosts.allow 和 /etc/hosts.deny 文件中，你可以使用 TCP Wrappers 设置更复杂的主机访问控制规则。例如，在 /etc/hosts.allow 中添加以下行： 1sshd: 192.168.1.0/255.255.255.0 这样将允许来自 192.168.1 网段的所有主机访问 SSH 服务。 5.4 修改 SSH 端口默认情况下，SSH 服务使用 22 端口。为了提高安全性，可以修改为其他非常用端口，比如 2222： 1Port 2222 确保保存修改并重新启动 SSH 服务。 5.5 使用 SSH Agent ForwardingSSH Agent Forwarding 是一种机制，允许你在本地系统上解锁私钥，然后通过安全地转发到远程主机，以在远程主机上进行身份验证。这意味着，如果你已经通过密钥身份验证登录到本地机器，你可以使用相同的身份验证在远程主机上执行操作，而无需再次输入密码或私钥。 在本地机器上启动 SSH Agent 1eval &quot;$(ssh-agent -s)&quot; 添加私钥到代理 1ssh-add ~/.ssh/id_rsa 在连接到远程主机时启用 Agent Forwarding 1ssh -A username@your_server_ip 确保替换 username 为你的用户名，your_server_ip 为目标服务器的 IP 地址。 现在，你可以在远程主机上执行需要私钥身份验证的操作，而无需再次输入密码或私钥。 尽管 SSH Agent Forwarding 提供了便利，但也需要注意一些安全性问题。确保遵循以下最佳实践： 只允许受信任的主机使用 Agent Forwarding：如果你连接到了不受信任的主机，可以通过使用 -A 参数禁用 Agent Forwarding。 定期检查代理并清除不再需要的密钥：使用 ssh-add -L 命令查看当前加载的密钥列表，并使用 ssh-add -D 清除不再需要的密钥。 在不需要 Agent Forwarding 的情况下禁用它：只有在确实需要在远程主机上执行私钥身份验证的操作时才启用 Agent Forwarding。 通过理解和正确配置 SSH Agent Forwarding，你可以在保持安全性的同时提高 SSH 的便利性。 5.6 定期更新密钥对为了增加安全性，定期更新 SSH 密钥对是一个好的实践。可以使用以下步骤生成新的密钥对并替换旧的密钥： 1234567891011# 生成新的密钥对ssh-keygen -t rsa -b 4096 -f ~/.ssh/new_key -C &quot;your_email@example.com&quot;# 复制新的公钥到目标服务器ssh-copy-id username@your_server_ip -i ~/.ssh/new_key.pub# 测试新密钥是否可以成功登录ssh -i ~/.ssh/new_key username@your_server_ip# 如果一切正常，可以删除旧密钥rm ~/.ssh/id_rsa* 5.7 监控和审计 SSH 访问启用 SSH 访问的监控和审计功能可以及时发现潜在的安全问题。可以使用工具如 fail2ban 来监控日志文件，自动封禁恶意的 SSH 连接尝试。 1234567# 安装 fail2bansudo apt install fail2ban # 对于 Debian/Ubuntusudo yum install fail2ban # 对于 Red Hat/CentOS# 启用 SSH 防护sudo systemctl enable fail2bansudo systemctl start fail2ban 通过审查系统日志文件，特别是 /var/log/auth.log（对于 Debian&#x2F;Ubuntu）或 /var/log/secure（对于 Red Hat&#x2F;CentOS），可以查看 SSH 访问的详细信息。 12sudo tail -f /var/log/auth.log # 对于 Debian/Ubuntusudo tail -f /var/log/secure # 对于 Red Hat/CentOS 通过监控和审计 SSH 访问，你可以及时发现异常情况，并采取相应的措施来保护系统安全。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"网络问题排查","slug":"Linux/network-manage/troubleshoot","date":"2025-08-21T03:00:49.000Z","updated":"2025-09-07T12:31:05.300Z","comments":true,"path":"Linux/network-manage/troubleshoot/","permalink":"https://aquapluto.github.io/Linux/network-manage/troubleshoot/","excerpt":"","text":"一、网卡丢包问题1.1 linux软硬件中断中断是系统用来影响硬件设备请求的一种机制，它会打断进程的正常调度和执行，然后调用内核中的终端处理程序来影响设备的请求。 中断是一个异步的事件处理机制，可以提高操作系统处理并发的能力。 1.2 全双工与半双工全双工模式是指交换机在发送数据的同时也能够接收数据，两者同步进行。 现代多数网络设备，如交换机、路由器、计算机的网卡等，通常支持全双工工作模式，能够提供更好的网络速率和性能。 半双工是指一个时间段内只有一个动作发生，随着技术的不断进步，半双工会逐渐退出历史舞台。 1.3 CRCCRC（循环冗余校验码）是数据通信领域中最常用的一种查错校验码，用于数据传输检错，其特征是信息字段和校验字段的长度可以任意选定。它对数据进行多项式计算，并将得到的结果附在帧的后面，接收设备也执行类似的算法，以保证数据传输的正确性和完整性 网卡接收数据的每一帧数据都包含一个CRC值。这个CRC值是基于数据帧中的数据计算得来的。发送数据的设备，如网卡，会在发送数据的时候计算CRC，并将CRC值附加在数据帧的尾部。接收数据的设备则会重新计算接收到的数据帧的CRC值，然后与接收到的CRC值进行比较。如果两个CRC值不相等，则表示数据在传输过程中发生了错误。 网卡进行CRC校验的原理主要有以下几步： 1、发送方网卡在发送数据包时，会首先对数据包进行CRC计算，得出一个CRC值。 2、发送方在数据包的尾部附加CRC值后，就将整个数据包发出。 3、当接收方网卡收到数据包后，会对除CRC值之外的数据重新进行CRC计算，得出一个新的CRC值。 4、接收方网卡会将计算出的新CRC值，和数据包尾部接收到的CRC值进行对比，如果一致则认为此数据没有在传输中被篡改，如果不一致则认为数据在传输中被篡改。 这就是网卡的CRC校验原理。虽然CRC不能完全确保数据的完整性，但是它在很大程度上提高了数据传输的可靠性。 1.4 网卡工作流程网卡发包： ip包+14个字节的mac头变成数据帧frame frame拷贝到网卡芯片内部的缓冲区，由网卡处理 网卡芯片为frame添加头部同步信息和CRC校验，此时才是真正可以发送的packet，然后发送该packet 网卡收包： 网络包packet到达网卡，网卡先检查包packet的CRC校验，保证其完整性和正确性，然后去掉它的头得到frame 网卡将frame拷贝到网卡内部的FIFO缓冲区 网卡驱动程序产生硬件中断，把frame从网卡拷贝到内存中，接下来就交给内核处理 网卡丢包 内核通常需要快速的拷贝网络数据包到系统内存！！！ 因为网卡上接收网络数据包的缓存大小固定，而且相比系统内存也要小得多。 所以上述拷贝动作一旦被延迟，必然造成网卡FIFO缓存溢出 - 进入的数据包占满了网卡的缓存，后续的包只能被丢弃，这也应该就是ifconfig里的overrun的来源。 1.5 丢包问题解决丢包排查：网卡工作在数据链路层，数据量链路层，会做一些校验，封装成帧。我们可以查看校验是否出错，确定传输是否存在问题。然后从软件层面，是否因为缓冲区太小丢包。 12345678910111213141516171819202122232425262728293031323334353637383940414243# 1 先查看硬件情况一台机器经常收到丢包的报警，先看看最底层的有没有问题:# 1.1 查看工作模式是否正常[root@egon ~]# ethtool ens33 | egrep &#x27;Speed|Duplex&#x27; Speed: 1000Mb/s Duplex: Full # 全双工 # 1.2 查看CRC校验是否正常 [root@egon ~]# ethtool -S ens33 | grep crc # crc错误值大通常是因为服务器外部的网络环境有问题导致的 rx_crc_errors: 0 -----------Speed，Duplex，CRC 之类的都没问题，基本可以排除物理层面的干扰---------- # 2 通过 ifconfig 可以看到 overruns 是否一直增大，如果查看结果是一直增大for i in `seq 1 100`; do ifconfig ens33 | grep RX | grep overruns; sleep 1; done # 3 调整网卡缓冲区[root@egon ~]# ethtool -g ens33Ring parameters for ens33:Pre-set maximums: # 最大可以设置的值RX: 4096RX Mini: 0RX Jumbo: 0TX: 4096Current hardware settings: # 当前设置的值RX: 256RX Mini: 0RX Jumbo: 0TX: 256[root@egon ~]# ethtool -G ens33 rx 2048 # 调大[root@egon ~]# ethtool -G ens33 tx 2048 # 调大[root@egon ~]# ethtool -g ens33Ring parameters for ens33:Pre-set maximums:RX: 4096RX Mini: 0RX Jumbo: 0TX: 4096Current hardware settings:RX: 2048RX Mini: 0RX Jumbo: 0TX: 2048 ethool网卡降速 1234567[root@egon ~]# ethtool -s ens33 speed 100 duplex full[root@egon ~]# ethtool -s ens33 speed 100 duplex full autoneg off # 关闭自适应才能设置成功[root@egon ~]# ethtool ens33 # 查看 若想完成永久设置，可以将上述ethtool设置写入/etc/rc.d/rc.local之中。然后记住必须要加一个x权限[root@egon ~]# chmod +x /etc/rc.d/rc.local 二、解决网络问题排查思路2.1 先搞清楚链路 思考清楚 你站在物理量的内部，访问某一台虚拟机，网络流向是什么 你站在某一台虚拟机中，访问另外一台虚拟机，网络流向是什么 你站在你们家其他物理设备上，访问某一台虚拟机，网络流向是什么 2.2 检查思路你在进行任何一次网络访问失败的时候，首先在纸上画出来它的链路 然后你就按照这个链路，去检查每一个环节 一遍查的过程，可以一遍google搜一下看看有没有人遇到过同类问题，90%的问题网上都有答案 2.3 检查手段2.3.1 定位网络问题工具1234mii-tool eth0 # 判断物理网卡是否接了网线ping命令telnet 目标ip 端口tcpdump + wireshark抓包分析 2.3.2 一些常规查询举例1、检查虚拟网卡与物理网卡 123456789101、ip a 查看网卡运行状态如果网卡处于done或者网卡无显示信息，systemctl restart network重启网卡、停止NetworkManager查看网卡配置文件具体信息2、vm层面检查虚拟网络编辑器 ---&gt; vmnet8 网卡设置 查看子网、掩码、网关等设置3、windows层面检查本地网卡检查 ---&gt; 控制面板\\网络和 Internet\\网络连接本地服务检查 ---&gt; 任务管理器 服务 vmware相关服务重启 2、测试网络连通性 123456789101112131415161718192021222324252627282930313233341、判断网卡是否能识别，是否接了有效的网线mii-tool eth0 # 判断物理网卡是否接了网线有可能明明连接了有效的网线，但是还是看不到link ok,可以先确定网卡配置文件是正确的，并且ONBOOT=yes ，然后重启network服务（systemctl restart network ）2、**ping 127.0.0.1**通，代表系统能够支持tcp/ip通信。不通，原因： 相关驱动损坏或者没有。防火墙iptables拦截了。3、**ping 网卡的ip**假设eth0配置10.1.1.22ping 10.1.1.22通，说明网卡是能够正常工作不通，可能是网卡驱动工作不正常，或iptables防火墙问题。或者是网卡未激活，可以尝试重启网络服务4、**ping 网关**不通原因： 网关有问题，或者IP冲突解决方法：ping 同一个网段中其他IP,或者用其他计算机也ping网关，如果能通，那就是自己机器的原因了5、**ping 外网（IP或域名）**# ping 外网IP通，只能说明通信没问题，网关是ok的。不通，很可能就是网关无法联网# ping 域名如果连域名对应的IP都无法返回，说明域名解析失败，原因：DNS设定有问题。6、**ping的错误类型****network unreachable (网络不可达)： 一般没有设定正确的网关****unknow host xxxx : 设定DNS无效** 2.4 Ping命令返回错误信息说明123456789101112131415161718192021222324252627282930313233343536373839# 1.Request timed out这是经常碰到的提示信息，很多文章中说这是对方机器置了过滤ICMP数据包，从上面工作过程来看，这是不完全正确的，至少有下几种情况。（1） 对方已关机，或者网络上根本没有这个地址：比如在上图中主机A中PING 192.168.0.7 ，或者主机B关机了，在主机A中PING 192.168.0.5 都会得到超时的信息。（2）对方与自己不在同一网段内，通过路由也无法找到对方，但有时对方确实是存在的，当然不存在也是返回超时的信息。（3）对方确实存在，但设置了ICMP数据包过滤（比如防火墙设置）。怎样知道对方是存在，还是不存在呢，可以用带参数 -a 的Ping命令探测对方，如果能得到对方的NETBIOS名称，则说明对方是存在的，是有防火墙设置，如果得不到，多半是对方不存在或关机，或不在同一网段内。（4）错误设置IP地址正常情况下，一台主机应该有一个网卡，一个IP地址，或多个网卡，多个IP地址（这些地址一定要处于不同的IP子网）。但如果一台电脑的“拨号网络适配器”（相当于一块软网卡）的TCP/IP设置中，设置了一个与网卡IP地址处于同一子网的IP地址，这样，在IP层协议看来，这台主机就有两个不同的接口处于同一网段内。当从这台主机Ping其他的机器时，会存在这样的问题： A.主机不知道将数据包发到哪个网络接口，因为有两个网络接口都连接在同一网段。 B.主机不知道用哪个地址作为数据包的源地址。因此，从这台主机去Ping其他机器，IP层协议会无法处理，超时后，Ping 就会给出一个“超时无应答”的错误信息提示。但从其他主机Ping这台主机时，请求包从特定的网卡来，ICMP只须简单地将目的、源地址互换，并更改一些标志即可，ICMP应答包能顺利发出，其他主机也就能成功Ping通这台机器了。# 2.Destination host Unreachable（1） 对方与自己不在同一网段内，而自己又未设置默认的路由，比如上例中A机中不设定默认的路由，运行Ping192.168.0.1.4就会出现“Destination host Unreachable”。（2）网线出了故障这里要说明一下“destination host unreachable”和 “time out”的区别，如果所经过的路由器的路由表中具有到达目标的路由，而目标因为其他原因不可到达，这时候会出现“time out”，如果路由表中连到达目标的路由都没有，那就会出现“destination host unreachable”。# 3.Bad IP address这个信息表示您可能没有连接到DNS服务器，所以无法解析这个IP地址，也可能是IP地址不存在。# 4.Source quench received这个信息比较特殊，它出现的机率很少。它表示对方或中途的服务器繁忙无法回应。# 5.Unknown host——不知名主机这种出错信息的意思是，该远程主机的名字不能被域名服务器（DNS）转换成IP地址。故障原因可能是域名服务器有故障，或者其名字不正确，或者网络管理员的系统与远程主机之间的通信线路有故障。# 6.No answer——无响应这种故障说明本地系统有一条通向中心主机的路由，但却接收不到它发给该中心主机的任何信息。故障原因可能是下列之一：中心主机没有工作；本地或中心主机网络配置不正确；本地或中心的路由器没有工作；通信线路有故障；中心主机存在路由选择问题。# 7.Ping 127.0.0.1127.0.0.1是本地循环地址.如果本地址无法Ping通，则表明本地机TCP/IP协议不能正常工作。# 8.no rout to host网卡工作不正常。# 9.transmit failed，error code：10043网卡驱动不正常。# 10.unknown host nameDNS配置不正确","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"网络管理","slug":"网络管理","permalink":"https://aquapluto.github.io/tags/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/"}]},{"title":"网桥配置","slug":"Linux/network-manage/bridge","date":"2025-08-21T03:00:39.000Z","updated":"2025-09-07T12:33:52.633Z","comments":true,"path":"Linux/network-manage/bridge/","permalink":"https://aquapluto.github.io/Linux/network-manage/bridge/","excerpt":"","text":"一、桥接原理桥接：把一台机器上的若干个网络接口“连接”起来。其结果是，其中一个网口收到的报文会被复制给其他网口并发送出去。以使得网口之间的报文能够互相转发。网桥就是这样一个设备，它有若干个网口，并且这些网口是桥接起来的。与网桥相连的主机就能通过交换机的报文转发而互相通信 主机A发送的报文被送到交换机S1的eth0口，由于eth0与eth1、eth2桥接在一起，故而报文被复制到eth1和eth2，并且发送出去，然后被主机B和交换机S2接收到。而S2又会将报文转发给主机C、D CentOS 8 取消brctl 工具,可以用下面方法查看网桥 1234#查看桥接情况[root@centos8 ~]#ip link show master virbr0[root@centos8 ~]#bridge link show 二、配置实现网桥工具包：bridge-utils,目前 CentOS 8 系统光盘里无此包,EPEL源有此包 原理：将centos7这台机器模拟成网桥，实现网桥的功能，使得A和B能够连接通讯 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#临时生效centos7机器准备两个网卡，一个网卡是仅主机模式对应vmnet1，另一个网卡是NAT模式对应vmnet8A主机的网卡是仅主机模式，对应vmnet1，B主机的网卡是NAT模式，对应vmnet8，这个时候A和B想要通就必须把centos7主机配置成交换机配置之前需要安装#yum install bridge-utils#centos7上创建网桥并启用up[root@centos7 ~]#brctl addbr br0[root@centos7 ~]#brctl show #查看[root@centos7 ~]#ip link set br0 up#将两网卡接到交换机上（网桥）[root@centos7 ~]#brctl addif br0 eth0[root@centos7 ~]#brctl addif br0 eth1#之后A和B就能通了,但是配置好后centos7就断了，原因是机器上的两网卡被br0所使用#加地址[root@centos7 ~]#ip a a 10.0.0.20/24 dev br0#如果将centos7上的两网卡删除，也不影响，因为交换机不配IP地址也不影响主机之间的通讯，配ip地址只是为了方便远程操控管理#永久生效第一种方法：nmcli命令#创建网桥nmcli con add type bridge con-name br0 ifname br0nmcli connection modify br0 ipv4.addresses 10.0.0.100/24 ipv4.method manualnmcli con up br0#加入物理网卡nmcli con add type bridge-slave con-name br0-port0 ifname eth0 master br0nmcli con add type bridge-slave con-name br0-port1 ifname eth1 master br0nmcli con up br0-port0nmcli con up br0-port1第二种方法：写配置文件vim /etc/sysconfig/network-scripts/ifcfg-br0DEVICE=br0STP=yesTYPE=Bridge#以下可以不要BOOTPROTO=staticIPADDR=10.0.0.100PREFIX=24vim /etc/sysconfig/network-scripts/ifcfg-br0-eth0TYPE=EthernetNAME=br0-eth0DEVICE=eth0ONBOOT=yesBRIDGE=br0vim /etc/sysconfig/network-scripts/ifcfg-br0-eth1TYPE=EthernetNAME=br0-eth1DEVICE=eth1ONBOOT=yesBRIDGE=br0#查看网桥brctl show#添加和删除网桥brctl addbr | delbr br0#删除之前先禁用：ip link set br0 down#添加和删除网桥中网卡brctl addif | delif br0 eth0 STP 生成树协议 123456正常情况下，三台交换机，连两条网线，但这种情况下，如果断掉了一条线，则网络就会中断，为了解决此问题，三台交换机，连三线网线，这样，如果断了一条线，网络还是可用的，但这样会形成一个环形网络，由于交换机执行广播请求，那这种网络会造成网络风暴，所以需要启用stp规避此问题#开启[root@centos7 ~]#brctl stp br0 on#关闭[root@centos7 ~]#brctl stp br0 off","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"网络管理","slug":"网络管理","permalink":"https://aquapluto.github.io/tags/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/"}]},{"title":"网络测试诊断工具","slug":"Linux/network-manage/test-diagnostic","date":"2025-08-21T03:00:33.000Z","updated":"2025-09-07T12:33:21.780Z","comments":true,"path":"Linux/network-manage/test-diagnostic/","permalink":"https://aquapluto.github.io/Linux/network-manage/test-diagnostic/","excerpt":"","text":"一、ping命令用于测试网络连接的常见命令行工具，通常用于检查目标主机是否可达以及测量网络往返时间（RTT）。 1234567891011121314151617-c N #ping N次后停止ping-s N #一个ping包的字节数大小-W N #第一个ping包的响应超时时间，单位S，其余ping包的响应超时时间为1s-w N #执行PING操作的超时时间，单位S-f #极限检测，快速连续ping一台主机[root@ubuntu2004 ~]#ping 10.0.0.179PING 10.0.0.179 (10.0.0.179) 56(84) bytes of data.64 bytes from 10.0.0.179: icmp_seq=1 ttl=64 time=0.809 ms64 bytes from 10.0.0.179: icmp_seq=2 ttl=64 time=4.58 ms64 bytes from 10.0.0.179: icmp_seq=3 ttl=64 time=4.73 ms64 bytes from 10.0.0.179: icmp_seq=4 ttl=64 time=4.44 ms64 bytes from 10.0.0.179: icmp_seq=5 ttl=64 time=2.58 ms^C--- 10.0.0.179 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4047msrtt min/avg/max/mdev = 0.809/3.429/4.733/1.525 ms 字段输出解释： PING：这是 ping 命令的第一行，显示正在执行的 ping 命令以及目标主机的 IP 地址或主机名。 bytes：每个 ICMP 报文的大小，通常默认为 64 字节。 icmp_seq：ICMP 报文的序列号，从 0 开始递增。 ttl：生存时间（Time to Live），表示报文在网络上能够存活的跳数（路由器数量）。 time：每个 ICMP 报文的往返时间（Round-Trip Time，RTT），以毫秒为单位。这是从发送 ICMP 报文到接收响应所经过的时间。 — 10.0.0.179 ping statistics —：表示一个 ICMP 报文的往返时间统计结束并显示 ping 统计信息的标题。 packets transmitted：发送的 ICMP 报文数量，表示发送的次数。 packets received：接收的 ICMP 响应报文数量，表示成功收到的次数。 packet loss：丢失的 ICMP 报文数量，表示未收到响应的次数，通常以百分比形式显示。 time：用于显示 RTT 的统计信息，通常包括最小、最大和平均 RTT 时间。 范例: 利用icmp协议判断网络状态 1234567891011121314151617181920[root@centos7 ~]#ping 10.0.0.8PING 10.0.0.8 (10.0.0.8) 56(84) bytes of data.64 bytes from 10.0.0.8: icmp_seq=1 ttl=64 time=0.307 ms64 bytes from 10.0.0.8: icmp_seq=2 ttl=64 time=0.344 ms[root@centos7 ~]#ping 10.0.0.81 #ping一个不存在地址出现的提示PING 10.0.0.81 (10.0.0.81) 56(84) bytes of data.From 10.0.0.7 icmp_seq=1 Destination Host Unreachable[root@centos7 ~]#iptables -A INPUT -s 10.0.0.8 -j REJECT[root@centos7 ~]#ping 10.0.0.8 #受防火墙规则限制的提示PING 10.0.0.8 (10.0.0.8) 56(84) bytes of data.From 10.0.0.8 icmp_seq=1 Destination Port UnreachableFrom 10.0.0.8 icmp_seq=2 Destination Port Unreachable[root@centos8 ~]#ping -s 65508 10.0.0.8 #指定发送65508大小的包Error: packet size 65508 is too large. Maximum is 65507 #最大只能是65507[root@centos8 ~]#ping -f -s 65507 172.18.0.200 #攻击一个机器，发送大量的包，消耗他的网络带宽PING 172.18.0.200 (172.18.0.200) 65507(65535) bytes of data. 二、mtr命令结合了 traceroute 和 ping 的功能，用于追踪网络路径和测量网络往返时间（RTT）。mtr 不仅可以显示路由路径，还能实时监控网络路径中每个跳的性能指标。 1yum install -y mtr 语法 12345mtr [选项] 目标主机-n：用于禁用域名解析。-c：用于指定发送数据包的次数。-i：用于设置报文发送的时间间隔。 范例：基本 mtr 测试 1mtr www.baidu.com 这会启动 mtr 并追踪到达 www.baidu.com 的网络路径，并实时显示每个路由器的 IP 地址、主机名、以及每个路由器的 RTT 时间。 字段输出解释： Host：显示路由路径上每个跳的主机名或 IP 地址，其中最后一行显示了目标主机（在本例中是 14.119.104.254）。 Loss%：损失率，表示到达每个跳的数据包丢失的百分比。在这个例子中，192.168.* 跳的损失率都是 0%，这意味着我本地没有数据包丢失，我本地网络正常，而中间网络就存在丢包了，说明中间路由网络是有延迟的。 Snt：发送的数据包数量，表示发送给每个跳的数据包总数。 Last：最后一个数据包的往返时间（RTT），以毫秒为单位。这是从源主机到达每个跳的最后一个数据包的时间。 Avg：平均往返时间，以毫秒为单位。这是从源主机到达每个跳的所有数据包的平均时间。 Best：最短往返时间，以毫秒为单位。这是从源主机到达每个跳的最快数据包的时间。 Wrst：最长往返时间，以毫秒为单位。这是从源主机到达每个跳的最慢数据包的时间。 StDev：往返时间的标准差，以毫秒为单位。它表示 RTT 变化的程度，越小越好。 通常网络出问题我们都会使用 ping 命令，但是该命令只是简单的网络连通性测试，却无法确定网络是在哪里出了问题，此时就会使用 traceroute 来查看数据包途径路由，或使用 nslookup 来查看 DNS 解析状态是否正常。而我们的 mtr 命令正好集成了这三个命令的功能，从而实现了我们的需求。 一般情况下 mtr 前几跳都是本地 ISP，后几跳属于服务商（如本次案例的百度），中间跳数则是中间网络互联节点，如果前几跳异常，需联系本地 ISP，如果后几跳出现问题，则需联系服务商（百度），如果中间几跳出现问题，则两边都无完全解决问题。 三、fpingfping是一个程序，用于将ICMP探测发送到网络主机，可以在命令行上定义任意数量的主机，或者指定包含要ping的IP地址或主机列表的文件, 常在shell 脚本中使用 12#-g 选项可以指定网段或地址范围[root@centos8 ~]#fping -g 10.0.0.0/24 四、tcpdump网络数据包截获分析工具。支持针对网络层、协议、主机、网络或端口的过滤。并提供and、or、not等逻辑语句帮助去除无用的信息 1234567891011121314151617181920212223242526272829tcpdump [-adeflnNOpqStvx][-c&lt;数据包数目&gt;][-dd][-ddd][-F&lt;表达文件&gt;][-i&lt;网络界面&gt;][-r&lt;数据包文件&gt;][-s&lt;数据包大小&gt;][-tt][-T&lt;数据包类型&gt;][-vv][-w&lt;数据包文件&gt;][输出数据栏位]参数说明：-a #尝试将网络和广播地址转换成名称。-c&lt;数据包数目&gt; #收到指定的数据包数目后，就停止进行倾倒操作。-d #把编译过的数据包编码转换成可阅读的格式，并倾倒到标准输出。-dd #把编译过的数据包编码转换成C语言的格式，并倾倒到标准输出。-ddd #把编译过的数据包编码转换成十进制数字的格式，并倾倒到标准输出。-e #显示链路层信息,默认不显示链路层-f #用数字显示网际网络地址。-F&lt;表达文件&gt; #指定内含表达方式的文件。-i&lt;网络接口&gt; #使用指定的网络截面送出数据包。-l #使用标准输出列的缓冲区。-n #不把主机的网络地址转换成名字-nn #数字化-N #不列出域名。-O #不将数据包编码最佳化。-p #不让网络界面进入混杂模式。-q #快速输出，仅列出少数的传输协议信息。-r&lt;数据包文件&gt; #从指定的文件读取数据包数据。-s&lt;数据包大小&gt; #设置每个数据包的大小。默认只截取前96个字节,使用-s0可以截取所有报文内容-S #用绝对而非相对数值列出TCP关联数。-t #在每列倾倒资料上不显示时间戳记。-tt #在每列倾倒资料上显示未经格式化的时间戳记。-T&lt;数据包类型&gt; #强制将表达方式所指定的数据包转译成设置的数据包类型。-v #详细显示指令执行过程。-vv #更详细显示指令执行过程。-x #用十六进制字码列出数据包资料。-w&lt;数据包文件&gt; #把数据包数据写入指定的文件。 常规过滤规则基于IP地址过滤：host使用 host 就可以指定 host ip 进行过滤 1$ tcpdump host 192.168.10.100 数据包的 ip 可以再细分为源ip和目标ip两种 12345# 根据源ip进行过滤$ tcpdump -i eth2 src 192.168.10.100# 根据目标ip进行过滤$ tcpdump -i eth2 dst 192.168.10.200 基于网段进行过滤：net若你的ip范围是一个网段，可以直接这样指定 1$ tcpdump net 192.168.10.0/24 网段同样可以再细分为源网段和目标网段 12345# 根据源网段进行过滤$ tcpdump src net 192.168# 根据目标网段进行过滤$ tcpdump dst net 192.168 基于端口进行过滤：port使用 port 就可以指定特定端口进行过滤 1$ tcpdump port 8088 端口同样可以再细分为源端口，目标端口 12345# 根据源端口进行过滤$ tcpdump src port 8088# 根据目标端口进行过滤$ tcpdump dst port 8088 如果你想要同时指定两个端口你可以这样写 1$ tcpdump port 80 or port 8088 但也可以简写成这样 1$ tcpdump port 80 or 8088 如果你的想抓取的不再是一两个端口，而是一个范围，一个一个指定就非常麻烦了，此时你可以这样指定一个端口段。 123$ tcpdump portrange 8000-8080$ tcpdump src portrange 8000-8080$ tcpdump dst portrange 8000-8080 对于一些常见协议的默认端口，我们还可以直接使用协议名，而不用具体的端口号 比如 http &#x3D;&#x3D; 80，https &#x3D;&#x3D; 443 等 1$ tcpdump tcp port http 基于协议进行过滤：proto常见的网络协议有：tcp, udp, icmp, http, ip,ipv6 等 若你只想查看 icmp 的包，可以直接这样写 1$ tcpdump icmp protocol 可选值：ip, ip6, arp, rarp, atalk, aarp, decnet, sca, lat, mopdl, moprc, iso, stp, ipx, or netbeui 过滤规则组合 and：所有的条件都需要满足，也可以表示为 &amp;&amp; or：只要有一个条件满足就可以，也可以表示为 || not：取反，也可以使用 ! 举个例子，我想需要抓一个来自10.5.2.3，发往任意主机的3389端口的包 1$ tcpdump src 10.5.2.3 and dst port 3389 当你在使用多个过滤器进行组合时，有可能需要用到括号，而括号在 shell 中是特殊符号，因为你需要使用引号将其包含。例子如下： 1$ tcpdump &#x27;src 10.0.2.4 and (dst port 3389 or 22)&#x27; 而在单个过滤器里，常常会判断一条件是否成立，这时候，就要使用下面两个符号 =：判断二者相等 ==：判断二者相等 !=：判断二者不相等 当你使用这两个符号时，tcpdump 还提供了一些关键字的接口来方便我们进行判断，比如 if：表示网卡接口名、 proc：表示进程名 pid：表示进程 id svc：表示 service class dir：表示方向，in 和 out eproc：表示 effective process name epid：表示 effective process ID 比如我现在要过滤来自进程名为 nc 发出的流经 en0 网卡的数据包，或者不流经 en0 的入方向数据包，可以这样子写 1$ tcpdump &quot;( if=en0 and proc =nc ) || (if != en0 and dir=in)&quot; 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#查看网卡[root@centos8 ~]#tcpdump -D#不指定任何参数，监听第一块网卡上经过的数据包。主机上可能有不止一块网卡，所以经常需要指定网卡。tcpdump#监听特定网卡tcpdump -i en0#监听特定主机，监听主机10.0.0.100 的通信包，注意：出、入的包都会被监听。tcpdump host 10.0.0.100#特定来源、目标地址的通信#特定来源tcpdump src host hostname#特定目标地址tcpdump dst host hostname#如果不指定src跟dst，那么来源或者目标是hostname的通信都会被监听tcpdump host hostname#面试题[root@centos8 ~]#tcpdump -i eth0 -nn icmp and src host 10.0.0.6 and dst host 10.0.0.7#特定端口tcpdump port 3000#监听TCP/UDP，服务器上不同服务分别用了TCP、UDP作为传输层，假如只想监听TCP的数据包tcpdump tcp#来源主机+端口+TCP，监听来自主机10.0.0.100在端口22上的TCP数据包tcpdump tcp port 22 and src host 10.0.0.100#监听特定主机之间的通信tcpdump ip host 10.0.0.101 and 10.0.0.102#10.0.0.101和除了10.0.0.1之外的主机之间的通信tcpdump ip host 10.0.0.101 and ! 10.0.0.1#限制抓包的数量，如下，抓到1000个包后，自动退出tcpdump -c 1000#保存到本地，tcpdump默认会将输出写到缓冲区，只有缓冲区内容达到一定的大小，或者tcpdump退出时，才会将输出写到本地磁盘,可以加上-U强制立即写到本地磁盘（一般不建议，性能相对较差）tcpdump -n -vvv -c 1000 -w /tmp/tcpdump_save.cap#详细示例tcpdump tcp -i eth1 -t -s 0 -c 100 and dst port ! 22 and src net 192.168.1.0/24 -w ./target.cap(1)tcp: ip icmp arp rarp 和 tcp、udp、icmp这些选项等都要放到第一个参数的位置，用来过滤数据报的类型(2)-i eth1 : 只抓经过接口eth1的包(3)-t : 不显示时间戳(4)-s 0 :设置为0表示使用默认值262144字节抓取每个包，以便与tcpdump的旧版本兼容(5)-c 100 : 只抓取100个数据包(6)dst port ! 22 : 不抓取目标端口是22的数据包(7)src net 192.168.1.0/24 : 数据包的源网络地址为192.168.1.0/24(8)-w ./target.cap : 保存成cap文件，方便用wireshark分析 输出内容结构121:26:49.013621 IP 172.20.20.1.15605 &gt; 172.20.20.2.5920: Flags [P.], seq 49:97, ack 106048, win 4723, length 48 从上面的输出来看，可以总结出： 第一列：时分秒毫秒 21:26:49.013621 第二列：网络协议 IP 第三列：发送方的ip地址+端口号，其中172.20.20.1是 ip，而15605 是端口号 第四列：箭头 &gt;， 表示数据流向 第五列：接收方的ip地址+端口号，其中 172.20.20.2 是 ip，而5920 是端口号 第六列：冒号 第七列：数据包内容，包括Flags 标识符，seq 号，ack 号，win 窗口，数据长度 length，其中 [P.] 表示 PUSH 标志位为 1，更多标识符见下面 Flags 标识符 使用 tcpdump 抓包后，会遇到的 TCP 报文 Flags，有以下几种： [S] : SYN（开始连接） [P] : PSH（推送数据） [F] : FIN （结束连接） [R] : RST（重置连接） [.] : 没有 Flag （意思是除上面四种类型外的其他情况，有可能是 ACK 也有可能是 URG） 抓包实战应用例子提取HTTP的User-Agent从 HTTP 请求头中提取 HTTP 的 User-Agent： 1$ tcpdump -nn -A -s1500 -l | grep &quot;User-Agent:&quot; 通过 egrep 可以同时提取User-Agent 和主机名（或其他头文件）： 1$ tcpdump -nn -A -s1500 -l | egrep -i &#x27;User-Agent:|Host:&#x27; 抓取HTTP GET和POST请求抓取 HTTP GET 请求包： 12345$ tcpdump -s 0 -A -vv &#x27;tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420&#x27;# or$ tcpdump -vvAls0 | grep &#x27;GET&#x27; 可以抓取 HTTP POST 请求包： 12345$ tcpdump -s 0 -A -vv &#x27;tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354&#x27;# or $ tcpdump -vvAls0 | grep &#x27;POST&#x27; 注意：该方法不能保证抓取到 HTTP POST 有效数据流量，因为一个 POST 请求会被分割为多个 TCP 数据包。 找出发包数最多的 IP找出一段时间内发包最多的 IP，或者从一堆报文中找出发包最多的 IP，可以使用下面的命令： 1$ tcpdump -nnn -t -c 200 | cut -f 1,2,3,4 -d &#x27;.&#x27; | sort | uniq -c | sort -nr | head -n 20 cut -f 1,2,3,4 -d ‘.’ : 以 . 为分隔符，打印出每行的前四列。即 IP 地址。 sort | uniq -c : 排序并计数 sort -nr : 按照数值大小逆向排序 抓取DNS请求和响应DNS 的默认端口是 53，因此可以通过端口进行过滤 1$ tcpdump -i any -s0 port 53 切割pcap文件当抓取大量数据并写入文件时，可以自动切割为多个大小相同的文件。例如，下面的命令表示每 3600 秒创建一个新文件 capture-(hour).pcap，每个文件大小不超过 200*1000000 字节： 1$ tcpdump -w /tmp/capture-%H.pcap -G 3600 -C 200 这些文件的命名为 capture-&#123;1-24&#125;.pcap，24 小时之后，之前的文件就会被覆盖。 提取HTTP POST请求中的密码从 HTTP POST 请求中提取密码和主机名： 1$ tcpdump -s 0 -A -n -l | egrep -i &quot;POST /|pwd=|passwd=|password=|Host:&quot; 提取HTTP请求的URL提取 HTTP 请求的主机名和路径： 1$ tcpdump -s 0 -v -n -l | egrep -i &quot;POST /|GET /|Host:&quot; 抓取HTTP有效数据包抓取 80 端口的 HTTP 有效数据包，排除 TCP 连接建立过程的数据包（SYN &#x2F; FIN &#x2F; ACK）： 1$ tcpdump &#x27;tcp port 80 and (((ip[2:2] - ((ip[0]&amp;0xf)&lt;&lt;2)) - ((tcp[12]&amp;0xf0)&gt;&gt;2)) != 0)&#x27; 五、nmap扫描远程主机工具，比发送 ICMP 报文的 ping 命令的功能要强大很多 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#仅列出指定网段上的每台主机,不发送任何报文到目标主机.[root@centos8 ~]#nmap -sL 10.0.0.0/24#可以指定一个IP地址范围[root@centos8 ~]#nmap -sP 10.0.0.1-10#批量扫描一个网段的主机存活数nmap -sP -v 192.168.1.0/24nmap –v –sn ip/24#有些主机关闭了ping检测,所以可以使用-P0跳过ping的探测,可以加快扫描速度.nmap -P0 192.168.1.100#扫描主机nmap –v –A IP #一次性扫描多台目标主机[root@centos8 ~]#nmap 10.0.0.6 10.0.0.7#探测目标主机开放的端口,可指定一个以逗号分隔的端口列表(如-PS22,443,80)[root@centos8 ~]#nmap -PS22,80,443 10.0.0.1#使用SYN半开放扫描[root@centos8 ~]#nmap -sS 10.0.0.1#扫描开放了TCP端口的设备[root@centos8 ~]#nmap -sT 10.0.0.1#扫描开放了UDP端口的设备[root@centos8 ~]#nmap -sU 10.0.0.1#只扫描UDP端口nmap –e eth1 -sU -O 10.0.0.1 #扫描TCP和UDP端口nmap -sTU -O 10.0.0.1#用于扫描目标主机服务版本号[root@centos8 ~]#nmap -sV 10.0.0.7#查看主机当前开放的端口nmap localhost #查看主机端口（1024-65535）中开放的端口nmap -p 1024-65535 localhost #探测目标主机开放的端口nmap -PS 10.0.0.1 #探测所列出的目标主机端口nmap -PS22,80,3306 10.0.0.1 #探测目标主机操作系统类型nmap -O 10.0.0.1#探测目标主机操作系统类型nmap -A 10.0.0.1 六、nc 实现任意TCP&#x2F;UDP端口的侦听，nc可以作为server以TCP或UDP方式侦听指定端口 端口的扫描，nc可以作为client发起TCP或UDP连接 机器之间传输文件 机器之间网络测速 普通用户不能监听1023以内（包括1023）的端口 12345678910111213141516nc [-hlnruz][-g&lt;网关...&gt;][-G&lt;指向器数目&gt;][-i&lt;延迟秒数&gt;][-o&lt;输出文件&gt;][-p&lt;通信端口&gt;][-s&lt;来源位址&gt;][-v...][-w&lt;超时秒数&gt;][主机名称][通信端口...]-g&lt;网关&gt; 设置路由器跃程通信网关，最多可设置8个。-G&lt;指向器数目&gt; 设置来源路由指向器，其数值为4的倍数。-h 在线帮助。-i&lt;延迟秒数&gt; 设置时间间隔，以便传送信息及扫描通信端口。-l 使用监听模式，管控传入的资料。-n 直接使用IP地址，而不通过域名服务器。-o&lt;输出文件&gt; 指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。-p&lt;通信端口&gt; 设置本地主机使用的通信端口。-r 乱数指定本地与远端主机的通信端口。-s&lt;来源位址&gt; 设置本地主机送出数据包的IP地址。-u 使用UDP传输协议-v 显示指令执行过程。-w&lt;超时秒数&gt; 设置等待连线的时间。-z 表示zero，表示扫描时不发送任何数据，只在扫描通信端口时使用 范例 12345678910111213141516#安装nc[root@ubuntu1804 ~]#apt -y install netcat-openbsd[root@centos8 ~]#yum -y install nc#探测TCP协议[root@ubuntu1804 ~]#nc -zv 10.0.0.101 22Connection to 10.0.0.101 22 port [tcp/ssh] succeeded![root@ubuntu1804 ~]#nc -zv 10.0.0.101 2222nc: connect to 10.0.0.101 port 2222 (tcp) failed: Connection refused#探测UDP协议[root@ubuntu1804 ~]#nc -zv -u 10.0.0.101 2049Connection to 10.0.0.101 2049 port [udp/nfs] succeeded![root@ubuntu1804 ~]#nc -zv -u 10.0.0.101 111Connection to 10.0.0.101 111 port [udp/sunrpc] succeeded![root@ubuntu1804 ~]#nc -zv -u 10.0.0.101 123 范例: 扫描远程服务 1234567891011121314151617#!/bin/bashnet=10.0.0port=5900begin=3end=254. /etc/os-releaseif [ $ID_LIKE =~ rhel ]];then rpm -q nc || yum -y install ncelse dpkg -i netcat-openbsd &amp;&gt; /dev/null || apt -y install netcat-openbsdfifor i in `eval echo &#123;$begin..$end&#125;`;do &#123; nc -z -v -w 1 $net.$i $port &amp;&gt; /dev/null &amp;&amp; echo $net.$i &gt; hosts.txt &#125;&amp;donewait","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"网络管理","slug":"网络管理","permalink":"https://aquapluto.github.io/tags/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/"}]},{"title":"网络配置","slug":"Linux/network-manage/configuration","date":"2025-08-21T03:00:25.000Z","updated":"2025-09-07T12:31:47.627Z","comments":true,"path":"Linux/network-manage/configuration/","permalink":"https://aquapluto.github.io/Linux/network-manage/configuration/","excerpt":"","text":"1 网络配置1.1 网络配置相关文件1.1.1 IP、MASK、GW、DNS相关的配置文件12345678910111213141516171819202122#rocky/etc/sysconfig/network-scripts/ifcfg-IFACE#Ubuntu/etc/netplan/*.yamlTYPE #接口类型；常见有的Ethernet, BridgeNAME #此配置文件应用到的设备DEVICE #设备名HWADDR #对应的设备的MAC地址UUID #设备的惟一标识BOOTPROTO #激活此设备时使用的地址配置协议，常用的dhcp, static, none, bootpIPADDR #指明IP地址NETMASK #子网掩码,如:255.255.255.0PREFIX #网络ID的位数, 如:24GATEWAY #默认网关DNS1 #第一个DNS服务器地址DNS2 #第二个DNS服务器地址DOMAIN #主机不完整时，自动搜索的域名后缀ONBOOT #在系统引导时是否激活此设备USERCTL #普通用户是否可控制此设备PEERDNS #如果BOOTPROTO的值为“dhcp”，YES将允许dhcp server分配的dns服务器信息直接覆盖至/etc/resolv.conf文件，NO不允许修改resolv.confNM_CONTROLLED #NM是NetworkManager的简写，此网卡是否接受NM控制 1.1.2 配置当前主机的主机名CentOS 6 之前版本 123/etc/sysconfig/networkHOSTNAME=在文件里修改完以后，还需要执行hostname xxxx才可以保存 CentOS 7 以后版配置文件: 123/etc/hostname默认没有此文件，通过DNS反向解析获取主机名，主机名默认为：localhost.localdomain删除文件/etc/hostname，恢复主机名localhost.localdomain Ubuntu 1/etc/hostname 1.1.3 DNS域名解析12345/etc/resolv.confnameserver DNS_SERVER_IP1nameserver DNS_SERVER_IP2nameserver DNS_SERVER_IP3search DOMAIN 常见公共DNS 123456789101112131415161718192021180.76.76.76 百度223.5.5.5 阿里223.6.6.6 阿里119.29.29.29 腾讯119.28.28.28 腾讯114.114.114.114 电信114.114.115.115 电信1.2.4.8 CNNIC210.2.4.8 CNNIC240c::6666 CNNIC240c::6644 CNNIC80.80.80.80 Freenom World80.80.81.81 Freenom World8.8.8.8 Google8.8.4.4 Google1.1.1.1 Cloudflare117.50.11.11 OneDNS117.50.22.22 OneDNS52.80.66.66 OneDNS117.50.10.10 OneDNS 52.80.52.52 OneDNS /etc/hosts 和 /etc/resolv.conf 的区别 /etc/hosts 主要用于本地主机名到 IP 地址的静态映射，而 /etc/resolv.conf 主要用于配置 DNS 解析器的设置，指定 DNS 服务器和其他相关选项。 /etc/hosts 的查询优先级高于 DNS，因为它是在本地执行的，而 /etc/resolv.conf 则是指示系统使用哪些 DNS 服务器进行域名解析。 1.1.4 修改 &#x2F;etc&#x2F;hosts和DNS的优先级123456789vim /etc/hosts10.0.0.8 www.baidu.comvim /etc/sysconfig/nerwork-scripts/ifcfg-eth0....DOMAIN=magedu.com....由于hosts比DNS优先级高，所以在/etc/hosts将www.baidu.com指向的IP地址为10.0.0.8后，去ping www.baidu.com得出的IP地址是10.0.0.8，就不是原先百度的地址 修改 12vim etc/nsswitch.confhosts: files dns 1.1.5 路由相关的配置文件12345678910/etc/sysconfig/network-scripts/route-IFACE两种风格：(1) TARGET via GW如：10.0.0.0/8 via 172.16.0.1(2) 每三行定义一条路由ADDRESS#=TARGETNETMASK#=maskGATEWAY#=GW 范例: CentOS7 创建&#x2F;etc&#x2F;sysconfig&#x2F;static-routes文件添加持久静态路由 1234567891011121314151617181920212223242526272829303132#查看network脚本调用路由文件[root@centos7 ~]#grep -A 3 &quot;/etc/sysconfig/static-routes&quot; /etc/init.d/network if [ -f /etc/sysconfig/static-routes ]; then if [ -x /sbin/route ]; then grep &quot;^any&quot; /etc/sysconfig/static-routes | while read ignore args ;do /sbin/route add -$args done else #查看当前路由[root@centos7 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0#创建文件[root@centos7 ~]#vim /etc/sysconfig/static-routes[root@centos7 ~]#cat /etc/sysconfig/static-routesany net 192.168.1.0/24 gw 10.0.0.254any net 192.168.2.0/24 gw 10.0.0.254[root@centos7 ~]#systemctl restart network#确认路由生效[root@centos7 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.1.0 10.0.0.254 255.255.255.0 UG 0 0 0 eth0192.168.2.0 10.0.0.254 255.255.255.0 UG 0 0 0 eth0 1.2 网卡配置注意：在虚拟机加网卡开着机加，不要关机加，不然会损坏虚拟机 1.2.1 网卡名称systemd对网络设备的命名方式 如果Firmware或BIOS为主板上集成的设备提供的索引信息可用，且可预测则根据此索引进行命名，如：eno1 如果Firmware或BIOS为PCI-E扩展槽所提供的索引信息可用，且可预测，则根据此索引进行命名，如：ens1 如果硬件接口的物理位置信息可用，则根据此信息命名，如：enp2s0 如果用户显式启动，也可根据MAC地址进行命名，如：enx2387a1dc56 上述均不可用时，则使用传统命名机制 基于BIOS支持启用biosdevname软件 12内置网卡：em1,em2 pci卡：pYpX Y：slot ,X:port 网卡组成格式 1234567en: Ethernet 有线局域网wl: wlan 无线局域网ww: wwan无线广域网o&lt;index&gt;: 集成设备的设备索引号s&lt;slot&gt;: 扩展槽的索引号x&lt;MAC&gt;: 基于MAC地址的命名p&lt;bus&gt;s&lt;slot&gt;: enp2s1 使用传统命名方式 1234567891011121314151617181920212223242526Rocky8.61 编辑文件/etc/default/grubGRUB_CMDLINE_LINUX=&quot;*quiet net.ifnames=0 biosdevname=0&quot;2 为grub2生成其配置文件基于UEFI模式引导的系统# grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg基于BIOS模式引导的系统# grub2-mkconfig -o /boot/grub2/grub.cfg# grub2-mkconfig -o /etc/grub2.cfg3 重启rebootUbuntu22.041 编辑文件/etc/default/grubGRUB_CMDLINE_LINUX=&quot; net.ifnames=0&quot;2 重新读取配置文件grub-mkconfig -o /boot/grub/grub.cfg2 重启reboot 自定义网卡名 1GRUB_CMDLINE_LINUX=&quot;... net.ifnames.prefix=&lt;required prefix&gt;&quot; 范例：自定义网卡名 12345678910111213141516[root@centos8 ~]# vi /etc/default/grubGRUB_CMDLINE_LINUX=&quot;net.ifnames.prefix=wang&quot;[root@centos8 ~]# grub2-mkconfig -o /boot/grub2/grub.cfg[root@centos8 ~]# reboot[root@centos8 ~]# ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULTgroup default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: wang0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPmode DEFAULT group default qlen 1000 link/ether 00:0c:29:15:9b:83 brd ff:ff:ff:ff:ff:ff3: wang1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPmode DEFAULT group default qlen 1000 link/ether 00:0c:29:15:9b:8d brd ff:ff:ff:ff:ff:ff 1.2.2 网卡地址配置配置选择（适用于Centos7,8） 1234567891011121314151617181920212223#必须配置DEVICE=ens33 #指定网卡设备名称NAME=ens33 #命令行下显示的配置名称BOOTPROTO=static #IP地址类型，获取IP方式，dhcp 动态获取，none|static 静态地址IPADDR=10.193.12.23 #IPV4的IP地址，多个地址可以写成 IPADDR2，IPADDR3NETMASK=255.255.255.0 | PREFIX=24 #传统写法 NETMASK=255.255.255.0；新写法 PREFIX=24，多地址可以写成PREFIX2GATEWAY=10.193.12.254 #网关，提供跨网段通讯功能DNS1=10.1.26.188#以下可选DOMAIN=magedu.com #域后缀TYPE=EthernetUUID=ef482e1b-4f2f-4d23-ae65-784122f24201ONBOOT=yes #网卡设备是否开机启用，yes 启用，no 禁用BROWSER_ONLY=noPROXY_METHOD=noneDEFROUTE=yesIPV4_FAILUREFATAL=noIPV6INIT=yesIPV6AUTOCONF=yesIPV6_DEFROUTE=yesIPV6FAILURE_FATAL=noIPV6ADDR_GEN_MODE=stable-privacy 范例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556571 进入目录[root@centos ~]#cd /etc/sysconfig/network-scripts2 配置文件[root@centos network-scripts]#vim ifcfg-eth0 #必须ifcfg开头，后面跟网卡名#动态获取地址DEVICE=eth0 NAME=eth0BOOTPROTO=dhcp[root@centos network-scripts]#nmcli con reload #centos8执行这两条命令[root@centos network-scripts]#nmcli con up eth0[root@centos ~]#systemctl restart network #centos7执行这条命令#静态获取地址（手工配置）DEVICE=eth0 NAME=eth0BOOTPROTO=static | none #二选一IPADDR=10.0.0.183NETMASK=255.255.255.0 | PREFIX=24 #子网掩码，二选一GATEWAY=10.0.0.2 #网关，不能随便写，看网络规划DNS1=10.0.0.2 #DNS至少配两个，如果第一个服务器坏了，可以通过第二个DNS服务器来做解析DNS2=100.76.76.76 #百度的DNS地址[root@centos network-scripts]#nmcli con reload #centos8执行这两条命令[root@centos network-scripts]#nmcli con up eth0[root@centos ~]#systemctl restart network #centos7执行这条命令#查看[root@rocky86 network-scripts]# nmcli connection#查看网关[root@centos ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0#查看DNS[root@centos ~]#cat /etc/resolv.conf # Generated by NetworkManagernameserver 10.0.0.2nameserver 100.76.76.76#一个网卡配置多个ip地址DEVICE=eth0 NAME=eth0BOOTPROTO=staticIPADDR=10.0.0.183PREFIX=24 IPADDR1=10.0.0.184PREFIX1=24 IPADDR2=10.0.0.185PREFIX2=24 GATEWAY=10.0.0.2 DNS1=10.0.0.2 DNS2=100.76.76.76 域后缀 12345678910111213141516171819202122[root@rocky86 network-scripts]# vim ifcfg-eth1DEVICE=eth1NAME=con-eth1IPADDR=10.0.0.88PREFIX=24GATEWAY=10.0.0.2DNS1=10.0.0.2DNS2=114.114.114.114DOMAIN=magedu.com#查看DNS和域后缀[root@rocky86 network-scripts]# cat /etc/resolv.conf# Generated by NetworkManagersearch localdomain magedu.comnameserver 10.0.0.2nameserver 114.114.114.114#默认补全[root@rocky86 network-scripts]# ping wwwPING www.magedu.com (160.121.140.246) 56(84) bytes of data.64 bytes from 160.121.140.246 (160.121.140.246): icmp_seq=1 ttl=128 time=54.3 ms...... 1.2.3 网卡别名将多个IP地址绑定到一个NIC上 每个IP绑定到独立逻辑网卡，即网络别名，命名格式： ethX:Y，如：eth0:1 、eth0:2、eth0:3 范例：ifconfig 命令 12ifconfig eth0:0 192.168.1.100/24 upifconfig eth0:0 down 范例：ip 命令 1234ip addr add 172.16.1.1/16 dev eth0ip addr add 172.16.1.2/16 dev eth0 label eth0:0ip addr del 172.16.1.2/16 dev eth0 label eth0:0ip addr flush dev eth0 label eth0:0 为每个设备别名生成独立的接口配置文件，格式为：ifcfg-ethX:xxx 范例： 给 lo 网卡加别名(此方式不适用于CentOS8) 123456789101112131415161718192021[root@centos7 network-scripts]#cat ifcfg-lo:1DEVICE=lo:1IPADDR=137.0.0.1NETMASK=255.0.0.0NETWORK=137.0.0.0# If you&#x27;re having problems with gated making 127.0.0.0/8 a martian,# you can change this to something else (255.255.255.255, for example)BROADCAST=137.255.255.255ONBOOT=yesNAME=loopback1[root@centos7 network-scripts]#ip addr show lo1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet 137.0.0.1/8 brd 137.255.255.255 scope global lo:1 valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 注意： 建议 CentOS 6 关闭 NetworkManager 服务 网卡别名必须使用静态地址 1.2.4 网卡模式繁杂模式：在默认情况下，网卡只会接收发给自己MAC地址的数据帧，其它的数据帧则会被过滤丢弃。但是当我们把网卡设置为繁杂模式时，网卡将接收通过该网卡的所有数据帧，无论这些数据帧的目标MAC地址是什么。这个特性在一些特定的情况下很有用，例如网络监听（sniffing）和网络调试。 多播模式：允许一个网络节点将数据发送到多个接收节点，但又不像广播那样向所有网络节点发送。多播通过使用特定的IP地址范围（224.0.0.0到239.255.255.255）和MAC地址范围，来标示一组特定的接收节点。当发送节点发送数据时，只有加入了相应多播组的节点会接收和处理这份数据，其它未加入的节点则会忽略这份数据。多播技术可有效减少网络流量，提升网络的利用效率，因此在一些需要数据共享的场景，例如视频直播和在线游戏，多播技术得到了广泛的应用。 12345ifconfig eth0 promisc # 开启繁杂模式ifconfig eth0 -promisc # 关闭繁杂模式ifconfig ens33 multicast # 开启多播ifconfig ens33 -multicast # 关闭多播 1.3 Ubuntu网络配置1.3.1 主机名12345678910111213#修改主机名root@ubuntu1804:~# hostnamectl set-hostname ubuntu1804.magedu.orgroot@ubuntu1804:~# cat /etc/hostnameubuntu1804.magedu.orgroot@ubuntu1804:~# hostnameubuntu1804.magedu.orgroot@ubuntu1804:~# echo $HOSTNAMEubuntu1804root@ubuntu1804:~# exitlogoutwang@ubuntu1804:~$ sudo -iroot@ubuntu1804:~# echo $HOSTNAMEubuntu1804.magedu.org 1.3.2 网卡名称12345678910111213141516#修改配置文件为下面形式root@ubuntu1804:~#vi /etc/default/grubGRUB_CMDLINE_LINUX=&quot;net.ifnames=0&quot;#或者sed修改root@ubuntu1804:~# sed -i.bak &#x27;/^GRUB_CMDLINE_LINUX=/s#&quot;$#net.ifnames=0&quot;#&#x27; /etc/default/grub#生效新的grub.cfg文件root@ubuntu1804:~# grub-mkconfig -o /boot/grub/grub.cfg#或者root@ubuntu1804:~# update-grubroot@ubuntu1804:~# grep net.ifnames /boot/grub/grub.cfg#重启生效root@ubuntu1804:~# reboot 1.3.3 网卡配置配置选项 123456dhcp4addressesgateway4 nameservers #DNSversion #版本，默认写2search #域后缀 范例 123456#两个写法等价addresses: [10.0.0.6/24,10.0.0.66/24]addresses: - 10.0.0.6/24 - 10.0.0.66/24 1.3.3.1 配置自动获取IP网卡配置文件采用YAML格式,必须以 &#x2F;etc&#x2F;netplan&#x2F;XXX.yaml 文件命名方式存放 可以每个网卡对应一个单独的配置文件,也可以将所有网卡都放在一个配置文件里 123456789101112root@ubuntu1804:~# cat /etc/netplan/01-netcfg.yaml# This file describes the network interfaces available on your system# For more information, see netplan(5).network: version: 2 renderer: networkd ethernets: #指定网卡类型：以太网 eth0: dhcp4: yes #通过ipv4自动获取 #修改网卡配置文件后需执行命令生效root@ubuntu1804:~#netplan apply 1.3.3.2 配置静态IP123456789101112131415161718192021222324252627282930313233343536#一个网卡network: version: 2 renderer: networkd ethernets: eth0: addresses: - 10.0.0.129/24 gateway4: 10.0.0.2 nameservers: #DNS search: [baidu.com, baidu.org] #实现域名访问的时候可以省略预后缀，比如正常来说是ping www.baidu.com，但是如果说写成ping www，那么就会自动添加baidu.com或者baidu.org来进行ping addresses: [10.0.0.2,180.76.76.76] #两个网卡network: version: 2 renderer: networkd ethernets: eth0: addresses: - 10.0.0.129/24 #gateway4: 10.0.0.2 #nameservers: #search: [baidu.com, baidu.org] #addresses: [10.0.0.2,180.76.76.76] eth1: addresses: [10.0.0.128/24， 1.1.1.1/24， 2.2.2.2/24] gateway4: 10.0.0.2 nameservers: search: [baidu.com] addresses: - 223.6.6.6 - 180.76.76.76 #一般来说，默认路由和DNS只需要其中一个网卡就行了，所以可以在eth0写，也可以在eth1写 查看ip和gateway 12root@ubuntu1804:~#ip addrroot@ubuntu1804:~#route -n 查看DNS 12root@ubuntu2004:~# resolvectl status #Ubuntu 20.04新命令root@ubuntu1804:~# systemd-resolve --status 1.4 Centos9网络配置1.4.1 NetworkManager和network服务network服务和NetworkManager是两种不同的网络管理工具，每种工具都有自己的特点和适用场景。 1、network服务：它是基于传统的Linux网络管理方法，即手动配置网络接口和服务。在使用network服务时， 管理员需要编辑&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;目录下的ifcfg文件来配置网络接口。 network服务在系统启动时读取这些配置文件，并应用这些设置。network服务的优势在于其稳定性和可预测性，特别适合服务器环境。 2、NetworkManager：它是一个更现代的网络管理工具，可以自动管理其他网络接口和服务， 主要用于桌面环境。NetworkManager可以管理各种类型的网络连接，例如有线网络、无线网络和VPN等， 并在网络环境改变时动态更新configuration。通常情况下，这两种服务不会同时运行，因为它们可能会产生冲突。 3、但是在CentOS 9中，NetworkManager已经成为默认的网络管理工具，而network服务已被移除。 也就是说，你需要使用NetworkManager来管理你的网络接口和连接。这相比早期的CentOS版本， 使得configuration更加灵活和自动化，特别是对于经常需要切换网络环境的桌面用户来说。 但是，这也可能需要服务器管理员熟悉新的网络管理方法。 123cenetos7.9中我们直接关闭掉它，而使用network服务，而在centos9中则只能用NetworkManager[root@egon ~]# systemctl stop NetworkManager[root@egon ~]# systemctl disable NetworkManager 1.4.2 配置静态ipcentos9中修改的则是&#x2F;etc&#x2F;NetworkManager&#x2F;system-connections&#x2F;下的文件 详见：https://rockylinux.cn/notes/rocky-linux-9-network-configuration.html 用命令直接设置静态ip 1234567nmcli con mod ens33 ipv4.addresses &quot;192.168.1.100/24&quot;nmcli con mod ens33 ipv4.gateway &quot;192.168.1.1&quot;nmcli con mod ens33 ipv4.dns &quot;8.8.8.8,8.8.4.4&quot; # 多个dnsnmcli con mod ens33 ipv4.method manual # 默认获取ip信息的method为auto，即DHCP模式，此处需要改为手动模式，不然会出现网络连接一会正常，一会中断的情况。这一点非常重要 nmcli con down ens33nmcli con up ens33 修改配置文件 12345678910111213141516171819202122[connection]id=ens18uuid=7f49fd62-02d9-323e-8f35-0c8249647a74type=ethernetautoconnect-priority=-999interface-name=ens18timestamp=1669365850[ethernet][ipv4]address1=192.168.11.172/24,192.168.11.254 # 第一个ip地址：前一个地址是ip，后一个是网关# address2=192.168.11.145/24,192.168.11.254 # 第二个ip地址：同上dns=114.114.114.114;223.6.6.6;dns-search=rockylinux.cn;rockylinux.org;method=manual[ipv6]addr-gen-mode=eui64method=disabled[proxy] 2 网络配置相关命令2.1 ifconfig命令来自于net-tools包 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#启用和禁用网卡[root@centos8 ~]#ifconfig eth0 up[root@centos8 ~]#ifconfig eth0 down#修改ip地址[root@centos8 ~]#ifconfig eth0 10.0.0.111/24#对一个网卡设置多个IP地址[root@centos8 ~]#ifconfig eth0:1 172.16.0.8/24#统计吞吐量[root@centos8 ~]#ifconfig -s [root@centos8 ~]#ifconfig -s eth0#清除eth0上面的IP地址[root@centos8 ~]#ifconfig eth0 0[root@egon ~]# ifconfig ens33ens33: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 # 从flags可知该接口已启用，支持广播、组播，MTU:1500（最大传输单元）：1500字节 # 其他了解知识： // UP：表示“接口已启用”。 // BROADCAST ：表示“主机支持广播”。 // RUNNING：表示“接口在工作中”。 // MULTICAST：表示“主机支持多播”。 // 可以了解一下繁杂模式：https://www.cnblogs.com/linhaifeng/articles/13949611.html inet 192.168.12.42 netmask 255.255.255.0 broadcast 192.168.12.255 # IPv4地址 子网掩码 广播地址 inet6 fe80::499e:c2c1:f5ed:3900 prefixlen 64 scopeid 0x20&lt;link&gt; # IPv6地址 掩码长度 作用域，link表示仅该接口有效 ether 00:0c:29:86:f8:59 txqueuelen 1000 (Ethernet) #网卡接口的MAC地址 传输队列长度 接口类型为Ethernet RX packets 5708 bytes 1061424 (1.0 MiB) # 表示开机后此接口累积接收的报文个数，总字节数 RX errors 0 dropped 833 overruns 0 frame 0 # 表示开机后此接口累积接收报文错误数，丢弃数，溢出数（由于速度过快而丢失的数据包数），冲突的帧数 TX packets 102 bytes 16768 (16.3 KiB) # 表示开机后此接口累积发送的报文个数，总字节数 TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 表示开机后此接口累积发送报文错误数，丢弃数，溢出数（由于速度过快而丢失的数据包数）， # carrier 载荷数(发生carrier错误而丢失的数据包数) # collisions 冲突数 2.2 route命令路由表管理命令 路由：路径的选择 路由表：导航，地图，但是不仅仅在路由器有，在任何通信的主机都有 路由表构成 Destination：一般来说是到达目标网络的网段地址，即不会精确到一台机器的IP地址，只能是这台机器所在的网段 Genmask:目标网络对应的netmask iface接口： 接口，要到达本路由记录的目标网络，需要从当前设备的哪个出口发送数据报文 gateway网关：如果本机与目标网络不在同一个网段，需要将数据包发送到下一个路由器邻近本机接口的接口的IP，即网关；如果目标网络和本机在同一个网段，则无需配置网关，gateway是0.0.0.0 Metric花费： 此值越小，路由记录的优先级越高 三种路由记录 主机路由：Destination为主机的IP地址，Genmask为255.255.255.255（不常用） 网络路由：Destination为目标网络的网段地址 默认路由：Destination为0.0.0.0，Genmask为0.0.0.0，优先级最低；比如路由表只有10网段和0.0.0.0,10网段的IP地址走10网段配置的路，不属于10网段的IP地址只能走0.0.0.0配置的路 添加路由 1route add [-net|-host|default] target [netmask Nm] [gw GW] [[dev] If] 删除路由 12345678910111213141516171819202122232425262728293031route del [-net|-host] target [gw Gw] [netmask Nm] [[dev] If]如果本机eth1这个网卡有172.16.10.0/24这个网段的IP,默认就会产生一条路由条目172.16.10.0 0.0.0.0 255.255.255.0 U 100 0 0 eth1 169.254.0.0/24 是保留网关192.168.100.2 是配置的网关 [root@egon ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.100.2 0.0.0.0 UG 100 0 0 eth0172.16.10.0 0.0.0.0 255.255.255.0 U 100 0 0 eth1192.168.0.0 192.168.100.70 255.255.0.0 UG 0 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.100.78 0.0.0.0 255.255.255.255 UH 0 0 0 eth0 各字段含义# 1、Destination：目标网络或目标主机# 2、Gateway：要达到目标所需要经过的网关。如果没有通过网关，这部分将显示为0.0.0.0。# 3、Genmask：和目标IP配对使用的网络掩码# 4、Flags：：表示特定的路由信息，有三个字符U 表示该路由处于up状态、该路由可用G 表示通过网关，路由项指向了一个网关H 表示路由项指向一个主机，而非整个网络 # 5、Metric：Metric用于距离衡量，一般用于路由选择，值越小优先级越高。# 6、Ref：使用/引用这个路由项的活动连接数量 (通常对普通用户没有意义)。# 7、Use：这个路由项被使用的次数。# 8、Iface：该路由所绑定的网络接口名，例如eth0，lo。 对于CentOS 6以上的系统，请忽略Metric和Ref两列，它们已经不被内核使用，只是有些路由软件可能会用上。 范例 123456789101112131415161718192021222324252627282930#查看路由表[root@centos8 ~]#routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault _gateway 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0[root@centos8 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 10.0.0.2 0.0.0.0 UG 100 0 0 eth010.0.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0#添加路由#目标：192.168.1.3 网关：172.16.0.1route add -host 192.168.1.3 gw 172.16.0.1 dev eth0#目标：192.168.0.0 网关：172.16.0.1route add -net 192.168.0.0 netmask 255.255.255.0 gw 172.16.0.1 dev eth0route add -net 192.168.0.0/24 gw 172.16.0.1 dev eth0route add -net 192.168.8.0/24 dev eth1 metric 200#默认路由，网关：172.16.0.1route add -net 0.0.0.0 netmask 0.0.0.0 gw 172.16.0.1route add -net 0.0.0.0/0 gw 172.16.0.1route add default gw 172.16.0.1#删除路由#目标：192.168.1.3 网关：172.16.0.1route del -host 192.168.1.3#目标：192.168.0.0 网关：172.16.0.1route del -net 192.168.0.0 netmask 255.255.255.0 2.2.1 配置路由表 配置 主机 角色 网卡 IP地址 系统 主机1 路由器R1 NAT 10.0.0.176 rocky86 桥接 10.132.65.73 主机2 路由器R2 NAT 10.0.0.182 rocky86 仅主机 192.168.10.254 主机3 客户端A 桥接 10.132.65.200 ubuntu2004 主机4 客户端B 仅主机 192.168.10.200 ubuntu2004 说明 主机1配置桥接，NAT两块网卡，充当路由 主机2配置仅主机，NAT两块网卡，充当路由 主机3配置一块桥接网卡，路由网关指向主机1桥接网卡上配置的IP地址 主机4配置一块仅主机网卡，路由网关指向主机2仅主机网卡上配置的IP地址 主机1和主机3的桥接网卡为一个网段 主机1和主机2的NAT网卡为一个网段 主机2和主机4的仅主机网卡为一个网段 通过上述配置，让主机3和主机4之间，经由主机1和主机2中转，实现连通 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889901 配置机器#R1DEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.176PREFIX=24ONBOOT=yesDEVICE=eth1NAME=eth1BOOTPROTO=staticIPADDR=10.132.65.73PREFIX=21ONBOOT=yes#R2DEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.182PREFIX=24ONBOOT=yesDEVICE=eth1NAME=eth1BOOTPROTO=staticIPADDR=192.168.10.254PREFIX=24ONBOOT=yes#Anetwork: version: 2 ethernets: eth0: addresses: - 10.132.65.200/21 gateway4: 10.132.65.73 #Bnetwork: version: 2 ethernets: eth0: addresses: - 192.168.10.200/24 gateway4: 192.168.10.254 2 测试主机A和路由器R1，路由器R1和路由器R2，路由器R2和主机B是否能相互连接[root@A ~]#ping 10.132.65.73[root@R1 ~]#ping 10.0.0.182[root@R2 ~]#ping 192.168.10.2003 配路由#R1和第一个网段和第二个网段直连，所以自动生成了10.132.64.0和10.0.0.0两个路由，就不需要配了，只需要加不直连的第三个网段路由[root@R1 ~]#route add -net 192.168.10.0/24 gw 10.0.0.182 dev eth0#R2和第二个网段和第三个网段直连，所以自动生成了10.0.0.0和192.168.10.0两个路由，就不需要配了，只需要加不直连的第一个网段路由[root@R2 ~]#route add -net 10.132.64.0/21 gw 10.0.0.176 dev eth04 排错第一个网段[root@A ~]#ping 10.132.65.73 #直连的一般能ping通第二个网段[root@A ~]#ping 10.0.0.182 #假如这里ping不通，原因要么是R1发给R2的数据报文失败或者R2返回给R1的数据报文返回失败[root@A ~]#tcpdump -i eth0 -nn icmpdropped privs to tcpdump tcpdump: verbose output suppressed,use -V or -vv for full protoco decodelistening on eth0，link-type EN10MB (Ethernet)，capture size 262144 bytes #发送不了[root@A ~]#ping 10.0.0.176 #假如这里也ping不通，原因是在A机器忘记配置网关了[root@A ~]#ping 10.0.0.176 #假如这里ping通了，而10.0.0.182不行，是因为R1在Linux中不是真正的路由器，Linux是一台电脑，有个默认行为是如果他收到一个数据包，发现这个数据包不是发给他的，就会抛弃掉，也就是A给R2发送数据报文，要经过R1转发给R2，但是由于其原因R1发现这个目标地址不是它，就直接抛弃，不转发给R2，而正常的路由器是会转发给R2的，所以要让R1扮演真正的路由器，就需要开启Linux的内核参数ip_forward，这个内核参数就是实现转发的功能#查看内核参数有无开启[root@R1 ~]#cat /proc/sys/net/ipv4/ip_forward#没开启就开启（临时）[root@R1 ~]#echo 1 &gt; /proc/sys/net/ipv4/ip_forward #永久[root@R1 ~]#vim /etc/sysctl.conf 加上net.ipv4.ip_forward=1[root@R1 ~]#sysctl -p#同理R2也需要开启内核参数[root@A ~]#ping 10.0.0.182 #这时候再ping就可以通了接着[root@A ~]#ping 192.168.10.200 #也成功了#查看经过哪些路由器mtr | tracepath | traceroute 192.168.10.200#如果路由器需要配置的网段只有一条路，只需要写默认路由 2.2.2 实现静态路由 12345678910111213141516171819202122232425262728293031四台主机：A主机：eth0 NAT模式R1主机：eth0 NAT模式,eth1 仅主机模式R2主机：eth0 桥接模式,eth1 仅主机模式B主机：eth0 桥接模式#配置A主机ifconfig eth0 10.0.0.123/8route add -net 10.0.0.0/8 dev eth0route add default gw 10.0.0.200 dev eth0#配置R1ifconfig eth0 10.0.0.200/8ifconfig eth1 192.168.0.200/24route add -net 10.0.0.0/8 dev eth0route add -net 192.168.0.0/24 dev eth1route add -net 172.16.0.0/16 gw 192.168.0.201 dev eth1echo 1 &gt; /proc/sys/net/ipv4/ip_forward#配置R2ifconfig eth0 172.16.0.200/16ifconfig eth1 192.168.0.201/24route add -net 192.168.0.0/24 dev eth1route add -net 172.16.0.0/16 dev eth0route add -net 10.0.0.0/8 gw 10.0.0.200 dev eth1echo 1 &gt; /proc/sys/net/ipv4/ip_forward#配置Bifconfig eth0 172.16.0.123/16route add -net 172.16.0.0/16 dev eth0route add default gw 172.16.0.200 dev eth0 2.2.3 三个路由器配置路由表 2.3 ip命令来自于iproute包，替代ifconfig 2.3.1 配置Linux网络属性1234567891011121314151617181920212223242526272829303132333435#查看链路层的信息ip link#显示ip地址ip a#禁用网卡ip link set eth1 down#网卡改名ip link set eth1 name wangnet #启用网卡ip link set wangnet up#增加ip地址ip addr add 172.16.100.100/16 dev eth0 #有个问题就是一个网卡名带有两个ip地址，不好分辨，所以加个别名#网卡别名ip addr add 172.16.100.100/16 dev eth0 label eth0:0#删除ipip addr del 172.16.100.100/16 dev eth0 label eth0:0 #先加新IP,再删除旧的IP#清除网络地址ip addr flush dev eth0#过10s后地址消失[root@centos8 ~]#ip addr change 10.0.0.18/24 dev eth0 preferred_lft 10 valid_lft 20#replace 代替现有地址信息[root@centos8 ~]#ip addr replace 10.0.0.18/24 dev eth0 preferred_lft 30 valid_lft 60#replace 也可实现新加IP[root@centos8 ~]#ip addr replace 10.0.0.28/24 dev eth0 preferred_lft 30 valid_lft 60 范例: 增加网卡别名实现一个网卡多个IP 12345678910111213141516171819202122232425262728293031323334[root@centos8 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:8a:51:21 brd ff:ff:ff:ff:ff:ff inet 10.0.0.8/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe8a:5121/64 scope link valid_lft forever preferred_lft forever [root@centos8 ~]#ip address add 10.0.0.18/24 dev eth0 label eth0:1[root@centos8 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:8a:51:21 brd ff:ff:ff:ff:ff:ff inet 10.0.0.8/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet 10.0.0.18/24 scope global secondary eth0:1 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe8a:5121/64 scope link valid_lft forever preferred_lft forever 2.3.2 管理路由1234567891011121314151617181920#添加路由：ip route add TARGET via GW dev IFACE src SOURCE_IPTARGET: 主机路由：IP 网络路由：NETWORK/MASK #添加网关：ip route add default via GW dev IFACE#删除路由：ip route del TARGET#显示路由：ip route show|list#清空路由表：ip route flush [dev IFACE] [via PREFIX]#查看路由过程ip route get IP 范例 1234ip route add 192.168.0.0/24 via 172.16.0.1ip route add 192.168.1.100 via 172.16.0.1ip route add default via 172.16.0.1ip route flush dev eth0 范例: 查看路由过程 123456[root@centos8 ~]#ip route get 10.0.0.710.0.0.7 dev eth0 src 10.0.0.8 uid 0 cache[root@centos8 ~]#ip route get 8.8.8.88.8.8.8 via 10.0.0.2 dev eth0 src 10.0.0.8 uid 0 cache 2.4 ss命令ss是Socket Statistics的缩写。顾名思义，ss命令可以用来获取socket统计信息，它可以显示和netstat类似的内容。但ss的优势在于它能够显示更多更详细的有关TCP和连接状态的信息，而且比netstat更快速更高效。当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢 12345678910111213141516171819-t: tcp协议相关-u: udp协议相关-w: 裸套接字相关-x：unix sock相关-l: 只显示listen状态的连接-a: 所有-n: 数字格式-p: 相关的程序及进程-e: 扩展的信息-m：显示连接内存使用情况-o：计时器信息-4：仅显示IPV4连接数据-6：仅显示IPV6连接数据-0：仅显示PACKET数据-M：仅显示mptcp数据-S：仅显示sctp数据-d：仅显示dccp数据--tipc：仅显示tipc数据--vsock：仅显示vsock数据 格式说明 123456789101112131415FILTER : [ state TCP-STATE ] [ EXPRESSION ]TCP的常见状态： tcp finite state machine: LISTEN: 监听（实际上就是机器上开启哪些了服务，远程用户可以通过其端口连接） ESTABLISHED：已建立的连接 FIN_WAIT_1 FIN_WAIT_2 SYN_SENT SYN_RECV CLOSEDEXPRESSION: dport = sport = [::]：表示ipv6的回环地址 范例 123456789101112131415161718192021222324#监控主机所有状态ss -nta#显示本地打开的所有端口ss -l#显示每个进程具体打开的socketss -pl#显示所有tcp socketss -t -a#显示所有的UDP Socektss -u -a#显示所有已建立的ssh连接ss -o state established &#x27;( dport = :ssh or sport = :ssh )&#x27;#显示所有已建立的HTTP连接ss -o state established &#x27;( dport = :http or sport = :http )&#x27;[root@centos8 ~]#ss -no state established &#x27;( dport = :21 or sport = :21 )&#x27;#列出当前socket详细信息[root@centos8 ~]#ss -s 查询网络连接情况： 1netstat -n | awk &#x27;/^tcp/ &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#x27; 2.5 nmcli命令可以自动生成网卡的配置文件 命令中的配置项和配置文件中的配置项对应关系： nmcli con mod|add ifcfg-* 文件 ipv4.method manual BOOTPROTO&#x3D;none ipv4.method auto BOOTPROTO&#x3D;dhcp ipv4.addresses 192.168.2.1&#x2F;24 IPADDR&#x3D;192.168.2.1 PREFIX&#x3D;24 ipv4.gateway 172.16.0.200 GATEWAY&#x3D;192.0.2.254 ipv4.dns 8.8.8.8 DNS0&#x3D;8.8.8.8 ipv4.dns-search example.com DOMAIN&#x3D;example.com ipv4.ignore-auto-dns true PEERDNS&#x3D;no connection.autoconnect yes ONBOOT&#x3D;yes connection.id（con-name） eth0 NAME&#x3D;eth0 connection.interface-name（ifname）eth0 DEVICE&#x3D;eth0 type Ethernet TYPE&#x3D;Ethernet 范例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#查看你当前的连接情况[root@centos ~]#nmcli conNAME UUID TYPE DEVICE eth0 5fb06bd0-0bb0-7ffb-45f1-d6edd65f3e03 ethernet eth0 ens33 ceda805a-2188-45fc-8edc-ae3cdf6780e7 ethernet -- Wired connection 1 8d83ec65-dbef-36bb-94d2-5ee21f2f436a ethernet -- #系统自动生成的#修改名称nmcli con modify Wired\\ connection\\ 1 con-name eth0-lan1#修改网卡的ip地址和网关并生效nmcli con modify eth0-lan1 ipv4.addresses 192.168.1.100/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli con reloadnmcli con up eth0-lan1#查看网络连接配置nmcli con show eth0-lan1#删除连接nmcli con del eth0#启用nmcli con up con-eth1#禁用nmcli con down con-eth1#显示所有活动连接nmcli con show --active#显示设备状态nmcli dev status#显示网络接口属性nmcli dev show eth0#修改配置文件执行生效nmcli con reload #刷新nmcli con up con-name#创建新连接default，IP自动通过dhcp获取nmcli con add con-name default type Ethernet ifname eth0#创建新连接static ，指定静态IP，不自动连接nmcli con add con-name static ifname eth0 autoconnect no type Ethernet ipv4.addresses 172.25.X.10/24 ipv4.gateway 172.25.X.254#新增网卡配置，自动生成配置文件nmcli con add con-name con-eth1 ipv4.addresses 10.0.0.110/24 ipv4.gateway 10.0.0.2 ipv4.dns 114.114.114.114 ipv4.method manual type ethernet ifname eth1#启用static连接配置nmcli con up static#启用default连接配置nmcli con up default#同一设备新增配置nmcli con mod con-eth1 +ipv4.addresses 10.0.0.119/24nmcli con mod con-eth1 +ipv4.dns 8.8.8.8#同一设备删除配置nmcli con mod con-eth1 -ipv4.addresses 10.0.0.119/24nmcli con mod con-eth1 -ipv4.dns 8.8.8.8#同一设备修改配置nmcli con mod con-eth1 connection.autoconnect nonmcli con mod con-eth1 ipv4.addresses 10.0.0.119/24nmcli con mod con-eth1 ipv4.dns 8.8.8.8#DNS设置存放在/etc/resolv.conf，PEERDNS=no 表示当IP通过dhcp自动获取时，dns仍是手动设置，不自动获取等价于下面命令nmcli con mod “system eth0” ipv4.ignore-auto-dns yes 3 linux上的网络3.1 linux处理数据包的过程 当向外界主机发送数据时，在它从网卡流入后需要对它做路由决策，根据其目标决定是流入本机的用户空间还是在内核空间就直接转发给其他主机 123456789# 1、如果是流入本机用户空间的数据，则数据会从内核空间进入用户空间(被应用程序接收并处理)；此时如果本机用户空间的应用程序不需要产生新的数据包对外发送，那便不再涉及到从某个网卡流出数据；但是如果本机用户空间的应用程序需要产生新的数据包对外发送，那便需要从某个网卡流出数据，但在流出之前，也需要做路由决策：根据目标决定从哪个网卡流出。 # 2、如果不是流入本机用户空间的数据，仅仅只是要经由本机把数据包转发给其他主机则必然涉及到从某个网卡流出，此时数据包必须从流入网卡完整地转发给流出网卡，这要求Linux主机能够完成这样的转发。但Linux主机默认未开启ip_forward功能，这使得数据包无法转发而被丢弃。 ps：Linux主机和路由器不同，路由器本身就是为了转发数据包，所以路由器内部默认就能在不同网卡间转发数据包，而Linux主机默认则不能转发。若要开启linux主机的转发功能，有很多方式，如下所示 临时开启linux主机的路由转发功能，重启网络服务则失效 12345# 方式1：echo 1 &gt; /proc/sys/net/ipv4/ip_forward # 方式2：sysctl -w net.ipv4.ip_forward=1 若要永久生效，则应该写入配置文件 1234567# 在CentOS 6中:将/etc/sysctl.conf文件中的&quot;net.ipv4.ip_forward&quot;值改为1即可 #在CentOS 7中:systemd管理了太多的功能，sysctl的配置文件也分化为多个，包括/etc/sysctl.conf、/etc/sysctl.d/*.conf和/usr/lib/sysctl.d/*.conf，并且这些文件中默认都没有net.ipv4.ip_forward项。当然，直接将此项写入到这些配置文件中也都是可以的，建议写在/etc/sysctl.d/*.conf中，这是systemd提供自定义内核修改项的目录。例如： echo &quot;net.ipv4.ip_forward=1&quot; &gt; /etc/sysctl.d/ip_forward.conf 注意了注意注意了：只有当本机被别人当成网关并且本机开启路由转发功能时，别人发来的请求包，本机才会帮忙转发，这一点很重要，请务必记住 3.2 网关Linux上分为3种路由： 主机路由：掩码位32位，Destination精确到某一台主机， 所以主机路由是直接指明到某台具体的主机怎么走，主机路由也就是所谓的静态路由 网络路由：掩码小于32位，Destination精确到某一个网段的主机 所以网络路由指明到某类网络怎么走 默认路由：掩码通常为0 不走主机路由的和网络路由的、全部都走默认路由。操作系统上设置的默认路由一般也称为网关。 路由是区分优先级的：若Linux上到某主机有多条路由可以选择，这时候会挑选优先级高的路由 12345678910大前提：主机范围越小、越精确、优先级越高，而缩小主机范围的恰恰就是子网掩码，掩码越长范围越小、越精确、优先级越高 优先级区分：# 1、在Linux中，路由条目的优先级确定方式是先匹配掩码位长度，掩码越长的优先级高也就是说，掩码位长的路由条目优先级一定比掩码位短的优先级高，所以主机路由的优先级最高，然后是直连网络(即同网段)的路由(也算是网络路由)次之，再是网络路由，最后才是默认路由即网关。 # 2、若路由条目的掩码长度相同，则比较节点之间的管理距离(比如metric)，管理距离短的生效。 例如1：在本机查看到路由表如下 12345678[root@egon ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.100.2 0.0.0.0 UG 100 0 0 eth0172.16.10.0 0.0.0.0 255.255.255.0 U 100 0 0 eth1192.168.0.0 192.168.100.70 255.255.0.0 UG 0 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.100.78 0.0.0.0 255.255.255.255 UH 0 0 0 eth0 在本机 ping 192.168.5.20 12345# 先检索掩码长的路由条件，即按照如下顺序255.255.255.255 &gt; 255.255.255.0 &gt; 255.255.0.0 &gt; 0.0.0.0 于是先对应192.168.100.78发现无法匹配，然后比对192.168.100.0，发现也无法匹配，接着再匹配192.168.0.0这条网络路由条目，发现能匹配，所以选择该路由条目，从eth0发出数据包 例2：如下路由表。由于两块网卡eth0和eth1都是192.168.100.0&#x2F;24网段地址，所以它们的路由条目在掩码长度的匹配上是相同的，但是和eth0直连的网段主机通信时，肯定会选择eth0这条路由条目（即第二条），因为eth1和该网段主机隔了一个eth0，距离增加了1。 123456[root@egon ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 192.168.100.2 0.0.0.0 UG 100 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.100.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1 3.3 ICMP范例：修改IP报文段的TTL值 1echo 100 &gt; /proc/sys/net/ipv4/ip_default_ttl 范例：发现IP冲突的主机 12345678[root@centos8 ~]#arping 10.0.0.6ARPING 10.0.0.6 from 10.0.0.8 eth0Unicast reply from 10.0.0.6 [00:0C:29:E0:2F:37] 0.779msUnicast reply from 10.0.0.6 [00:0C:29:32:80:38] 0.798msUnicast reply from 10.0.0.6 [00:0C:29:32:80:38] 0.926msUnicast reply from 10.0.0.6 [00:0C:29:32:80:38] 0.864ms^CSent 3 probes (1 broadcast(s))Received 4 response(s) 范例: 禁用IPv6 12345678910111213141516171819202122232425262728#默认启动IPv6#修改内核配置[root@centos8 ~]#vim /etc/sysctl.conf#加下面两行net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1[root@centos8 ~]#sysctl -p#注意:禁用IPv6可能会影响一些服务的启动,如:ssh,postfix,mysql等[root@centos8 ~]#vim /etc/ssh/sshd_config#AddressFamily any 此行修改为以下行AddressFamily inet[root@centos8 ~]#systemctl restart sshd[root@centos8 ~]#vim /etc/postfix/main.cf#inet_interfaces = localhost 此行修改为以下行inet_interfaces = 127.0.0.1 [root@centos8 ~]#systemctl restart postfix[root@centos8 ~]#ss -ntlState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 100 127.0.0.1:25 0.0.0.0:*","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"网络管理","slug":"网络管理","permalink":"https://aquapluto.github.io/tags/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/"}]},{"title":"逻辑卷管理器LVM","slug":"Linux/disk-manage/LVM","date":"2025-08-21T02:59:57.000Z","updated":"2025-09-07T05:41:27.777Z","comments":true,"path":"Linux/disk-manage/LVM/","permalink":"https://aquapluto.github.io/Linux/disk-manage/LVM/","excerpt":"","text":"1 LVM概念我们在对磁盘分区的大小进行规划时，往往不能确定每个分区使用的空间大小，只能凭经验分配一个大小，而我们通常使用的 fdisk、gdisk 等工具对磁盘分区后，每个分区的大小就固定死了，这么做的问题是 如果分区设置的过大，就白白浪费了磁盘空间； 如果分区设置的过小，就会导致空间不够用的情况出现。 对于分区过小的问题，我们可以重新划分磁盘的分区，或者通过软连接的方式将此分区的目录链接到另外一个分区。这样做虽然能够临时解决问题，但是给管理带来了麻烦。 逻辑卷管理LVM是硬盘的一个系统工具。无论在Linux或者其他类似的系统，都是非常的好用。传统分区使用固定大小分区，重新调整大小十分麻烦。但是，LVM可以创建和管理“逻辑”卷，而不是直接使用物理硬盘。可以让管理员弹性的管理逻辑卷的扩大缩小，通过交换PE来进行资料的转换，将原来LV内的PE转移到其他的设备中以降低LV的容量，或将其他设备中的PE加到LV中以加大容量，操作简单，而不损坏已存储的数据。可以随意将新的硬盘添加到LVM，以直接扩展已经存在的逻辑卷。LVM并不需要重启就可以让内核知道分区的存在。 通过 LVM 技术，可以屏蔽掉磁盘分区的底层差异，在逻辑上给文件系统提供了一个卷的概念，然后在这些卷上建立相应的文件系统。下面是 LVM 中主要涉及的一些概念 物理卷(physical volume PV)：把常规的磁盘设备通过pvcreate命令对其进行初始化，形成了物理卷。其实就是硬盘或分区（面粉） 卷组(volume group VG)：把多个物理卷组成一个逻辑的整体，这样卷组的大小就是多个硬盘之和。或者理解就是由一个或多个PV组成的整体（面团） 逻辑卷(logical volume LV)：从卷组中划分需要的空间大小出来。用户仅需对其格式化然后即可挂载使用。从VG中切割出的空间用于创建文件系统（切成馒头） 基本单元(physical extend PE)：分配的逻辑大小的最小单元，默认为4MB的基本块，假设分配100MB逻辑空间，则需要创建25个PE LVM的优点 12345# 1、可以在系统运行的状态下动态的扩展文件系统的大小。# 2、文件系统可以跨多个磁盘，因此文件系统大小不会受物理磁盘的限制。# 3、可以增加新的磁盘到LVM的存储池中。# 4、可以以镜像的方式冗余重要的数据到多个物理磁盘。# 5、可以方便的导出整个卷组到另外一台机器。 LVM的缺点 123456# 1、因为加入了额外的操作，存取性能受到影响。# 2、当卷组中的一个磁盘损坏时，整个卷组都会受到影响。解释：LVM如果有一个磁盘损坏,整个lvm都坏了，lvm只有动态扩展作用方案：底层用RAID + 上层LVM = 既有冗余又有动态扩展# 3、在从卷组中移除一个磁盘的时候必须使用reducevg命令（该命令要求root权限,并且不允许在快照卷组中使用） 2 实现逻辑卷2.1 硬盘变成逻辑卷步驟 相关工具来自于 lvm2 包 硬盘变成逻辑卷步驟 把硬盘或者分区通过pvcreate变成物理卷，物理卷是用固定大小的物理区域（Physical Extent，PE）来定义的 把多个物理卷通过vgcreate变成卷组 把卷组通过lvcreate分出空间给逻辑卷 创建文件系统 挂载 12345678910111213141516#/dev/sdb 20G 假如不想20G全部都加逻辑卷，先分区#分区要改ID，改成8e（硬盘不用改）fdisk /dev/sdb#将硬盘和分区一起创建1 pvcreate /dev/sdc /dev/sdb1（pvcreate /dev/sd&#123;b1,c&#125;）#-s指定PE的大小为16M，默认为4M；testvg0随意起名字，PE是分配将来空间以及进行容量扩容的时候的最小单位，扩容一个P一个P的扩2 vgcreate -s 16 testvg0 /dev/sdc /dev/sdb1 #-L指定从testvg0卷组中分配空间大小，大小不能超过整个卷组的空间大小（假如现在卷组空间大小为15G）；-n起名字3 lvcreate -L 6G -n lv-mysql testvg0 4 mkfs.ext4 /dev/testvg0/lv-mysql5 mount /dev/testvg0/lv-mysql /data/mysql 第一个逻辑卷对应设备名：/dev/dm-# dm：device mapper，将一个或多个底层块设备组织成一个逻辑设备的模块 软链接： /dev/mapper/VG_NAME-LV_NAME /dev/VG_NAME/LV_NAME 范例 1234567[root@rocky86 ~]# ll /dev/mapper/rl*lrwxrwxrwx 1 root root 7 Aug 1 09:38 /dev/mapper/rl-home -&gt; ../dm-2lrwxrwxrwx 1 root root 7 Aug 1 09:38 /dev/mapper/rl-root -&gt; ../dm-0lrwxrwxrwx 1 root root 7 Aug 1 09:38 /dev/mapper/rl-swap -&gt; ../dm-1[root@ubuntu2204 ~]# ll /dev/mapper/*lvlrwxrwxrwx 1 root root 7 May 14 12:11 /dev/mapper/ubuntu--vg-ubuntu--lv -&gt;../dm-0 2.2 pv管理工具将块设备创建为物理卷，本质上就是给块设备打一个标签 块设备数量和物理卷数量是对应的，有几个块设备，就可以创建几个物理卷 块设备容量大小不限，可以跨分区 123456789# 1、显示pvpvs #简要pv信息显示pvdisplay #详细 # 2、创建PV，可以对分区做、也可以对整块盘做pvcreate /dev/DEVICE# 3、删除PVpvremove /dev/DEVICE 2.3 vg管理工具显示卷组 123456789101112vgsvgdisplay#示例[root@ubuntu2204 ~]# vgdisplay testvg.....VG Size &lt;24.97 GiB #总大小PE Size 16.00 MiB #PE大小Total PE 1598 #总PE数量Alloc PE / Size 0 / 0 #己分配的PEFree PE / Size 1598 / &lt;24.97 GiB #可用PE..... 创建卷组 12345vgcreate [-s Size ] vgname pv1 [pv2...]-s 指定PE大小，默认4M，数字加单位，单位为 k|K|m|M|g|G|t|T|p|P|e|E#示例vgcreate -s 16M vg0 /dev/sdb /dev/sdc 管理卷组 12vgextend vgname pv1 [pv2...] #扩展卷组，将新的物理卷添加到已有的卷组vgreduce vgname pv1 [pv2...] #从卷组中移除物理卷 范例：扩展vg 123456789101112131415161718[root@ubuntu2204 ~]# pvsPV VG Fmt Attr PSize PFree/dev/sda3 ubuntu-vg lvm2 a-- &lt;198.00g 99.00g/dev/sdb1 testvg lvm2 a-- 4.98g 4.98g/dev/sdb2 lvm2 --- 5.00g 5.00g/dev/sdc testvg lvm2 a-- 19.98g 19.98g#往testvg中增加 pv[root@ubuntu2204 ~]# vgextend testvg /dev/sdb2Volume group &quot;testvg&quot; successfully extended#再次查看pv[root@ubuntu2204 ~]# pvsPV VG Fmt Attr PSize PFree/dev/sda3 ubuntu-vg lvm2 a-- &lt;198.00g 99.00g/dev/sdb1 testvg lvm2 a-- 4.98g 4.98g/dev/sdb2 testvg lvm2 a-- 4.98g 4.98g/dev/sdc testvg lvm2 a-- 19.98g 19.98g 删除卷组 123#删除vg之前，要先把对应的pv解除绑定 1、pvmove2、vgremove 2.4 lv管理工具显示逻辑卷 12lvsLvdisplay 创建逻辑卷 12345678lvcreate -L #[mMgGtT] -n NAME VolumeGroup-L #指定大小，创建一个特定大小的逻辑卷，允许你手动指定逻辑卷的大小-n #逻辑卷名称-l #指定PE个数,也可用百分比，通常跟一个加号（+）和设备名，用来指定新的逻辑卷应该使用该设备上的所有剩余空间，用于自动分配所有可用的空间#范例lvcreate -l +60%VG -n mylv testvglvcreate -l +100%FREE -n yourlv testvg 扩展逻辑卷 12345lvextend &#123;-L N[mMgGtT]|-l N&#125; LV_NAME-L[+]Size[mMgGtT] #N个单位大小，也可写成+10M-l[+]Number[PERCENT] #N个PE，也可以写成+10，表示在原基础上加10个PE大小，+100%free 表示把剩下空间都用完-r #自动重置文件系统大小 缩减逻辑卷 1234lvreduce &#123;-L N[mMgGtT]|-l N&#125; LV_NAME-L #指定逻辑卷的大小，N个单位大小，也可写成-10M-l #指定逻辑卷的大小，N个PE，也可以写成-10，表示在原基础上减10个PE大小 删除逻辑卷 1lvremove /dev/VG_NAME/LV_NAME 重设文件系统大小 修改了逻辑卷大小后，要同步文件系统 12345678fsadm [options] resize device [new_size[BKMGTEP]]xfs_growfs /mountpoint #只支持xfs系列文件系统resize2fs [-f] [-F] [-M] [-P] [-p] device [new_size] #只支持ext系列文件系统-f：强制执行调整操作，不需要检查文件系统的完整性。-F：显示当前文件系统的特性（例如，是否启用了日志功能）。-M：显示文件系统的超级块备份信息。-P：显示文件系统的备用超级块位置。-p：在调整文件系统大小之前，检查并修复文件系统的错误。 范例：创建lvm 123456789101112131415#从 testvg 中创建lv1,大小为100个PE[root@ubuntu2204 ~]# lvcreate -l 100 -n lv1 testvgLogical volume &quot;lv1&quot; created.#创建lv2，大小为5G[root@ubuntu2204 ~]# lvcreate -L 5G -n lv2 testvgLogical volume &quot;lv2&quot; created.#创建lv3,大小为testvg卷组中所有物理卷剩下可用PE数量的20%[root@ubuntu2204 ~]# lvcreate -l 20%free -n lv3 testvgLogical volume &quot;lv3&quot; created.#创建lv4,大小为testvg卷组总大小的10%[root@ubuntu2204 ~]# lvcreate -l 10%VG -n lv4 testvgLogical volume &quot;lv4&quot; created. 逻辑卷的使用跟硬盘分区使用一样，要先创建文件系统，再进行挂载 1234567891011121314151617[root@ubuntu2204 ~]# mkfs.xfs /dev/vg1/lv1_from_vg1 [root@ubuntu2204 ~]# mkfs.xfs /dev/vg1/lv2_from_vg1 [root@ubuntu2204 ~]# mkfs.xfs /dev/vg2/lv1_from_vg2 [root@ubuntu2204 ~]# mkfs.xfs /dev/vg2/lv2_from_vg2 [root@ubuntu2204 ~]# mount /dev/vg1/lv1_from_vg1 /test1/[root@ubuntu2204 ~]# mount /dev/vg1/lv2_from_vg1 /test2/[root@ubuntu2204 ~]# mount /dev/vg2/lv1_from_vg2 /test3/[root@ubuntu2204 ~]# mount /dev/vg2/lv2_from_vg2 /test4/[root@ubuntu2204 ~]# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/mapper/vg1-lv1_from_vg1 98980 5344 93636 6% /test1/dev/mapper/vg1-lv2_from_vg1 201380 10464 190916 6% /test2/dev/mapper/vg2-lv1_from_vg2 303780 15584 288196 6% /test3/dev/mapper/vg2-lv2_from_vg2 406180 20704 385476 6% /test4 2.5 扩展和缩减逻辑卷2.5.1 在线扩展逻辑卷扩展逻辑卷之前，必须保证卷组有足够的空间，若是卷组没有空间了，就要新增一个磁盘，打造为物理卷，添加到卷组中才可以进行扩容 两步实现，先扩展逻辑卷，再扩容文件系统 12345678#第一步实现逻辑卷的空间扩展lvextend -L [+]#[mMgGtT] /dev/VG_NAME/LV_NAME#第二步实现文件系统的扩展#针对extresize2fs /dev/VG_NAME/LV_NAME#针对xfsxfs_growfs MOUNTPOINT 一步实现容量和文件系统的扩展 1234567lvresize [option] /dev/VG_NAME/LV_NAME#选项说明-L #指定逻辑卷的大小, 单位为“kKmMgGtT”字节 -l #指定逻辑卷的大小（LE数）-r #可以同时扩展卷和文件系统-A #自动调整逻辑卷的大小，如果扩展失败可以使用这个选项 范例 123456#扩容之前先vgs看看卷组还有多少空间#+100%free是把卷组所剩空间全部都扩容lvextend -L +100%free /dev/testvg0/lv-mysql #只会先扩逻辑卷空间resize2fs /dev/testvg0/lv-mysql #扩文件系统lvresize -r -l +100%FREE /dev/testvg0/lv-mysql #一步实现 学习问题1 问题：一开始创建逻辑卷，sdb1和sdb2都关联到了test-mysql逻辑卷，然后将sdb3扩展到test卷组，但是为什么扩展到了之后，sdb3没有关联到test-mysql卷组 解答：之所以test-mysql存在sdb1和sdb2上，是因为它需要6G的那个空间，而sdb1和sdb2都只有5G，加起来10G，所以它才会同时存在这两个地方，已经确保有6G足够可用的空间，test-mysql存在了sdb1和2上，已经存好了，3上不用存了，test-mysql需要的空间已经足够了，那么扩展卷组的用处就是比如原本test卷组10G容量，然后已经分完容量给一个逻辑卷，想扩展逻辑卷的话没有容量了，那么扩展卷组后多了容量，就可以扩展逻辑卷了，所以也就是说，当test-mysql扩容到10G，到达了test卷组的最大容量，sdb3有10G，扩展到test卷组，那么test卷组就有了20G的容量，那么test-mysql又可以继续扩容了，或者创建新的逻辑卷 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@rocky8 ~]#lvcreate -L 6G -n mysql testWARNING: xfs signature detected on /dev/test/mysql at offset 0. Wipe it? [y/n]: y Wiping xfs signature on /dev/test/mysql. Logical volume &quot;mysql&quot; created. [root@rocky8 ~]#lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert root rl -wi-ao---- &lt;17.00g swap rl -wi-ao---- 2.00g mysql test -wi-a----- 6.00g [root@rocky8 ~]#lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part ├─rl-root 253:0 0 17G 0 lvm / └─rl-swap 253:1 0 2G 0 lvm [SWAP]sdb 8:16 0 20G 0 disk ├─sdb1 8:17 0 5G 0 part │ └─test-mysql 253:2 0 6G 0 lvm ├─sdb2 8:18 0 5G 0 part │ └─test-mysql 253:2 0 6G 0 lvm └─sdb3 8:19 0 10G 0 part sr0 11:0 1 10.5G 0 rom [root@rocky8 ~]#vgs VG #PV #LV #SN Attr VSize VFree rl 1 2 0 wz--n- &lt;19.00g 0 test 2 1 0 wz--n- &lt;9.97g &lt;3.97g [root@rocky8 ~]#pvs PV VG Fmt Attr PSize PFree /dev/sda2 rl lvm2 a-- &lt;19.00g 0 /dev/sdb1 test lvm2 a-- 4.98g 0 /dev/sdb2 test lvm2 a-- 4.98g &lt;3.97g /dev/sdb3 lvm2 --- &lt;10.00g &lt;10.00g [root@rocky8 ~]#vgextend test /dev/sdb3 Volume group &quot;test&quot; successfully extended [root@rocky8 ~]#pvs PV VG Fmt Attr PSize PFree /dev/sda2 rl lvm2 a-- &lt;19.00g 0 /dev/sdb1 test lvm2 a-- 4.98g 0 /dev/sdb2 test lvm2 a-- 4.98g &lt;3.97g /dev/sdb3 test lvm2 a-- 9.98g 9.98g [root@rocky8 ~]#lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part ├─rl-root 253:0 0 17G 0 lvm / └─rl-swap 253:1 0 2G 0 lvm [SWAP]sdb 8:16 0 20G 0 disk ├─sdb1 8:17 0 5G 0 part │ └─test-mysql 253:2 0 6G 0 lvm ├─sdb2 8:18 0 5G 0 part │ └─test-mysql 253:2 0 6G 0 lvm └─sdb3 8:19 0 10G 0 part sr0 11:0 1 10.5G 0 rom 学习问题2 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#根分区已满[root@ubuntu2004 ~]#df -hFilesystem Size Used Avail Use% Mounted onudev 1.9G 0 1.9G 0% /devtmpfs 389M 41M 349M 11% /run/dev/mapper/ubuntu--vg-ubuntu--lv 9.8G 9.8G 0 100% /tmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/sda2 1.5G 209M 1.2G 16% /boot/dev/loop1 68M 68M 0 100% /snap/lxd/21835/dev/loop2 92M 92M 0 100% /snap/lxd/24061/dev/loop3 41M 41M 0 100% /snap/snapd/20290/dev/loop4 64M 64M 0 100% /snap/core20/2015tmpfs 389M 0 389M 0% /run/user/0/dev/loop5 64M 64M 0 100% /snap/core20/2264#两种命令扩容报错[root@ubuntu2004 ~]#lvresize -r -L +8G /dev/mapper/ubuntu--vg-ubuntu--lv /etc/lvm/archive/.lvm_ubuntu2004_50678_558404538: write error failed: No space left on device[root@ubuntu2004 ~]#lvextend -L +8G /dev/mapper/ubuntu--vg-ubuntu--lv /etc/lvm/archive/.lvm_ubuntu2004_50678_558404538: write error failed: No space left on device #但是卷组还有空间[root@ubuntu2004 ~]#vgdisplay --- Volume group --- VG Name ubuntu-vg System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 2 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 1 Act PV 1 VG Size &lt;18.50 GiB PE Size 4.00 MiB Total PE 4735 Alloc PE / Size 2560 / 10.00 GiB Free PE / Size 2175 / &lt;8.50 GiB VG UUID d0xfxL-9ith-021w-xHBF-GALw-41Nk-TYZnQ1 #解决#该命令重新调整了 /dev/mapper/ubuntu 的 lv 大小，并刷新了文件系统[root@ubuntu2004 ~]#lvresize -A n -L +8G /dev/mapper/ubuntu--vg-ubuntu--lv Size of logical volume ubuntu-vg/ubuntu-lv changed from 10.00 GiB (2560 extents) to 18.00 GiB (4608 extents). WARNING: This metadata update is NOT backed up. Logical volume ubuntu-vg/ubuntu-lv successfully resized. [root@ubuntu2004 ~]#resize2fs -p /dev/mapper/ubuntu--vg-ubuntu--lvresize2fs 1.45.5 (07-Jan-2020)Filesystem at /dev/mapper/ubuntu--vg-ubuntu--lv is mounted on /; on-line resizing requiredold_desc_blocks = 2, new_desc_blocks = 3The filesystem on /dev/mapper/ubuntu--vg-ubuntu--lv is now 4718592 (4k) blocks long.[root@ubuntu2004 ~]#df -hFilesystem Size Used Avail Use% Mounted onudev 1.9G 0 1.9G 0% /devtmpfs 389M 1.6M 388M 1% /run/dev/mapper/ubuntu--vg-ubuntu--lv 18G 9.8G 7.0G 59% /tmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/sda2 1.5G 209M 1.2G 16% /boot/dev/loop1 68M 68M 0 100% /snap/lxd/21835/dev/loop2 92M 92M 0 100% /snap/lxd/24061/dev/loop3 41M 41M 0 100% /snap/snapd/20290/dev/loop4 64M 64M 0 100% /snap/core20/2015tmpfs 389M 0 389M 0% /run/user/0/dev/loop5 64M 64M 0 100% /snap/core20/2264 2.5.2 缩减逻辑卷注意：缩减有数据损坏的风险，建议先备份再缩减，不支持在线缩减，要先取消挂载，xfs文件系统不支持缩减 范例: 缩减ext4文件系统的逻辑卷 1234561 取消挂载umonut /data/mysql2 缩减逻辑卷和文件系统lvreduce -L 8G -r /dev/testvg0/lv-mysql #只支持ext4系统3 重新挂载mount /dev/testvg0/lv-mysql /data/mysql 范例: 缩减XFS文件系统的逻辑卷 123456789101112131415161718192021#先备份XFS文件系统数据[root@centos8 ~]#yum -y install xfsdump#备份/data挂载点对应的逻辑卷#注意挂载点后面不要加/,否则会出错:xfsdump: ERROR: /data/ does not identify a file system[root@centos8 ~]#xfsdump -f data.img /data#卸载文件系统[root@centos8 ~]#umount /data#缩减逻辑卷[root@centos8 ~]#lvreduce -L 10G /dev/vg0/lv0#重新创建文件系统[root@centos8 ~]#mkfs.xfs -f /dev/vg0/lv0#重新挂载[root@centos8 ~]#mount /dev/vg0/lv0 /data#还原数据[root@centos8 ~]#xfsrestore -f data.img /data 2.6 跨主机迁移卷组源计算机上 （1）在旧系统中，umount 所有卷组上的逻辑卷 （2）禁用卷组 12vgchange -a n vg0lvdisplay （3）导出卷组 123vgexport vg0pvscanvgdisplay （4）拆下旧硬盘在目标计算机上,并导入卷组 1vgimport vg0 （5）启用 1vgchange -ay vg0 （6）mount 所有卷组上的逻辑卷 2.7 拆除即将坏掉的硬盘拆除一个硬盘之前要先把它上面所存的数据挪到同一个卷组的其他成员，首先先查看这个硬盘存放数据的大小，再查看卷组的大小，若卷组的大小大于硬盘大小，则可以直接挪，反之要新增加一个硬盘，把它加入到卷组中，扩大卷组的大小，才可以挪，挪完就可以删了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[root@ubuntu2204 ~]# vgdisplay testvg --- Volume group ---VG Name testvgSystem ID Format lvm2Metadata Areas 3Metadata Sequence No 6VG Access read/writeVG Status resizableMAX LV 0Cur LV 4Open LV 0Max PV 0Cur PV 3Act PV 3VG Size 29.95 GiBPE Size 16.00 MiBTotal PE 1917Alloc PE / Size 910 / &lt;14.22 GiBFree PE / Size 1007 / 15.73 GiBVG UUID lm1nmp-0SS1-icze-jnWP-UHhb-VYma-AxOaJW[root@ubuntu2204 ~]# pvdisplay --- Physical volume ---PV Name /dev/sdb1VG Name testvgPV Size 5.00 GiB / not usable 16.00 MiBAllocatable yesPE Size 16.00 MiBTotal PE 319Free PE 28Allocated PE 291PV UUID rjdv3i-minb-ge0w-cL0Y-b6ye-2fxo-GHeCk7 --- Physical volume ---PV Name /dev/sdcVG Name testvgPV Size 20.00 GiB / not usable 16.00 MiBAllocatable yesPE Size 16.00 MiBTotal PE 1279Free PE 660Allocated PE 619PV UUID PC5EE2-1duK-dvF2-08GF-zzxI-qoNA-GVhrpD --- Physical volume ---PV Name /dev/sdb2VG Name testvgPV Size 5.00 GiB / not usable 16.00 MiBAllocatable yesPE Size 16.00 MiBTotal PE 319Free PE 319Allocated PE 0PV UUID rqn1EO-bmaI-ak0M-0bT3-rEp4-r3cf-2kDHQb............#把 testvg 上的某个 pv 拆除，前提是 testvg上还有足够多的空间，能容纳要拆除的 pv 上的数据#把 pv 上的数据先移走[root@ubuntu2204 ~]# pvmove /dev/sdb1/dev/sdb1: Moved: 8.59%/dev/sdb1: Moved: 34.36%/dev/sdb1: Moved: 100.00%#再次查看 /dev/sdb1 上己经没有数据了[root@ubuntu2204 ~]# pvdisplay --- Physical volume ---PV Name /dev/sdb1VG Name testvgPV Size 5.00 GiB / not usable 16.00 MiBAllocatable yesPE Size 16.00 MiBTotal PE 319Free PE 319Allocated PE 0PV UUID rjdv3i-minb-ge0w-cL0Y-b6ye-2fxo-GHeCk7 --- Physical volume ---PV Name /dev/sdcVG Name testvgPV Size 20.00 GiB / not usable 16.00 MiBAllocatable yesPE Size 16.00 MiBTotal PE 1279Free PE 369Allocated PE 910PV UUID PC5EE2-1duK-dvF2-08GF-zzxI-qoNA-GVhrpD --- Physical volume ---PV Name /dev/sdb2VG Name testvgPV Size 5.00 GiB / not usable 16.00 MiBAllocatable yesPE Size 16.00 MiBTotal PE 319Free PE 319Allocated PE 0PV UUID rqn1EO-bmaI-ak0M-0bT3-rEp4-r3cf-2kDHQb[root@ubuntu2204 ~]# pvsPV VG Fmt Attr PSize PFree/dev/sda3 ubuntu-vg lvm2 a-- &lt;198.00g 99.00g/dev/sdb1 testvg lvm2 a-- 4.98g 4.98g/dev/sdb2 testvg lvm2 a-- 4.98g 4.98g/dev/sdc testvg lvm2 a-- 19.98g &lt;5.77g#从 vg 中拆除[root@ubuntu2204 ~]# vgreduce testvg /dev/sdb1Removed &quot;/dev/sdb1&quot; from volume group &quot;testvg&quot;[root@ubuntu2204 ~]# pvsPV VG Fmt Attr PSize PFree/dev/sda3 ubuntu-vg lvm2 a-- &lt;198.00g 99.00g/dev/sdb1 lvm2 --- 5.00g 5.00g/dev/sdb2 testvg lvm2 a-- 4.98g 4.98g/dev/sdc testvg lvm2 a-- 19.98g &lt;5.77g#删除 pv[root@ubuntu2204 ~]# pvremove /dev/sdb1Labels on physical volume &quot;/dev/sdb1&quot; successfully wiped. 3 逻辑卷快照3.1 逻辑卷快照原理LVM 机制还提供了对 LV 做快照的功能，也就是说可以给文件系统做一个备份，这也是设计 LVM 快照的主要目的。LVM 的快照功能采用写时复制技术(Copy-On-Write，COW)，这比传统的备份技术的效率要高很多。创建快照时不用停止服务，就可以对数据进行备份。说明：LVM 还支持 thin 类型的快照，但是本文中的快照都是指 COW 类型的快照。 LVM 采用的写时复制，是指当 LVM 快照创建的时候，仅创建到实际数据的 inode 的硬链接(hark-link)而已。只要实际的数据没有改变，快照就只包含指向数据的 inode 的指针，而非数据本身。快照会跟踪原始卷中块的改变，一旦你更改了快照对应的文件或目录，这个时候原始卷上将要改变的数据会在改变之前拷贝到快照预留的空间。 逻辑卷快照工作原理 创建快照实际上也是创建了一个逻辑卷，只不过该卷的属性与普通逻辑卷的属性有些不一样。我们可以通过上图来理解快照数据卷（图中的实线框表示快照区域，虚线框表示文件系统）： 左图为最初创建的快照数据卷状况，LVM 会预留一个区域 (比如左图的左侧三个 PE 区块) 作为数据存放处。 此时快照数据卷内并没有任何数据，而快照数据卷与源数据卷共享所有的 PE 数据， 因此你会看到快照数据卷的内容与源数据卷中的内容是一模一样的 等到系统运行一阵子后，假设 A 区域的数据被更新了(上面右图所示)，则更新前系统会将该区域的数据移动到快照数据卷中， 所以在右图的快照数据卷中被占用了一块 PE 成为 A，而其他 B 到 I 的区块则还是与源数据卷共享 强调：由于快照区与原本的 LV 共享很多 PE 区块，因此快照区与被快照的 LV 必须要在同一个 VG 上头，下面两点非常重要： VG中需要预留存放快照本身的空间，不能全部被占满。 快照所在的 VG 必须与被备份的 LV 相同，否则创建快照会失败。 总结：快照的本质就是一个特殊的lv，创建快照后，如果源数据卷中的文件被更新了，会将老数据赋给快照的空间，这就要求快照的空间也是够用的 3.2 实现逻辑卷快照范例 1234567891011121314151617假设/data/mysql下有f&#123;1,2,3&#125;.txt文件1 创建快照lvcreate -n lv-mysql-snapshot -s -p r -L 1G /dev/testvg0/lv-mysql #-n起名字；-s快照；-p r实现只读，防止被误操作；-L指定大小；对/dev/testvg0/lv-mysql做快照2 挂载mount /dev/testvg0/lv-mysql-snapshot /mnt这个时候不管你f&#123;1,2,3&#125;.txt文件如何增删改，/mnt下都保留着f&#123;1,2,3&#125;.txt文件的原始数据，可以利用快照还原数据3 取消所有挂载umount /data/mysql4 恢复lvconvert --merge /dev/testvg0/lv-mysql-snapshot5 重新挂载mount /dev/testvg0/lv-mysql /data/mysql 范例：利用快照恢复单个文件 12345678910111213141516171819202122232425262728293031323334353637# 1、准备好初始数据[root@ubuntu2204 ~]# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/mapper/vg1-lv1_from_vg1 98980 5348 93632 6% /test1[root@ubuntu2204 ~]# echo &quot;hello&quot; &gt; /test1/1.txt # 2、查看vg1容量是否充足# lv1_from_vg1 属于卷组vg1，而vg1有足够的容量来分配给快照卷[root@ubuntu2204 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg1 2 2 0 wz--n- 20.99g &lt;20.70g # 3、在vg1卷组里创建一个lv1_from_vg1的逻辑卷[root@ubuntu2204 ~]# lvcreate -L 1G -s -n lv1_from_vg1_snap /dev/vg1/lv1_from_vg1 # 4、查看[root@ubuntu2204 ~]# lvs LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert lv1_from_vg1 vg1 owi-aos--- 100.00m lv1_from_vg1_snap vg1 swi-a-s--- 104.00m lv1_from_vg1 0.01 # 5、修改文件/test/1.txt[root@ubuntu2204 ~]# echo &quot;say ladygaga&quot; &gt;&gt; /test1/1.txt [root@ubuntu2204 ~]# cat /test1/1.txt hellosay ladygaga # 6、恢复数据# 挂载快照，注意：快照在挂载的时候由于和原来的lvm是同一个UUID，而XFS是不允许相同UUID的文件系统挂载，所以需要加选项 -o nouuid[root@ubuntu2204 ~]# mount -o nouuid /dev/vg1/lv1_from_vg1_snap /opt/[root@ubuntu2204 ~]# cat /opt/1.txt hello[root@egon ~]# cp /opt/1.txt /test1/1.txt [root@egon ~]# cat /test1/1.txt hello 范例：如果要恢复的文件个数过多，可以直接合并 123456789101112131415161718192021222324252627282930mount /dev/vg1/lv1_from_vg1 /test1/echo hello &gt; /test1/1.txt # -L指快照的大小，不能超过源的大小，超了也没啥用lvcreate -L 1G -s -n lv1_from_vg1_snap /dev/vg1/lv1_from_vg1echo aaaa &gt;&gt; /test1/1.txtecho aaaa &gt;&gt; /test1/1.txt echo aaaa &gt;&gt; /test1/1.txtecho aaaa &gt;&gt; /test1/1.txtecho aaaa &gt;&gt; /test1/1.txtecho aaaa &gt;&gt; /test1/1.txtmount -o nouuid /dev/vg1/lv1_from_vg1_snap /opt/ [root@egon ~]# cat /opt/1.txt hello[root@egon ~]# cat /test1/1.txt helloaaaaaaaaaaaaaaaaaaaa 先卸载数据源与快照，再进行合并，快照会自动删除，一次性的 [root@egon ~]# umount /test1[root@egon ~]# umount /opt[root@egon ~]# lvconvert --mergesnapshot /dev/vg1/lv1_from_vg1_snap [root@egon ~]# mount /dev/vg1/lv1_from_vg1 /test1/[root@egon ~]# cat /test1/1.txt # 数据还原回来了hello","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://aquapluto.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"}]},{"title":"Swap分区","slug":"Linux/disk-manage/swap","date":"2025-08-21T02:59:52.000Z","updated":"2025-09-09T15:48:24.240Z","comments":true,"path":"Linux/disk-manage/swap/","permalink":"https://aquapluto.github.io/Linux/disk-manage/swap/","excerpt":"","text":"1 文件页和匿名页1、文件页 当发生了内存泄漏时，或者运行了大内存的应用程序，导致系统的内存资源紧张时，系统又会如何应对呢？内存回收和 OOM 杀死进程。 内存回收，也就是系统释放掉可以回收的内存，比如缓存和缓冲区，就属于可回收内存，因为linux系统会把空闲的内存用作page cache以优化磁盘的读写。它们在内存管理中，通常被叫做文件页（File-backed Page）。大部分文件页，都可以直接回收，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。这些脏页，一般可以通过两种方式写入磁盘。 可以在应用程序中，通过系统调用 fsync ，把脏页同步到磁盘中；也可以交给系统，由内核线程 pdflush 负责这些脏页的刷新。除了缓存和缓冲区，通过内存映射获取的文件映射页，也是一种常见的文件页。它也可以被释放掉，下次再访问的时候，从文件重新读取。 2、匿名页 应用程序动态分配的堆内存，也就是我们在内存管理中说到的匿名页（Anonymous Page），这些内存自然不能直接释放。但是如果这些内存在分配后很少被访问，似乎也是一种资源浪费。是不是可以把它们暂时先存在磁盘里，释放内存给其他更需要的进程？ 这正是 Linux 的 Swap 机制。Swap 把这些不常访问的RSS的匿名内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。 2 Swap分区工作原理储备知识：物理内存和虚拟内存；页面调度和页面缺失 2.1 LRU 算法LRU：Least Recently Used 近期最少使用算法（喜新厌旧），释放内存 内存淘汰策略，让进程自己能依据某种规则来管理内存空间，保证程序不会因为内存耗尽而无法工作。 假设序列为 4 3 4 2 3 1 4 2， 物理块有3个，则 12345678第1轮 4调入内存 4第2轮 3调入内存 3 4第3轮 4调入内存 4 3第4轮 2调入内存 2 4 3第5轮 3调入内存 3 2 4第6轮 1调入内存 1 3 2第7轮 4调入内存 4 1 3第8轮 2调入内存 2 4 1 2.2 工作原理Swap Space（交换空间）是 Linux 中虚拟内存的一个实现方式，当物理内存不足时，拿出部分硬盘空间当 Swap 分区（虚拟成内存）使用。说白了就是把一块磁盘空间或者一个本地文件当成内存来使用。它包括换出和换入两个过程 换出：由 Linux 系统内核根据 LRU 算法来判断哪些是进程暂时不用的内存数据，将其存储到磁盘中，并释放这些数据占用的物理内存，即回收内存。 换入：在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。Swap 其实是把系统的可用内存变大了。即使服务器的内存不足，也可以运行大内存的应用程序。事实上，内存再大，对应用程序来说，也有不够用的时候。 系统总是在物理内存不够时，才进行Swap交换。Swap大小是有上限的，一旦Swap使用完，操作系统会触发OOM-Killer机制，把消耗内存最多的进程kill掉以释放内存 Redhat官方推荐系统 swap 空间 系统中的 RAM 量 推荐的 swap 空间 允许休眠的建议 swap 空间大小 低于 2 GB RAM 量的2倍数 RAM 容量的三倍 2 GB - 8 GB 等于 RAM 量 RAM 量的倍数 8 GB - 64 GB 4 GB 到 RAM 容量的 0.5 倍 RAM 容量的 1.5 倍 8 GB - 64 GB 独立负载（至少 4GB） 不建议使用休眠功能 范例 1234567891011121314[root@ubuntu2204 ~]# free -h total used free shared buff/cache availableMem: 1.9Gi 347Mi 896Mi 1.0Mi 697Mi 1.4GiSwap: 2.0Gi 0B 2.0Gi#Mem和Swap加起来都没有30G，所以失败了[root@ubuntu2204 ~]# dd if=/dev/zero of=/dev/null bs=30G count=1dd: memory exhausted by input buffer of size 32212254720 bytes (30 GiB)#这里文件大小虽然超过了，但有SWAP空间，所以可以执行成功[root@ubuntu2204 ~]# dd if=/dev/zero of=/dev/null bs=3G count=10+1 records in0+1 records out2147479552 bytes (2.1 GB, 2.0 GiB) copied, 7.40064 s, 290 MB/s 查询swap内存有哪些程序在使用 1234567891011for i in $(cd /proc;ls | grep &quot;^[0-9]&quot; | awk &#x27;$0&gt;100&#x27;); do awk &#x27;/Swap:/&#123;a=a+$2&#125;END&#123;print &quot;$i&quot;,a/1024&quot;M&quot;&#125;&#x27; /proc/$i/smaps;done| sort -k2nr命令解析# 1、遍历/proc目录下的所有进程目录，只考虑数字开头的目录（即进程ID）for i in $(cd /proc;ls | grep &quot;^[0-9]&quot; | awk &#x27;$0&gt;100&#x27;); do# 2、对于每个进程，读取其smaps文件，统计Swap内存的使用量（单位为KB），并将其转换为MBawk &#x27;/Swap:/&#123;a=a+$2&#125;END&#123;print &quot;$i&quot;,a/1024&quot;M&quot;&#125;&#x27; /proc/$i/smaps# 3、将结果按照第二列（即Swap内存使用量）从大到小排序sort -k2nr 12345#查看占用内存最多的前10个进程ps aux --sort=-%mem | head#筛选出使用了swap内存的进程ps aux --sort=-%mem | grep -E &#x27;S|Z&#x27; | head 3 Swap使用典型场景一个很典型的场景就是，即使内存不足时，有些应用程序也并不想被 OOM 杀死，而是希望能缓一段时间，等待人工介入，或者等系统自动释放其他进程的内存，再分配给它。我们常见的笔记本电脑的休眠和快速开机的功能，也基于 Swap 。休眠时，把系统的内存存入磁盘，这样等到再次开机时，只要从磁盘中加载内存就可以。这样就省去了很多应用程序的初始化过程，加快了开机速度。 Swap 是为了回收内存，那么 Linux 到底在什么时候需要回收内存呢？又该怎么来衡量内存是不是紧张呢？ 1、直接内存回收 在内存分配时发现没有足够空闲内存时会立刻触发内存回收。一个最容易想到的场景就是，有新的大块内存分配请求，但是剩余内存不足。这个时候系统就需要回收一部分内存（比如前面提到的缓存），进而尽可能地满足新内存请求。这个过程通常被称为直接内存回收。 2、定期回收内存 开启了一个守护进程（swapd进程）周期性对系统内存进行检查，在可用内存降低到特定阈值之后主动触发内存回收。专门的内核线程用来定期回收内存，也就是 kswapdo。为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是 页最小阈值（pages_min）： 通过内核选项 /proc/sys/vm/min_free_kbytes 来间接设置 页低阈值（pages_low）： 根据页最小阈值计算，计算方式如下：pages_low = pages_min*5/4 页高阈值（pages_high）： 根据页最小阈值计算，计算方式如下：pages_high = pages_min*3/2 剩余内存，则使用pages_free 表示。 kswapdo定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，一旦剩余内存小于页低阈值，就会进行内存的回收操作 剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。 剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。 剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。 剩余内存大于页高阈值，说明剩余内存比较多，没有内存压力。 4 Swap分区实现过程 创建交换分区或者文件，然后使用mkswap写入特殊签名 可以使用整个磁盘，或者一个disk-partition，使用 mkswap 命令制作swap分区 mkswap /dev/sdb1 通过文件制作，本质上还是磁盘 dd if=/dev/zero of=/swap_file bs=1M count=200 chmod 0600 /swap_file mkswap -f /swap_file 在&#x2F;etc&#x2F;fstab文件中添加适当的条目 激活swap分区：swapon 磁盘路径 关闭swap分区：swapoff 磁盘路径 12345678910111213141516171819202122232425262728293031323334swapon [OPTION]... [DEVICE]#选项-a|--all #激活 /etc/fstab 中的所有交换区-d|--discard[=policy] #根据条件禁用(once|pages)-e|--ifexists #自动跳过不存在的设备而不提示-f|--fixpgsz #必要时重新初始化交换区-o|--options list #指定选项，swapon -o pri=1,discard=pages,nofail /dev/sda2-p|--priority N #指定交换设备的优先级(-1到32767)，值越大优先级越高,也可在/etc/fstab第4列指定pri=value-s|--summary #显示已使用交换设备的摘要--show[=columns] #以可自定义的表格形式打印摘要 swapon --show|swapon--show=NAME,TYPE--noheadings #不打印表头，配合 --show 选项--raw #使用原生输出格式，配合 --show 选项--bytes #在 --show 输出中以字节数显示交换区大小-v|--verbose #显示详细信息#&lt;spec&gt; 参数：-L label #同 LABEL=label-U uuid #同 UUID=uuidLABEL=label #按交换区标签指定设备UUID=uuid #按交换区 UUID 指定设备PARTLABEL=label #按分区标签指定设备PARTUUID=uuid #按分区 UUID 指定设备device #要使用设备的名称filename #要使用文件的名称#可显示列NAME #设备文件或分区路径TYPE #设备的类型SIZE #交换区大小USED #已使用字节数PRIO #交换优先级UUID #uuidLABEL #label 创建swap分区 123456789101112#第一种（以硬盘方式）1 给硬盘设备修改分区类型为Linux swap2 mkswap [device] #创建3 写入/etc/fstab文件中4 swapon -a #使其生效#第二种（以文件方式）1 dd if=/dev/zero of=/swapfile bs=1M count=10242 mkswap /swapfile3 chmod 600 /swapfile 4 写入/etc/fstab文件中5 swapon -a #使其生效 范例：以文件实现swap分区 123456789101112[root@centos8 ~]#dd if=/dev/zero of=/swapfile bs=1M count=1024[root@centos8 ~]#mkswap /swapfile[root@centos8 ~]#blkid /swapfile &gt;&gt; /etc/fstab[root@centos8 ~]#vim /etc/fstab/swapfile swap swap defaults 0 0 #不要用UUID,使用文件的路径[root@centos8 ~]#chmod 600 /swapfile[root@centos8 ~]#swapon -a[root@centos8 ~]#swapon -sFilename Type Size Used Priority/dev/sda5 partition 2097148 0 -2/swapfile file 1048572 0 -3 禁用swap分区 12345678910111213141516171819#第一种方法（立即生效）swapon -s 查看设备名称swapoff [OPTION]... [DEVICE]#第二种方法（下一次重启才会生效）1 在/etc/fstab中注释或删除swap那一行2 sed -i &#x27;/swap/s/^/#/&#x27; /etc/fstab#常用选项-a|--all #禁用 /proc/swaps 中的所有交换区-v|--verbose #显示过程#spec 参数-L label #要使用设备的标签-U uuid #要使用设备的 UUIDLABEL=label #要使用设备的标签UUID=uuid #要使用设备的 UUIDdevice #要使用设备的名称filename #要使用文件的名称 查看swap分区 Priority越大，优先级越高，会被优先使用 123456[root@ubuntu2204 ~]# swapon -sFilename Type Size Used Priority/swap.img file 2097148 0 -2/dev/sdc3 partition 2097148 0 -3[root@ubuntu2204 ~]# cat /proc/swaps Swap的优先级 可以指定swap分区0到32767的优先级，值越大优先级越高 如果用户没有指定，那么核心会自动给swap指定一个优先级，这个优先级从-1开始，每加入一个新的没有用户指定优先级的swap，会给这个优先级减一 先添加的swap的缺省优先级比较高，除非用户自己指定一个优先级，而用户指定的优先级(是正数)永远高于核心缺省指定的优先级(是负数) 永久禁用swap 1234567891011#删除swap行[root@ubuntu2204 ~]#sed -i.bak &#x27;/swap/d&#x27; /etc/fstab#或注释swap行[root@ubuntu2204 ~]#sed -i.bak &#x27;/swap/s@^@#@&#x27; /etc/fstab#禁用swap，由于修改了配置文件，所以重启也不会有SWAP[root@ubuntu2204 ~]#swapoff -a#需要重启生效[root@ubuntu2204 ~]#systemctl mask swap.target 关闭Swap交换分区的优点 提高性能：Swap交换分区通常会在物理内存不足时被使用，这会导致额外的I&#x2F;O操作和延迟。当系统使用物理内存满足所有应用程序的需求时，关闭Swap交换分区可以避免这种情况的发生，从而提高服务器的性能。 减少磁盘使用：Swap交换分区通常占用一部分硬盘空间，关闭Swap交换分区可以减少磁盘使用，从而为其他用途释放空间。 减少系统管理复杂性：当系统有Swap交换分区时，需要定期检查Swap交换分区的使用情况，并可能需要调整Swap交换分区的大小。关闭Swap交换分区可以避免这些问题，从而简化系统管理。 关闭Swap交换分区也存在一些缺点： 内存压力增大：如果系统物理内存不足，关闭Swap交换分区会导致系统无法使用硬盘空间作为额外的内存，这可能导致应用程序崩溃或性能下降。 系统不稳定：在极端情况下，如果系统物理内存不足并且没有Swap交换分区可用，可能会导致系统不稳定或无法正常运行。 5 Swap的使用策略内存回收的内存既包括了文件页，又包括了匿名页。 对文件页的回收，就是直接回收缓存page cache，或者把脏页写回磁盘后再回收。 对匿名页的回收，其实就是通过 Swap 机制，把它们写入磁盘后再释放内存。 1234567# cat /proc/meminfo | grep -i active Active: 233836 kB Inactive: 1280348 kB Active(anon): 138780 kB # 匿名页 Inactive(anon): 26740 kB Active(file): 95056 kB # 文件页 Inactive(file): 1253608 kB page cache与swap分区都是内存的优化方案，两种不同的内存回收机制，在物理内存不足时，是释放page cache还是使用swap分区呢？说白了，单独用谁都不合理，因为如果只释放page cache，有可能需要频繁读写一个文件，释放了page cache必然引起系统性能下降，如果只写把匿名内存写入swap分区，那一旦从rss释放到的匿名内存马上又需要使用，你又必须从swap分区里把刚刚写入的匿名内存数据重新读入内存，这同样会导致系统性能下降 所以Linux 提供了一个 /proc/sys/vm/swappiness 选项，用来调整使用 Swap 的积极程度。该值代表的是一个权重，用来定义page cache与匿名内存的释放比例。swappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。 1234# cat /proc/sys/vm/swappiness30# sysctl -a |grep vm.swappinessvm.swappiness=30 上面的30代表的是：当物理内存被使用 (100-30)% 的时候才会使用到swap vm.swappiness&#x3D;0：表示尽可能避免使用swap，会最大限度释放page cache，当真的没招了的时候才使用swap vm.swappiness=100：表示非常积极的使用swap分区 值得注意的是，swappiness不是内存的百分比，而是调整 Swap 积极程度的权重，即使你把它设置成 0，当剩余内存 + 文件页小于页高阈值时，还是会发生 Swap 1234567891011121314151617#说明：CentOS7和8默认值为30，Centos6默认值为60[root@centos8 ~]# cat /proc/sys/vm/swappiness30[root@centos7 ~]# cat /proc/sys/vm/swappiness30[root@centos6 ~]# cat /proc/sys/vm/swappiness60#临时修改[root@centos8 ~]#echo 1 &gt; /proc/sys/vm/swappiness#永久修改[root@rocky86 ~]# vim /etc/sysctl.confvm.swappiness=0#生效[root@rocky86 ~]# sysctl -pvm.swappiness = 0 6 NUMA与Swap有时候会发现：Linux 物理内存还有很多，而 Swap 的数据占用却很大，这是什么原因呢？ 第一个原因：如果一个内存占用很大的进程正在运行，必然就会耗费大量的内存资源，此时 Linux 内核就会将一些不常用的页面文件交换到 Swap Space 中。当该进程终止后，Linux 就会释放该进程占用的大量内存资源，而此时被交换出去的页面文件数据并不会自动又交换到物理内存中来（除非有这个必要），那此时看到的就是物理内存空间空闲，Swap Space 占用较大的现象了。 第二个原因：处理器的 NUMA （Non-Uniform Memory Access）架构导致的。 6.1 NUMANUMA概念 既然 NUMA 架构下的每个 Node 都有自己的本地内存空间，那么，在分析内存的使用时，我们也应该针对每个 Node 单独分析。numactl 命令，可以查看处理器在 Node 的分布情况，以及每个 Node 的内存使用情况。 12$ numactl --hardwareavailable1 nodes (0)node 0 cpus: 0 1node 0 size: 7977 MBnode 0 free: 4416 MB... 输出信息中，系统中只有一个Node 0 ，而且编号为 0 和 1 的两个 CPU， 都位于 Node 0 上。另外，Node 0 的内存大小为 7977 MB，剩余内存为 4416 MB。 6.2 与swap的关系在 NUMA 架构下，即使系统整体物理内存还有剩余，也可能出现某一节点的内存被耗尽，而其他节点内存空闲的情况。此时，内核可能会将该节点上不活跃的页面换入 Swap，而不是 “跨节点” 使用其他空闲内存，因为跨节点访问内存的延迟较高，内核默认优先保护本地内存的访问效率 三个内存阈值（页最小阈值、页低阈值和页高阈值），都可以通过内存域在 proc 文件系统中的接口 /proc/zoneinfo 来查看 1234567891011121314$ cat /proc/zoneinfo...Node 0, zone Normal pages free 227894 min 14896 low 18620 high 22344... nr_free_pages 227894 nr_zone_inactive_anon 11082 nr_zone_active_anon 14024 nr_zone_inactive_file 539024 nr_zone_active_file 923986... pages 处的 min、low、high，就是上面提到的三个内存阈值，而 free 是剩余内存页数，它跟后面的 nr_free_pages 相同。 nr_zone_active_anon 和 nr_zone_inactive_anon，分别是活跃和非活跃的匿名页数。 nr_zone_active_file 和 nr_zone_inactive_file，分别是活跃和非活跃的文件页数。 从这个输出结果可以发现，剩余内存远大于页高阈值，所以此时的 kswapdo 不会回收内存。 当然，某个 Node 内存不足时，系统可以从其他 Node 寻找空闲内存，也可以从本地内存中回收内存。具体选哪种模式，可以通过 /proc/sys/vm/zone_reclaim_mode 来调整。它支持以下几个选项 默认为0，表示既可以从其他 Node 寻找空闲内存，也可以从本地回收内存 1、2、4 都表示只回收本地内存，2 表示可以回写脏数据回收内存，4 表示可以用 Swap 方式回收内存。 7 Swap使用高定位分析开启 Swap 后，/proc/sys/vm/min_free_kbytes 来调整系统定期回收内存的阈值，/proc/sys/vm/swappiness 来调整文件页和匿名页的回收倾向。那么，当 Swap 使用升高时，要如何定位和分析呢？如下案例 要清楚的是，Linux 本身支持两种类型的 Swap，即 Swap 分区和 Swap 文件。以 Swap 文件为例 12345678# 创建Swap文件，Swap 文件的大小为 8GB$ fallocate -l 8G /mnt/swapfile# 修改权限只有根用户可以访问$ chmod 600 /mnt/swapfile# 配置Swap文件$ mkswap /mnt/swapfile# 开启Swap$ swapon /mnt/swapfile 1、执行 free 命令，确认 Swap 配置成功，free 输出中，Swap 空间以及剩余空间都从 0 变成了 8GB，说明 Swap 已经正常开启。 2、dd 命令，模拟大文件的读取 12# 写入空设备，实际上只有磁盘的读请求$ dd if=/dev/sda1 of=/dev/null bs=1G count=2048 3、运行 sar 命令，查看内存各个指标的变化情况 1234567891011121314151617181920# 间隔1秒输出一组数据，-r表示显示内存使用情况，-S表示显示Swap使用情况$ sar -r -S 104:39:56 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty04:39:57 6249676 6839824 1919632 23.50 740512 67316 1691736 10.22 815156 841868 404:39:56 kbswpfree kbswpused %swpused kbswpcad %swpcad04:39:57 8388604 0 0.00 0 0.0004:39:57 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty04:39:58 6184472 6807064 1984836 24.30 772768 67380 1691736 10.22 847932 874224 2004:39:57 kbswpfree kbswpused %swpused kbswpcad %swpcad04:39:58 8388604 0 0.00 0 0.00…04:44:06 kbmemfree kbavail kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty04:44:07 152780 6525716 8016528 98.13 6530440 51316 1691736 10.22 867124 6869332 004:44:06 kbswpfree kbswpused %swpused kbswpcad %swpcad04:44:07 8384508 4096 0.05 52 1.27 可以看到，sar 的输出结果是两个表格，第一个表格（即kbmemfree这行开始）表示内存的使用情况，第二个表格（即kbswpfree这行开始）表示 Swap 的使用情况。其中各个指标名称前面的 kb 前缀，表示这些指标的单位是 KB。 指标 含义 kbmemfree 空闲物理内存大小（未被任何进程使用的内存）。 kbavail 可用物理内存大小（对应用程序来说可立即分配的内存，包括空闲内存 + 可回收缓存）。 kbmemused 已使用的物理内存大小（总内存 - 空闲内存 - 内核预留内存）。 %memused 物理内存使用率（kbmemused 占总物理内存的百分比）。 kbbuffers 内核缓冲区占用的内存（用于缓存磁盘 I&#x2F;O 数据，如文件元信息）。 kbcached 页缓存占用的内存（用于缓存文件内容，可被应用程序共享或回收）。 kbcommit 当前系统负载需要的内存。它实际上是为了保证系统内存不溢出，对需要内存的估计值 %commit kbcommit 占总可用内存的百分比 kbactive 活跃内存（最近被频繁访问的内存，不容易被系统回收）。 kbinact 非活跃内存（最近较少访问的内存，系统可能在内存紧张时回收）。 kbdirty 脏页内存（已被修改但尚未写入磁盘的内存数据，需要通过 sync 刷盘）。 结合具体数值来分析相关的现象。可以清楚地看到，总的内存使用率（%memused）在不断增长，从开始的 23% 一直长到了 98%，并且主要内存都被缓冲区（kbbuffers）占用。具体来说：刚开始，剩余内存（kbmemfree）不断减少，而缓冲区（kbbuffers）则不断增大，由此可知，剩余内存不断分配给了缓冲区。一段时间后，剩余内存已经很小，而缓冲区占用了大部分内存。这时候，Swap 的使用开始逐渐增大，缓冲区和剩余内存则只在小范围内波动。 4、运行 cachetop 命令，观察缓存的使用情况： 12345$ cachetop 512:28:28 Buffers MB: 6349 / Cached MB: 87 / Sort: HITS / Order: ascendingPID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT%18280 root python 22 0 0 100.0% 0.0%18279 root dd 41088 41022 0 50.0% 50.0% 通过 cachetop 的输出，我们看到 dd 进程的读写请求只有 50% 的命中率，并且未命中的缓存页数（MISSES）为 41022（单位是页）。这说明，正是案例开始时运行的 dd，导致了缓冲区使用升高。 5、为什么 Swap 也跟着升高了呢？直观来说，缓冲区占了系统绝大部分内存，还属于可回收内存，内存不够用时，不应该先回收缓冲区吗？进一步通过 /proc/zoneinfo ，观察剩余内存、内存阈值以及匿名页和文件页的活跃情况。 观察 /proc/zoneinfo 中这几个指标的变化情况： 12345678910111213141516171819# -d 表示高亮变化的字段# -A 表示仅显示Normal行以及之后的15行输出$ watch -d grep -A 15 &#x27;Normal&#x27; /proc/zoneinfoNode 0, zone Normal pages free 21328 min 14896 low 18620 high 22344 spanned 1835008 present 1835008 managed 1796710 protection: (0, 0, 0, 0, 0) nr_free_pages 21328 nr_zone_inactive_anon 79776 nr_zone_active_anon 206854 nr_zone_inactive_file 918561 nr_zone_active_file 496695 nr_zone_unevictable 2251 nr_zone_write_pending 0 可以发现，剩余内存（pages_free）在一个小范围内不停地波动。当它小于页低阈值（pages_low) 时，又会突然增大到一个大于页高阈值（pages_high）的值。结合刚刚用 sar 看到的剩余内存和缓冲区的变化情况，我们可以推导出，剩余内存和缓冲区的波动变化，正是由于内存回收和缓存再次分配的循环往复。当剩余内存小于页低阈值时，系统会回收一些缓存和匿名内存，使剩余内存增大。其中，缓存的回收导致 sar 中的缓冲区减小，而匿名内存的回收导致了 Swap 的使用增大。紧接着，由于 dd 还在继续，剩余内存又会重新分配给缓存，导致剩余内存减少，缓冲区增大。 6、如果多次运行 dd 和 sar，你可能会发现，在多次的循环重复中，有时候是 Swap 用得比较多，有时候 Swap 很少，反而缓冲区的波动更大。换句话说，系统回收内存时，有时候会回收更多的文件页，有时候又回收了更多的匿名页。 123# 查看 swappiness 的配置：$ cat /proc/sys/vm/swappiness60 swappiness 显示的是60，这是一个相对中和的配置，所以系统会根据实际运行情况，选择合适的回收类型，比如回收不活跃的匿名页，或者不活跃的文件页。 7、Swap 换出的是哪些进程的内存？proc 文件系统用来查看进程 Swap 换出的虚拟内存大小，它保存在 &#x2F;proc/pid/status 中的 VmSwap 中。查看使用 Swap 最多的进程： 1234567# 按VmSwap使用量对进程排序，输出进程名称、进程ID以及SWAP用量$ for file in /proc/*/status ; do awk &#x27;/VmSwap|Name|^Pid/&#123;printf $2 &quot; &quot; $3&#125;END&#123; print &quot;&quot;&#125;&#x27; $file; done | sort -k 3 -n -r | headdockerd 2226 10728 kBdocker-containe 2251 8516 kBsnapd 936 4020 kBnetworkd-dispat 911 836 kBpolkitd 1004 44 kB 从这里你可以看到，使用 Swap 比较多的是 dockerd 和 docker-containe 进程，所以，当 dockerd 再次访问这些换出到磁盘的内存时，也会比较慢。这也说明了一点，虽然缓存属于可回收内存，但在类似大文件拷贝这类场景下，系统还是会用 Swap 机制来回收匿名内存，而不仅仅是回收占用绝大部分内存的文件页。 8 总结在内存资源紧张时，Linux 会通过 Swap 把不常访问的匿名页换出到磁盘中，下次访问的时候再从磁盘换入到内存中来。可以设置 /proc/sys/vm/min_free_kbytes来调整系统定期回收内存的阈值；也可以设置 /proc/sys/vm/swappiness，来调整文件页和匿名页的回收倾向。 当 Swap 变高时，可以用 sar、/proc/zoneinfo、/proc/pid/status 等方法查看系统和进程的内存使用情况，进而找出 Swap 升高的根源和受影响的进程。反过来说，通常降低 Swap 的使用，可以提高系统的整体性能。要怎么做呢？几种常见的降低方法。 禁止 Swap，现在服务器的内存足够大，所以除非有必要，禁用 Swap 就可以了。随着云计算的普及，大部分云平台中的虚拟机都默认禁止 Swap。 如果实在需要用到 Swap，可以尝试降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。 响应延迟敏感的应用，如果它们可能在开启 Swap 的服务器中运行，还可以用库函数 mlock() 或者 mlockall() 锁定内存，阻止它们的内存换出。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://aquapluto.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"}]},{"title":"文件系统","slug":"Linux/disk-manage/file-system","date":"2025-08-21T02:59:47.000Z","updated":"2025-09-07T05:14:22.121Z","comments":true,"path":"Linux/disk-manage/file-system/","permalink":"https://aquapluto.github.io/Linux/disk-manage/file-system/","excerpt":"","text":"1 概念文件系统：操作系统内核中用来控制硬盘的一种程序 磁盘必须格式化制作文件系统，然后挂载才能使用 可以为整个磁盘格式化制作文件系统 也可以为某个MBR或者GPT分区格式化制作文件系统 每个硬盘分区都要有一个文件系统 硬盘分区—-》打隔断，分割出一个个小空间 硬盘的最小存取单位-》扇区（512字节，相当于0.5KB） 文件系统—-》对一个个小空间做装修，负责把空间的数据组织好 操作系统的最小存取单位-》block块（4KB，8个扇区） 操作系统读取硬盘的时候，不会一个扇区一个扇区地读取，这样效率太低，于是操作系统中的文件系统负责将磁盘的多扇区组织成一个个的block块，这样操作系统就可以一次性读取一个”块”（block），即一次性连续读取多个扇区，所以文件系统组织好了之后带来的方便之处 使用者—–》block块（文件系统）—–》n个扇区（硬盘的读写单位） 一个文件系统包含的三大类块 inode block块：存放文件的元数据 ls -l 的结果如权限、属主、数组 对一个文件来说，inode block就1个 data block块：存放文件的内容数据 cat看到的结果，真正的内容 对一个文件来说，如果过大，data block可能有多个 superblock超级块：记录此filesystem的整体信息，包括inode&#x2F;block的总量、使用量、剩余量，以及文件系统的格式与相关信息等； superblock一个文件系统整体就一个 查看当前内核支持的文件系统 1cat /lib/modules/`uname -r`/kernel/fs 查看当前系统可用的文件系统 1cat /proc/filesystems 当前系统支持的文件系统和当前系统可用的文件系统是两回事，modules 中的文件系统在编译时选择了才是可用的，而可用的文件系统包含了默认支持的文件系统，如果需要使用某个文件系统，而该文件系统又不在proc 中，则需要重新编译 固态硬盘（SSD）的分区格式实际是指文件系统。文件系统是操作系统用来控制如何在硬盘上存储和检索数据的一种方法。选择正确的文件系统对于最大化SSD 性能和兼容性至关重要 从系统角度来看，文件系统是对文件存储设备的空间进行组织和分配，负责文件存储并对存入的文件进行保护和检索的系统。具体地说，它负责为用户建立文件，存入、读出、修改、转储文件，控制文件的存取，安全控制，日志，压缩，加密等。 2 文件系统的组成文件系统的组成部分 内核中的模块：ext4, xfs, vfat Linux的虚拟文件系统：VFS 用户空间的管理工具：mkfs.ext4, mkfs.xfs, mkfs.vfat VFS（Virtual Filesystem Switch）称为虚拟文件系统或虚拟文件系统转换，是一个内核软件层，在具体的文件系统之上抽象的一层，用来处理与 Posix 文件系统相关的所有调用，表现为能够给各种文件系统提供一个通用的接口，使上层的应用程序能够使用通用的接口访问不同文件系统，同时也为不同文件系统的通信提供了媒介。VFS 并不是一种实际的文件系统，它只存在于内存中，不存在任何外存空间，VFS 在系统启动时建立，在系统关闭时消亡。VFS由超级块、inode、dentry、vfsmount 等结构来组成。 超级块 一个超级块对应一个文件系统(已经安装的文件系统类型如ext2，此处是实际的文件系统，不是VFS)。超级块是反映了文件系统整体的控制信息。超级块能够以多种的方式存在，对于基于磁盘的文件系统，它以特定的格式存在于磁盘的固定区域（取决于文件系统类型）上。在挂载文件系统时，该超级块中的内容被读入磁盘中，从而构建出位于内存中的新的超级块。 块组描述符表(GDT) ext文件系统每一个块组信息使用32字节描述，这32个字节称为块组描述符，所有块组的块组描述符组成块组描述符表GDT(group descriptor table)。虽然每个块组都需要块组描述符来记录块组的信息和属性元数据，但是不是每个块组中都存放了块组描述符。将所有块组的块组信息组成一个GDT保存,并将该GDT存放于某些块组中，类似存放superblock和备份superblock的块 3 文件系统类型 分类 类型 &#x2F; 名称 特点与说明 本地文件系统 ext2 适用于小分区、低更新频率场景（如 &#x2F;boot），无日志功能 ext3 ext2 的改进版，支持日志功能，可从异常关机中恢复 ext4 ext 系列最新版，支持 1EB 文件系统、16TB 单个文件，纳秒级时间戳，快速修复，多块分配 xfs 源自 SGI，支持 8EB 文件系统，日志恢复快，性能接近裸设备 I&#x2F;O，全 64 位支持 swap 交换分区专用文件系统 iso9660 光盘专用文件系统 btrfs Oracle 开发，支持快照、动态卷等高级功能 reiserfs 早期高性能日志文件系统，擅长小文件处理 F2FS 闪存专用文件系统，优化 SSD、USB 等设备性能 Windows文件系统 FAT32 兼容性好（跨系统），支持最大 16TB 文件系统和 4GB 单个文件，适合移动存储 NTFS 支持 16EB 文件系统和单个文件，具备数据恢复、加密、磁盘配额等高级功能 exFAT 介于 FAT32 和 NTFS 之间，适合大容量移动设备，支持大于 4GB 文件 Unix 文件系统 FFS（fast） 快速文件系统，早期 Unix 常用 UFS（unix） 多数类 Unix 系统默认磁盘文件系统 JFS2 IBM 开发的日志文件系统，适用于 AIX 等系统 网络文件系统 NFS 基于网络的文件共享协议，允许远程计算机访问本地文件系统 CIFS（SMB） Windows 主导的网络文件共享协议，支持跨平台文件访问（如 Samba 服务） 集群文件系统 GFS2 基于 x86_64，最大支持 100TB 文件系统，适合集群环境 OCFS2 Oracle 开发的集群文件系统，适用于数据库集群等场景 分布式文件系统 fastdfs 轻量级分布式文件系统，适合中小规模文件存储 ceph 企业级对象存储生态，支持文件、块、对象存储，高可用、可扩展 moosefs 简单分布式文件系统，易于部署和管理 mogilefs 基于 HTTP 的分布式文件存储系统，适合图片等静态资源 glusterfs 开源分布式文件系统，支持横向扩展和多种卷类型 Lustre 高性能分布式文件系统，常用于超级计算机和大规模集群 特殊文件系统 RAW 裸文件系统，未经格式化，直接使用原始磁盘扇区 虚拟文件系统 procfs（&#x2F;proc） 内核抽象层，提供进程和系统信息访问（不占用磁盘空间） sysfs（&#x2F;sys） 管理设备和内核参数的虚拟文件系统 tmpfs 基于内存的临时文件系统，速度快，重启后数据丢失 虚拟化文件系统 9pfs 用于 QEMU 虚拟机，实现虚拟机与主机的文件共享 日志文件系统 （包含 ext3&#x2F;4、xfs 等） 通过日志记录文件系统变化，提高数据一致性和故障恢复速度 闪存文件系统 F2FS（见上文） 专为闪存设备设计，减少写入损耗，优化读写性能 4 创建文件系统mkfs命令 12345678910111213141516171819201 mkfs.FS_TYPE /dev/DEVICE#FS_TYPE选择如下：ext4xfs # 专为大数据产生btrfsvfat#/dev/DEVICE可以是一个硬盘，也可以是一个硬盘分区2 mkfs -t FS_TYPE /dev/DEVICE-b #指定块 block 大小 (1024|2048|4096)-L LABEL #设置卷标-V|--verbose #显示创建过程-j #同 -t ext3-i N #为数据空间中每多少个字节创建一个inode；不应该小于block大小-N N #指定分区中创建多少个inode-I N #一个inode记录占用的磁盘空间大小，128---4096-m N #默认5%,为管理人员预留空间占总空间的百分比-O FEATURE[,...] #启用指定特性-O ^FEATURE #关闭指定特性 范例 12345678#主流mkfs.ext4mkfs.xfs#创建ext4 文件系统[root@ubuntu2204 ~]# mkfs.ext4 /dev/sdc1#创建xfs文件系统[root@ubuntu2204 ~]# mkfs.xfs /dev/sdc2 mke2fs：ext系列文件系统专用管理工具 123456789101112mke2fs [OPTION]... DEVICE-t #&#123;ext2|ext3|ext4|xfs&#125; 指定文件系统类型-b #&#123;1024|2048|4096&#125; 指定块 block 大小-L #‘LABEL’ 设置卷标-j #相当于 -t ext3， mkfs.ext3 = mkfs -t ext3 = mke2fs -j = mke2fs -t ext3-i # 为数据空间中每多少个字节创建一个inode；不应该小于block大小-N # 指定分区中创建多少个inode-I #一个inode记录占用的磁盘空间大小，128---4096-m # 默认5%,为管理人员预留空间占总空间的百分比-O FEATURE[,...] #启用指定特性-O ^FEATURE #关闭指定特性 5 管理文件系统5.1 e2label管理ext系列文件系统的LABEL 1e2label DEVICE [LABEL] 5.2 tune2fs重新设定ext系列文件系统可调整参数的值 1234567-l #查看指定文件系统超级块信息；super block-L &#x27;LABEL’ #修改卷标-m N #修预留给管理员的空间百分比-j #将ext2升级为ext3-O #文件系统属性启用或禁用, -O ^has_journal-o #调整文件系统的默认挂载选项，-o ^acl-U UUID #修改UUID号 1[root@ubuntu2204 ~]# tune2fs -l /dev/sdc1 5.3 dumpe2fs显示ext文件系统信息，将磁盘块分组管理 1-h #查看超级块信息，不显示分组信息 超级块存放整个文件系统的数据，都有备份，万一文件系统被破坏了，利用超级块的备份块可以修复 1234[root@ubuntu2204 ~]# dumpe2fs /dev/sdc1#查看超级块信息[root@ubuntu2204 ~]# dumpe2fs -h /dev/sdc1 5.4 xfs_info显示示挂载或已挂载的 xfs 文件系统信息 1xfs_info mountpoint|devname 1[root@centos8 ~]#xfs_info /dev/sda7 6 xfs文件系统备份与恢复xfsdump备份工具： 完全备份：每一次都是对所有数据的备份 增量备份：和第一次备份进行比较，仅备份有差异的数据 xfsdump使用限制 必须用root权限 只能备份已挂载的文件系统 只能备份XFS文件系统 只能用xfsrestore解释 透过文件系统的UUID来分辨备份档，因此不能备份相同UUID的文件系统 xfsdump选项 123456-l：注意不是大写字母L而是小写，就是指定level，有0~9共10个等级，默认为0，即完整备份。-L：xfsdump会记录每次备份的session Label，这里可以填写针对此文件系统的简易说明；-M：xfsdump可以记录存储Media Label，这里可以填写此媒体的简易说明。-f：后面接产生的文件和destination file 。例如/dev/st0设备文件名或其他一般文件文件名-I：大写的“i”，从/var/lib/xfsdump/inventory 列出目前备份的信息状态。 xfsdump备份和xfsrestore恢复 12345678910111213141516171819# 1、数据备份# 1.1 先做全量备份，切记“备份的源路径”末尾不要加左斜杠/注意：（1）-L与-M后的你起的名字保持一致就行，也方便你记忆（2）备份的源路径写/dev/sda1这种文件系统名字是通用写法，虽然在centos7.9中写挂载点路径虽然也可以，但是还是推荐用通用的靠谱一些xfsdump -l 0 -L sdb3_bak -M sdb3_bak -f 全量备份的成果路径1 备份的源路径 # 1.2 再做增量备份xfsdump -l 1 -L sdb3_bak -M sdb3_bak -f 增量备份的成果路径2 备份的源路径 xfsdump -l 1 -L sdb3_bak -M sdb3_bak -f 增量备份的成果路径3 备份的源路径 xfsdump -l 1 -L sdb3_bak -M sdb3_bak -f 增量备份的成果路径4 备份的源路径 # 2、数据恢复# 2.1、先恢复全量备份xfsrestore -f 全量备份的成果路径1 数据恢复的路径# 2.2、再依次恢复增量xfsrestore -f 增量备份的成果路径2 数据恢复的路径xfsrestore -f 增量备份的成果路径2 数据恢复的路径xfsrestore -f 增量备份的成果路径2 数据恢复的路径 数据备份示例 12345678910111213141516171819202122232425262728293031323334353637383940# 1、准备一个分区并制作好xfs文件系统，挂载好后给它加一点初始数据[root@localhost ~]# df文件系统 1K-块 已用 可用 已用% 挂载点。。。。。。/dev/sdb3 1038336 76836 961500 8% /opt[root@localhost ~]# cp -r /etc/ /opt/[root@localhost ~]# echo 111 &gt; /opt/1.txt[root@localhost ~]# ls /opt/1.txt etc[root@localhost ~]# # 2、先做全量备份[root@localhost ~]# xfsdump -l 0 -L sdb3_bak -M sdb3_bak -f /all.bak /opt # 3、往/opt下新增文件2.txt，然后作增量备份[root@localhost ~]# echo 222 &gt; /opt/2.txt[root@localhost ~]# xfsdump -l 1 -L sdb3_bak -M sdb3_bak -f /add.bak1 /opt # 4、往/opt下新增文件3.txt，然后作增量备份[root@localhost ~]# echo 333 &gt; /opt/3.txt[root@localhost ~]# xfsdump -l 1 -L sdb3_bak -M sdb3_bak -f /add.bak2 /opt 注意上面你备份的都是挂载点/opt,对应文件系统/dev/sdb3，这个在centos7.9中还可用，但是到了rockylinux9.3中会报错：xfsdump: version 3.1.12 (dump format 3.0) - type ^C for status and controlxfsdump: ERROR: /mydata/ does not identify a file systemxfsdump: usage: xfsdump [ -a (dump DMF dualstate files as offline) ]解决办法就是把挂载点/opt换成/dev/sdb3如：xfsdump -l 0 -L sdb3_bak -M sdb3_bak -f /all.bak /dev/sdb3 # 5、查看一下备份文件大小[root@localhost ~]# du -sh /opt/41M /opt/ [root@localhost ~]# ll -h /all.bak # 全量备份大小-rw-r--r--. 1 root root 37M 11月 4 18:44 /all.bak[root@localhost ~]# ll -h /add.bak1 # 增量备份大小-rw-r--r--. 1 root root 22K 11月 4 18:45 /add.bak1[root@localhost ~]# ll -h /add.bak2 # 增量备份大小-rw-r--r--. 1 root root 23K 11月 4 18:46 /add.bak2 数据恢复示例 1234567891011[root@localhost ~]# rm -rf /opt/*[root@localhost ~]# xfsrestore -f /all.bak /opt/ # 先恢复全量......[root@localhost ~]# ls /opt/1.txt etc[root@localhost ~]# xfsrestore -f /add.bak1 /opt/ # 再恢复增量1[root@localhost ~]# ls /opt/1.txt 2.txt etc[root@localhost ~]# xfsrestore -f /add.bak2 /opt/ # 再恢复增量2[root@localhost ~]# ls /opt/1.txt 2.txt 3.txt etc 7 挂载Linux 系统中“一切皆文件”，所有文件都放置在以根目录为树根的树形目录结构中。在Linux 看来，任何硬件设备也都是文件，它们各有自己的一套文件系统 (文件目录结构)。因此产生的问题是，当在 Linux 系统中使用这些硬件设备时，只有将Linux本身的文件目录与硬件设备的文件目录合二为一，硬件设备才能为我们所用。合二为一的过程称为“挂载”，如果不挂载，通过Linux系统中的图形界面系统可以查看找到硬件设备，但命令行方式无法找到。 挂载：将磁盘挂载到某一个目录下(最好是空目录)，这个目录下的文件就会存储在这个磁盘中，所挂载的目录称为挂载点 卸载：为解除此关联关系的过程 7.1 挂载文件系统12345678910111213141516171819202122232425262728293031323334353637383940mount [-fnrsvw] [-t fstype] [-o options] device mountpoint-t fstype #指定要挂载的设备上的文件系统类型,如:ext4,xfs-r #readonly，只读挂载-w #read and write, 读写挂载,此为默认设置,可省略-n #不更新/etc/mtab，mount不可见-a #自动挂载所有支持自动挂载的设备(定义在了/etc/fstab文件中，且挂载选项中有auto功能)-L &#x27;LABEL&#x27; #以卷标指定挂载设备-U &#x27;UUID&#x27; #以UUID指定要挂载的设备-B #绑定目录到另一个目录上device：指明要挂载的设备，可以是设备名称，也可以是卷标，或者UUID -L #同 LABEL=label -U #同 UUID=uuid LABEL=label #用label值指定设备 UUID=uuid #用uuid值指定设备 PARTLABEL=label #按PARTLABEL值指定设备 PARTUUID=uuid #按PARTUUID值指定设备 device #按路径指定设备 directory #绑定式挂载的挂载点(参阅 --bind/rbind) file #用于设置回环设备的常规文件mountpoint：挂载点目录，必须事先存在，建议使用空目录-o options：(挂载文件系统的选项)，多个选项使用逗号分隔 async #异步模式,内存更改时,写入缓存区buffer,过一段时间再写到磁盘中，效率高，但不安全 sync #同步模式,内存更改时，同时写磁盘，安全，但效率低下 atime/noatime #包含目录和文件 diratime/nodiratime #目录的访问时间戳 auto/noauto #是否支持开机自动挂载，是否支持-a选项 exec/noexec #是否支持将文件系统上运行应用程序 dev/nodev #是否支持在此文件系统上使用设备文件 suid/nosuid #是否支持suid和sgid权限 remount #重新挂载 ro/rw #只读、读写 user/nouser #是否允许普通用户挂载此设备，/etc/fstab使用 acl/noacl #启用此文件系统上的acl功能 loop #使用loop设备 _netdev #当网络可用时才对网络资源进行挂载，如：NFS文件系统 defaults #相当于rw, suid, dev, exec, auto, nouser, async 挂载规则 一个挂载点同一时间只能挂载一个设备 一个挂载点同一时间挂载了多个设备，只能看到最后一个设备的数据，其它设备上的数据将被隐藏 一个设备可以同时挂载到多个挂载点 通常挂载点一般是已存在空的目录 范例 123#只读挂载[root@ubuntu2204 ~]# mount --source /dev/sdc2 -o ro /sdc2mount /dev/sdb1 /opt/ 往目录&#x2F;opt下新建的文件存到了哪里？ 存到了 /dev/sdb1磁盘分区下 卸载&#x2F;opt后，数据是否存在？ 存在，因为数据已经存到了磁盘，重新挂载还是可以看到数据 重新格式化&#x2F;dev&#x2F;sdb1后，数据是否依然存在？ 不存在了 同一个分区&#x2F;文件系统挂载到不同的文件夹下，数据的来源是否一致？ 一致，因为他们的数据都是会存在同一个磁盘中，可以共享里面的数据 sdb1挂载到 &#x2F;，sdb2挂载到 &#x2F;opt，那么sdb1可以存储 &#x2F;opt 下的数据吗？ 不可以，虽然opt是在 &#x2F; 下，但是 &#x2F;opt 已经挂载到sdb2上了，那么sdb2就是它存储数据的硬盘，如果说 &#x2F;opt 没有被挂载到哪个硬盘上，那么数据就会存储在sdb1上 7.2 卸载文件系统卸载时：可使用设备，也可以使用挂载点 1umount 设备名|挂载点 7.3 查看挂载情况查看挂载 123456#通过查看/etc/mtab文件显示当前已挂载的所有设备mount#查看内核追踪到的已挂载的所有设备cat /proc/mounts#通过查看/etc/mtab文件显示当前已挂载的所有设备cat /etc/mtab 查看挂载点情况 1findmnt MOUNT_POINT|device 范例 1234567#默认查看整个系统挂载树[root@ubuntu2204 ~]# findmnt......#查看指定设备或挂载点[root@ubuntu2204 ~]# findmnt /dev/sdc1TARGET SOURCE FSTYPE OPTIONS/sdc1 /dev/sdc1 ext4 rw,relatime 查看正在访问指定文件系统的进程 12lsof MOUNT_POINTfuser -v MOUNT_POINT 终止所有在正访问指定文件系统的进程 1fuser -km MOUNT_POINT 7.4 持久挂载开机自动挂载两种方式 将挂载命令写入/etc/rc.local中，注意这是个软链接，需要给源文件加 x 执行权限 在/etc/fstab按照格式写入信息，UUID可以通过blkid命令查看 范例：在 /etc/fstab 按照格式写入信息 1234567891011[root@rocky8 ~]#cat /etc/fstab /dev/mapper/rl-root / xfs defaults 0 0UUID=a1564c83-06da-4ee2-9442-409727529931 /boot xfs defaults 0 0/dev/mapper/rl-swap none swap defaults 0 0第一项： 设备的标识 (LABEL=label | UUID=uuid | /dev/sda1)第二项： mountpoint ，必须是事先存在的目录第三项： 文件系统 ext4 ,iso9660第四项: 挂载规则，defaults ，acl，bind，ro，rw 等第五项： 备份频率 0 不做备份; 1 每天转储; 2 每隔一天转储第六项; fsck检查的文件系统的顺序：0 不自检 ; 1 首先自检，一般只有rootfs才用（挂载点为 / ）；2 非rootfs使用 在 /etc/fstab 填写完，需要执行下面命令生效，此命令只针对文件新增行或删除行有效，如果在中间修改了挂载规则，则此命令无效 1mount -a 修改了 &#x2F;etc&#x2F;fstab 文件中的挂载规则，无法通过 mount -a 生效，要执行重新挂载 1mount -o remount MOUNTPOINT 7.5 故障修复当写入文件的内容不小心写错的时候，导致重启开机失败，我们先在VM里以故障状态登陆root，然后打开 &#x2F;etc&#x2F;fstab，将有错误的更改或者注释，就可以了 范例：centos7, 8 &#x2F;etc&#x2F;fstab 的分区UUID错误，无法启动 1234不需要其他操作，自动进入emergency mode,系统提示输入root 密码#cat /proc/mounts 可以查看到/ 以rw方式挂载#vim /etc/fstab#reboot 范例：centos 6 &#x2F;etc&#x2F;fstab 的分区UUID错误，无法启动 123456如果/etc/fstab 的挂载设备出错，比如文件系统故障，并且文件系统检测项（即第6项为非0），将导致无法启动自动进入emergency mode,输入root 密码#cat /proc/mounts 可以查看到/ 以ro方式挂载，无法直接修改配置文件#mount -o remount,rw /#vim /etc/fstab将故障行的最后1项，即第6项修改为0，开机不检测此项挂载设备的健康性，从而忽略错误，能实现启动","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://aquapluto.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"}]},{"title":"磁盘分区","slug":"Linux/disk-manage/disk-partition","date":"2025-08-21T02:59:41.000Z","updated":"2025-09-07T04:54:09.915Z","comments":true,"path":"Linux/disk-manage/disk-partition/","permalink":"https://aquapluto.github.io/Linux/disk-manage/disk-partition/","excerpt":"","text":"1 linux中的磁盘1.1 设备文件Linux 哲学思想：一切皆文件。对于硬件设备，在Linux系统中，也是以文件的形式呈现出来的/sys 和 /proc 一样，是一个内存文件系统 ，在磁盘上并没有对应的内容。通常称其为sysfs，这是内核 “暴露” 给用户空间的一个 驱动模型层次结构的展现。因此，host0, host1 这些 “文件”是内核根据设备驱动程序 “发现” 的设备后在内存中创建的对应的 “文件” 和 “文件层次”。 设备文件：关联至一个设备驱动程序，进而能够跟与之对应硬件设备进行通信 1234567#设备文件[root@ubuntu2204 ~]# ll /dev/sdabrw-rw---- 1 root disk 8, 0 Jul 29 08:51 /dev/sda#设备文件[root@ubuntu2204 ~]# ll /dev/tty0crw--w---- 1 root tty 4, 0 Jul 29 08:51 /dev/tty0 设备号码： 主设备号：major number, 标识设备类型 次设备号：minor number, 标识同一类型下的不同设备 设备类型： 块设备：block，存取单位“块”，磁盘 字符设备：char，存取单位“字符”，键盘 磁盘设备的设备文件命名 设备类型 设备文件命名 描述 SAS,SATA,SCSI,IDE,USB &#x2F;dev&#x2F;sda; &#x2F;dev&#x2F;sdb; &#x2F;dev&#x2F;sdc; …… d 代表disk；s 代表接口；sda代表第一块盘；sdb代表第二块；sda1代表第一块盘的第一个分区 nvme协议硬盘（固态盘建议使用） &#x2F;dev&#x2F;nvme0n1; &#x2F;dev&#x2F;nvme0n2; &#x2F;dev&#x2F;nvme0n3; …… nvme0代表第一块盘；nvme1代表第二块硬盘；nvme0n1表示第一块硬盘的第一个namespace命名空间，这是一个逻辑概念，一个命名空间就相当于一块隔离的硬盘，相当于硬盘的虚拟化技术；nvme0n1p1表示n1下的第一个分区 虚拟磁盘 &#x2F;dev&#x2F;vda; &#x2F;dev&#x2F;vdb; &#x2F;dev&#x2F;xvda; &#x2F;dev&#x2F;xvdb; …… 逻辑卷LVM &#x2F;dev&#x2F;dm-0;&#x2F;dev&#x2F;dm-1;…… RAID设备 &#x2F;dev&#x2F;md0;&#x2F;dev&#x2F;md1;…… 块设备 &#x2F;dev&#x2F;loop1;&#x2F;dev&#x2F;loop2;…… 不同磁盘标识：a-z,aa,ab… 1/dev/sda，/dev/sdb, ... 同一设备上的不同分区：1,2, … 12/dev/sda1/dev/sda5 范例 12345[root@Rocky ~]#ll /dev/nvme0*crw------- 1 root root 243, 0 8月 20 11:50 /dev/nvme0brw-rw---- 1 root disk 259, 0 8月 20 11:50 /dev/nvme0n1brw-rw---- 1 root disk 259, 1 8月 20 11:50 /dev/nvme0n1p1brw-rw---- 1 root disk 259, 2 8月 20 11:50 /dev/nvme0n1p2 1.2 磁盘操作范例：查看CHS 1234567891011121314151617181920212223242526#rocky8.6默认只有扇区信息，ubuntu2204同样[root@rocky8 ~]#fdisk -l /dev/sdaDisk /dev/sda: 20 GiB, 21474836480 bytes, 41943040 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x43a6eadaDevice Boot Start End Sectors Size Id Type/dev/sda1 * 2048 2099199 2097152 1G 83 Linux/dev/sda2 2099200 41943039 39843840 19G 8e Linux LVM#加参数后可显示更详细信息[root@rocky8 ~]#fdisk -u=cylinder -l /dev/sdaDisk /dev/sda: 20 GiB, 21474836480 bytes, 41943040 sectorsGeometry: 255 heads, 2 sectors/track, 2610 cylindersUnits: cylinders of 510 * 512 = 261120 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x43a6eadaDevice Boot Start End Cylinders Size Id Type/dev/sda1 * 5 4117 4113 1G 83 Linux/dev/sda2 4117 82242 78126 19G 8e Linux LVM 范例：识别SSD和机械硬盘类型 123456789101112131415161718192021222324#1表示机械，0表示SSD[root@centos8 ~]#lsblk -d -o name,rotaNAME ROTAsda 1sr0 1nvme0n1 0nvme0n2 0[root@centos8 ~]#ls /sys/block/nvme0n1 nvme0n2 sda sr0[root@centos8 ~]#cat /sys/block/*/queue/rotational0011[root@centos8 ~]#cat /sys/block/sda/queue/rotational1[root@centos8 ~]#cat /sys/block/sr0/queue/rotational1[root@centos8 ~]#cat /sys/block/nvme0n1/queue/rotational0[root@centos8 ~]#cat /sys/block/nvme0n2/queue/rotational0 范例: 测速 123[root@ubuntu1804 ~]#dd | hdparm -t /dev/sda/dev/sda:Timing buffered disk reads: 1854 MB in 3.00 seconds = 617.80 MB/sec 自动检测新的硬盘 123echo &#x27;- - -&#x27; &gt; /sys/class/scsi_host/hostx/scanfor i in /sys/class/scsi_host/host*/scan;do echo &#x27;- - -&#x27; &gt; $i;done ubuntu 自动检测新的硬盘 12345678910#两种写法1 循环遍历[root@ubuntu2204 ~]# for i in `ls /sys/class/scsi_host/`;do echo &#x27;- - -&#x27; &gt;/sys/class/scsi_host/$i/scan;done2 找出SPI总线对应的 host，只扫描该 host[root@ubuntu2204 ~]# grep mptspi /sys/class/scsi_host/host*/proc_name/sys/class/scsi_host/host32/proc_name:mptspi[root@ubuntu2204 ~]# echo &#x27;- - -&#x27; &gt; /sys/class/scsi_host/host32/scan#此处是重新扫描 SCSI 总线来添加设备，之所以是 SCSI 总线，是因为我们添加的 SCSI 类型的硬盘 2 磁盘分区2.1 为什么分区 优化I&#x2F;O性能 实现磁盘空间配额限制 提高修复速度 隔离系统和程序 安装多个OS 采用不同文件系统 2.2 分区类型分区主要分为三类：主分区&lt;— 扩展分区&lt;— 逻辑分区 逻辑分区属于扩展分区，扩展分区属于主分区 主分区又叫做引导分区，是可以安装系统的分区 2.3 分区格式2.3.1 MBR分区MBR 分区，MBR 的意思是 “主引导记录”。MBR 最大支持 2TB 容量，在容量方面存在着极大的瓶颈。并且只支持创建最多4个主分区，也可以3主分区+1扩展(N个逻辑分区)。而GPT分区方式就没有这些限制 划分分区的单位： CentOS 5 之前按整柱面划分 CentOS 6 版本后可以按Sector划分 0磁道0扇区：512bytes 446字节存放计算机启动的程序 64字节存放划分分区的信息，每个分区占16字节 2字节 55AA 标识位 硬盘主引导记录MBR由4个部分组成 主引导程序（偏移地址0000H–0088H），它负责从活动分区中装载，并运行系统引导程序 出错信息数据区，偏移地址0089H–00E1H为出错信息，00E2H–01BDH全为0字节 分区表（DPT,Disk Partition Table）含4个分区项，偏移地址01BEH–01FDH,每个分区表项长16个字节，共64字节为分区项1、分区项2、分区项3、分区项4 结束标志字，偏移地址01FE–01FF的2个字节值为结束标志55AA 为什么不能超过4个主分区？ 因为在0磁道0扇区上只留了64bytes空间存储分区表信息，而一个分区的关键信息要占用16个字节来存放 为什么单分区不能超2T？ 一个分区信息占用16个字节，其中记录分区开始位置的空间为4个字节，记录分区结束位置的空间也是4个字节； 一个字节8位，4个字节是32位，则起始位最小值为32个0，结束位最大值为32个1， 所以一个分区，最大就是2的32次方个扇区，一个扇区512字节，则最大空间是 232*29 &#x3D; 241 字节； 240是T，那么241表示不超过2T 范例：二进制查看扇区情况 1hexdump -C -n 512 /dev/sda 范例：备份MBR的分区表,并破坏后恢复 123456789101112131415161718192021222324252627#备份MBR分区表[root@centos8 ~]#dd if=/dev/sda of=/data/dpt.img bs=1 count=64 skip=446bs=1：块大小1个字节 count=64：备份64字节 skip=446：不要前面446字节，取后面的64字节/dev/sda：使用lsblk查看当前虚拟机的硬盘名称/data/dpt.img：备份到的文件[root@centos8 ~]#hexdump -C /data/dpt.img[root@centos8 ~]#scp /data/dpt.img 10.0.0.102:#破坏MBR分区表[root@centos8 ~]#dd if=/dev/zero of=/dev/sda bs=1 count=64 seek=446#无法启动[root@centos8 ~]#reboot#进入虚拟机，手速要快按下ESC键进入一个选择界面，选择CD-ROM Drive,进入Linux安装光盘界面，选择Troubleshooting，选择第2项rescue，选第1项continue，回车继续，配置网络，将之前备份到远程主机的文件拷贝过来#配置网络ifconfig ens160 10.0.0.8/24ip a a 10.0.0.8/24 dev ens160#传送文件scp 10.0.0.102:/root/dpt.img .#恢复MBR分区表dd if=dpt.img of=/dev/sda bs=1 count=64 seek=446reboot 问题：如何利用分区策略相同的另一台主机的分区表来还原和恢复当前主机破环的分区表？ 123456789# 在正常主机上备份分区表，使用 sfdisk 导出分区表sudo sfdisk -d /dev/sdX &gt; partition_table.sfdisk# 将分区表传输到目标主机scp partition_table.sfdisk user@target_host:/path/to/destination/# 在目标主机上恢复分区表sudo sfdisk --verify /dev/sdX &lt; partition_table.sfdisksudo sfdisk /dev/sdX &lt; partition_table.sfdisk 2.3.2 GPT分区GPT 分区（ubuntu装系统默认就是GPT分区），GPT 意为 GUID 分区表，它支持的磁盘容量比 MBR 大得多。这是一个正逐渐取代 MBR 的新标准，它是由 UEFI 辅住而形成的，将来 UEFI 用于取代老旧的 BIOS，而 GPT 则取代老旧的 MBR。 GPT：GUID（Globals Unique Identifiers） partition table 支持128个分区，当数据较大时可以使用GPT分区，使用64位，支持8Z（ 512Byte&#x2F;block ）64Z （ 4096Byte&#x2F;block） 使用128位UUID(Universally Unique Identifier) 表示磁盘和分区，GPT分区表自动备份在头和尾两份， 并有CRC校验位，自动备份 UEFI (Unified Extensible Firmware Interface 统一可扩展固件接口)硬件支持GPT，使得操作系统可以启动 2.4 管理分区2.4.1 parted 命令高级分区操作，可以是交互或非交互方式 注意：parted的操作都是实时生效的，小心使用 格式 123456789101112131415161718192021222324parted [选项]... [设备 [命令 [参数]...]...]#常用选项-l|--list #显示所有硬盘分区信息 -s|--script #不输出提示信息 #常用子命令align-check TYPE N #检查分区是否满足对齐(最小|最佳)类型的对齐方式help [COMMAND] #显示命令帮助mklabel|mktable LABEL-TYPE #指定磁盘的分区类型 gpt|msdos(mbr)mkpart PART-TYPE [FS-TYPE] START END #新建分区,指定分区类型，文件系统，开始结束位置name NUMBER NAME #重命名指定分区print [devices|free|list,all|NUMBER] #显示quit #退出rescue START END #空间碎片整理resizepart NUMBER END #重置分区大小rm NUMBER #删除指定分区select DEVICE #选择设备disk_set FLAG STATE #为设备打标签disk_toggle [FLAG] #修改flagset NUMBER FLAG STATE #设置flagtoggle [NUMBER [FLAG]] #修改flagunit UNIT #设置默认单位, 默认为MB，B|KB|MB|GB|TBversion #显示版本 范例 12345parted /dev/sdb mklabel gpt|msdos #选择使用GPT还是MBR分区parted /dev/sdb print #打印分区表情况parted /dev/sdb mkpart primary 1 200（默认M） #指定从第一个分区到第200个分区进行创建主分区parted /dev/sdb rm 1 #删除分区（1是硬盘编号）parted -l #列出所有硬盘分区信息 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101[root@centos8 ~]#parted /dev/sdb printError: /dev/sdb: unrecognised disk labelModel: VMware, VMware Virtual S (scsi) Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: unknownDisk Flags:[root@centos8 ~]#parted /dev/sdb mklabel gptInformation: You may need to update /etc/fstab.[root@centos8 ~]#parted /dev/sdb printModel: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags[root@centos8 ~]#parted /dev/sdb mkpart primary 1 1001Information: You may need to update /etc/fstab.[root@centos8 ~]#parted /dev/sdb printModel: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags1 1049kB 1001MB 1000MB primary[root@centos8 ~]#parted /dev/sdb mkpart primary 1002 1102Information: You may need to update /etc/fstab.[root@centos8 ~]#parted /dev/sdb printModel: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags1 1049kB 1001MB 1000MB primary2 1002MB 1102MB 99.6MB primary[root@centos8 ~]#parted /dev/sdb rm 2Information: You may need to update /etc/fstab.[root@centos8 ~]#parted /dev/sdb print Model: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags1 1049kB 1001MB 1000MB primary[root@centos8 ~]#parted /dev/sdb mklabel msdosWarning: The existing disk label on /dev/sdb will be destroyed and all data onthis disk will be lost. Do you want to continue?Yes/No? YInformation: You may need to update /etc/fstab.[root@centos8 ~]#parted /dev/sdb print Model: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 21.5GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags:Number Start End Size Type File system Flags[root@centos8 ~]#parted /dev/sdb #交互式操作GNU Parted 3.2Using /dev/sdbWelcome to GNU Parted! Type &#x27;help&#x27; to view a list of commands.(parted) helpalign-check TYPE N check partition N for TYPE(min|opt)alignmenthelp [COMMAND] print general help, or help onCOMMANDmklabel,mktable LABEL-TYPE create a new disklabel (partitiontable)mkpart PART-TYPE [FS-TYPE] START END make a partitionname NUMBER NAME name partition NUMBER as NAMEprint [devices|free|list,all|NUMBER] display the partition table,available devices, free space, all found partitions, or a particular partitionquit exit programrescue START END rescue a lost partition near STARTand ENDresizepart NUMBER END resize partition NUMBER rm NUMBER delete partition NUMBERselect DEVICE choose the device to editdisk_set FLAG STATE change the FLAG on selected devicedisk_toggle [FLAG] toggle the state of FLAG on selecteddevice set NUMBER FLAG STATE change the FLAG on partition NUMBERtoggle [NUMBER [FLAG]] toggle the state of FLAG on partitionNUMBERunit UNIT set the default unit to UNITversion display the version number andcopyright information of GNU Parted(parted) 2.4.2 分区工具fdisk和gdisk格式 123fdisk -l [-u] [device...] #查看分区fdisk [device...] #管理MBR分区gdisk [device...] #类fdisk的管理GPT分区工具，用法跟fdisk一样 选项 1234567891011-b|--sector-size &lt;size&gt; #指定扇区大小，默认512字节-L|--color[=color] #显示时是否添加颜色(auto|always|never)默认启用颜色-l|--list #显示-o|--output &lt;list&gt; #只显示指定列-u|--units[=&lt;unit&gt;] #设置显示单位 cylinders|sectors，默认sectors-s|--getsz #显示设备有多少个扇区-b|--bytes N #以指定的字节大小来计算扇区数量-t|--type type #只显示指定类型的分区表-C|--cylinders N #指定柱面数-H|--heads N #指定磁头数-S|--sectors N #指定每条磁道的扇区数 子命令 123456789101112131415161718192021222324252627fdiskp #分区列表t #更改分区类型n #创建新分区d #删除分区v #校验分区u #转换单位w #保存并退出q #不保存并退出x #高级功能(专家模式)gdiskb #备份分区表到指定文件c #修改分区名d #删除分区i #显示分区详细信息l #列出所有分区类型n #新建分区o #创建新的分区表p #查看分区q #退出r #恢复和转换选项，非专业人士勿用s #排序t #修改分区类型，默认 8300,表示普通分区v #检测硬盘是否有问题w #保存退出x #额外功能，专家模式 同步分区表：将操作系统内存中缓存的磁盘分区信息（分区表）与磁盘实际存储的分区表数据进行一致性同步的操作。 当我们对磁盘进行分区操作（如创建、删除、调整分区）时，操作通常先在内存中的分区表缓存中生效，而磁盘上的实际分区表可能并未即时更新。如果此时直接操作磁盘（如格式化新分区），操作系统可能因读取的是旧缓存信息而无法识别新分区，甚至导致数据错误。 1234# CentOS 7,8partprobe# CentOS6partx 范例 12345#显示指定列fdisk -lo id,size,type /dev/sda#查看内核是否已经识别新的分区cat /proc/partitions 范例：MBR分区 123456789101112131415161718192021222324[root@centos7 ~]#fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0x057e0640.Command (m for help): n #输入子命令Partition type: p primary (0 primary, 0 extended, 4 free) #选择主分区 e extended #选择扩展分区Select (default p): Partition number (1-4, default 1): #选择分区编号First sector (2048-41943039, default 2048): #选择开始扇区位置，2048表示起始位置是2MLast sector, +sectors or +size&#123;K,M,G&#125; (2048-41943039, default 41943039): #选择大小Command (m for help): w #w保存The partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks. 范例：GPT分区 123456789101112131415161718192021[root@centos7 ~]#gdisk /dev/sdcCommand (? for help): n # 新建Partition number (1-128, default 1): # 直接回车，默认分区号为1First sector (34-5242879966, default = 2048) or &#123;+-&#125;size&#123;KMGTP&#125;: #直接回车，使用默认的起始位置Last sector (2048-5242879966, default = 5242879966) or &#123;+-&#125;size&#123;KMGTP&#125;: +1G # 设定1G的空间Current type is &#x27;Linux filesystem&#x27;Hex code or GUID (L to show codes, Enter = 8300): # 直接回车用默认8300就好，或者输入L，可以看到一系列文件系统信息，默认的8300编号代表Linux filesystem，用它就好Changed type of partition to &#x27;Linux filesystem&#x27; Command (? for help): p # 打印查看......Number Start (sector) End (sector) Size Code Name 1 2048 2099199 1024.0 MiB 8300 Linux filesystem Command (? for help): w # 保存Final checks complete. About to write GPT data. THIS WILL OVERWRITE EXISTINGPARTITIONS!! Do you want to proceed? (Y/N): y # 输入y保存OK; writing new GUID partition table (GPT) to /dev/sdc.The operation has completed successfully. 范例：非交互式创建分区 1234567[root@centos8 ~]#echo -e &#x27;n\\np\\n\\n\\n+2G\\nw\\n&#x27; | fdisk /dev/sdc[root@centos8 ~]#fdisk /dev/sdb &lt;&lt;EOFnp+1GwEOF 范例：硬盘分区问题 123456789101112131415161718192021222324252627282930313233#看出磁盘的第二个分区的起始位置为1026048，计算：1026048*512=525M（因为512个字节一个扇区），结束位置为1599M，1599-525=1074，相当于分了1G空间[root@centos7 ~]#fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x166a0e04 Device Boot Start End Blocks Id System/dev/sdb2 1026048 3123199 1048576 83 Linux#现在想要在第一个分区分5G的容量[root@centos7 ~]#fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Command (m for help): nPartition type: p primary (1 primary, 0 extended, 3 free) e extendedSelect (default p): pPartition number (1,3,4, default 1): 1First sector (2048-41943039, default 2048): Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-1026047, default 1026047): +5GValue out of range. #失败了#失败的原因是在一个磁盘空间里，之间在525M到1599M的区间分了一个分区，在第一个分区分的起始位置只能是2048(2M)，而2M到525M的区间不足5G，因为分区必须是连续空间，不能是东凑一块西凑一块，要分5G空间就只能是在起始位置1599(3123199)开始分第三个分区 范例：修改分区类型 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@centos7 ~]#fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x166a0e04 Device Boot Start End Blocks Id System/dev/sdb1 2048 1026047 512000 83 Linux #现在是83/dev/sdb2 1026048 3123199 1048576 83 Linux [root@centos7 ~]#fdisk /dev/sdbWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Command (m for help): t #更改分区类型Partition number (1,2, default 2): 1 #选择分区编号Hex code (type L to list all codes): L #列出所有分区类型 0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden C: c6 DRDOS/sec (FAT- 4 FAT16 &lt;32M 40 Venix 80286 85 Linux extended c7 Syrinx 5 Extended 41 PPC PReP Boot 86 NTFS volume set da Non-FS data 6 FAT16 42 SFS 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS/exFAT 4d QNX4.x 88 Linux plaintext de Dell Utility 8 AIX 4e QNX4.x 2nd part 8e Linux LVM df BootIt 9 AIX bootable 4f QNX4.x 3rd part 93 Amoeba e1 DOS access a OS/2 Boot Manag 50 OnTrack DM 94 Amoeba BBT e3 DOS R/O b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad hi eb BeOS fs e W95 FAT16 (LBA) 53 OnTrack DM6 Aux a5 FreeBSD ee GPT f W95 Ext&#x27;d (LBA) 54 OnTrackDM6 a6 OpenBSD ef EFI (FAT-12/16/10 OPUS 55 EZ-Drive a7 NeXTSTEP f0 Linux/PA-RISC b11 Hidden FAT12 56 Golden Bow a8 Darwin UFS f1 SpeedStor 12 Compaq diagnost 5c Priam Edisk a9 NetBSD f4 SpeedStor 14 Hidden FAT16 &lt;3 61 SpeedStor ab Darwin boot f2 DOS secondary 16 Hidden FAT16 63 GNU HURD or Sys af HFS / HFS+ fb VMware VMFS 17 Hidden HPFS/NTF 64 Novell Netware b7 BSDI fs fc VMware VMKCORE 18 AST SmartSleep 65 Novell Netware b8 BSDI swap fd Linux raid auto1b Hidden W95 FAT3 70 DiskSecure Mult bb Boot Wizard hid fe LANstep 1c Hidden W95 FAT3 75 PC/IX be Solaris boot ff BBT 1e Hidden W95 FAT1 80 Old Minix Hex code (type L to list all codes): 8e #选择分区类型编号Changed type of partition &#x27;Linux&#x27; to &#x27;Linux LVM&#x27;Command (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@centos7 ~]#fdisk -l /dev/sdbDisk /dev/sdb: 21.5 GB, 21474836480 bytes, 41943040 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x166a0e04 Device Boot Start End Blocks Id System/dev/sdb1 2048 1026047 512000 8e Linux LVM #改成8e（逻辑卷）/dev/sdb2 1026048 3123199 1048576 83 Linux 学习问题：就是这个sda2挂载了&#x2F;boot，sda3那个lvm挂载了&#x2F;，疑问就是&#x2F;boot所占的空间，会影响到&#x2F;的空间吗，因为boot也是在&#x2F;下的 解答：&#x2F;boot是单独分出来的sda2，跟sda3没关系，不会互相影响。 12345678910[root@ubuntu2004 ~]#df -hFilesystem Size Used Avail Use% Mounted onudev 1.9G 0 1.9G 0% /devtmpfs 389M 1.6M 388M 1% /run/dev/mapper/ubuntu--vg-ubuntu--lv 18G 2.8G 15G 17% /tmpfs 1.9G 4.0K 1.9G 1% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/locktmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/sda2 1.5G 209M 1.2G 16% /boottmpfs 389M 0 389M 0% /run/user/0 2.4.3 lsblk123456789101112131415161718192021222324252627282930#常用选项-a|--all #输出所有设备信息-b|--bytes #以字节为单位显示设备大小-d|--nodeps #不显示分区信息-e|--exclude &lt;list&gt; #以主设备号排除设备-f|--fs #显示文件系统-i|--ascii #只使用 ascii 字符输出-I|--include &lt;list&gt; #仅显示指定的设备-J|--json #以json格式显示输出-l|--list #以列表显示-T|--tree #以树状结构显示，默认项-m|--perms #显示属主属组及权限-n|--noheadings #不显示表头-o|--output &lt;list&gt; #只显示指定列-O|--output-all #显示所有列-p|--paths #显示设备全路径-P|--pairs #以 k=&gt;v 的格式显示输出-r|--raw #原样输出-s|--inverse #反向显示关联信息-S|--scsi #显示scsi设备(small computer system interface device,小型计算机接口设备)-t|--topology #显示拓扑信息#-o常用字段NAME #设备名称MAJ:MIN #主设备号:次设备号RM #是否是可移动设备SIZE #设备容量大小RO #是否是只读设备TYPE #设备类型MOUNTPOINT #挂载点 范例 123456#查看指定设备[root@ubuntu2204 ~]# lsblk -f /dev/sdc#查看所有设备[root@rocky86 ~]# lsblk -f#查看指定列[root@ubuntu2004 ~]#lsblk -d -o NAME,SIZE 2.4.4 blkid可以查看块设备属性信息 1234blkid [OPTION]... [DEVICE]-U UUID #根据指定的UUID来查找对应的设备-L LABEL #根据指定的LABEL来查找对应的设备 范例 1234567891011121314151617#显示所有[root@ubuntu2004 ~]#blkid/dev/sr0: UUID=&quot;2022-02-23-09-27-00-00&quot; LABEL=&quot;Ubuntu-Server 20.04.4 LTS amd64&quot; TYPE=&quot;iso9660&quot; PTUUID=&quot;492bdcc4&quot; PTTYPE=&quot;dos&quot;/dev/sda2: UUID=&quot;04c587f7-1f97-4632-970a-0dd3e08fb398&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;118bc546-3a2d-4411-9c74-25f5a37bdbf3&quot;/dev/sda3: UUID=&quot;8LrQLS-6cfS-HTCn-w7Vo-flx7-90dy-qDukq7&quot; TYPE=&quot;LVM2_member&quot; PARTUUID=&quot;b07f78ba-02aa-4575-ae26-7c97b4a43b73&quot;/dev/mapper/ubuntu--vg-ubuntu--lv: UUID=&quot;8ef803b3-d204-4c23-b2e7-1a5ba91ea557&quot; TYPE=&quot;ext4&quot;/dev/loop1: TYPE=&quot;squashfs&quot;/dev/loop2: TYPE=&quot;squashfs&quot;/dev/loop3: TYPE=&quot;squashfs&quot;/dev/loop4: TYPE=&quot;squashfs&quot;/dev/loop5: TYPE=&quot;squashfs&quot;/dev/loop6: TYPE=&quot;squashfs&quot;/dev/sda1: PARTUUID=&quot;b96ee049-d3f9-49d0-a7dd-10cdd163f152&quot;#查询[root@ubuntu2004 ~]#blkid -U &quot;04c587f7-1f97-4632-970a-0dd3e08fb398&quot;/dev/sda2 PARTUUID是分区的唯一标识符，而UUID是文件系统的唯一标识符。以下是它们的具体解释： PARTUUID（Partition UUID）：是针对GPT（GUID Partition Table）分区方案的分区表级别的唯一标识符。它是在创建分区时生成的，并且即使对分区进行重新格式化或更改文件系统类型，该标识符仍然保持不变。因为PARTUUID是从分区表中直接检索得到的，所以它可以访问分区而不依赖于分区内的文件系统内容。 UUID（Universally Unique Identifier）：通常是指文件系统级别的唯一标识符。它用于唯一识别一个特定的文件系统实例，比如一个ext4或NTFS分区。当文件系统被创建（例如，通过格式化操作）时，会生成一个UUID，并存储在文件系统的元数据中。如果文件系统被删除或重新格式化，UUID会改变。 简而言之，PARTUUID与分区结构相关联，而UUID与分区内的文件系统相关联。 2.4.5 findfs查找分区 12findfs [options] LABEL=&lt;label&gt;findfs [options] UUID=&lt;uuid&gt; 范例 123456[root@centos8 ~]#findfs UUID=f7f53add-b184-4ddc-8d2c-5263b84d1e15/dev/sda2[root@ubuntu2204 ~]# findfs LABEL=&#x27;Ubuntu-Server 22.04 LTS amd64&#x27;/dev/sr0[root@centos8 ~]#findfs `sed -En &#x27;/data/s#^([^ ]+).*#\\1#p&#x27; /etc/fstab`/dev/sda3 2.5 格式化制作文件系统与挂载磁盘必须格式化制作文件系统，然后挂载才能使用，请看 文件系统 3 磁盘管理工具3.1 df查看磁盘分区使用情况：当你需要了解每个挂载点或磁盘分区的已用、可用以及总空间时，应该使用df命令。df可以快速给出整个文件系统空间实际真正占用等信息，因为它直接从文件系统获取信息，而不是通过计算文件大小。只能查看挂载下的文件系统 12345678910111213141516df [OPTION]... [device]...-a|--all #显示所有-B|--block-size=SIZE #显示时指定块大小--direct #将挂载点那一列标题显示为文件-h|--human-readable #以方便阅读的方式显示-H|--si #以1000为单位，而不是1024-i|--inodes #显示inode 信息而非块使用量-k #同 --block-size=1K-l|--local #只显示本机的文件系统--output[=FIELD_LIST] #只显示指定字段-P|--portability #以Posix兼容的格式输出--total #最后加一行汇总数据-t|--type=TYPE #根据文件系统类型过滤-T|--print-type #显示文件系统类型-x|--exclude-type=TYPE #根据文件系统类型反向过滤 范例 1234567891011121314#显示文件系统的inode使用情况，而不是磁盘空间使用情况df -i#显示文件系统类型df -T#显示总磁盘空间使用情况，包括所有文件系统df -h --total#以MB为单位显示磁盘空间使用情况df -m#显示指定文件系统类型之外的文件系统df -x tmpfs -x devtmpfs 3.2 du查看某文件或某目录总体占用空间：如果你想要知道某个目录或一组文件具体占用了多少磁盘空间，可以使用du命令。du会遍历指定目录中的所有文件和子目录，逐一报告它们的实际占用状态，单位为KB。 1234567891011121314151617du [OPTION]... DIRdu [OPTION]... --files0-from=F-0 #输出时以NULL分割，而不是换行-a #显示所有文件和目录大小-B #指定块大小-b #同 --apparent-size --block-size=1-c #汇总-d #指定最大目录层级-h #以方便阅读的格式显示-k #同 --block-size=1K-m #同 --block-size=1M--si #友好显示，以1000为单位，而不是1024-s #只显示外层目录-X #根据文件名忽略--exclude=PATTERN #根据正则表达式忽略-x #忽略不在同一个文件系统的目录 范例 1234567891011121314151617181920#查看目录占用多大空间[root@ubuntu2204 ~]# du -sh /etc/#查看目录下每个文件或子目录大小[root@ubuntu2204 ~]# du -sh /var/log/* | head -n 3#显示目录下所有文件和子目录的磁盘使用情况du -a /path/to/directory#以人类可读的格式显示目录下一级子目录的磁盘使用情况du -h --max-depth=1 /path/to/directory#显示指定目录的磁盘使用情况，排除挂载的文件系统du -shx /path/to/directory#显示多个目录的总磁盘使用情况du -c /path/to/directory1 /path/to/directory2#跟踪符号链接指向的文件或目录的磁盘使用情况du -L /path/to/symlink 3.3 df和du的差别df命令关注的是文件系统的磁盘空间使用情况，它包括了被文件和程序占用的空间，甚至包括已经删除但尚未由操作系统释放的文件所占用的空间。这意味着df命令提供的是一个更为全面的磁盘使用概览，它可以帮助我们了解磁盘的空闲空间以及整体的使用状况。 du命令则专注于计算文件或目录占用的磁盘空间，它是一个面向文件的命令，只会统计当前存在的文件所占用的空间。du命令通过累加各个文件的大小来计算总占用空间，这使得它特别适合于查找特定目录下的磁盘使用详情。例如，当我们需要找出哪个文件或哪个目录占用了大量空间时，du命令就会非常有用。 特点 df du 功能 显示文件系统的磁盘空间使用情况 估算文件和目录的磁盘空间使用情况 范围 显示已挂载文件系统的磁盘空间使用情况 递归计算指定目录下所有文件和子目录的磁盘空间使用情况 输出信息 文件系统的磁盘空间使用情况，包括总容量、已使用、可用和挂载点等信息 文件和目录的磁盘空间使用量 查找大文件 否 是 用途 查看整个文件系统的使用情况和可用空间 定位存储占用问题，查找占用空间较大的文件或目录 3.4 dddd命令的全称是disk dump，意为磁盘转储。在Unix&#x2F;Linux中，用于复制和转换文件。可以读取&#x2F;或写入到硬件的设备驱动（如硬盘）和特殊设备文件（如&#x2F;dev&#x2F;zero和&#x2F;dev&#x2F;random），就像普通文件一样，即按照指定的规则复制文件、转换文件格式以及创建镜像文件 注意：虽然dd命令强大，但使用时务必小心。一不小心错误的命令参数就可能导致数据丢失 12345678910111213141516171819202122232425dd if=/PATH/FROM/SRC of=/PATH/TO/DEST bs=# count=#/PATH/FROM/SRC #要复制内容的源文件/PATH/TO/DEST #目标文件，将接收复制的内容if=file #从所命名文件读取而不是从标准输入，即输入文件（源文件）of=file #写到所命名的文件而不是到标准输出，即输出文件（目标文件）ibs=size #一次从源文件读size个byteobs=size #一次在目标文件写size个bytebs=size #block size, 块大小，用于指定每次读取和写入的数据块大小cbs=size #一次转化size个byteskip=blocks #从开头忽略blocks个ibs大小的块seek=blocks #从开头忽略blocks个obs大小的块count=n #复制n个bsconv=conversion[,conversion...] #用指定的参数转换文件#conversion 转换参数:ascii #转换 EBCDIC 为 ASCIIebcdic #转换 ASCII 为 EBCDIClcase #把大写字符转换为小写字符ucase #把小写字符转换为大写字符nocreat #不创建输出文件noerror #出错时不停止notrunc #不截短输出文件sync #把每个输入块填充到ibs个字节，不足部分用空(NUL)字符补齐fdatasync #写完成前，物理写入输出文件 范例：创建空白文件 123456789dd if=/dev/zero of=test bs=block_size count=number_of_blocks#这个代码是使用dd命令来创建一个文件，具体地说，它会生成一个填充了零（null bytes）的文件。下面是对这个命令的详细解释：1 dd: 这是命令本身，代表disk dump。2 if=/dev/zero: 这是输入文件（input file）的路径。这里指定的是/dev/zero，这是一个特殊的设备文件，它只产生零字节（null bytes）。3 of=test: 这是输出文件（output file）的路径。在这个例子中，输出的文件名为test。4 bs=block_size: 这是块大小（block size）。这意味着每次读取和写入的数据块的大小。例如，如果block_size是4K，那么每次读取和写入的数据块将是4KB。5 count=number_of_blocks: 这指定了要读取和写入的块数。因此，如果block_size是4K，并且number_of_blocks是100，那么将创建400KB的文件。#所以，这个命令的总体效果是从/dev/zero读取零字节，并将这些零字节写入到test中，总共写入number_of_blocks个块，每个块的大小为block_size,文件内容为0 范例 1234567dd if=/dev/zero of=test bs=1M count=1000在当前目录下会生成一个1000M的test文件，文件内容为全0（因从/dev/zero中读取，/dev/zero为0源），但是这样为实际写入硬盘，文件产生速度取决于硬盘读写速度，如果欲产生超大文件，速度很慢。在某种场景下，我们只想让文件系统认为存在一个超大文件在此，但是并不实际写入硬盘则可以dd if=/dev/zero of=test bs=1M count=0 seek=100000此时创建的文件在文件系统中的显示大小为100000MB，但是并不实际占用block，因为count=0，因此创建速度与内存速度相当，seek的作用是跳过输出文件中指定大小的部分，这就达到了创建大文件，但是并不实际写入的目的。当然，因为不实际写入硬盘，所以你在容量只有10G的硬盘上创建100G的此类文件都是可以的。 范例：随机生成1百万个1K的文件 1seq 1000000 | xargs -i dd if=/dev/zero of=&#123;&#125;.dat bs=1024 count=1 范例：创建随机数据文件 12#通过读取/dev/urandom设备生成的随机数据，你可以轻松创建一个用于测试或加密目的的文件dd if=/dev/urandom of=output_file bs=block_size count=number_of_blocks 范例：制作设备镜像 12#dd不仅可以操作文件，还可以处理设备。比如制作设备的镜像，特别是在备份或克隆存储设备时dd if=input_device of=output_file bs=4M 范例 1234567891011121314151617181920212223242526272829[root@centos8 ~]#cat f1.txt;abcdef[root@centos8 ~]#cat f2.txt123456789[root@centos8 ~]#dd if=f1.txt of=f2.txt bs=1 count=2 skip=3 seek=4 2+0 records in2+0 records out2 bytes copied, 6.6515e-05 s, 30.1 kB/s[root@centos8 ~]#cat f2.txt1234de[root@centos8 ~]#echo 123456789 &gt; f2.txt[root@centos8 ~]#cat f2.txt123456789[root@centos8 ~]#cat f1.txtabcdef[root@centos8 ~]#cat f1.txt; cat f2.txtabcdef123456789[root@centos8 ~]#dd if=f1.txt of=f2.txt bs=1 count=2 skip=3 seek=4 conv=notrunc2+0 records in2+0 records out2 bytes copied, 7.6153e-05 s, 26.3 kB/s[root@centos8 ~]#cat f2.txt1234de789 范例 12345678910111213141516171819202122232425262728293031323334353637383940414243#备份MBRdd if=/dev/sda of=/tmp/mbr.bak bs=512 count=1#破坏MBR中的bootloaderdd if=/dev/zero of=/dev/sda bs=64 count=1 seek=446#有一个大与2K的二进制文件fileA。现在想从第64个字节位置开始读取，需要读取的大小是128Byts。又有fileB, 想把上面读取到的128Bytes写到第32个字节开始的位置，替换128Bytes，实现如下dd if=fileA of=fileB bs=1 count=128 skip=63 seek=31 conv=notrunc#将本地的/dev/sdx整盘备份到/dev/sdydd if=/dev/sdx of=/dev/sdy#将/dev/sdx全盘数据备份到指定路径的image文件dd if=/dev/sdx of=/path/to/image#备份/dev/sdx全盘数据，并利用gzip压缩，保存到指定路径dd if=/dev/sdx | gzip &gt;/path/to/image.gz#将备份文件恢复到指定盘dd if=/path/to/image of=/dev/sdx#将压缩的备份文件恢复到指定盘gzip -dc /path/to/image.gz | dd of=/dev/sdx#将内存里的数据拷贝到root目录下的mem.bin文件dd if=/dev/mem of=/root/mem.bin bs=1024#拷贝光盘数据到root文件夹下，并保存为cdrom.iso文件dd if=/dev/cdrom of=/root/cdrom.iso#销毁磁盘数据dd if=/dev/urandom of=/dev/sda1#通过比较dd指令输出中命令的执行时间，即可确定系统最佳的block size大小dd if=/dev/zero of=/root/1Gb.file bs=1024 count=1000000dd if=/dev/zero of=/root/1Gb.file bs=2048 count=500000 dd if=/dev/zero of=/root/1Gb.file bs=4096 count=250000#测试硬盘写速度dd if=/dev/zero of=/root/1Gb.file bs=1024 count=1000000#测试硬盘读速度dd if=/root/1Gb.file bs=64k | dd of=/dev/null 范例：定制化dd命令 123456789101112#显示进度，在处理大文件或设备时，你可能想要知道操作的进度。通过使用status=progress参数，dd会在执行过程中显示进度信息，让你对操作的完成情况有更直观的了解，status=progress: 该参数显示复制进度信息，你可以在终端中看到复制的进度百分比和已经复制的数据量dd if=input_file of=output_file bs=4M status=progress#错误处理，dd可以通过conv=sync,noerror参数进行错误处理，即使在读取错误的情况下，它也会继续进行操作。这在处理损坏的设备或损坏的文件时可能很有用dd if=input_file of=output_file bs=4M conv=sync,noerror#跳过部分数据，有时候，你可能只对文件的一部分数据感兴趣。使用skip参数可以告诉dd跳过输入文件的前几个块dd if=input_file of=output_file bs=4M skip=10#合并多个文件，通过使用if参数指定多个输入文件，你可以使用dd将它们合并成一个输出文件，seek=1表示在输出文件中跳过一个块，以确保新的数据追加到正确的位置dd if=input_file1 of=output_file bs=4Mdd if=input_file2 of=output_file bs=4M seek=1","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://aquapluto.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"}]},{"title":"作业管理与并行运行","slug":"Linux/process-manage/job-parallel","date":"2025-08-21T02:59:12.000Z","updated":"2025-09-10T15:07:22.055Z","comments":true,"path":"Linux/process-manage/job-parallel/","permalink":"https://aquapluto.github.io/Linux/process-manage/job-parallel/","excerpt":"","text":"一、作业管理 Linux的作业控制 前台作业：通过终端启动，且启动后一直占据终端 后台作业：可通过终端启动，但启动后即转入后台运行（释放终端） 让作业运行于后台 运行中的作业： Ctrl+z（变成stop态） 尚未启动的作业： COMMAND &amp; 后台作业虽然被送往后台运行，但其依然与终端相关；退出终端，将关闭后台作业。如果希望送往后台后，剥离与终端的关系 nohup COMMAND &amp;&gt;/dev/null &amp; screen；COMMAND tmux；COMMAND 后台运行的进程和终端关系 进程虽然放到了后台执行，但还是终端下的子进程 查看当前终端所有作业编号 1234567[root@ubuntu ~]# jobs[1] Stopped ping www.baidu.com[2]- Stopped ./a.sh 123[3]+ Stopped ./a.sh 456-：倒数第二个后台进程+：最后一个后台进程 作业控制 1234fg [[%]JOB_NUM] 把指定的后台作业调回前台bg [[%]JOB_NUM] 让送往后台的作业在后台继续运行kill [%JOB_NUM] 终止指定的作业cmd &amp; 直接是后台运行 范例：作业控制，让后台stop进程继续运行 123456789101112131415[root@ubuntu ~]# jobs[1] Stopped ./a.sh 123[2]- Stopped ./a.sh 456[3]+ Stopped ./a.sh 789#让后台stop 进程继续running[root@ubuntu ~]# bg[3]+ ./a.sh 789 &amp;[root@ubuntu ~]# bg 1[1]- ./a.sh 123 &amp;[root@ubuntu ~]# jobs[1] Running ./a.sh 123 &amp;[2]+ Stopped ./a.sh 456[3]- Running ./a.sh 789 &amp; 范例: 后台运行的进程和终端关系 123456789101112#终端1运行后台进程[root@centos8 ~]#ping 127.0.0.1 &amp;[1] 30545#终端2 可以查看到进程[root@centos8 ~]#ps aux|grep pingroot 30545 0.0 0.2 32408 2416 pts/0 S 12:25 0:00 ping 127.0.0.1root 30547 0.0 0.1 12108 988 pts/2 S+ 12:25 0:00 grep --color=auto ping#关闭终端1后,在终端2查看不到进程[root@centos8 ~]#ps aux|grep pingroot 30552 0.0 0.1 12108 1084 pts/2 S+ 12:25 0:00 grep --color=auto ping 范例：将进程从前台运行（占用终端窗口，不会输出其他命令结果）变成后台运行（仍处于运行状态，但可以输出其他命令） 123456789101112131415161718192021222324252627282930313233[root@centos ~]#ping 10.0.0.183PING 10.0.0.183 (10.0.0.183) 56(84) bytes of data.64 bytes from 10.0.0.183: icmp_seq=1 ttl=64 time=0.100 ms64 bytes from 10.0.0.183: icmp_seq=2 ttl=64 time=0.051 ms64 bytes from 10.0.0.183: icmp_seq=3 ttl=64 time=0.052 ms64 bytes from 10.0.0.183: icmp_seq=4 ttl=64 time=0.135 ms64 bytes from 10.0.0.183: icmp_seq=5 ttl=64 time=0.057 ms64 bytes from 10.0.0.183: icmp_seq=6 ttl=64 time=0.153 ms64 bytes from 10.0.0.183: icmp_seq=7 ttl=64 time=0.056 ms^Z #Ctrl+z停止运行[1]+ Stopped ping 10.0.0.183#通过jobs查看作业编号1[root@centos ~]#jobs [1]+ Stopped ping 10.0.0.183#实现后台执行（或者kill -18 11596）[root@centos ~]#bg 1 [1]+ ping 10.0.0.183 &amp;[root@centos ~]#64 bytes from 10.0.0.183: icmp_seq=8 ttl=64 time=0.032 ms64 bytes from 10.0.0.183: icmp_seq=9 ttl=64 time=0.059 ms64 bytes from 10.0.0.183: icmp_seq=10 ttl=64 time=0.046 msls #不占用终端资源，可以输出命令结果anaconda-ks.cfg apps data dir motd_peiqi passwd pwd[root@centos ~]#64 bytes from 10.0.0.183: icmp_seq=11 ttl=64 time=0.037 ms64 bytes from 10.0.0.183: icmp_seq=12 ttl=64 time=0.156 ms64 bytes from 10.0.0.183: icmp_seq=13 ttl=64 time=0.045 ms[root@centos ~]#pidof ping11596[root@centos ~]#kill -19 11596 #停止运行[root@centos ~]#fg 1 #恢复前台 范例: nohup（在后台运行，不会随着窗口的关闭而停止运行，而是将内容重定向到一个文件） 1234567891011[root@centos8 ~]#nohup ping 127.0.0.1nohup: ignoring input and appending output to &#x27;nohup.out&#x27;[root@centos8 ~]#cat nohup.out64 bytes from 127.0.0.1: icmp_seq=16 ttl=64 time=0.037 ms64 bytes from 127.0.0.1: icmp_seq=17 ttl=64 time=0.040 ms64 bytes from 127.0.0.1: icmp_seq=18 ttl=64 time=0.042 ms64 bytes from 127.0.0.1: icmp_seq=19 ttl=64 time=0.047 ms#如果不想它重定向到一个文件，放进垃圾箱并后台执行[root@centos8 ~]#nohup ping 127.0.0.1 &amp;&gt; /dev/null &amp; 二、并行运行利用后台执行，实现并行功能，即同时运行多个进程，提高效率 方法1：写脚本 12345cat all.shf1.sh&amp;f2.sh&amp;f3.sh&amp;#在脚本里最后要加上wait才能退出 方法2 1(f1.sh&amp;);(f2.sh&amp;);(f3.sh&amp;) 方法3 1f1.sh&amp;f2.sh&amp;f3.sh&amp; 停止运行 1killall cmd 多组命令实现并行并停止 12[root@centos8 ~]#&#123; ping -c3 127.1; ping 127.2; &#125;&amp; &#123; ping -c3 127.3 ;ping 127.4;&#125;&amp;[root@centos8 ~]#killall ping 范例：网段检测 12345678910111213141516[root@centos8 ~]#cat scanhost.sh#!/bin/bash#顺序执行net=10.0.0for i in &#123;1..254&#125;;do ping -c1 -W1 $net.$i &amp;&gt; /dev/null &amp;&amp; echo $net.$i is up || echo $net.$i is downdone#并发执行NET=10.0.0for i in &#123;1..254&#125;;do &#123; ping -c1 -W1 $&#123;NET&#125;.$&#123;i&#125; &amp;&gt; /dev/null &amp;&amp; echo $&#123;NET&#125;.$&#123;i&#125; is up || echo $&#123;NET&#125;.$&#123;i&#125; is down &#125;&amp;donewait 三、导致子进程脱离父进程被祖宗进程接管的几种执行模式123456789101112131415以下几种命令执行格式会导致子进程脱离父进程的直接管束，最终在父进程退出后被祖宗进程接管- 宿主机中是PID为1的systemd进程- 容器中是0号进程1. 使用子shell语法并后台运行： (command &amp;) 解释：括号创建子shell，command在后台运行，子shell立即退出，导致command成为孤儿进程，被祖宗进程接管。2. 使用 nohup 命令： nohup command &amp; 解释：nohup 使进程忽略SIGHUP信号，即使父shell退出，进程仍继续运行，最终被祖宗进程接管。3. 使用 setsid 启动新会话： setsid command 解释：command 运行在新的会话中，与原shell完全脱离，成为会话首进程，若其父退出，则被祖宗进程接管。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://aquapluto.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"信号发送","slug":"Linux/process-manage/signal","date":"2025-08-21T02:58:59.000Z","updated":"2025-09-11T02:25:38.295Z","comments":true,"path":"Linux/process-manage/signal/","permalink":"https://aquapluto.github.io/Linux/process-manage/signal/","excerpt":"","text":"一、HUP信号HUP信号，主要涉及两种应用场景 终端关闭时，系统会向与该终端相连的所有进程发送SIGHUP信号，这会导致这些进程被关闭(所有才有了脱离终端运行的需求) 用kill命令给进程发送SIGHUP信号实现该进程的平滑重启 需要注意的是，并非所有的进程或应用都能处理SIGHUP信号，只有程序内写过专门的捕获并处理该信号的代码才行，nginx的源代码里就写了，并且会响应该信号来实现平滑重启 当我们的进程在前台或者后台运行的时候，会因为用户退出、网络断开、终端关闭导致一起关闭，要想解决有两种的方法 让进程忽略HUP信号 让进程运行在新会话中，从而成为不属于此终端的子进程，就不会在当前终端挂掉的情况下一起挂掉 nohup命令：忽略HUP信号，和&amp;一起使用，输出信息(1,2)会缺省重定向到 nohup.out 文件。当当前终端关掉后，该终端的进程的父id会变成1 setsid命令：直接将进程的父id变成1 二、kill命令kill：内部命令，可用来向进程发送控制信号，以实现对进程管理,每个信号对应一个数字，信号名称以SIG开头（可省略），不区分大小写 2.1 执行流程kill命令的执行流程：用户空间的kill命令—–》内核空间（sys_kill()—&gt;_send_signal()—&gt;sig_task_ignored()）——》用户空间的另外一个进程 没有被sig_task_ignored()忽略掉的信号才会发送给用户态的的另外一个进程，sig_task_ignored()就相当于一层关卡，被它放行的信号才会通知给用户态的指定进程（如下图所示pid为1的进程） sig_task_ignored()代码如下，主要有三个判断条件，任意一个条件成立，都会返回true，代表应该忽略（Ignore）当前信号，我们主要看一下第二个if判断 1t-&gt;signal-&gt;flags &amp; SIGNAL_UNKILLABLE 进程必须是SIGNAL_UNKILLABLE该条件才成立， 在每个Namespace中1号进程创建时，都会被打上SIGNAL_UNKILLABLE的标签 也就是说只要是容器内的1号进程，该条件一定成立 1handler == SIG_DFL 如果用户进程没有针对本信号注册自己的handler处理函数，那么会有一个默认的handler程序 这个默认的handler就叫SIG_DFL 所以，总结一句话就是如果你进程没有针对本信号注册handler，那该条件就程序 ps：如果进程使用的是默认&#x2F;缺省行为那该条件就成立 如果目标进程对-15信号注册了handler处理函数，那么该条件就为false 如果没有注册该条件就为true 1!(force &amp;&amp; sig_kernel_only(sig) force结果的真假取决于发送信号的进程与接收信号的进程是否在同一个namespace里， 如果是那结果就为0，否则结果就为1 1sig_kernel_only(sig) 只有信号为-9信号或者-19结果才true 2.2 使用方式格式 12345678910kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec]-SIGNAL：信号名称-s：指定信号-u uid: 生效者-U uid: 真正发起运行命令者-t terminal: 与指定终端相关的进程-l: 显示进程名（pgrep可用）-a: 显示完整格式的进程名（pgrep可用）-P pid: 显示指定进程的子进程 显示当前系统可用信号 12345678910111213kill -l trap -l#常用信号0) 检测进程是否正常，此方式有局限性，即使进程处于停止或僵尸状态，此方式仍然认为是进程是健康的1) SIGHUP 无须关闭进程而让其重读配置文件2) SIGINT 中止正在运行的进程；相当于Ctrl+c3) SIGQUIT 相当于ctrl+\\9) SIGKILL 强制杀死正在运行的进程,可能会导致数据丢失,慎用!10) SIGUSR1 用来触发一些特定的动作或者事件，比如重新加载配置文件、启动某些特定的操作等，不需要重启15) SIGTERM 终止正在运行的进程，默认信号18) SIGCONT 继续运行19) SIGSTOP 后台休眠 指定信号的方法 : 信号的数字标识：1, 2, 9 信号完整名称：SIGHUP，sighup 信号的简写名称：HUP，hup 向进程发送信号： 1、按PID 123456kill -1 pid …kill -n 9 pidkill -s SIGINT pid[root@centos8 ~]#kill -int `pidof ping`[root@centos8 ~]#kill -sigint `pidof ping` 2、按名称：killall 来自于psmisc包 1234killall [-SIGNAL] comm…#范例killall nginx 范例：查看HUP信号 123#许多服务的支持的reload操作，实际就是发送了HUP信号#service httpd reload 即相当于 killall -1 httpd[root@centos6 ~]#grep -A 10 -w reload -m 1 /etc/init.d/httpd 范例：利用 0 信号实现进程的健康性检查 123456789[root@centos8 ~]#killall -0 ping[root@centos8 ~]#echo $?0[root@centos8 ~]#killall -0 pingping: no process found[root@centos8 ~]#echo $?1#此方式有局限性，即使进程处于停止或僵尸状态，此方式仍然认为是进程是健康的 3、按模式 1pkill [options] pattern 范例：踢出指定终端，高版本内核无效 1[root@rocky ~]# pkill -t pts/1 范例: pkill和pgrep支持正则表达式 123456[root@centos8 ~]#pkill &#x27;^p&#x27;[root@centos8 ~]#pgrep -a &#x27;^p&#x27;9278 pickup -l -t unix -u9281 ping 1.1.1.19311 ping 2.2.2.2 范例: nginx服务的信号 1234567891011121314151617181920[root@centos8 ~]#man nginx SIGUSR1 Reopen log files. SIGUSR2 Upgrade the nginx executable on the fly. SIGWINCH Shut down worker processes gracefully. [root@wang-liyun-pc ~]# cat /etc/logrotate.d/nginx/apps/nginx/logs/*.log &#123; daily rotate 100 missingok notifempty nocompress delaycompress create 644 nginx nginx postrotate if [ -f /apps/nginx/logs/nginx.pid ]; then kill -USR1 `cat /apps/nginx/logs/nginx.pid` #发送USR1信号,重新打开日志文件 fi endscript&#125; 范例：关掉指定端口的进程 1234567891011[root@ubuntu ~]# lsof -i:80COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEapache2 1220 root 4u IPv6 38019 0t0 TCP *:http (LISTEN)apache2 1225 www-data 4u IPv6 38019 0t0 TCP *:http (LISTEN)apache2 1226 www-data 4u IPv6 38019 0t0 TCP *:http (LISTEN)#强制关闭使用 TCP 端口 80 的所有进程[root@ubuntu ~]# fuser -k -9 80/tcp80/tcp: 1220 1225 1226[root@ubuntu ~]# lsof -i:80 三、-9和-15信号当我们使用kill pid时，实际相当于kill -15 pid。也就是说默认信号为15。使用kill -15时，系统会发送一个SIGTERM的信号给对应的程序。当程序接收到该信号后，具体要如何处理自己可以决定。这时候，应用程序可以选择： 立即停止程序 释放响应资源后停止程序 忽略该信号，继续执行程序 因为kill -15信号只是通知对应的进程要进行”安全、干净的退出”，程序接到信号之后，退出前一般会进行一些”准备工作”，如资源释放、临时文件清理等等，如果准备工作做完了，再进行程序的终止。 但是，如果在”准备工作”进行过程中，遇到阻塞或者其他问题导致无法成功，那么应用程序可以选择忽略该终止信号。 这也就是为什么我们有的时候使用kill命令是没办法”杀死”应用的原因，因为默认的kill信号是SIGTERM（15），而SIGTERM（15）的信号是可以被阻塞和忽略的。 和kill -15相比，kill -9就相对强硬得多，系统会发出SIGKILL信号，他要求接收到该信号的程序应该立即结束运行，不能被阻塞或者忽略。 所以，kill -9在执行时，应用程序是没有时间进行”准备工作”的，所以这通常会带来一些副作用，数据丢失或者终端无法恢复到正常状态等。 四、特权信号进程收到信号后的三种反应 忽略（Ignore）: 就是对该信号不做任何处理， 捕获（Catch）：就是让用户进程可以注册自己针对这个信号的handler，一旦进程收到该信号后，就会触发handler的功能运行 默认&#x2F;缺省行为（Default）：linux系统为每个信号都定义了默认要做的事情，我们可以通过man 7 signal来查看，对于大多数信号来说，我们都不需要捕获并注册自己的handler，使用默认的就好 两个特权信号 SIGKILL（-9）：强制杀死 SIGSTOP（-19）：暂停进程的运行，恢复运行需要发送-18信号 这两个特权信号特权就特权在： 无法被忽略 无法被捕获 因为这两信号是linux系统为内核和超级用户准备的特权，可以删除任意的进程，任何进程只要收到了这俩信号，只能执行缺省行为，任何进程收到了这俩信号都只能乖乖听话。至于SIGTERM（-15）信号，代表的是平滑关闭，该信号是可以被忽略或者说捕获的","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://aquapluto.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"进程管理命令","slug":"Linux/process-manage/command","date":"2025-08-21T02:58:53.000Z","updated":"2025-09-09T15:50:30.649Z","comments":true,"path":"Linux/process-manage/command/","permalink":"https://aquapluto.github.io/Linux/process-manage/command/","excerpt":"","text":"一、查看进程相关信息linux启动的第一个进程是0号进程，是静态创建的 在0号进程启动后会接连创建两个进程，分别是1号进程和2和进程。 1号进程最终会去调用可init可执行文件，init进程最终会去创建所有的应用进程。 2号进程会在内核中负责创建所有的内核线程 所以说0号进程是1号和2号进程的父进程；1号进程是所有用户态进程的父进程；2号进程是所有内核线程的父进程。 1.1 进程树pstreepstree 可以用来显示进程的父子关系，以树形结构显示 123456pstree [OPTION] [ PID | USER ]-p 显示PID-T 不显示线程thread,默认显示线程-u 显示用户切换-H pid 高亮显示指定进程及其前辈进程 范例 1234567891011#高亮显示前辈进程[root@centos8 ~]#pstree -pH 1780#显示进程切换[root@ubuntu ~]# pstree -u | grep jose#显示指定用户的进程 [root@ubuntu ~]# pstree jose#不显示线程[root@ubuntu ~]# pstree -pT | grep httpd 1.2 进程信息ps查看进程状态，默认显示当前终端中的进程，Linux系统各进程的相关信息均保存在 /proc/PID 目录下的各文件中 12345678910111213141516171819202122232425ps [OPTION]...a #查看所有终端的进程u #选项显示进程所有者的信息x #选项包括不链接终端的进程，即后台运行的进程（守护进程）f #选项显示进程树o #属性… 选项显示定制的信息，pid、cmd、%cpu、%mem、nik #对属性排序，属性前加 - 表示倒序L #显示支持的属性列表-ef #显示列 C 表示cpu利用率-L #显示线程-e #显示所有进程，相当于-A-f #显示完整格式程序信息-F #显示更完整格式的进程信息-H #以进程层级格式显示进程相关信息-u #userlist 指定有效的用户ID或名称-U #userlist 指定真正的用户ID或名称-g #gid或groupname 指定有效的gid或组名称-G #gid或groupname 指定真正的gid或组名称-p #pid 显示指pid的进程-M #显示SELinux信息，相当于Z--ppid pid #显示属于pid的子进程--sort #对属性排序，属性前加 - 表示倒序-t ttylist #指定tty,相当于 t-C cmdlist #指定命令，多个命令用，分隔 ps 输出属性 字段名 含义解析 USER(UID) 进程的所属用户（用户 ID） PID 进程的唯一标识 ID PPID 父进程的 ID（创建当前进程的进程） %CPU(C) 进程的 CPU 占用率（C为取整后的利用率） %MEM 进程的内存占用率（占物理内存的比例） VSZ 虚拟内存集，进程已分配的线性内存空间大小，即在程序代码中申请的内存大小（包括未实际使用的部分，如内存映射、动态库等）linux系统的内存是超配的，进程最先申请到的都是VSZ虚拟内存 RSS 常驻内存集，进程实际占用的物理内存大小也称之为匿名内存，这部分内存大都是没有刷入磁盘的数据，所以也叫脏数据，因为没有落盘所以数据不安全容易丢 TTY 进程关联的终端（如pts/0为伪终端，?表示无终端关联的后台进程） STAT 进程状态： R：运行中 S：可中断睡眠 D：不可中断睡眠 T：停止 Z：僵尸进程 +：前台进程 &#96; START(STIME) 进程的启动时间 TIME 累计分配给进程的 CPU 运行时长 COMMAND(CMD) 启动进程的程序及参数： 带[]：内核态进程；不带[]：用户态进程 UID 同USER(UID)，进程属主的用户 ID ni 进程的优先级调整值（-20~19，值越小优先级越高） pri 进程的实际优先级（与ni相关，值越小优先级越高） rtprio 进程实时优先级（仅实时进程有效，0~99，值越大优先级越高） psr 进程当前运行的 CPU 核心编号，即进程运行在哪个处理器上 euser 有效用户，进程的实际拥有者（可能与启动用户不同） ruser 真实用户，启动进程的用户 范例 123456789101112131415161718192021222324252627282930#查看进程详细信息[root@centos8 ~]#ps -ef#可以更精确看到CPU使用情况和内存使用情况[root@centos8 ~]#ps aux#查看进程的父子关系[root@centos8 ~]#ps auxf#查看进程的特定属性[root@centos8 ~]#ps axo pid,cmd,%mem,%cpu#按CPU利用率倒序排序[root@centos8 ~]#ps aux k -%cpu[root@centos8 ~]#ps axo pid,cmd,%cpu,%mem k -%cpu#按内存倒序排序[root@centos8 ~]#ps axo pid,cmd,%cpu,%mem --sort %mem#linux下获取占用内存资源最多的10个进程ps aux|head -1;ps aux|grep -v PID|sort -rn -k +4|head#内存消耗最大进程ps aux --sort -rss | head#输出中仅展示有关内存消耗过程的特定信息ps -eo pid,ppid,%mem,%cpu,cmd --sort=-%mem | head#只想查看命令名称而不是命令的绝对路径ps -eo pid,ppid,%mem,%cpu,comm --sort=-%mem | head 范例：有效用户和实际用户 1234567[wang@centos8 ~]$passwdChanging password for user wang.Current password:[root@centos8 ~]#ps axo pid,cmd,%cpu,%mem,user,euser,ruser | grep passwd 1965 passwd 0.0 1.0 root root wang 1970 grep --color=auto passwd 0.0 0.1 root root root 范例 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#查询你拥有的所有进程ps -x#显示指定实际用户名(RUID)或用户ID的进程ps -fU apacheps -fU 48#显示指定有效用户名(EUID)或用户ID的进程ps -fu wangps -fu 1000#查看以root用户权限（实际和有效ID）运行的每个进程ps -U root -u root#列出某个组拥有的所有进程（实际组ID：RGID或名称）ps -fG nginx#列出有效组名称（或会话）所拥有的所有进程ps -fg mysqlps -fg 27#显示指定的进程ID对应的进程ps -fp 1234#以父进程ID来显示其下所有的进程，如显示父进程为1234的所有进程ps -f --ppid 1234#显示指定PID的多个进程ps -fp 1204,1239,1263#要按tty显示所属进程ps -ft pts/0#以进程树显示系统中的进程如何相互链接ps -e --forest#以进程树显示指定的进程ps -f --forest -C sshdps -ef --forest | grep -v grep | grep sshd#要显示一个进程的所有线程,将显示LWP（轻量级进程）以及NLWP（轻量级进程数）列ps -fL -C nginx#要列出所有格式说明符ps L#查看进程的PID，PPID，用户名和命令ps -eo pid,ppid,user,cmd#自定义格式显示文件系统组,ni值开始时间和进程的时间ps -p 1234 -o pid,ppid,fgroup,ni,lstart,etime#使用其PID查找进程名称：ps -p 1244 -o comm=#要以其名称选择特定进程，显示其所有子进程ps -C sshd,bash#查找指定进程名所有的所属PID，在编写需要从std输出或文件读取PID的脚本时这个参数很有用ps -C httpd,sshd -o pid=#检查一个进程的执行时间ps -eo comm,etime,user | grep nginx#排序，查找占用最多内存和CPU的进程ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | headps -eo pid,ppid,cmd,%mem,%cpu --sort=-%cpu | head#显示安全信息ps -eMps --context#使用以下命令以用户定义的格式显示安全信息ps -eo euser,ruser,suser,fuser,f,comm,label#使用watch实用程序执行重复的输出以实现对就程进行实时的监视，如下面的命令显示每秒钟的监视watch -n 1 &#x27;ps -eo pid,ppid,cmd,%mem,%cpu --sort=-%mem | head&#x27; 范例：查看优先级和CPU绑定关系 12345678910[root@centos8 ~]#ps axo pid,cmd,ni,pri,psr,rtprio |grep migration 11 [migration/0] - 139 0 99 16 [migration/1] - 139 1 99 2246 grep --color=auto migration 0 19 0 -[root@centos8 ~]#ps axo pid,cmd,ni,pri,psr |grep dd 2 [kthreadd] 0 19 1 138 [ipv6_addrconf] -20 39 0 2153 dd if=/dev/zero of=/dev/nul 19 0 0 2228 grep --color=auto dd 0 19 1 1.3 查看相关命令的进程编号pidof123456789101112131415161718192021pidof [options] [program [...]]-x 按脚本名称查找pid-s 多个结果时只显示一条[root@centos8 ~]#pidof bash19035 18813 18789 1251#只显示一个进程号[root@ubuntu ~]# pidof -s bash8274[root@centos7 ~]#cat ping.sh#!/bin/bashping 127.0.0.1#centos8 执行命令可以查看到pid[root@centos7 ~]#pidof ping.sh#ping.sh必须有shebang机制,否则pidof -x 也无法查找到[root@centos7 ~]#pidof -x ping.sh19035 1.4 查看进程实时状态toptop 命令是一个用于实时监视 Linux 系统中进程的命令行工具，它提供了有关系统性能和进程活动的实时信息。它没有参数，直接运行它将显示实时的系统性能信息和进程列表。默认情况下，top会按CPU利用率降序排列进程。 top命令统计的状态信息就是从 /proc/stat 中取的，系统所有进程的运行状态都在 /proc 下 cpu：/proc/cpuinfo 内存：/proc/meminfo 内核启动参数：/proc/cmdline 1234567891011121314151617181920212223242526272829303132333435363738394041[root@centos ~]#toptop - 17:19:29 up 1 day, 9:55, 2 users, load average: 0.00, 0.01, 0.05Tasks: 109 total, 1 running, 108 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.0 us, 0.0 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.2 si, 0.0 stKiB Mem : 1863028 total, 1305272 free, 187488 used, 370268 buff/cacheKiB Swap: 2097148 total, 2097148 free, 0 used. 1508764 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND #第一行：uptime格式17:19:29：系统当前时间up 1 day, 9:55：系统己开机运行时长2 users：当前有两个用户登录load average: 0.00, 0.01, 0.05：系统的平均负载，1分钟，5分钟，15分钟#第二行：一共109个进程，其中1个在运行，108个在睡觉，0个停止态。0个僵尸态#第三行：us：高优先级进程的用户态进程占用cpu时间的百分比sy：内核态进程占用cpu时间的百分比ni：nice值为1-19的进程，即低优先级进程的用户态占cpu时间的百分比id：CPU空闲时间的百分比wa：CPU等待I/O完成的时间的百分比，该时间不计入进程的CPU时间hi：处理硬件中断的时间的百分比，该时间不计入进程的CPU时间si：处理软件中断的时间的百分比，该时间不计入进程的CPU时间st：被嵌套虚拟机跑进程所使用的时间，即同一宿主机上的其他虚拟机抢走的CPU时间#第四行：Mem：物理总内存大小，未使用内存大小，已使用物理内存大小，内核缓存占用内存大小#第五行：swap：交换区大小，未使用交换区大小，已使用交换区大小#第六行PID：进程号USER：用户PR：优先级VIRT：虚拟物理内存，包括所有代码、数据和共享库，以及已交换的页面和已映射但未使用的内存RES：实际物理内存，共享的内存比如动态库也会计算在内SHR：共享物理内存，并非所有共享的内存都是常驻的S：进程状态%CPU：CPU占用率%MEM：内存占用率TIME+：时间片累加值 为了便于理解，假设只有一个cpu，按照从上到下从左到右的顺序以此解析每个框的含义 第一个框us：一个用户程序开始运行，那么就对应于着第一个“us”框，代表linux用户态的Cpu Usage。普通的用户程序代码中，只要不是调用系统调用（System Call），这些代码的指令消耗的CPU就都属于“us”。 第二个框sys：当这个用户程序代码调用了系统调用，比如read()去读取一个文件，这时候这个用户的进程就会从用户态切换到内核态。内核态read()系统调用在读到真正disk上的文件前，就会进行一些文件系统层的操作，那么这些代码指令的消耗就属于“sys”，代表内核cpu使用 第三个框wa：接下来，这个read()系统调用会向linux的Block Layer发出一个I&#x2F;O Request,触发一个真正的磁盘读取操作，此时，这个进程一般会被置为不可中断睡眠状态TASK_UNINTERUPTIBLE。而linux会把这段时间标识成“wa”， 第四个框sys：紧接着，当磁盘返回数据时，进程在内核态拿到数据，这里仍旧是内核态CPU使用中的“sy” 第五个框us：然后进程再从内核态切回用户态，在用户态得到文件数据，这里进程又回到用户态CPU使用，即“us” 第六个框id：好，在这里我们假设一下，这个用户进程在读取数据之后，没事可做就休眠了，并且假设此时在这个CPU上也没有其他需要运行的进程了，那么系统就会进入“id”这个步骤，代表系统处于空闲状态 第七个框hi：如果此时这台机器收到一个网络包，网卡就会发出一个中断（interrupt），该中断为硬中断，cpu必须响应，cpu响应后进入中断服务程序，此时cpu就会进入“hi”，代表cpu处理硬中断的开销。 第八个框si：由于我们的中断服务需要关闭中断，所以这个硬中断的时间不能太长。但发生中断之后的工作是必须要完成的，如果这些工作比较耗时怎么办？linux中有一个软中断的概念（softirq），它可以完成这些耗时比较长的工作。从网卡收到的数据包的大部分工作，都是通过软中断来最终处理的。 强调：无论是hi还是si，占用的cpu时间，都不会计入进程的cpu时间，因为本来中断程序就是单独的程序，它们在处理时本就不属于任何一个进程。 此外还有两个类型的cpu：一个是“ni”，另外一个是“st” “ni”是nice的缩写，这里表示如果进程的nice值是正值（1-19），代表优先级比较低的进程运行时所占用的cpu “st”是steal的缩写，是虚拟机里用的cpu使用类型，表示有多少时间是被同一个宿主机上的其他虚拟机抢走的 强调：无论是hi还是si，占用的cpu时间，都不会计入进程的cpu时间，因为本来中断程序就是单独的程序，它们在处理时本就不属于任何一个进程，即不属于用户态也不属于内核态，因此容器中的cgroup不会限制它们 交互环境下子命令 12345678910111213141516171819202122帮助：h 或 ？ ，按 q 或esc 退出帮助排序：P：以占据的CPU百分比,%CPUM：占据内存百分比,%MEMT：累积占据CPU时长,TIME+N：以PID的大小排序R：对排序进行反转f：自定义显示字段1：显示所有CPU的负载s：改变画面更新频率首部信息显示：uptime信息：l命令tasks及cpu信息：t命令cpu分别显示：1 (数字)memory信息：m命令退出命令：q修改刷新时间间隔：s终止指定进程：k保存文件：W 快捷键 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#h：获取帮助按下h键，top命令将为您展示所有可用的快捷键和功能说明。这是您使用top命令时的得力助手，随时提供帮助和指导。#k：结束进程选中要结束的进程，按下k键，然后输入要结束的进程的PID（进程ID），即可终止选定的进程。这是一个强大而危险的功能，可用于关闭不响应或占用过多资源的进程。#r：修改进程优先级按下r键，可以修改选定进程的优先级。通过输入要修改的进程的PID和新的优先级值，您可以更好地管理系统资源分配，提高系统性能。#f：选择显示字段按下f键，进入字段选择界面，允许您自定义top命令中显示的字段。您可以根据需求选择要显示的字段，如CPU使用率、内存占用等，以便更全面地了解系统性能。#o：按指定字段排序按下o键，可以按指定字段对进程列表进行排序。输入要排序的字段名称，top命令将根据该字段的值对进程进行排序，如按CPU使用率排序、按内存占用排序等。#s：更改刷新间隔按下s键，可以修改top命令的刷新间隔。输入新的刷新间隔值（以秒为单位），默认为3秒。通过调整刷新频率，您可以更准确地监控系统的实时性能。#q：退出top命令按下q键，可以退出top命令，返回到命令行界面。#1：切换到单核模式按下数字键1，可以切换到单核模式，只显示一个CPU核心的性能数据。这对于单核处理器的系统非常有用，可以更清晰地查看单个核心的性能情况。#i：隐藏空闲和僵尸进程按下i键，可以切换是否显示空闲和僵尸进程。当显示大量进程信息时，隐藏空闲和僵尸进程可以使界面更清晰，专注于关注的活动进程。#m：切换单位显示按下m键，可以切换单位显示方式。您可以在字节、千字节、兆字节和吉字节之间进行切换，以适应不同的内存规模。#t：显示进程和CPU信息摘要按下t键，可以在顶部显示进程和CPU信息的摘要。这个摘要可以帮助您更快速地了解系统的整体性能情况。#W：保存当前设置为配置文件按下大写字母W键，可以将当前top命令的设置保存为配置文件。下次启动top命令时，将使用保存的配置，节省您的时间和努力。#L：切换显示平均负载和启动时间按下大写字母L键，可以切换top命令的显示模式，显示平均负载和系统的启动时间。这对于监控系统负载和了解系统运行时间非常有用。#F：跟踪选定进程按下F键，可以跟踪选定的进程。在进程列表中选中要跟踪的进程，然后按下F键，top命令将仅显示该进程的信息，以便更详细地监控其性能和资源消耗。#e：切换显示所有进程按下e键，可以切换显示所有进程。默认情况下，top命令仅显示活动进程，按下e键后，将显示所有进程，包括空闲和僵尸进程。#n：设置显示进程数量按下n键，可以设置top命令显示的进程数量。通过输入新的进程数量值，您可以控制显示的进程数量，以适应您的需求。##：切换显示进程编号按下#键，可以切换是否显示进程编号。当处理大量进程时，隐藏进程编号可以使界面更清晰，减少混乱。#&amp;：设置进程筛选条件按下&amp;键，可以设置进程筛选条件。您可以输入一个或多个条件，如进程ID、进程名称等，以便于快速筛选和定位特定的进程。#u：仅显示指定用户的进程按下u键，可以仅显示指定用户的进程。输入用户名后，top命令将仅显示该用户的进程信息，有助于对特定用户的进程进行监控和管理。#z：切换颜色/显示模式按下z键，可以切换top命令的颜色和显示模式。您可以选择不同的配色方案或切换到单色模式，以适应您的偏好和环境。 选项 123456-d #指定刷新时间间隔，默认为3秒-b #全部显示所有进程-n #刷新多少次后退出-H #线程模式-p #指定进程号^%Cpu #过滤以 %CPU 开头的行 示例 1top -H -p `pidof mysqld` 范例 1234567891011121314151617181920212223#10s刷新一次[root@ubuntu ~]# top -d 10#显示指定用户的进程统计[root@ubuntu ~]# top -u jose#显示指定进程的线程[root@ubuntu ~]# top -Hp 1444#显示进程的具体命令[root@ubuntu ~]# top -p 1331 -c#输出展示多个 CPU 的情况top -b -n1 | grep ^%Cpu#获取包含百分比符号及保留两位小数的 CPU 占用率top -b -n1 | grep ^%Cpu | awk &#x27;&#123;printf(&quot;Current CPU Utilization is : %.2f%&quot;), 100-$8&#125;&#x27;#在 Linux 中查找内存消耗最大的进程top -c -b -o +%MEM | head -n 20 | tail -15#只想查看命令名称而不是命令的绝对路径top -b -o +%MEM | head -n 20 | tail -15 1.5 进程对应的内存映射pmap进程和内存之间的使用情况，查看进程占了哪部分内存 12345pmap [options] pid [...]-x: 显示详细格式的信息-X：显示更详细信息、-d：显示设备 另外一种实现 1cat /proc/PID/maps 范例 1234567[root@centos8 ~]#pmap 3347733477: ping 127.0.0.1000055f708aa7000 56K r-x-- ping000055f708cb5000 4K r---- ping000055f708cb6000 4K rw--- ping000055f708cb7000 140K rw--- [ anon ]000055f70a7cc000 132K rw--- [ anon ] 1.6 查看进程打开文件lsof查看当前系统文件的工具。在linux环境下，一切皆文件，用户通过文件不仅可以访问常规数据，还可以访问网络连接和硬件如传输控制协议 (TCP) 和用户数据报协议 (UDP)套接字等，系统在后台都为该应用程序分配了一个文件描述符 lsof使用注意事项 需要root权限才能使用lsof命令。 lsof命令需要一定时间才能完成扫描，因此不应在生产环境下滥用。 使用lsof命令时应确保使用的是最新版本，以防止出现已知的bug。 使用时应仔细查看命令输出，尤其是对于打开套接字的程序及其连接，以避免意外暴露敏感信息。 lsof命令的扫描范围包括所有已打开的文件和网络套接字，因此执行时可能会对系统性能产生一定的影响，如果对性能敏感，应考虑使用其他更轻量级的工具。 在使用lsof命令时，应确保已经对电脑进行了必要的安全保护，以避免受到黑客攻击或数据泄露。 命令选项 12345678910111213-a：列出打开文件存在的进程-c&lt;进程名&gt;：列出指定进程所打开的文件-g：列出GID号进程详情-d&lt;文件号&gt;：列出占用该文件号的进程+d&lt;目录&gt;：列出目录下被打开的文件+D&lt;目录&gt;：递归列出目录下被打开的文件-n&lt;目录&gt;：列出使用NFS的文件-i&lt;条件&gt;：列出符合条件的进程(4、6、协议、:端口、 @ip )-p&lt;进程号&gt;：列出指定进程号所打开的文件-u：列出UID号进程详情-h：显示帮助信息-v：显示版本信息。-n: 不反向解析网络名字 字段说明 COMMAND列：打开文件的进程的名称 PID列：打开文件的进程的标识符 USER列：打开文件的进程的所有者 FD列：打开文件的进程的文件描述符 TYPE列：打开文件的类型，如REG（常规文件）、DIR（目录）、CHR（字符设备）、FIFO（管道）、SOCK（套接字）等 DEVICE列：打开文件所在的设备的编号 SIZE&#x2F;OFF列：文件的大小或偏移量 NODE列：打开文件的节点号码 NAME列：打开文件的路径和文件名。 范例：查看某个进程打开的所有文件 123456789101112131415#例如查询sshd服务进程的PID号[root@jeven ~]# ps aux |grep ssh root 9347 0.0 0.0 112756 4312 ? Ss 06:22 0:00 /usr/sbin/sshd -D root 30102 0.0 0.0 161316 6052 ? Ss 17:14 0:00 sshd: root@pts/1 root 30109 0.0 0.0 161312 6040 ? Ss 17:14 0:00 sshd: root@notty root 30154 0.0 0.0 74176 2940 ? Ss 17:14 0:00 /usr/libexec/openssh/sftp-server root 31429 0.0 0.0 112712 968 pts/1 S+ 18:57 0:00 grep --color=auto ssh#使用lsof查询该进程打开的所有文件[root@jeven ~]#lsof -p 9347 #或者这种方法[root@centos8 ~]#lsof -p `pidof ssh`#也可以使用/proc/PID/fd/来查看 范例：查看某个用户打开的所有文件 1[root@jeven ~]# lsof -u apache |head 范例：查看某个文件被哪些进程打开 1[root@centos8 ~]#lsof /var/log/messages 范例：查看某个端口被哪些进程占用 12345#查看所有网络连接[root@jeven ~]#lsof -i #查看某个端口被哪些进程占用[root@jeven ~]# lsof -i :22 范例 123456789101112131415161718192021222324252627282930#lsof 列出当前所有打开的文件[root@centos8 ~]#lsof | head#查看由登陆用户启动而非系统启动的进程[root@centos8 ~]#lsof /dev/pts/1[root@centos8 ~]#lsof `tty`#查看指定程序打开的文件[root@centos8 ~]#lsof -c httpd[root@centos8 ~]#lsof -c bc#查看指定目录下被打开的文件，参数+D为递归列出目录下被打开的文件，参数+d为列出目录下被打开的文件[root@centos8 ~]#lsof +D /var/log/[root@centos8 ~]#lsof +d /var/log/ #查看打开某个类型文件的进程列表[root@jeven ~]# lsof -t /usr/sbin/httpd #查看所有网络连接，通过参数-i查看网络连接的情况，包括连接的ip、端口等以及一些服务的连接情况，例如：sshd等。也可以通过指定ip查看该ip的网络连接情况[root@centos8 ~]#lsof -i –n [root@centos8 ~]#lsof -i@127.0.0.1#查看端口连接情况，通过参数-i:端口可以查看端口的占用情况，-i参数还有查看协议，ip的连接情况等[root@centos8 ~]#lsof -i :80 -n#查看指定进程打开的网络连接，参数-i、-a、-p等，-i查看网络连接情况，-a查看存在的进程，-p指定进程[root@centos8 ~]#lsof -i –n -a -p 9527#查看指定状态的网络连接，-n:no host names, -P:no port names,-i TCP指定协议，-s指定协议状态通过多个参数可以清晰的查看网络连接情况、协议连接情况等[root@centos8 ~]#lsof -n -P -i TCP -s TCP:ESTABLISHED 范例：利用 lsof 恢复正在使用中的误删除的文件 12345678910111213141516171819202122232425262728#打开文件[root@ubuntu ~]# tail -f /var/log/syslog#删除文件[root@ubuntu ~]# rm -rf /var/log/syslog[root@ubuntu ~]# ls /var/log/syslogls: cannot access &#x27;/var/log/syslog&#x27;: No such file or directory#找出进程[root@ubuntu ~]# ps aux | grep tailroot 6037 0.0 0.1 217128 976 pts/4 S+ 02:29 0:00 tail -f /var/log/syslog#查看内存映射[root@ubuntu ~]# ll /proc/6037/fd/total 0lrwx------ 1 root root 64 May 29 02:32 0 -&gt; /dev/pts/4lrwx------ 1 root root 64 May 29 02:32 1 -&gt; /dev/pts/4lrwx------ 1 root root 64 May 29 02:32 2 -&gt; /dev/pts/4lr-x------ 1 root root 64 May 29 02:32 3 -&gt; &#x27;/var/log/syslog (deleted)&#x27;lr-x------ 1 root root 64 May 29 02:32 4 -&gt; anon_inode:inotify#打开，文件内容还在[root@ubuntu ~]# cat /proc/6037/fd/3#恢复[root@ubuntu ~]# cat /proc/6037/fd/3 &gt; /var/log/syslog[root@ubuntu ~]# ll /var/log/syslog-rw-r--r-- 1 root root 1885719 May 29 02:38 /var/log/syslog 1.7 查看线程个数123#297是PID[root@centos ~]#cat /proc/297/statusThreads: 1 1.8 查看进程的二进制文件1/proc/PID/exe 是这个进程对应的磁盘文件的路径 范例：如果你发现这个进程出现了异常，这个异常进程占用了大量的资源，有可能是病毒，需要清理掉，就可以通过这种方法查看磁盘文件路径，删掉之后命名一个和它同名的文件，并设置属性chattr，这样可以防止病毒重新生成 123456[root@centos ~]#ll /proc/920/exelrwxrwxrwx 1 root root 0 Aug 31 13:51 /proc/920/exe -&gt; /usr/sbin/rsyslogd[root@centos ~]#rm -f /usr/sbin/rsyslogd[root@centos ~]#touch /usr/sbin/rsyslogd[root@centos ~]#chattr +i /usr/sbin/rsyslogd 二、管理进程2.1 设置和调整进程优先级一个进程在运行的时候会涉及到内核态与用户态的转换，关于进程的优先级，可以查看优先级概念，top命令查看到两个相关的值PR与NI PR –&gt; 内核态使用的值 NI –&gt; 用户态使用的值 系统调度器最终是根据PR的值来决定进程的运行顺序 PR值越小优先级越高 PR值 &#x3D; 20+NICE值 ps -l 命令可以查看PR值，输出的字段信息如下 12345UID: 执行者的身份PID: 这个进程的代号PPID：这个进程是由哪个进程发展衍生而来的，亦即父进程的代号PRI：这个进程可被执行的优先级，其值越小越早被执行NI：这个进程的nice值 用户无法直接修改内核态的PR值，但是我们可用nice命令修改用户态得NI值，来影响内核态的PR值 需要注意的是普通用户只能在0～19之间调整应用程序的优先权值，只有超级用户有权调整更高的优先权值（从-20～19） 静态优先级范围是100-139，进程默认启动时的nice值为0，对应的优先级为120 123系统优先级：0-139, 数字越小优先级越高实时优先级: 99-0，值最大优先级最高nice值：-20到19，对应系统优先级100-139，数字越小优先级越高 nice命令：以指定的优先级来启动进程 12nice [OPTION] [COMMAND [ARG]...]-n, --adjustment=N add integer N to the niceness (default 10) 123[root@ubuntu ~]# nice -n 11 ping www.baidu.com[root@ubuntu ~]# ps axo pid,cmd,nice | grep ping 3887 ping www.baidu.com 11 renice命令：调整正在执行中的进程的优先级 1renice -n 优先级号 pid 123456789[root@centos8 ~]#renice -n -20 21182106 (process ID) old priority -10, new priority -20[root@centos8 ~]#ps axo pid,cmd,nice |grep ping 2118 ping 127.0.0.1 -20 2200 grep --color=auto ping 0#越界不会报错[root@ubuntu ~]# renice -n -30 21183734 (process ID) old priority 0, new priority -20 2.2 搜索进程pgrep按条件搜索进程 ps 选项 | grep ‘pattern’ 灵活 pgrep 按预定义的模式 &#x2F;sbin&#x2F;pidof 按确切的程序名称查看pid 12345678910111213pgrep [options] pattern-u uid #生效者-U uid #真正发起运行命令者-t terminal #与指定终端相关的进程-l #显示进程名-a #显示完整格式的进程名-x #根据指定的命令匹配-w #显示线程ID-c #统计匹配到的进程数量-P pid #显示指定进程的子进程-s &lt;SID,...&gt; #根据会话ID显示-F &lt;file&gt; #从文件中读取PID作为条件 范例 123456789101112131415161718192021222324[root@centos8 ~]#pgrep -u wang23032330[root@centos8 ~]#pgrep -lu wang2303 bash2330 dd#错误写法[root@centos8 ~]#pgrep -ul wangpgrep: invalid user name: l[root@centos8 ~]#pgrep -au wang2303 -bash2330 dd if=/dev/zero of=/dev/null[root@centos8 ~]#pgrep -aP 23032330 dd if=/dev/zero of=/dev/null#查找指定终端的进程[root@centos8 ~]#pgrep -at pts/21482 -bash2302 su - wang2303 -bash2330 dd if=/dev/zero of=/dev/null 范例：发起用户和生效用户 12345678910111213141516#普通用户发起passwd 进程[jose@ubuntu ~]$ passwdChanging password for user jose.Current password:#实际进程属主是root[root@ubuntu ~]# ps aux | grep passwdroot 15440 0.0 0.4 331172 8564 pts/1 S+ 22:24 0:00 passwd#查看user 和 ruser[root@ubuntu ~]# ps axo pid,cmd,user,ruser#根据 real user查找[root@ubuntu ~]# pgrep -aU jose2843 -bash15440 passwd 2.3 给进程发信号信号发送 2.4 杀死僵尸进程12345678910111213141516171819202122[root@centos8 ~]#bash#当前shell进程的ID，等同于$$[root@centos8 ~]#echo $BASHPID1809#父进程的ID[root@centos8 ~]#echo $PPID1436#将父进程设为停止态，无法回收其资源[root@centos8 ~]#kill -19 1436#杀死子进程，使其进入僵尸态[root@centos8 ~]#kill -9 1809[root@centos8 ~]#ps aux #可以看到上面图示的结果，STAT为Z，表示为僵尸态#方法1:恢复父进程[root@centos8 ~]#kill -18 1436#方法2:杀死父进程[root@centos8 ~]#kill -9 1436#再次观察，可以僵尸态的进程不存在了[root@centos8 ~]#ps aux 三、内存性能分析3.1 freefree 命令用于显示Linux系统上的内存使用情况。它提供了有关物理内存（RAM）和交换空间（swap）的信息，包括已使用、空闲、缓冲区和缓存内存等 12345678-b #以字节为单位-m #以MB为单位-g #以GB为单位-h #易读格式-o #不显示-/+buffers/cache行-t #显示RAM + swap的总和-s n #刷新间隔为n秒-c n #刷新n次后即退出 1234[root@ubuntu2004 ~]#free -h total used free shared buff/cache availableMem: 3.8Gi 352Mi 425Mi 1.0Mi 3.0Gi 3.2GiSwap: 0B 0B 0B 字段说明 total：物理内存的总量，包括实际可用内存和内核保留的内存 used：已使用的但目前还没有释放的物理内存量，包括用于进程和系统的内存，这部分内存不包括 “buff&#x2F;cache” 这行所表示的内存 free：空闲的，当前完全没有被程序使用的物理内存量，尚未分配给任何进程 shared：被共享的内存量，通常用于共享内存段的进程（如进程间通信机制），即多个进程共享的内存 buff&#x2F;cache：缓冲区和缓存的内存量。这包括系统数据的缓存，以及用于文件I&#x2F;O的内存缓冲区，统称 page cache available：可用内存量，是真正表明系统当前可供新进程使用的内存 Mem：物理内存的相关信息，包括总内存空间、使用、剩余等相关信息。 Swap：交换空间的信息，包括总交换空间、已使用的交换空间和空闲交换空间。 3.1.1 缓存和缓冲区的说明缓冲区（buffers）是操作系统用来临时存储I&#x2F;O数据的内存区域。例如，当读写文件时，内核会将磁盘上的数据暂时存放在缓冲区内，然后再批量写，以减少磁盘碎片和硬盘反复寻道，快速处理后续的I&#x2F;O请求。这样可以提高系统性能，减少对磁盘的直接访问，主要用于硬盘与内存之间的数据交互，缓存(cached) 是指文件的内容要被多个进程使用的时候，则可以将内容放入缓存区，则后续就可以直接从内存中读，而不用再消耗IO 缓存（cache）主要指的是页面缓存或文件缓存。这部分内存用来存储最近访问过的文件内容，使得再次访问同一文件时能更快地从内存而不是硬盘中读取数据。还包括inode缓存等内核用于加速文件系统操作的数据结构，主要作用于CPU和内存之间的数据交互（本来要用IO读硬盘文件，现在变成了读内存）。在 free 结果中，cache 是包括了 Page Cache 和 Slab 的总和，在有需要时，是可以被释放出来以供其它进程使用的，但并不是所有cache都可以释放，比如当前被用作ramfs的内存 Page Cache：页面缓存，用于文件的缓存，即以页为单位，为了加快文件的读写，内核中提供了page cache作为缓存 Slab：内存对象缓存层，它包括了两个子系统 dentry cache（目录条目缓存）和 inode cache（索引节点缓存）。这两者以及其它的 slab 缓存也会被算入 free 结果中的 cache 向 /proc/sys/vm/drop_caches 中写入相应的修改值，会清理缓存。建议先执行sync（sync 命令将所有未写的系统缓冲区写到磁盘中，包含已修改的 inode、已延迟的块 I&#x2F;O 和读写映射文件）。执行echo1、2、3 至 /proc/sys/vm/drop_caches，达到不同的清理目的 如果因为是应用会存在像内存泄露、溢出的问题时，从swap的使用情况是可以比较快速可以判断的，但通过执行 free 反而比较难查看。但核心并不会因为内存泄露等问题并没有快速清空buffer或cache（默认值是0），生产也不应该随便去改变此值。 一般情况下，应用在系统上稳定运行了，free值也会保持在一个稳定值的。当发生内存不足、应用获取不到可用内存、出现OOM错误等问题时，还是更应该去分析应用方面的原因，否则，清空buffer，强制腾出free的大小，可能只是把问题给暂时屏蔽了。 排除内存不足的情况外，除非是在软件开发阶段，需要临时清掉buffer，以判断应用的内存使用情况；或应用已经不再提供支持，即使应用对内存的时候确实有问题，而且无法避免的情况下，才考虑定时清空buffer。 范例: 清理缓存 1234567891011121314[root@centos8 ~]#cat /proc/sys/vm/drop_caches0[root@centos8 ~]#dd if=/dev/zero of=/f1.img bs=1M count=1024[root@centos8 ~]#free -h total used free shared buff/cache availableMem: 1.8Gi 355Mi 724Mi 9.0Mi 726Mi 1.2GiSwap: 2.0Gi 0B 2.0Gi[root@centos8 ~]#echo 3 &gt; /proc/sys/vm/drop_caches[root@centos8 ~]#free -h total used free shared buff/cache availableMem: 1.8Gi 320Mi 1.3Gi 9.0Mi 152Mi 1.3GiSwap: 2.0Gi 0B 2.0Gi 3.2 smemsmem 是一个用于查看 Linux 系统中进程内存使用情况的工具。它提供了详细的内存统计信息，包括物理内存、虚拟内存、共享内存、缓冲区和缓存等各种内存指标。smem 命令的功能比标准的 ps 或 top 命令更加详细，可以帮助我们更好地了解各个进程占用内存的情况。 该命令需要额外安装，具体安装方式如下： 12yum install -y epel* # 需安装扩展源（因为本地软件仓库中没有找到smem软件包）或下载源码进行编译安装yum install -y smem 123456789101112smem [选项]-r：按照内存使用量的逆序（从高到低）排序显示进程列表。-u：以用户模式显示内存使用情况，按照用户分类显示内存使用情况。-U：显示虚拟内存（VIRT）的信息。-P：显示共享内存（SHR）的信息。-c：显示缓冲区的信息。-C：显示缓存的信息。-k：指定按KB显示。-p：指定按百分比显示。-P：指定具体的进程。-s：指定排序规则（如 -s uss，表示对 USS 列进行排序 - 默认为升序） 字段输出解释： PID：进程ID。 User：进程所属用户。 Command：进程的命令行。 Swap：进程占用的交换空间。 USS：唯一内存使用（Unique Set Size），表示进程独占的内存。只计算进程独自占用的内存大小,不包含任何共享的部分 PSS：共享内存使用（Proportional Set Size），表示进程独占内存加上共享内存的平均值。 RSS：物理内存使用（Resident Set Size），表示进程当前实际占用的物理内存。 默认情况下，smem 命令以物理内存（RES）的信息排序，并显示前10个进程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990[root@ubuntu2004 ~]#smemPID User Command Swap USS PSS RSS670 root /sbin/agetty --noclear tty1 0 172 214 864652 root /usr/sbin/irqbalance --fore 0 376 458 1016618 audit /sbin/auditd 0 332 562 1088643 root /usr/sbin/wmsta -f 0 680 736 1260643 root /usr/lib/systemd/systemd-lo 0 680 736 1260646 dbus /usr/bin/dbus-daemon --syst 0 776 892 17441071 root /usr/libexec/postfix/master 0 1156 1233 2216503 root /usr/libexec/openssh/sshd -D 0 1176 1261 2232872 root /usr/sbin/sshd -D 0 1092 1684 43601076 postfix pickup -l -t unix -u 0 1128 1712 41041077 postfix qmgr -l -t unix -u 0 1220 1740 3062472 root /usr/lib/systemd/systemd-jo 0 1328 1768 43241523 root -bash 0 2648 2775 38601871 root /usr/lib/systemd/systemd -- 0 1088 1876 3672871 root /usr/sbin/NetworkManager 0 2428 3134 59561521 root -bash 0 5720 6329 79441662 root python /usr/bin/smem 0 8428 9470 12924649 root /usr/sbin/NetworkManager -- 0 8852 9787 12322641 polkitd /usr/lib/polkit-1/polkitd 0 13668 14035 17496875 root /usr/bin/python3 -Es /usr/s 0 22132 33151 33584880 root /usr/bin/containerd 0 53044 53151 54244# 显示内存单位并指定字段进行排序[root@ubuntu2004 ~]#smem -k -s ussPID User Command Swap USS PSS RSS670 root /sbin/agetty --noclear tty1 0 172.0K 214.0K 864.0K652 root /usr/sbin/irqbalance --fore 0 376.0K 458.0K 1.2M692 root /usr/sbin/lvmetad -f 0 524.0K 613.0K 1.5M618 audit /sbin/auditd 0 680.0K 756.0K 1.1M665 root /usr/sbin/crond -n 0 532.0K 592.0K 1.6M643 root /usr/lib/systemd/systemd-lo 0 776.0K 832.0K 1.7M646 dbus /usr/bin/dbus-daemon --syst 0 864.0K 1.1M 2.4M872 root /usr/sbin/sshd -D 0 1.0M 1.6M 4.3M1071 root /usr/libexec/postfix/master 0 1.1M 1.6M 2.2M503 root /usr/lib/systemd/systemd-ud 0 1.1M 1.2M 2.2M1076 postfix pickup -l -t unix -u 0 1.2M 1.7M 4.0M1077 postfix qmgr -l -t unix -u 0 1.2M 1.7M 4.0M472 root /usr/lib/systemd/systemd-jo 0 1.3M 1.8M 3.6M1523 root -bash 0 1.8M 1.7M 2.0M1521 root -bash 0 2.4M 3.0M 5.6M877 root sshd: root@pts/0 0 2.6M 3.1M 4.8M1 root /usr/lib/systemd/systemd -- 0 5.6M 2.7M 7.8M1683 root python /usr/bin/smem -k -s 0 8.2M 6.3M 3.8M649 root /usr/sbin/NetworkManager -- 0 8.6M 9.9M 12.6M641 polkitd /usr/lib/polkit-1/polkitd 0 12.4M 13.7M 17.1M875 root /usr/bin/python2 -Es /usr/s 0 31.8M 32.1M 53.0M880 root /usr/bin/containerd -H fd:// - 0 51.8M 52.1M 53.8M# 以百分比显示并指定字段进行排序[root@ubuntu2004 ~]#smem -p -s ussPID User Command Swap USS PSS RSS670 root /sbin/agetty --noclear tty1 N/A 0.00% 0.01% 0.02%652 root /usr/sbin/irqbalance --fore N/A 0.01% 0.01% 0.03%489 root /usr/sbin/lvmetad -f N/A 0.01% 0.02% 0.04%618 audit /sbin/auditd N/A 0.02% 0.02% 0.04%665 root /usr/sbin/crond -n N/A 0.02% 0.02% 0.05%643 root /usr/lib/systemd/systemd-lo N/A 0.02% 0.03% 0.06%646 dbus /usr/bin/dbus-daemon --syst N/A 0.03% 0.03% 0.11%872 root /usr/sbin/sshd -D N/A 0.03% 0.03% 0.06%1071 root /usr/libexec/postfix/master N/A 0.03% 0.03% 0.06%503 root /usr/lib/systemd/systemd-ud N/A 0.03% 0.04% 0.11%1076 postfix pickup -l -t unix -u N/A 0.03% 0.05% 0.11%1077 postfix qmgr -l -t unix -u N/A 0.03% 0.06% 0.08%472 root /usr/lib/systemd/systemd-jo N/A 0.05% 0.05% 0.07%1523 root -bash N/A 0.06% 0.06% 0.16%1521 root -bash N/A 0.06% 0.08% 0.07%877 root sshd: root@pts/0 N/A 0.07% 0.07% 0.10%1 root /usr/lib/systemd/systemd -- N/A 0.07% 0.07% 0.12%1685 root python /usr/bin/smem -p -s N/A 0.12% 0.15% 0.21%649 root /usr/sbin/NetworkManager -- N/A 0.25% 0.27% 0.33%641 polkitd /usr/lib/polkit-1/polkitd N/A 0.23% 0.25% 0.42%875 root /usr/bin/python2 -Es /usr/s N/A 0.23% 0.35% 0.45%1067 root /usr/bin/containerd N/A 1.37% 1.36% 1.04%880 root /usr/bin/dockerd -H fd:// - N/A 1.37% 1.36% 1.04%# 查看每个用户使用内存的情况[root@ubuntu2004 ~]#smem -uUser Count Swap USS PSS RSSdbus 1 0 864 1109 2484postfix 2 0 2412 3452 8236polkitd 1 0 8852 9770 12332root 19 0 130860 137130 164112# 查看指定进程（这里是dockerd）使用系统内存的情况[root@ubuntu2004 ~]#smem -k -P dockerdPID User Command Swap USS PSS RSS1692 root python /usr/bin/smem -k -P 0 4.7M 5.4M 6.9M1067 root /usr/bin/dockerd -H fd:// - 0 51.8M 51.9M 53.0M 3.3 虚拟内存信息vmstat用于监控系统性能和虚拟内存统计的命令。它提供了关于CPU、内存、磁盘I&#x2F;O和系统上下文切换等方面的信息。 1234[root@centos ~]#vmstatprocs -----------memory------------- ---swap-- -----io--- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 0 1305664 2108 368540 0 0 1 1 20 36 0 0 100 0 0 字段说明 procs：显示队列和等待状态 r 列：表示运行队列中的进程数，即当前正在运行的进程数。如果这个值长期大于系统 CPU 个数，说明 CPU 紧张，需进行 CPU 升级（即增加系统 CPU）。 b 列：表示在等待资源的进程数，即处于不可中断（blocked）状态的进程数，通常是等待 I&#x2F;O、内存交换完成的进程数。由于硬盘速度特别慢而导致内存同步的时候没成功，那么现在告诉程序，说你先不要产生数据，这就是阻塞 b越大证明硬盘压力很大 memory：显示物理内存状态 swpd 列：表示交换（swap）的虚拟内存使用量，表示从实际物理内存已经交换到交换空间的数据量，我这里的值为 0，因为我压根就没有启用 Swap Space。如果你启用了交换空间，发现swpd 列下的值很大，只要 swap 字段下的 si、so 列的值长期为 0，这也不会影响系统性能。 free 列：表示当前可用的空闲物理内存。 buff 列：Buffers Cache，表示内存缓冲区缓存的数据量，一般对块设备的读写才需要缓冲（即通常用于文件I&#x2F;O缓存）。 cache 列：Page Cache，表示内存的页高速缓存的数据量，一般作为文件系统的缓存（即通常用于文件系统缓存），频繁访问的文件都会被缓存，如果 cache 列的值比较大，说明页缓存的文件数较多，如果此时 io 字段中的 bi 列的值较小，说明文件系统效率较好。 swap：显示交换分区读写情况 si 列：表示每秒从磁盘（即交换空间）交换到内存的数据量（swap in)（单位KB&#x2F;s），内存进，从swap出。 so 列：表示每秒从内存交换到磁盘（即交换空间）的数据量（swap out）（单位KB&#x2F;s），内存出，进到swap里面，腾出内存空间运行应用程序。 说明：如果 si、so 长期不为 0，那我们的 Linux 系统的物理内存资源肯定不足了。为什么呢？你想一想，根据内存的相关机制，我们知道长期不为 0，说明数据频繁地在物理内存和交换空间中交换数据，如：需要使用内存的进程会在内存中运行，然后内存会将不常用的文件数据交换到 Swap Space，这样的话 si、so 值势必是不会为 0 的，而且会存在频繁波动。如果发现si、so里面有数据，说明内存可能不够用了 io：显示磁盘读写情况 bi 列：表示每秒从块设备（磁盘）读取的块数量（blocks in）（单位KB&#x2F;s），从块设备读入数据到系统的速率(kb&#x2F;s)，进内存。 bo 列：表示每秒写入块设备（磁盘）的块数量（blocks out）（单位KB&#x2F;s），保存数据至块设备的速率，出内存 说明：如果 bi + bo 的值大于 1000KB&#x2F;s，且 wa 值较大，则表示系统磁盘 I&#x2F;O 有瓶颈，应该提高磁盘的读写性能。 system：显示采集间隔内发生的中断数 in 列：每秒中断的数量，包括时钟中断、网络中断等。 cs 列：每秒上下文切换的数量，包括进程切换和内核线程切换。 说明：如果这两个值越大，说明内核消耗的 CPU 时间会越多。 cpu：显示 CPU 的使用状态 us 列：用户空间占用CPU时间的百分比。如果长期大于 50%，就需要考虑优化程序或算法。 sy 列：内核空间占用CPU时间的百分比。如果 us + sy 长期大于 80%，说明 CPU 资源不足。 id 列：CPU空闲时间的百分比。 wa 列：等待 I&#x2F;O 完成的CPU时间的百分比。wa 越高说明 IO 等待越严重，一般如果 wa 超过 20%，说明 IO 等待严重（可能是因为磁盘大量的随机读写造成）。 st 列：用于虚拟机监控程序（hypervisor）的CPU时间的百分比（仅在虚拟化环境中可见） 选项 12345678-a #分开显示活动和非活动内存-s #显示事件统计-d #统计磁盘设备相关信息-D #综合统计磁盘-p &lt;dev&gt; #统计指定分区-S &lt;char&gt; #指定显示单位 k|K|m|M-w #以宽格式显示-t #显示时间 范例 12345678#每秒显示一次[root@ubuntu ~]# vmstat 1#使用vmstat检测，每隔1秒刷新一次，共刷新3次[root@localhost proc]# vmstat 1 3#显示统计数据[root@ubuntu ~]# vmstat -s 范例 123456# vmstat -1 # dd if=/dev/zero of=/aa bs=1G count=1 //这条命令之后查看bo的数值，发现bo产生数据 记录了1+0 的读入 记录了1+0 的写出 1073741824字节(1.1 GB)已复制，4.90872 秒，219 MB/秒#find / //这条命令之后查看bi，发现bi产生数据 如果 一直开着vmstat发现bo 5秒钟一个数，这就是因为脏数据5秒钟一次 如果要拿这个数据做图，bo的第一个数据一定要剔除到，这个数字是上一次重启到敲vmstat这条命令之间的平均值，所以这个数字没用 四、CPU性能分析4.1 负载查询uptime&#x2F;proc&#x2F;uptime 包括两个值，单位 s 系统启动时长 空闲进程的总时长（按总的CPU核数计算）uptime 和 w 显示以下内容 当前时间 系统已启动的时间 当前上线人数 系统平均负载（1、5、15分钟的平均负载，一般不会超过1，超过5时建议警报，15分钟的平均CPU负载，数字越小越好） 负载知识请看 12345678910111213[root@centos8 ~]#uptime09:38:34 up 1 day, 1:04, 2 users, load average: 0.00, 0.00, 0.0009:38:34 #系统当前时间up 1 day, 1:04 #系统己开机运行时长2 users #当前登录到系统的用户数量load average: 0.00, 0.00, 0.00 #系统的平均负载，1分钟，5分钟，15分钟[root@centos8 ~]#w09:38:29 up 1 day, 1:04, 2 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.0.0.1 Wed08 0.00s 0.32s 0.00s wroot pts/1 10.0.0.1 09:10 5:25 0.06s 0.00s /bin/bash./ping.sh 如果CPU利用率不高，但是平均负载高，说明不可中断睡眠的进程比较多，因为R状态进程一定会提高CPU的使用率，那么这时候就可以考虑进行IO的优化 如果CPU利用率高、平均负载也高，说明 CPU密集型进程多 进程同时消耗cpu和进行I&#x2F;O等待操作 进程竞争CPU资源 中断处理过多 大量短时进程，如短时间频繁创建&#x2F;销毁进程 4.2 显示CPU相关统计mpstatmpstat 是 Multiprocessor Statistics（即多处理器统计），它用于显示多核CPU系统中每个CPU核心的性能统计信息。这个命令可以帮助系统管理员监控和分析系统的CPU使用情况，尤其是在多核 CPU 的环境中。一般地，有些 Linux 发行版默认没有安装次工具，需我们手动安装。来自于sysstat包 该命令与 vmstat 命令类似，mpstat 是通过 &#x2F;proc&#x2F;stat 里面的状态信息进行统计的，mpstat 的好处在于，它可以查看多核 CPU 中每个 CPU 计算核的统计数据的情况，而 vmstat 只能查看系统整体 CPU 的情况。 1234567891011121314[root@centos ~]#mpstatLinux 3.10.0-1160.el7.x86_64 (centos) 09/16/2023 _x86_64_ (2 CPU)04:49:20 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle04:49:20 PM all 0.03 0.00 0.05 0.00 0.00 0.00 0.00 0.00 0.00 99.92#每一秒查看CPU利用情况[root@centos ~]#mpstat 1 #查看第一颗cpu运行情况[root@ubuntu ~]# mpstat -P 0#查看所有[root@ubuntu ~]# mpstat -P ALL 字段输出解释： Linux 3.10.0-1160.el7.x86_64 (centos)：显示操作系统版本和主机名。 09&#x2F;16&#x2F;2023：显示当前日期。 x86_64：显示系统架构（在此示例中为x86_64，表示64位系统）。 (2 CPU)：显示 CPU 核心数量，本例中有 2 个CPU核心。 04:49:20 PM：显示统计信息的时间戳。 CPU：处理器的 ID 号，采集时不指定则默认为系统整体 CPU 情况（all 表示系统整体 CPU 情况，其他如 CPU 0、CPU 1 等）。 %usr：用户空间占用 CPU 时间的百分比。 %nice：优先级较高的用户空间占用 CPU 时间的百分比。 %sys：内核空间占用 CPU 时间的百分比。 %iowait：CPU 等待 I&#x2F;O 操作完成的百分比。涉及到磁盘的IO或者网络的IO，值过大说明磁盘和网络的性能较差 %irq：CPU 处理硬件中断的百分比。 %soft：CPU 处理软件中断的百分比。 %steal：CPU 被虚拟机监控程序（hypervisor）”偷取”的百分比，即被嵌套虚拟机跑进程所使用的时间 %guest：运行虚拟机中的操作系统时，CPU花费在虚拟机中的百分比，即运行虚拟化主机cpu自身时间的占比 %gnice：虚拟机中运行的优先级较高的用户空间占用CPU时间的百分比，即运行虚拟cpu上的nice 进程的占比 %idle：CPU空闲时间的百分比，值越高，CPU利用率越低 Average：平均值，如果指定了采集次数，系统为自动为我们计算出相关字段的平均值。 五、磁盘IO性能分析5.1 统计CPU和设备IO信息 iostatiostat 命令是用于监视 Linux 系统中磁盘和 CPU 使用情况的命令行工具。它可以提供有关系统的磁盘 I&#x2F;O 和 CPU 使用的详细统计信息 此工具由sysstat包提供 123456常用选项:-c 只显示CPU行-d 显示设备（磁盘）使用状态-k 以KB(千字节)为单位显示输出-t 在输出中包括时间戳-x 在输出中包括扩展的磁盘指标 范例 12345678910111213141516[root@centos8 ~]#iostatLinux 4.18.0-80.el8.x86_64 (centos8.localdomain) 01/09/2020 _x86_64_ (4CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.01 0.00 0.06 0.00 0.00 99.93Device tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 0.31 2.57 3.52 238227 326708scd0 0.01 0.14 0.00 13140 0#每秒刷新一次数据[root@ubuntu ~]# iostat 1#每隔1秒刷新一次，共刷新3次[root@centos8 ~]#iostat 1 3#指定设备（sda）的磁盘 I/O 统计信息，包括磁盘的读写速度、I/O 请求的响应时间等[root@centos8 ~]#iostat -d sda -t -k 1 3 字段说明 %usr：用户空间进程占用的CPU %nice：调整优先级占用的CPU %system：内核空间进程占用的CPU %iowait：等待IO，涉及到磁盘的IO或者网络的IO，值过大说明磁盘和网络的性能较差 %steal：被嵌套虚拟机跑进程所使用的时间 %idle：空闲值，值越高，CPU利用率越低 Device：磁盘设备的名称，表示正在监视的磁盘或分区。 tps：每秒传输的 I&#x2F;O 操作次数。这个值表示每秒完成的读取和写入磁盘的总操作数，即该设备每秒的传输次数，”一次传输”意思是”一次I&#x2F;O请求”。多个逻辑请求可能会被合并为”一次I&#x2F;O请求”。”一次传输”请求的大小是未知的 kB_read&#x2F;s：每秒从磁盘读取的数据量。这个值表示每秒从磁盘读取的数据量，以千字节（KB）为单位。 kB_wrtn&#x2F;s：每秒写入磁盘的数据量。这个值表示每秒写入磁盘的数据量，以千字节（KB）为单位。 kB_read：自系统启动以来从磁盘读取的总数据量。这个值表示自系统启动以来累积的总读取数据量，以千字节（KB）为单位。 kB_wrtn：自系统启动以来写入磁盘的总数据量。这个值表示自系统启动以来累积的总写入数据量，以千字节（KB）为单位。 范例 1234[root@centos8 ~]#iostat -d sda 1 3 -xLinux 4.18.0-193.el8.x86_64 (centos8.wangxiaochun.com) 11/24/2020 _x86_64_(2 CPU)Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %utilsda 12.70 2.93 523.99 183.34 0.05 0.78 0.41 20.93 0.37 0.75 0.00 41.26 62.53 0.50 0.78 字段说明 r&#x2F;s: 每秒合并后读的请求数 w&#x2F;s: 每秒合并后写的请求数 rsec&#x2F;s：每秒读取的扇区数 wsec&#x2F;：每秒写入的扇区数 rKB&#x2F;s：每秒向设备发出的读取请求数 wKB&#x2F;s：每秒向设备发出的写入请求数 rrqm&#x2F;s：每秒这个设备相关的读取请求有多少被合并了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同块的数据，FS会将这个请求合并） wrqm&#x2F;s：每秒这个设备相关的写入请求有多少被合并了 %rrqm: 读取请求在发送到设备之前合并在一起的百分比 %wrqm: 写入请求在发送到设备之前合并在一起的百分比 avgrq-sz：平均请求扇区的大小 avgqu-sz：是平均请求队列的长度。毫无疑问，队列长度越短越好 await：每一个IO请求的处理的平均时间（单位是微秒毫秒）。这里可以理解为IO的响应时间，一般地系统IO响应时间应该低于5ms，如果大于10ms就比较大了。这个时间包括了队列时间和服务时间，也就是说，一般情况下，await大于svctm，它们的差值越小，则说明队列时间越短，反之差值越大，队列时间越长，说明系统出了问题。 svctm：表示平均每次设备I&#x2F;O操作的服务时间（以毫秒为单位）。如果svctm的值与await很接近，表示几乎没有I&#x2F;O等待，磁盘性能很好，如果await的值远高于svctm的值，则表示I&#x2F;O队列等待太长，系统上运行的应用程序将变慢。 %util：在统计时间内所有处理IO时间，除以总共统计时间。例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的%util &#x3D; 0.8&#x2F;1 &#x3D; 80%，所以该参数暗示了设备的繁忙程度。一般地，如果该参数是100%表示设备已经接近满负荷运行了（当然如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。 5.2 监视磁盘I&#x2F;O iotop来自于iotop包 iotop命令是一个用来监视磁盘I&#x2F;O使用状况的top类工具，iotop具有与top相似的UI，其中包括PID、用户、I&#x2F;O、进程等相关信息，可查看查看哪些进程正在读取或写入磁盘，可定位哪个进程消耗IO的时间最长 123456789[root@centos8 ~]#iotopTotal DISK READ : 0.00 B/s | Total DISK WRITE : 0.00 B/sActual DISK READ: 0.00 B/s | Actual DISK WRITE: 0.00 B/sTID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt; COMMAND#第一行：Read和Write速率总计#第二行：实际的Read和Write速率 字段输出解释： TID：线程或进程的唯一标识符（Thread&#x2F;Task ID），表示正在进行磁盘 I&#x2F;O 操作的进程或线程的ID。 PRIO：进程的优先级（Priority），通常用于指示进程的执行优先级。 USER：执行磁盘 I&#x2F;O 操作的用户的用户名。 DISK READ：磁盘读取速率，表示进程正在从磁盘读取数据的速度。以字节&#x2F;秒（Bytes per Second）为单位显示。 DISK WRITE：磁盘写入速率，表示进程正在向磁盘写入数据的速度。以字节&#x2F;秒为单位显示。 SWAPIN：表示进程从交换空间中读取数据的速率，通常用于虚拟内存操作。以字节&#x2F;秒为单位显示。 IO&gt;：磁盘 I&#x2F;O 操作的总和，包括读取和写入。以百分比形式表示，表示磁盘 I&#x2F;O 占用的总带宽。 COMMAND：正在进行磁盘 I&#x2F;O 操作的进程或线程的命令名称。 iotop常用参数 1234567891011121314-o #只显示正在产生I/O的进程或线程，除了传参，可以在运行过程中按o生效-b #非交互模式，一般用来记录日志-n NUM|--iter=NUM #设置监测的次数，默认无限。在非交互模式下很有用-d SEC|--delay=SEC #设置每次监测的间隔，默认1秒，接受非整形数据例如1.1-p PID|--pid=PID #指定监测的进程/线程-u USER|--user=USER #指定监测某个用户产生的I/O-P #仅显示进程，默认iotop显示所有线程-a #显示累积的I/O，而不是带宽-k #使用kB单位，而不是对人友好的单位。在非交互模式下，脚本编程有用-t #加上时间戳，非交互非模式-q #禁止头几行，非交互模式，有三种指定方式-q #只在第一次监测时显示列名-qq #永远不显示列名-qqq #永远不显示I/O汇总 交互按键 1234567left和right方向键：改变排序r：反向排序o：切换至选项--onlyp：切换至--processes选项a：切换至--accumulated选项q：退出i：改变线程的优先级 六、网络性能分析6.1 显示网络带宽使用情况 iftop和ethtool通过EPEL源的 iftop 包 可以看到机器和哪个远程主机的网络带宽占用比较高 选项 123456789101112-h #显示帮助-n #以IP的形式显示主机-N #直接显示端口号，而不是服务名-p #显示同一网段其它主机的流量-b #不显示图形化的进度条-B #以 bytes 为单位显示网络带宽-a #以数据包为单位显示带宽-i #指定监听的网卡-F #只显示ipv4流量-G #只显示ipv6流量-l #显示本地ipv6流量，默认关闭-P #显示流量端口号 范例 1234567891011[root@centos8 ~]#iftop -ni eth0TX：总的发送数据包速度RX：总的接收数据包速度peak：峰值#只监听ens160上 ipv4的流量[root@ubuntu ~]# iftop -nF ens160#显示进出流量的端口号[root@ubuntu ~]# iftop -nPF ens160 ethtool可以看到总带宽 1[root@rocky8 ~]#ethtool eth0 6.2 查看网络实时吞吐量 nloadnload 是一个实时监控网络流量和带宽使用情况，以数值和动态图展示进出的流量情况,通过EPEL源安装 选项 12345-m #显示所有设备-t N #数据刷新频率，单位毫秒，默认500-u h|b|k|m|g|H|B|K|M|G #显示单位(h: auto b: Bit/s k: kBit/s m: MBit/s H:auto B: Byte/s K: kByte/s M: MByte/s)-U h|b|k|m|g|H|B|K|M|G #总流量显示单位devices #指定设备 字段说明 12345Curr #当前流量Avg #平均流量Min #最小流量Max #最大流量Ttl #总流量 界面操作 123上下方向键、左右方向键、enter键或者tab键都就可以切换查看多个网卡的流量情况按 F2 显示选项窗口按 q 或者 Ctrl+C 退出 nload 范例 1234567891011121314#查看所有网络设备进出流量，第一页显于一个设备[root@ubuntu ~]# nload#一屏显示所有设备流量[root@ubuntu ~]# nload -m#在nload后面指定网卡，可以指定多个,按左右键分别显示网卡状态[root@ubuntu ~]#nload eth0 eth1#1000 毫秒刷新一次数据[root@ubuntu ~]# nload -t 1000 ens160#设置单位：显示两种单位一种是显示Bit/s、一种是显示Byte/s，默认是以Bit/s，也可不显示/s[root@ubuntu ~]#nload -u M eth0 6.3 查看进程网络带宽的使用情况 nethogsNetHogs是一个开源的命令行工具（类似于Linux的top命令），用来按进程或程序实时统计网络带宽使用率 红帽系统的nethogs包来自于EPEL源 选项 123456-d #数据刷新时间间隔，默认1秒-v #显示单位(0：KB/s，1：total KB，2：total B，3：total MB)-t #跟踪显示-s #按发送数据量排序-a #显示所有设备数据，包含回环设备device #指定设备 交互式命令 1234q #退出s #根据发送数据量排序r #根据接收数据量排序m #切换显示单位 范例 12345#默认[root@ubuntu ~]# nethogs#显示指定设备数据[root@ubuntu ~]# nethogs ens160 6.4 网络监视工具iptraf-ng来自于iptraf-ng包,可以进网络进行监控,对终端窗口大小有要求 选项 12345678-i &lt;iface&gt; #指定网络设备，默认所有-d &lt;iface&gt; #指定网络设备，显示详细信息-s &lt;iface&gt; #指定网络设备，统计tcp和udp信息-z &lt;iface&gt; #指定网络设备，统计数据包-B #在后台执行-f #清除所有锁和计数器-t N #仅统计指定的时长，单位分钏-L &lt;logfile&gt;#指定日志文件 范例 12#对终端窗口大小有要求,如果太小否则无法显示[root@centos8 ~]#iptraf-ng","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://aquapluto.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"OOM","slug":"Linux/process-manage/OOM","date":"2025-08-21T02:58:45.000Z","updated":"2025-09-09T15:36:13.174Z","comments":true,"path":"Linux/process-manage/OOM/","permalink":"https://aquapluto.github.io/Linux/process-manage/OOM/","excerpt":"","text":"1 内存不足时linux系统的处理流程阶段1：Buffers 和 Cache 清理：首先，当内存开始变得紧张时，系统会开始回收Buffers和Cache中的内存。这些缓存的内容如果需要，能很快被重新生成，因此它们是最先被系统考虑回收的部分。 阶段2：启动 Swapping：然后，如果确定缓存无法满足内存需求，即便在清理了Buffers和Cache后，内存仍然紧张，那么系统会开始 Swapping。这个阶段系统开始将一部分内存中的内容（通常是最近不活跃的进程）放入swap磁盘空间中，以腾出内存空间。这个阶段是有性能损失的，因为与读写内存相比，读写硬盘的速度要慢得多。 阶段3：启动 OOM Killer：最后，如果在启动了Swapping之后内存压力仍然很大，系统就会启动OOM Killer。OOM Killer会选择一些“牺牲品”，杀掉这些进程，以此释放内存。选择牺牲品的策略有很多，但通常来说，被杀掉的进程会是那些占用内存较多、优先级较低、启动时间较长的进程。 2 OOM案例演示C语言内存空间分配函 malloc() 数简介 12345678910linux系统里的程序都是调用接口malloc()来申请内存 调用形式：(类型说明符*)malloc(size)功能：在内存的动态存储区中分配一块长度为“size”字节的连续区域。函数的返回值为该区域的首地址。说明：（1）“类型说明符”表示把该区域用于何种数据类型。（2）（类型说明符*）表示把返回值强制转换为该类型指针。（3）“size”是一个无符号数。例如：pc=(char*)malloc(100);表示分配100个字节的内存空间，并强制转换为字符数组类型，函数的返回值为指向该字符数组的指针，把该指针赋予指针变量pc。 mem_alloc.c 1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;malloc.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt; #define BLOCK_SIZE (1024*1024) int main(int argc, char **argv)&#123; int thr, i; char *p1; if (argc != 2) &#123; printf(&quot;Usage: mem_alloc &lt;num (MB)&gt;\\n&quot;); exit(0); &#125; thr = atoi(argv[1]); /*此处指定我们for循环的次数，后面我们启一个for循环每次申请1M*/ printf(&quot;Allocating,&quot; &quot;set to %d MBytes\\n&quot;, thr); sleep(30); for (i = 0; i &lt; thr; i++) &#123; p1 = malloc(BLOCK_SIZE); /*单位为Bytes，此处代表申请1M的内存，得到的就是一个虚拟地址*/ memset(p1, 0x00, BLOCK_SIZE); /*单位为Bytes，此处是根据虚拟地址真正应用物理内存*/ &#125; sleep(6000); return 0;&#125; 编译生成可执行文件 1gcc -o mem_alloc mem_alloc.c 运行测试，该进程启动后30s后，会开始循环申请内容，每次申请1M，当内存不足时，就会被系统杀死，可以调整下面1000这个值来控制总共申请内存的大小 1./mem_alloc 1000 3 OOM介绍储备知识：了解MMU、TLB与大页内存HugePages 3.1 什么是OOMOOM 全称 Out of Memory（内存不足&#x2F;内存溢出），是linux系统的一种保护机制，当物理内存不够用时，linux系统的killer会杀死某个正在运行的进程来释放内存，当然不会随机杀进程，具体杀哪个进程是会对进程进行评分的，后面会说 3.2 何时会发生OOMOOM分为整个系统级别，以及cgroup控制组级别 1、无论如何，只要内存不足时肯定会触发OOM，是站在整个系统的角度去杀进程，所有进程都是目标 2、内存充足，但是所有进程对内存的占用超过了规定的限制（使用mem cgroup限制），也会触发OOM killer，只能杀死本控制组内进程 3.3 为何一定要发生OOM在linux中，内存申请是通过调用 malloc() 完成的，一旦内存不足，该方法调用失败就可以了，不是吗？为何要杀死进程呢？ 这个理解是错误的，因为linux系统的内存都是超配的overcommit。即程序申请的内存其实是虚拟内存，操作系统给程序的只是一个地址范围，该地址范围对应到物理内存上，而且只有在程序往这个地址对应的空间里写入数据时才会真正占用物理内存，所以程序申请的内存是可以超过物理内存大小的。例如物理内存有1G，但是 malloc() 可以申请2G的内存。 示例1：申请内存，获得虚拟地址，过30s后，根据虚拟地址应用物理内存 12345678910111213141516171819202122[root@test04 test]# cat overcommit_test.c #include &lt;stdio.h&gt;#include &lt;malloc.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt; int main(int argc, char **argv)&#123; char *p1; p1 = malloc(100 * 1024 * 1024); /*申请100M的内存，得到的就是一个虚拟地址，拿到指针p1*/ sleep(30); memset(p1, 0x00, 50 * 1024 * 1024); /*根据p1指针的指向的虚拟地址真正应用物理内存*/ sleep(6000); return 0;&#125; [root@test04 test]# gcc -o overcommit_test overcommit_test.c [root@test04 test]# ./overcommit_test 打开另外一个终端查看，第一次查看VSZ为106624KB（~100M），RSS只有616KB 1234[root@test04 ~]# ps aux |head -1USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND[root@test04 ~]# ps aux |grep overcommit_test |grep -v grep root 91190 0.0 0.0 106624 616 pts/3 S+ 14:13 0:00 ./overcommit_test 过大概30s后查看。VSZ不变，RSS物理内存被占用到了53960KB（~50M），RSS才是真正占用物理内存的大小，RRS内存包括了代码段内存、栈内存、堆内存、共享库的内存，这些内存是进程运行必须要有的，我们代码里 malloc/memset 得到的内存就是堆内存，具体RSS大小可以查看 /proc/[pid]/smaps 文件 1234[root@test04 ~]# ps aux |head -1USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND[root@test04 ~]# ps aux |grep overcommit_test |grep -v greproot 91190 0.0 2.6 106624 53960 pts/3 S+ 14:13 0:00 ./overcommit_test 这种overcommit的内存申请模式的好处是，可以有效提高系统内存的利用率，但坏处就是在所有进程都试图真的用完申请的内存空间时，物理内存可能会发生不够用，系统为了保命，只能杀掉一些进程，这就是OOM。 3.4 OOM发生时什么进程会被杀掉当OOM发生时，并不会随机杀进程，具体杀哪个进程是会调用内核了一个 oom_badness() 函数来评定，主要就是两个指标 进程已经使用的物理内存页面数 每个进程的OOM校准值oom_score_adj 在&#x2F;proc文件系统中，每个进程都有一个 /proc/oom_score_adj 的接口文件，我们可以在该文件中输入 -1000到1000 之间的任意一个数值，来调整该进程被OOM kill的几率 oom_badness() 函数会计算一个评分：系统总的可用页面数 * oom_score_adj值 + 进程已经使用的物理页面数 该评分的值越大，被OOM kill掉的几率就越大。 强调：OOM只针对内存超出的情况，cpu使用率过高是不会被杀掉的，但是会一直限制cpu的最大使用，（cpu时间片都是共享的，你超出了最多不给你用了，不会杀你。但是内存在申请的时候都是超配的，你执行系统调用malloc得到的就只是一个虚拟的地址与物理内存对应，这就是overcommit机制，这有效增强了内存利用率，同时也埋下了隐患，就是如果所有进程都来真的，都真的去用完这些内存，把内存占满了，这就必须杀掉几个来释放了。最后说一句：内存你占用了，你不释放你就一直占用着，但是cpu可不是这样，操作系统会随时夺走你的cpu执行权限，不管你是谁，不管你在干什么） 3.5 内存不足时一定会OOM吗我们知道，由于Linux默认是允许memory overcommit（内存过度使用）的，只要你来申请内存我就给你，有可能实际上用不到那么多内存，但万一用到那么多，且让主机存储不足了呢？Linux就设计了一个OOM killer机制挑选一个进程出来杀死，以腾出部分内存，如果还不够就继续。但是杀死进程有可能造成业务中断，所以Linux提供了下列内核参数来优化OOM 通过设置内核参数 vm.panic_on_oom 的值为2，可以让系统在 OOM 时直接重启，避免持续杀进程 通过内核参数 vm.overcommit_memory 禁止memory overcommit vm.overcommit_memory 接受三种取值 123450 – Heuristic overcommit handling. 这是缺省值，它允许overcommit，但过于明目张胆的overcommit会被拒绝，比如malloc一次性申请的内存大小就超过了系统总内存。Heuristic的意思是“试探式的”，内核利用某种算法猜测你的内存申请是否合理，它认为不合理就会拒绝overcommit。1 – Always overcommit. 允许overcommit，对内存申请来者不拒。内核执行无内存过量使用处理。使用这个设置会增大内存超载的可能性，但也可以增强大量使用内存任务的性能2 – Don’t overcommit. 禁止overcommit。 内存拒绝等于或者大于总可用 swap 大小以及overcommit_ratio 指定的物理 RAM 比例的内存请求。如果希望减小内存过度使用的风险，这个设置就是最好的。 4 如何定位OOM问题与解决OOM级别分为两个维度 维度 系统级 OOM 容器级 OOM 管理边界 整个主机（全局） 单个容器（隔离的命名空间） 触发原因 主机整体内存耗尽 容器内存使用超过预设限制 依赖机制 OOM Killer + 全局内存管理 cgroups 内存限制 影响范围 杀死主机内的进程（包括容器） 仅杀死属于容器的进程 核心目标 保护系统不崩溃 隔离资源，防止单个容器滥用 如何定位OOM问题与解决的演示请看该文章第3节 补充：在 Java 程序中，当 JVM 因为没有足够的内存来为对象分配空间并且垃圾回收器也已经没有空间可回收时，就会抛出OOM 原因： 给应用分配内存太少：比如虚拟机本身可使用的内存（一般通过启动时的VM参数指定）太少。 应用用的太多，并且用完没释放，浪费了。此时就会造成内存泄露或者内存溢出。 使用的解决办法： 限制 java进程的max heap，并且降低 java 程序的worker数量，从而降低内存使用 给系统增加swap空间（没招了才使用） 设置内核参数 内核参数 含义与作用 常见取值 对 JVM OOM 的影响 vm.panic_on_oom 控制系统 OOM 时的行为（是否触发内核 panic） 0（默认）、1、2 设为 0 时，内核会启动 OOM Killer 杀死进程释放内存（可能包括 JVM 进程）；设为 2 可能导致系统重启。 vm.oom_kill_allocating_task 触发 OOM 时优先杀死 “申请内存的进程” 还是 “得分最高的进程” 0（默认）、1 设为 1 时，若 JVM 进程因申请内存触发 OOM，会被优先杀死，避免其他进程被误杀。 vm.overcommit_memory 控制内存超额分配策略（内核是否允许进程申请超过实际可用的内存） 0（默认）、1、2 - 设为 0：内核动态判断是否允许分配，可能导致 JVM 申请内存时被拒绝（触发 OOM）。 - 设为 1：允许超额分配，可能掩盖内存问题，最终导致系统级 OOM。 - 设为 2：严格限制分配上限，避免 JVM 过度申请内存。 vm.overcommit_ratio 当 vm.overcommit_memory=2 时，允许超额分配的物理内存百分比（相对于总物理内存） 50（默认）、0-100 配合 vm.overcommit_memory=2 使用，例如设为 80 时，JVM 最大可申请内存 &#x3D; 物理内存 ×80% + Swap。 vm.swappiness 控制内存页被交换到 Swap 的倾向（值越高，越容易换出） 0-100（默认 60） 降低值（如 10-20）可减少 JVM 堆内存被换出到 Swap 的频率，避免因 Swap I&#x2F;O 导致 JVM 性能下降。 vm.min_free_kbytes 系统保留的最小空闲内存（KB），防止内存完全耗尽 动态计算（默认较小值） 适当调大（如物理内存 16GB 时设为 65536）可保留更多空闲内存，减少 JVM 因系统内存耗尽而触发 OOM 的概率。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"进程管理","slug":"进程管理","permalink":"https://aquapluto.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"}]},{"title":"软件包管理","slug":"Linux/software-manage","date":"2025-08-21T02:46:07.000Z","updated":"2025-09-07T12:54:27.551Z","comments":true,"path":"Linux/software-manage/","permalink":"https://aquapluto.github.io/Linux/software-manage/","excerpt":"","text":"1 软件运行与编译1.1 C语言程序实现过程gcc编译过程 12345678#分步骤编译运行gcc -E hello.c -o hello.i 对hello.c文件进行预处理，生成了hello.i 文件gcc -S hello.i -o hello.s 对预处理文件进行编译，生成了汇编文件gcc -c hello.s -o hello.o 对汇编文件进行编译，生成了目标文件gcc hello.o -o hello 对目标文件进行链接，生成可执行文件#一步实现编译过程gcc hello.c -o hello 直接编译链接成可执行目标文件 1.2 模块（库）文件查看二进制程序所依赖的库文件 123ldd /PATH/TO/BINARY_FILE[root@rocky8 ~]# ldd /usr/bin/ls 管理及查看本机装载的库文件 12345#加载配置文件中指定的库文件ldconfig#显示本机已经缓存的所有可用库文件名及文件路径映射关系/sbin/ldconfig –p 配置文件 12/etc/ld.so.conf/etc/ld.so.conf.d/*.conf 缓存文件：/etc/ld.so.cache 范例：库文件破坏后，将导致依赖的程序无法正常运行，需要通过救援模式恢复 123456789101112131415161718192021222324[root@centos8 ~]#ldd /bin/lslinux-vdso.so.1 (0x00007ffc509fd000)libselinux.so.1 =&gt; /lib64/libselinux.so.1 (0x00007fc6ef24a000)libcap.so.2 =&gt; /lib64/libcap.so.2 (0x00007fc6ef044000)libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fc6eec81000)libpcre2-8.so.0 =&gt; /lib64/libpcre2-8.so.0 (0x00007fc6ee9fd000)libdl.so.2 =&gt; /lib64/libdl.so.2 (0x00007fc6ee7f9000)/lib64/ld-linux-x86-64.so.2 (0x00007fc6ef698000)libpthread.so.0 =&gt; /lib64/libpthread.so.0 (0x00007fc6ee5d9000)[root@centos8 ~]#ldd /bin/catlinux-vdso.so.1 (0x00007ffe335dd000)libc.so.6 =&gt; /lib64/libc.so.6 (0x00007fa34749e000)/lib64/ld-linux-x86-64.so.2 (0x00007fa347a6b000)[root@centos8 ~]#mv /lib64/libc.so.6 /tmp[root@centos8 ~]#lsls: error while loading shared libraries: libc.so.6: cannot open shared objectfile: No such file or directory[root@centos8 ~]#catcat: error while loading shared libraries: libc.so.6: cannot open shared objectfile: No such file or directory 2 软件包和包管理器2.1 使用光盘NFS服务时，会将挂载信息写入”&#x2F;etc&#x2F;fstab”，系统每次开机都会自动将其挂载，但是这样会消耗很多的本地资源，给网络带宽和服务器的硬件资源带来很大的负载。autofs服务程序则是在用户需要使用该文件系统时才动态的去挂载，从而节约了网络资源和服务器的硬件资源。 123456789101112131415161718#神奇的光盘挂载目录#CentOS[root@centos8 ~]#rpm -q autofs || yum -y install autofs[root@centos8 ~]#systemctl enable --now autofs[root@centos ~]#cd /misc/cd[root@centos cd]#lsCentOS_BuildTag EULA images LiveOS repodata RPM-GPG-KEY-CentOS-Testing-7EFI GPL isolinux Packages RPM-GPG-KEY-CentOS-7 TRANS.TBL[root@centos cd]#cd Packages/ #存放红帽编译打包的各种rpm包[root@centos Packages]#ls[root@centos ~]#ls /var/lib/rpm #不能破坏#Ubunturoot@ubuntu2004:~# apt install autofs -yroot@ubuntu2004:~# vim /etc/auto.master/misc /etc/auto.miscroot@ubuntu2004:~# systemctl restart autofs 2.2 软件包种类rpm包：预先编译打包，安装简单，但缺乏可定制的灵活性 源码包：手动编译打包，安装繁琐，但可定制性强 二进制包：解压即可使用, 安装简单，但缺乏可定制的灵活性，与rpm包相比它可以改变安装目录 2.3 软件包中的文件分类 二进制文件 库文件 配置文件 帮助文件 范例：利用 cpio工具查看包文件列表 12rpm2cpio 包文件|cpio –itv 预览包内文件rpm2cpio 包文件|cpio –id &quot;*.conf&quot; 释放包内文件 2.4 系统发版的光盘或官方网站CentOS 镜像 123456https://www.centos.org/download/http://mirrors.aliyun.comhttps://mirrors.huaweicloud.com/https://mirror.tuna.tsinghua.edu.cn/http://mirrors.sohu.comhttp://mirrors.163.com Ubuntu 镜像 12http://cdimage.ubuntu.com/releases/http://releases.ubuntu.com 3 Centos软件管理3.1 rpm包管理器3.1.1 安装12345rpm &#123;-i|--install&#125; [install-options] PACKAGE_FILE…-v: verbose-vv:-h: 以#显示程序包管理执行进度 常用组合 1rpm -ivh PACKAGE_FILE ... 选项 12345678910--test: 测试安装，但不真正执行安装，即dry run模式--nodeps：忽略依赖关系--replacepkgs | replacefiles--nosignature: 不检查来源合法性--nodigest：不检查包完整性--noscripts：不执行程序包脚本 %pre: 安装前脚本 --nopre %post: 安装后脚本 --nopost %preun: 卸载前脚本 --nopreun %postun: 卸载后脚本 --nopostun 3.1.2 升级和降级rpm包升级 12rpm &#123;-U|--upgrade&#125; [install-options] PACKAGE_FILE...rpm &#123;-F|--freshen&#125; [install-options] PACKAGE_FILE... 对应选项 12345upgrade：安装有旧版程序包，则&quot;升级&quot;，如果不存在旧版程序包，则&quot;安装&quot;freshen：安装有旧版程序包，则&quot;升级&quot;， 如果不存在旧版程序包，则不执行升级操作--oldpackage：降级--force: 强制安装 常用组合 12rpm -Uvh PACKAGE_FILE ...rpm -Fvh PACKAGE_FILE ... 升级注意项： 不要对内核做升级操作；Linux支持多内核版本并存，因此直接安装新版本内核 如果原程序包的配置文件安装后曾被修改，升级时，新版本提供的同一个配置文件不会直接覆盖老版本的配置文件，而把新版本文件重命名(FILENAME.rpmnew)后保留 3.1.3 包查询123456789-p rpmfile #针对尚未安装的程序包文件做查询操作-q PACKAGE #查询有没有安装到该包-qi PACKAGE #了解包的信息-ql PACKAGE #列出这个包的所有文件列表-q --scripts PACKAGE #查看该包的脚本-qf file #查询磁盘上的文件或命令来自于哪个包，自己建的文件查不到-qc PACKAGE #只列出这个包里面包含的配置文件-qd PACKAGE #只列出这个包里面包含的文档-qa --last #查看最近安装的包 范例: 查看最近安装的包 1[root@centos8 ~]# rpm -qa --last|head 范例：查询包详细信息 123456#没有安装，可以指定程序包[root@rocky86 h]# rpm -qi httpdpackage httpd is not installed#-p指定rpm 包文件，新版可以省略此选项[root@rocky86 h]# rpm -qip httpd-2.4.37-47.module+el8.6.0+823+f143cee1.1.x86_64.rpm 范例：根据文件查询包信息 12345678910111213[root@rocky86 h]# rpm -qf /usr/sbin/nginxnginx-1.14.1-9.module+el8.4.0+542+81547229.x86_64[root@centos ~]#rpm -qf `which ifconfig`net-tools-2.0-0.25.20131004git.el7.x86_64#还没安装的包，此处不能省略-p参数[root@rocky86 h]# rpm -qpf httpd-2.4.37-47.module+el8.6.0+823+f143cee1.1.x86_64.rpmhttpd-2.4.37-47.module+el8.6.0+823+f143cee1.1.x86_64#自己编译的，不属于任何包[root@rocky86 0727]# rpm -qf testfile /root/0727/test is not owned by any package 3.1.4 包卸载1rpm &#123;-e|--erase&#125; [--allmatches] [--nodeps] [--noscripts] [--notriggers] [--test] PACKAGE_NAME ... 注意：当包卸载时，对应的配置文件不会删除， 以FILENAME.rpmsave形式保留 范例：强行删除rpm包，并恢复 123456789[root@centos7 ~]#rpm -e rpm --nodeps#重启进入rescue模式#mkdir /mnt/cdrom#mount /dev/sr0 /mnt/cdrom如果已经有挂载直接df查看挂载点#rpm -ivh /mnt/cdrom/Packages/rpm-4.11.3-40.el7.x86_64.rpm --root=/mnt/sysimagerpm -ivh /run/install/repo/BaseOS/Packages/r/rpm-4.xxx.rpm --root=/mnt/sysroot --force#reboot 注意：当包卸载时，对应的配置文件不会删除(前提是该文件被改动过)， 以FILENAME.rpmsave形式保留 3.1.5 包校验在安装包时，系统也会检查包的来源是否是合法的 检查包的完整性和签名 1rpm -K 【PATH】rpmfile 范例 1234567[root@centos Packages]#rpm -K zip-3.0-11.el7.x86_64.rpm zip-3.0-11.el7.x86_64.rpm: rsa sha1 (md5) pgp md5 OK[root@centos ~]#rpm -K zip-3.0-11.el7.x86_64.rpm error: zip-3.0-11.el7.x86_64.rpm: open failed: No such file or directory[root@centos ~]#rpm -K /misc/cd/Packages/zip-3.0-11.el7.x86_64.rpm /misc/cd/Packages/zip-3.0-11.el7.x86_64.rpm: rsa sha1 (md5) pgp md5 OK 在检查包的来源和完整性前，必须导入所需要公钥 1234567891011#导入公钥[root@rocky86 v]# rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficialrpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7#查看己导入的公钥[root@rocky86 v]# rpm -qa &quot;gpg-pubkey&quot;gpg-pubkey-2f86d6a1-5cf7cefbgpg-pubkey-6d745a60-60287f36#查看[root@rocky86 v]# rpm -qi gpg-pubkey-6d745a60-60287f36 软件在安装时，会将包里的每个文件的元数据，如：大小，权限，所有者，时间等记录至rpm相关的数据库中，可以用来检查包中的文件是否和当初安装时有所变化 123456789101112131415rpm -V PACKAGErpm -Va 检测所有安装好的包里面的所有文件的属性，看看自从这些包安装好后，哪些文件的信息发生了变化#字段说明S #文件大小不一样M #文件权限不一样或文件类型不一样5 #md5 校验值不一样D #版本号值不一样L #链接路径不一样U #属主发生了改变G #属组发生了改变T #修改时间发生了改变P #功能发生了改变c|d|g|l|r #文件类型 c配置文件, d数据文件，g该文件不属于此处，l许可文件(licens file)，r自述文件(READ ME)#如果占位符是 . 则表示该处与安装时没有任何改变 3.1.6 数据库维护rpm包安装时生成的信息，都放在rpm数据库中 &#x2F;var&#x2F;lib&#x2F;rpm 可以重建数据库 123rpm &#123;--initdb|--rebuilddb&#125;initdb: 初始化，如果事先不存在数据库，则新建之，否则，不执行任何操作rebuilddb：重建已安装的包头的数据库索引目录 3.1.7 包更新日志1rpm -q --changelog packageName 3.2 yum和dnfCentOS 使用 yum, dnf 解决rpm的包依赖关系 yum&#x2F;dnf 工作原理 yum&#x2F;dnf 是基于C&#x2F;S 模式 yum 服务器存放rpm包和相关包的元数据库 yum 客户端访问yum服务器进行安装或查询等 yum 实现过程 先在yum服务器上创建 yum repository（仓库），在仓库中事先存储了众多rpm包，以及包的相关的元数据文件（放置于特定目录repodata下） 当yum客户端利用yum&#x2F;dnf工具进行安装时包时，会自动下载repodata中的元数据，查询远数据是否存在相关的包及依赖关系，自动从仓库中找到相关包下载并安装。 3.2.1 yum客户端配置yum客户端配置文件 12/etc/yum.conf #为所有仓库提供公共配置/etc/yum.repos.d/*.repo： #为每个仓库的提供配置文件，配置文件必须以repo为后缀 repo仓库配置文件指向的定义 123456789101112[repositoryID] #仓库的标识，如果有多个仓库，内容必须唯一，不能带有空格name=Some name for this repository #描述这个仓库是干什么的baseurl=url://path/to/repository/ #仓库的路径，可添加多个路径，防止网站没了enabled=&#123;1|0&#125; #可以不加，要想禁用仓库的话就要加并且等于0gpgcheck=&#123;1|0&#125; #校验包是否合法，默认为1，禁用就是0gpgkey=URL#以下可以不写mirrorlist=http://list/ #仓库地址列表，在这里写了多个 baseurl指向的地址enablegroups=&#123;1|0&#125; #是否启用yum group,默认值为 1failovermethod=&#123;roundrobin|priority&#125; #有多个baseurl，此项决定访问规则，roundrobin 随机，priority:按顺序访问roundrobin：意为随机挑选，默认值，priority:按顺序访问cost=1000 #开销，或者是成本，YUM程序会根据此值来决定优先访问哪个源,默认为1000 yum服务器的baseurl形式 1234baseurl=file:///cdrom/AppStream/baseurl=https://mirrors.aliyun.com/rockylinux/8.6/AppStream/x86_64/os/baseurl=http://mirrors.aliyun.com/rockylinux/8.6/AppStream/x86_64/os/baseurl=ftp://10.0.0.159/ 注意：yum仓库指向的路径一定必须是repodata目录所在目录 相关变量 123456yum的repo配置文件中可用的变量：$releasever: 当前OS的发行版的主版本号，如：8，7，6$arch: CPU架构，如：aarch64, i586, i686，x86_64等$basearch：系统基础平台；i386, x86_64$contentdir：表示目录，比如：centos-8，centos-7$YUM0-$YUM9:自定义变量 范例 123http://server/centos/$releasever/$basearch/http://server/centos/7/x86_64http://server/centos/6/i386 范例：CentOS 8 配置文件 12345678910[root@centos8 ~]# ll /etc/yum.conflrwxrwxrwx. 1 root root 12 May 14 2019 /etc/yum.conf -&gt; dnf/dnf.conf[root@centos8 ~]#cat /etc/yum.conf[main]gpgcheck=1 #安装包前要做包的合法和完整性校验installonly_limit=3 #同时可以安装3个包，最小值为2，如设为0或1，为不限制clean_requirements_on_remove=True #删除包时，是否将不再使用的包删除best=True #升级时，自动选择安装最新版，即使缺少包的依赖skip_if_unavailable=False #跳过不可用的 范例：CentOS 7 的配置文件 1234567891011121314151617[root@centos7 ~]# ll /etc/yum.conf-rw-r--r--. 1 root root 970 Aug 8 19:57 /etc/yum.conf[root@centos7 ~]# cat /etc/yum.conf[main]cachedir=/var/cache/yum/$basearch/$releasever #缓存路径keepcache=0 #如果为1,则下载rpm并缓存下来,不删除,默认安装rpm后会删除rpm包debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release baseurl 指向的路径 阿里云提供了写好的CentOS,Rocky和ubuntu的仓库文件下载链接 1http://mirrors.aliyun.com/repo/ Rocky 系统的yum源 12345678#南京大学https://mirror.nju.edu.cn/rocky/$releasever/#上海交大https://mirrors.sjtug.sjtu.edu.cn/rocky/$releasever/#山东大学https://mirrors.sdu.edu.cn/rocky/$releasever/ CentOS系统的yum源 1234567891011#阿里云https://mirrors.aliyun.com/centos/$releasever/#腾讯云https://mirrors.cloud.tencent.com/centos/$releasever/#华为云https://repo.huaweicloud.com/centos/$releasever/#清华大学https://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/ EPEL的yum源 1234567891011#阿里云https://mirrors.aliyun.com/epel/$releasever/x86_64#腾讯云https://mirrors.cloud.tencent.com/epel/$releasever/x86_64#华为云https://mirrors.huaweicloud.com/epel/$releasever/x86_64#清华大学https://mirrors.tuna.tsinghua.edu.cn/epel/$releasever/x86_64 查看本地磁盘密钥路径 12[root@centos ~]#ls /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 RPM-GPG-KEY-CentOS-Debug-7 RPM-GPG-KEY-CentOS-Testing-7 范例：为CentOS7用系统安装光盘作的本地yum仓库 12345678910#挂载光盘至某目录,如/mnt/cdrommount /dev/cdrom /mnt/cdrom#创建配置文件[root@centos7 ~]#vim /etc/yum.repos.d/centos7.repo[CentOS7]name=CentOS 7baseurl=file:///mnt/cdromgpgcheck=0enabled=1 范例：为CentOS 8 配置 yum 的系统和EPEL源仓库 12345678910111213141516171819202122[root@centos8 ~]#cat /etc/yum.repos.d/base.repo[BaseOS]name=BaseOSbaseurl=file:///misc/cd/BaseOSgpgcheck=1gpgkey=/etc/pki/rpm-gpg/RPM-GPG-KEY-centosofficial [AppStream]name=AppStreambaseurl=file:///misc/cd/AppStreamgpgcheck=0[epel]name=EPELbaseurl=http://mirrors.aliyun.com/epel/$releasever/Everything/$basearchgpgcheck=0enabled=1[extras]name=extrasbaseurl=https://mirrors.aliyun.com/centos/$releasever/extras/$basearch/osgpgcheck=0 范例：安装epel仓库包 12345678910111213141516[root@centos ~]#cd /etc/yum.repos.d/[root@centos yum.repos.d]#lsCentOS-Base.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-Vault.repoCentOS-CR.repo CentOS-fasttrack.repo CentOS-Sources.repo CentOS-x86_64-kernel.repo[root@centos yum.repos.d]#vim test.repo[root@centos yum.repos.d]#cat test.repo [epel]name=epel repobaseurl=https://mirrors.aliyun.com/epel/$releasever/$basearch https://mirrors.tuna.tsinghua.edu.cn/epel/$releasever/$basearchgpgcheck=1gpgkey=https://mirrors.aliyun.com/epel/RPM-GPG-KEY-EPEL-7 https://mirrors.tuna.tsinghua.edu.cn/epel/RPM-GPG-KEY-EPEL-7 [root@centos ~]#yum -y install sl epel属于额外的包，安装需要操作系统自身的包，操作系统的包在光盘里(&#x2F;misc&#x2F;cd)，所以光盘这个包是要加上去的 centos7中所有软件包都在一个仓库里，所以repodata文件是单独的，不需要另外建仓库，而centos8中把所有软件包分成两个仓库，所 以repodata的文件是有两个的，一个在AppStream里，一个在BaseOS里，需要另外建两个仓库 1234567891011121314151617181920#centos8[BaseOS]name=BaseOSbaseurl=file:///misc/cd/BaseOSgpgcheck=1gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7[AppStream]name=AppStreambaseurl=file:///misc/cd/AppStreamgpgcheck=0[epel]name=epel repobaseurl=https://mirrors.aliyun.com/epel/$releasever/$basearch https://mirrors.tuna.tsinghua.edu.cn/epel/$releasever/$basearchgpgcheck=1gpgkey=https://mirrors.aliyun.com/epel/RPM-GPG-KEY-EPEL-7 https://mirrors.tuna.tsinghua.edu.cn/epel/RPM-GPG-KEY-EPEL-7 范例2：升级内核（慎重） 1234567891011121314151617yum install https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm #centos7或者红帽7yum install https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm #centos8或者红帽8[root@centos ~]#yum list kernel*发现都是已经安装好的了，没有最新版的文件，原因是脚本里设置禁用[root@centos ~]#cd /etc/yum.repos.d/[root@centos yum.repos.d]#lsCentOS-Base.repo CentOS-Debuginfo.repo CentOS-Media.repo CentOS-Vault.repo elrepo.repoCentOS-CR.repo CentOS-fasttrack.repo CentOS-Sources.repo CentOS-x86_64-kernel.repo test.repo[root@centos yum.repos.d]#vim elrepo.repo 找到[elrepo-kernel]，将enabled=0改成1[root@centos ~]#yum list kernel*就可以发现有最新的了，安装推荐lt版[root@centos yum.repos.d]#yum -y install kernel-lt 3.2.2 yum-config-manager命令可以生成yum仓库的配置文件及启用或禁用仓库，来自于yum-utils包 格式 123456#增加仓库yum-config-manager --add-repo URL或file#禁用仓库yum-config-manager --disable &quot;仓库名&quot;#启用仓库yum-config-manager --enable &quot;仓库名&quot; 范例：创建仓库配置 1234567[root@centos8 ~]#rpm -qf `which yum-config-manager `dnf-utils-4.0.2.2-3.el8.noarch[root@centos8 ~]#yum-config-manager --add-repohttps://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoAdding repo from: https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo[root@centos8 ~]#ls /etc/yum.repos.d/backup base.repo docker-ce.repo 范例：创建仓库配置 123456789#生成172.16.0.1_cobbler_ks_mirror_8_.repo[root@centos8 ~]#yum-config-manager --add-repo=http://172.16.0.1/cobbler/ks_mirror/8/Adding repo from: http://172.16.0.1/cobbler/ks_mirror/8/[root@centos8 ~]#cat /etc/yum.repos.d/172.16.0.1_cobbler_ks_mirror_8_.repo[172.16.0.1_cobbler_ks_mirror_8_]name=created by dnf config-manager from http://172.16.0.1/cobbler/ks_mirror/8/baseurl=http://172.16.0.1/cobbler/ks_mirror/8/enabled=1 范例：创建仓库配置 123456[root@centos8 ~]#ls /etc/yum.repos.d/backup base.repo[root@centos8 ~]#yum-config-manager --add-repo /data/docker-ce.repoAdding repo from: file:///data/docker-ce.repo[root@centos8 ~]#ls /etc/yum.repos.d/backup base.repo docker-ce.repo 范例：启用和禁用仓库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@centos8 ~]#yum repolist[root@centos8 ~]#yum-config-manager --disable epel[root@centos8 ~]#cat /etc/yum.repos.d/base.repo[BaseOS]name=BaseOSbaseurl=file:///misc/cd/BaseOSgpgcheck=0[AppStream]name=AppStreambaseurl=file:///misc/cd/AppStreamgpgcheck=0[epel]name=EPELbaseurl=http://mirrors.aliyun.com/epel/$releasever/Everything/$basearch http://mirrors.huaweicloud.com/epel/$releasever/Everything/$basearchgpgcheck=0enabled=0[extras]name=extrasbaseurl=https://mirrors.aliyun.com/centos/$releasever/extras/$basearch/os http://mirrors.huaweicloud.com/centos/$releasever/extras/$basearch/osgpgcheck=0enabled=1[root@centos8 ~]#yum repolistBaseOS 3.8 MB/s | 3.9 kB 00:00 AppStream 4.2 MB/s | 4.3 kB 00:00 extras 10 kB/s | 1.5 kB 00:00 repo id repo name statusAppStream AppStream 4,755BaseOS BaseOS 1,659extras extras 12[root@centos8 ~]#yum-config-manager --disable extras[root@centos8 ~]#yum repolistBaseOS 3.8 MB/s | 3.9 kB 00:00 AppStream 4.2 MB/s | 4.3 kB 00:00 repo id repo name statusAppStream AppStream 4,755BaseOS BaseOS 1,659[root@centos8 ~]#yum-config-manager --enable extras[root@centos8 ~]#yum repolistBaseOS 3.8 MB/s | 3.9 kB 00:00 AppStream 4.2 MB/s | 4.3 kB 00:00 extras 12 kB/s | 1.5 kB 00:00 repo id repo name statusAppStream AppStream 4,755BaseOS BaseOS 1,659extras extras 12 3.2.3 yum命令1234567yum [options] [command] [package ...]-y #自动回答为&quot;yes&quot;-q #静默模式--nogpgcheck #禁止进行gpg check--enablerepo=repoidglob #临时启用此处指定的repo，支持通配符，如：&quot;*&quot;--disablerepo=repoidglob #临时禁用此处指定的repo,和上面语句同时使用，放在后面的生效 3.2.3.1 列出所有仓库12[root@centos yum.repos.d]#yum repolist[root@centos yum.repos.d]#yum repolist -v 可以看到仓库有多少个包 3.2.3.2 显示程序包查看包来自哪个仓库 1[root@centos ~]#yum list sl 只查看已经安装的包 1[root@centos8 ~]#yum list installed|head 查看可安装的包 1[root@centos8 ~]#yum list available |head 查看可以升级的包 1[root@centos8 ~]#yum list updates 范例: 查看指定的包 123[root@centos8 ~]#yum list exim#支持通配符[root@centos8 ~]#yum list exim* 3.2.3.3 安装程序包12345yum install package1 [package2] [...]yum reinstall package1 [package2] [...] #重新安装--downloadonly #只下载相关包默认至/var/cache/yum/x86_64/7/目录下,而不执行install/upgrade/erase--downloaddir=&lt;path&gt;, --destdir=&lt;path&gt; #--downloaddir选项来指定下载的目录,如果不存在自动创建 范例：只下载相关的依赖包,而不安装 123456789101112#/data/目录如果不存在,会自动创建[root@centos8 ~]#yum -y install --downloadonly --downloaddir=/data/httpd httpd[root@centos8 ~]#ls /data/httpd/apr-1.6.3-9.el8.x86_64.rpm httpd-2.4.37-16.module_el8.1.0+256+ae790463.x86_64.rpmapr-util-1.6.1-6.el8.x86_64.rpm httpd-filesystem-2.4.37-16.module_el8.1.0+256+ae790463.noarch.rpmapr-util-bdb-1.6.1-6.el8.x86_64.rpm httpd-tools-2.4.37-16.module_el8.1.0+256+ae790463.x86_64.rpmapr-util-openssl-1.6.1-6.el8.x86_64.rpm mailcap-2.1.48-3.el8.noarch.rpmcentos-logos-httpd-80.5-2.el8.noarch.rpm mod_http2-1.11.3-3.module_el8.1.0+213+acce2796.x86_64.rpm 注意: 下载包也可以通过启用配置文件实现 1234[root@centos7 ~]# cat /etc/yum.conf[main]cachedir=/var/cache/yum/$basearch/$releasever #缓存路径keepcache=1 #如果为1,则下载rpm并缓存下来,不删除,默认安装rpm后会删除rpm包 3.2.3.4 卸载包1[root@centos ~]#yum remove httpd 3.2.3.5 升级和降级检查可用升级 1yum check-update 升级和降级 123yum upgrade|update [package1] [package2] [...]yum upgrade-minimal #最小化升级yum downgrade package1 [package2] [...] (降级) 3.2.3.6 查询查看包的信息 1[root@centos ~]#yum info sl 查看没有安装的文件（命令）来自于哪个包 12345678910111213141516171819yum provides file#注意：文件要写全路径，而不只是文件名，否则可能无法查询到#注意要写文件全路径才能查询到（文件路径可以用which查）[root@centos8 ~]#yum provides vsftpd.confLast metadata expiration check: 0:56:45 ago on Fri 10 Apr 2020 11:24:00 AM CST.Error: No Matches found[root@centos8 ~]#yum provides `which ifconfig`[root@centos8 ~]# yum provides /etc/vsftpd/vsftpd.confLast metadata expiration check: 0:33:13 ago on Fri 27 Dec 2019 03:47:34 PM CST.vsftpd-3.0.3-28.el8.x86_64 : Very Secure Ftp DaemonRepo : AppStreamMatched from:Filename : /etc/vsftpd/vsftpd.conf#使用通配符[root@centos8 ~]#yum provides */vsftpd.conf[root@centos8 ~]#yum provides */updatedb* 查看yum安装卸载历史 12yum history ...info 编号 #查看该编号的包的信息 以指定的关键字搜索程序包名 12yum search string1 [string2] [...]yum search mysql #查看关于MySQL的包 查看指定包所依赖的capabilities 1yum deplist package1 [package2] [...] 3.2.3.7 仓库缓存清除目录&#x2F;var&#x2F;cache&#x2F;yum&#x2F;缓存 1yum clean all 构建缓存 1yum makecache 3.2.3.8 查看yum事务历史yum 执行安装卸载命令会记录到相关日志中 日志文件： 123456#CentOS 7以前版本日志/var/log/yum.log#CentOS 8 版本日志/var/log/dnf.rpm.log/var/log/dnf.log 查看yum安装卸载历史 1234yum history [info|list|packages-list|packages-info|summary|addon-info|redo|undo|rollback|new|sync|stats]info N #查看该编号的包的信息undo N #回滚第N条记录redo N #重新执行第N条记录 3.2.3.9 安装及升级本地程序包12yum localinstall|install rpmfile1 [rpmfile2] [...]yum localupdate|update rpmfile1 [rpmfile2] [...] 3.2.3.10 查看包的安全警报1yum updateinfo --summary|--list|--info 3.2.3.11 包组管理的相关命令包组管理（不建议安装包组） 12345yum grouplist [hidden] [groupwildcard] [...]yum groupinstall group1 [group2] [...]yum groupupdate group1 [group2] [...]yum groupremove group1 [group2] [...]yum groupinfo group1 [...] 范例：最小化安装的系统安装图形环境 123[root@centos8 ~]#yum grouplist[root@centos8 ~]#yum groupinstall &quot;Server with GUI&quot;[root@centos8 ~]#init 5 3.2.3.12 yum安装失败原因12345678yum的配置文件格式或路径错误解决方法：检查/etc/yum.repos.d/*.repo文件格式yum cache解决方法：yum clean all网络不通：解决方法：网卡配置 3.2.4 dnf命令配置文件：&#x2F;etc&#x2F;dnf&#x2F;dnf.conf 仓库文件：&#x2F;etc&#x2F;yum.repos.d&#x2F; *.repo 日志：&#x2F;var&#x2F;log&#x2F;dnf.rpm.log、&#x2F;var&#x2F;log&#x2F;dnf.log dnf 用法与yum一致 123456789101112dnf [options] &lt;command&gt; [&lt;arguments&gt;...]dnf --versiondnf repolistdnf reposyncdnf install httpddnf remove httpddnf clean alldnf makecachednf list installeddnf list availablednf search nanodnf history undo 1 范例: CentOS 8 查看未安装包的文件列表 1234567[root@centos8 ~]#rpm -q memcachedpackage memcached is not installed[root@centos8 ~]#dnf repoquery -l memcachedLast metadata expiration check: 2:35:45 ago on Tue 14 Jul 2020 08:56:26 AM CST./etc/sysconfig/memcached/usr/bin/memcached...... 范例: CentOS 7 查看未安装包的文件列表 1234567[root@centos7 ~]#rpm -q memcachedpackage memcached is not installed[root@centos7 ~]#yum -y install yum-utils[root@centos7 ~]#repoquery -ql memcached/etc/sysconfig/memcached/usr/bin/memcached...... 用yum安装软件时是先把元数据下载到缓存中，缓存存在&#x2F;var&#x2F;cache&#x2F;dnf，在有些情况下会影响软件的安装，比如仓库的信息发生变化，缓存不会及时更新，还是旧的缓存信息，这样会影响安装新软件，为了避免缓存影响我们安装软件，可以清理缓存 1dnf clean all 3.3 制作本地yum源 先准备rpm包，扔到一个文件夹中&#x2F;my_rpms 123456rpm包的来源：1、镜像2、网上的yum源3、使用yum命令把某个仓库里的包下载到本地（只下载不安装）$ yum install --downloadonly --downloaddir=/my_rpms httpd -y4、开启keepcache=1 在上述文件夹&#x2F;my_rpms下生成repodata目录（存放依赖性关系） 12yum install createrepo -y createrepo /my_rpms 安装软件，通过该软件把你的文件&#x2F;my_rpms共享出去 12345678910客户端与服务都关闭selinux与防火墙systemctl stop firewalldsetenforce 0 服务端安装yum install vsftpd -y 把服务机器的yum仓库放到/var/ftp目录下ln -s /my_rpms /var/ftp/my_rpms或者用cp、mv都可以 使用者：配置一个repo文件，baseurl指向上面的yum源地址 123456789101112131415cat &gt; my.repo &lt;&lt; EOF[local]name=localbaseurl=file:///my_rpmsenabled=1gpgcheck=0EOF cat &gt; ftp.repo &lt;&lt; EOF[local]name=localbaseurl=ftp://服务都ip地址/my_rpmsenabled=1gpgcheck=0EOF 3.4 实现yum私有仓库原理：例如epel源，它是在互联网上的，互联网上的包要在公司内部搭，要先从互联网上下载到本机，然后把本机下载到的所有数据利用http协议发布出去，共享出来，那么公司内部的机器就可以利用http协议从创建好的http的共享服务器上获取仓库信息 利用http协议要先下载软件httpd，下载后可以搭建公司内部的http服务器，它的共享数据目录是在&#x2F;var&#x2F;www&#x2F;html 启动http服务：systemctl enable --now httpd 启动之后就相当于外部网站了，可以在微软等软件上输入机器的ip地址就可以访问了 从互联网上下载epel源 123456789#默认只下载rpm包，不下载 meta数据，需要指定--download-metadata 才能下载 metadnf reposync --repoid=REPOID --download-metadata -p /path--repoid=REPOID：你要下载哪个仓库--download-metadata -p /path：下载元数据，下载到哪个地方（/path），这个地方要用http协议共享出来[root@repo-server ~]#dnf reposync --repoid=epel --download-metadata -p /var/www/html然后就可以在网站输入10.0.0.8/epel/访问了 在新机器上下载epel 12345678修改yum仓库配置[epel]name=epel repobaseurl=https://10.0.0.8/epel/gpgcheck=0enabled=1然后就可以正常下载了 4 Ubuntu软件管理4.1 dpkg包管理器dpkg是一个底层的包管理工具，主要用于安装、卸载和管理.deb格式的软件包。它不会自动解决软件包之间的依赖关系，也就是说，如果一个软件包依赖于其他软件包，用户需要手动安装这些依赖包。此外，dpkg不会从远程仓库获取软件包，只能安装本地已经下载的.deb文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#安装包，不支持包的依赖dpkg -i package.deb#删除包，不建议，不自动卸载依赖于它的包dpkg -r package#删除包（包括配置文件）dpkg -P package#列出当前已安装的包，类似rpm -qadpkg -l#显示该包的简要说明dpkg -l package#列出该包的状态，包括详细信息，类似rpm –qidpkg -s package#列出该包中所包含的文件，类似rpm –qldpkg -L package#搜索包含pattern的包，类似rpm –qfdpkg -S &lt;pattern&gt;#配置包，-a 使用，配置所有没有配置的软件包dpkg --configure package#列出 deb 包的内容，类似rpm –qpldpkg -c package.deb#解开 deb 包的内容dpkg --unpack package.deb#列出系统上安装的所有软件包dpkg -l#列出软件包安装的文件dpkg -L bash#查看/bin/bash来自于哪个软件包dpkg -S /bin/bash#安装本地的 .deb 文件dpkg -i /mnt/cdrom/pool/main/z/zip/zip_3.0-11build1_amd64.deb#卸载软件包dpkg -r zip 注意：一般建议不要使用dpkg卸载软件包。因为删除包时，其它依赖它的包不会卸载，并且可能无法再正常运行 4.2 apt1234567891011121314151617apt install #安装软件包apt remove #移除软件包（删不干净）apt purge #移除软件包及配置文件（删的干净）apt update #刷新存储库索引（安装软件之前先执行这个命令更新一下）apt upgrade #升级所有可升级的软件包（谨慎使用）apt autoremove #自动删除不需要的包apt full-upgrade #升级整个系统，必要时可以移除旧软件包apt search #搜索应用程序apt show #显示安装细节apt list #列出包含条件的包（已安装，可升级等）apt edit-sources #编辑源列表apt-cache madison #查看仓库中软件包有哪些版本可以安装#查看文件来自于哪个包,类似redhat中的yum provides &lt;filename&gt;，需要安装apt-file search &#x27;string&#x27; #默认是包含此字符串的文件apt-file search -x &#x27;正则表达式&#x27;apt-file search -F /path/file APT包索引配置文件（相当于yum的仓库配置文件） 123/etc/apt/sources.list/etc/apt/sources.list.d#可以修改上面文件为国内的安装源，提高速度 apt命令操作（如安装和删除软件包）日志文件 1/var/log/dpkg.log 4.2.1 查看文件来自于哪个包范例: 查找存在的文件来自于哪个包 123#dpkg -S filename ：在当前安装的包里查找文件[root@ubuntu1804 ~]#dpkg -S /bin/lscoreutils: /bin/ls 范例: 查找不存在的文件存在于哪个包 1234567891011121314[root@ubuntu1804 ~]#apt -y install apt-file[root@ubuntu1804 ~]#apt update[root@ubuntu1804 ~]#apt-file search -x &#x27;/sl$&#x27;espeak-data: /usr/lib/x86_64-linux-gnu/espeak-data/voices/test/slespeak-ng-data: /usr/lib/x86_64-linux-gnu/espeak-ng-data/lang/zls/sllanguage-pack-sl-base: /var/lib/locales/supported.d/slpython-langdetect: /usr/lib/python2.7/dist-packages/langdetect/profiles/slpython3-langdetect: /usr/lib/python3/dist-packages/langdetect/profiles/slqemu-system-common: /usr/share/qemu/keymaps/slrdesktop: /usr/share/rdesktop/keymaps/slsl: /usr/games/slvirtualbox: /usr/share/virtualbox/rdesktop-vrdp-keymaps/sl[root@ubuntu1804 ~]#apt-file search -F /usr/games/slsl: /usr/games/sl 4.2.2 查看包相关信息12345678910#显示系统安装包的统计信息,可以统计已经安装包的数量，大小，占用空间等[root@ubuntu1804 ~]#apt-cache stats#显示xxx包的信息,可以看到某个包的源、版本等信息#apt-cache show xxx #更详细#apt show xxx[root@ubuntu1804 ~]#apt show keepalived[root@ubuntu1804 ~]#apt-cache show keepalived 4.2.3 查看仓库中的指定软件的所有版本123456789101112[root@ubuntu1804 ~]#apt-cache madison docker-cedocker-ce | 5:19.03.13~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:19.03.12~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:19.03.11~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:19.03.10~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages#安装指定版本[root@ubuntu1804 ~]#apt -y install docker-ce=5:19.03.13~3-0~ubuntu-bionic 4.2.4 查看文件的依赖1234567891011#查询软件xxx依赖哪些包#apt depends xxx#apt-cache depends xxx[root@ubuntu1804 ~]#apt depends keepalived#查询软件xxx被哪些包依赖#apt rdepends xxx#apt-cache rdepends xxx[root@ubuntu1804 ~]#apt rdepends bash 5 源码编译安装5.1 编译安装过程利用编译工具，通常只需要三个大的步骤 ./configure (1) 通过选项传递参数，指定安装路径、启用特性等；执行时会参考用户的指定以及Makefile.in文件生成Makefile (2) 检查依赖到的外部环境，如依赖的软件包 make 根据Makefile文件，会检测依赖的环境，进行构建应用程序 make install 复制文件到相应路径 注意：安装前可以通过查看README，INSTALL获取帮助 5.1.1 编译安装准备准备：安装相关的依赖包 开发工具：make, gcc (c&#x2F;c++编译器GNU C Complier) 开发环境：开发库（glibc：标准库），头文件，可安装开发包组 Development Tools 软件相关依赖包 生产实践：基于最小化安装的系统建议安装下面相关包 1yum install gcc make autoconf gcc-c++ glibc glibc-devel pcre pcre-devel openssl openssl-devel systemd-devel zlib-devel vim lrzsz tree tmux lsof tcpdump wget net-tools iotop bc bzip2 zip unzip nfs-utils man-pages 5.1.2 编译安装第一步：运行 configure 脚本，生成 Makefile 文件 其选项主要功能： 可以指定安装位置 指定启用的特性 安装路径设定 123--prefix=/PATH：指定默认安装位置,默认为/usr/local/--sysconfdir=/PATH：配置文件安装位置System types：支持交叉编译 软件特性和相关指定： 1234567Optional Features: 可选特性--disable-FEATURE--enable-FEATURE[=ARG]Optional Packages: 可选包--with-PACKAGE[=ARG] 依赖包--without-PACKAGE 禁用依赖关系 注意：通常被编译操作依赖的程序包，需要安装此程序包的”开发”组件，其包名一般类似于name-devel-VERSION 第二步：make 1make -j x #加多x个进程，提高速度（x的个数取决于内核的数量） 第三步：make install 注意：make和make install执行失败的原因 121 有一些依懒性的包没安装，看报错提示安装 xxx-devel2 可能因为一些杂糅，可以删除解压后的包，重新解压，再重复之前的操作 5.1.3 安装后配置 二进制程序目录导入至PATH环境变量中，编辑文件/etc/profile.d/NAME.sh 1export PATH=/PATH/TO/BIN:$PATH 相关用户及文件 有些开源软件编译完成后，还需要创建相关的用户及文件 导入帮助手册 编辑&#x2F;etc&#x2F;man.config|man_db.conf文件，添加一个MANPATH 5.2 编译安装实战5.2.1 编译安装新版 tree源码建议放在&#x2F;usr&#x2F;local&#x2F;src 1.进入官方网站查询最新版本，然后复制下载链接（FTP或者HTTP的都行） 12345678910111213141516171819202122[root@centos ~]#rpm -qi treeName : treeVersion : 1.6.0 #版本Release : 10.el7Architecture: x86_64Install Date: Wed 16 Aug 2023 03:33:17 PM CSTGroup : Applications/FileSize : 89505License : GPLv2+Signature : RSA/SHA256, Fri 04 Jul 2014 01:36:46 PM CST, Key ID 24c6a8a7f4a80eb5Source RPM : tree-1.6.0-10.el7.src.rpmBuild Date : Tue 10 Jun 2014 03:28:53 AM CSTBuild Host : worker1.bsys.centos.orgRelocations : (not relocatable)Packager : CentOS BuildSystem &lt;http://bugs.centos.org&gt;Vendor : CentOSURL : http://mama.indstate.edu/users/ice/tree/ #官方网站Summary : File system tree viewerDescription :The tree utility recursively displays the contents of directories in atree-like format. Tree is basically a UNIX port of the DOS treeutility. 2.下载 1234[root@centos ~]#wget http://mama.indstate.edu/users/ice/tree/src/tree-2.1.1.tgz[root@centos src]#lstree-2.1.1.tgz 3.解压 123456789[root@centos src]#tar xf tree-2.1.1.tgz[root@centos src]#lstree-2.1.1 tree-2.1.1.tgz[root@centos src]#cd tree-2.1.1[root@centos tree-2.1.1]#lsCHANGES doc filter.c html.c INSTALL LICENSE Makefile strverscmp.c tree.c unix.ccolor.c file.c hash.c info.c json.c list.c README TODO tree.h xml.c#因为该文件太简单了，所以官方没给configure脚本，直接给了Makefile文件 4.查看说明书 12[root@centos tree-2.1.1]#less README[root@centos tree-2.1.1]#less INSTALL 5.根据需要修改Makefile文件（安装路径设定，软件特性和相关指定） 12[root@centos tree-2.1.1]#vim Makefile PREFIX=/apps/tree 6.改版本号（选做） 12345678910111213[root@centos tree-2.1.1]#grep 2.1.1 *CHANGES:Version 2.1.1 (05/31/2023)grep: doc: Is a directoryMakefile:VERSION=2.1.1tree.c:char *version = &quot;$Version: $ tree v2.1.1 %s 1996 - 2023 by Steve Baker, Thomas Moore, Francesc Rocher, Florian Sesser, Kyosuke Tokoro $&quot;;tree.c:char *hversion= &quot;\\t\\t tree v2.1.1 %s 1996 - 2023 by Steve Baker and Thomas Moore &lt;br&gt;\\n&quot;[root@centos tree-2.1.1]#sed -i &#x27;s/v2.1.1/v6.6.6/&#x27; tree.c[root@centos tree-2.1.1]#grep 6.6.6 *grep: doc: Is a directorytree.c:char *version = &quot;$Version: $ tree v6.6.6 %s 1996 - 2023 by Steve Baker, Thomas Moore, Francesc Rocher, Florian Sesser, Kyosuke Tokoro $&quot;;tree.c:char *hversion= &quot;\\t\\t tree v6.6.6 %s 1996 - 2023 by Steve Baker and Thomas Moore &lt;br&gt;\\n&quot; 7.make 1234567891011121314[root@centos tree-2.1.1]#makegcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o tree.o tree.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o list.o list.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o hash.o hash.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o color.o color.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o file.o file.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o filter.o filter.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o info.o info.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o unix.o unix.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o xml.o xml.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o json.o json.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o html.o html.cgcc -ggdb -std=c11 -pedantic -Wall -D_LARGEFILE64_SOURCE -D_FILE_OFFSET_BITS=64 -c -o strverscmp.o strverscmp.cgcc -o tree tree.o list.o hash.o color.o file.o filter.o info.o unix.o xml.o json.o html.o strverscmp.o 8.make install 123456789101112131415161718[root@centos tree-2.1.1]#make installinstall -d /apps/tree/bininstall -d /apps/tree/man/man1install tree /apps/tree/bin/tree; \\install -m 644 doc/tree.1 /apps/tree/man/man1/tree.1[root@centos tree-2.1.1]#cd[root@centos ~]#ls /apps/tree/bin man[root@centos ~]#/apps/tree/bin/tree --versiontree v6.6.6 © 1996 - 2023 by Steve Baker, Thomas Moore, Francesc Rocher, Florian Sesser, Kyosuke Tokoro[root@centos ~]#tree /apps/tree/apps/tree├── bin│ └── tree└── man └── man1 └── tree.1 9.将旧的tree改为新的tree 123456789101112131415161718192021222324252627282930#用的还是旧的[root@centos ~]#which tree/usr/bin/tree#旧的路径是/usr/bin/[root@centos ~]#echo $PATH /usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/wu/.local/bin:/home/wu/bin#第一种方法将/apps/tree/bin/tree创建软链接在/usr/bin/之前，优先匹配它就行了在/usr/bin/前面的只有/usr/local/bin[root@centos ~]#ln -s /apps/tree/bin/tree /usr/local/bin[root@centos ~]#tree --version #因为原来的路径有缓存，所以还是旧版本tree v1.6.0 (c) 1996 - 2011 by Steve Baker, Thomas Moore, Francesc Rocher, Kyosuke Tokoro [root@centos ~]#hash -r #清除缓存[root@centos ~]#which tree/usr/local/bin/tree[root@centos ~]#tree --versiontree v6.6.6 © 1996 - 2023 by Steve Baker, Thomas Moore, Francesc Rocher, Florian Sesser, Kyosuke Tokoro#第二种方法修改PATH变量，将/apps/tree/bin/tree放在/usr/bin/之前[root@centos ~]#vim /etc/profile.d/tree.shPATH=/apps/tree/bin:$PATH[root@centos ~]#. /etc/profile.d/tree.sh #生效[root@centos ~]#echo $PATH /apps/tree/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/wu/.local/bin:/home/wu/bin[root@centos ~]#which tree/apps/tree/bin/tree[root@centos ~]#tree --versiontree v6.6.6 © 1996 - 2023 by Steve Baker, Thomas Moore, Francesc Rocher, Florian Sesser, Kyosuke Tokoro 5.2.2 编译安装nginx12345678910111213yum install gcc gcc-c++ autoconf automake make -yyum -y install zlib zlib-devel openssl openssl-devel pcre pcre-devel wget https://nginx.org/download/nginx-1.24.0.tar.gztar xf nginx-1.24.0.tar.gzcd nginx-1.24.0# 1、定制安装信息./configure --prefix=/usr/local/nginx_1.24.0 --with-http_stub_status_module --with-http_ssl_module --with-stream --with-http_gzip_static_module --with-http_sub_module# 2、编译，并发4个任务去编译make -j 4# 3、安装make install# 使用ln -s /usr/local/nginx_1.24.0 /usr/local/nginx 5.3 源码编译安装和二进制安装的区别源码包：源代码包里面包括了程序原始的程序代码，需要在你的计算机上进行编译成二进制程序后才可以产生可以运行程序 优点： 开源，可修改源代码 可自由选择所需要的功能 编译安装，更加适合自己的系统，更加稳定效率更高 卸载方便也更加干净 缺点： 安装步骤过多，容易出现错误 编译过程较长，安装比二进制包安装时间长 二进制包：二进制包里面包括了已经经过编译，可以马上运行的程序。你只需要下载和解包（安装）它们以后，就马上可以使用 优点： 包管理系统简单，只需要几个命令即可实现安装，卸载等等 安装速度比源码包安装快很多 缺点： 不是开源的，看不到源代码 功能选择不如源代码灵活 安装软件包需要依赖于其他的软件包","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"软件包管理","slug":"软件包管理","permalink":"https://aquapluto.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86/"}]},{"title":"文本管理工具","slug":"Linux/basics/text-command","date":"2025-08-21T02:44:27.000Z","updated":"2025-08-28T12:33:05.260Z","comments":true,"path":"Linux/basics/text-command/","permalink":"https://aquapluto.github.io/Linux/basics/text-command/","excerpt":"","text":"1 文本编译工具vim常用选项 1234567+N #打开文件后让光标处于第N行的行首，+默认行尾+/PATTERN #让光标处于第一个被PATTERN匹配到的行行首-b file #二进制方式打开文件-d file1 file2… #比较多个文件，相当于 vimdiff-m file #只读打开文件-e file #直接进入ex模式，相当于执行ex file-y file #Easy mode (like &quot;evim&quot;, modeless)，直接可以操作文件，ctrl+o:wq|q! 保存和不保存退出 范例 1234[root@ubuntu2204 ~]# vim +3 f1.txt #光标在第3行[root@ubuntu2204 ~]# vim + f1.txt #光标在最后一行[root@ubuntu2204 ~]# vim +/^abc f2.txt #模式匹配定位[root@ubuntu2204 ~]# vim +/^bc* f2.txt #模式匹配定位 1.1 三种模式和切换三种模式 命令模式：实现移动光标，剪切或粘贴文本 插入模式：修改文本 扩展命令模式：保存，退出等 切换模式 命令模式–&gt;插入模式 123456i insert，在光标所在处输入I 在当前光标所在行的行首输入a 在光标所在处后面输入A 在当前光标所在行的行尾输入o 在当前光标所在行的下方打开一个新行O 在当前光标所在行的上方打开一个新行 插入模式 — ESC—–&gt; 命令模式 命令模式 —- : —-&gt; 扩展命令模式 扩展命令模式 —-ESC,enter—-&gt; 命令模式 范例: 插入颜色字符 123456789101112131 切换至插入模式2 按ctrl+v+[ 三个键,显示^[3 后续输入颜色信息,如:^[[32mhello^[[0m4 切换至扩展命令模式,保存退出5 cat 文件可以看到下面显示[root@centos7 ~]#vim color.txt^[[1;31mHello,vim^[[0m^[[1;32mGreen^[[0m[root@centos7 ~]#cat color.txt Hello,vimGreen 1.2 扩展命令模式1.2.1 基本命令12345678910w #写（存）磁盘文件wq #写入并退出x #写入并退出X #加密q #退出q！ #不存盘退出，即使更改都将丢失r #filename 读文件内容到当前文件中w #filename 将当前文件内容写入另一个文件!command #执行命令r!command #读入命令的输出 1.2.2 地址定界格式：:start_pos,end_pos CMD 地址定界格式 123456789101112# #具体第#行，例如2表示第2行#,# #从左侧#表示起始行，到右侧#表示结尾行#,+# #从左侧#表示的起始行，加上右侧#表示的行数，范例：2,+3 表示2到5行. #当前行$ #最后一行.,$-1 #当前行到倒数第二行% #全文, 相当于1,$/pattern/ #从当前行向下查找，直到匹配pattern的第一行,即:正则表达式/pat1/,/pat2/ #从第一次被pat1模式匹配到的行开始，一直到第一次被pat2匹配到的行结束#,/pat/ #从指定行开始，一直找到第一个匹配pattern的行结束/pat/,$ #向下找到第一个匹配patttern的行到整个文件的结尾的所有行 编辑命令 123456d #删除，3,6d：删掉第三行到第六行的内容；%d：全文删除y #复制，.,$y：复制当前行到最后一行的内容，p按键粘贴w file #将范围内的行另存至指定文件中r file #在指定位置插入指定文件中的所有内容t #行号，将前面指定的行复制到#行后m #行号，将前面指定的行移动到#行后 范例 123456789:2d #删除第2行:2,4d #删除第2到第4行:2;+3y #复制第2到第5行，总共4行:3;+4w test #将第2到第5行，总共4行内容写入新文件:5r /etc/issue #将/etc/issue 文件读取到第5行:t2 #将光标所在行复制到第2行:2;+3t10 #将第2到第5行，总共4行内容复制到第10行之后:.d #删除光标所在行:$y #复制最后一行 1.2.3 查找并替换格式 12:s/要查找的内容/替换为的内容/修饰符:%s 表示全文查找替换 说明 12要查找的内容：可使用基本正则表达式模式替换为的内容：不能使用模式，但可以使用\\1, \\2, ...等后向引用符号；还可以使用“&amp;”引用前面查找时查找到的整个内容 修饰符 1234不添加 #如果一行中有多个相同的字符串，只会替换第一个i #忽略大小写g #全局替换，默认情况下，每一行只替换第一次出现gc #全局替换，每次替换前询问 范例 12%s/root/ROOT #在全文查找root并替换为ROOT，但只会替换第一个（懒惰模式）%s/root/ROOT/g #在全文查找root并替换为ROOT，全部root都替换（贪婪模式） 查找替换中的分隔符&#x2F;可替换为其它字符，如：#,@ 12s@/etc@/var@gs#/boot#/#i 1.3 定制vim的工作特性扩展命令模式的配置只是对当前vim进程有效，可将配置存放在文件中持久保存 配置文件 12/etc/vimrc #全局~/.vimrc #个人 1.3.1 行号12显示：set number，简写 set nu取消显示：set nonumber, 简写 set nonu 1.3.2 忽略字符大小写12启用：set ignorecase，简写 set ic不忽略：set noic 1.3.3 自动缩进12启用：set autoindent，简写 set ai禁用：set noai 1.3.4 复制保留格式12启用：set paste禁用：set nopaste 1.3.5 显示Tab ^I和换行符 和$显示有些文件是不支持tab键 12启用：set list禁用：set nolist 1.3.6 高亮搜索12启用：set hlsearch禁用：set nohlsearch 简写：nohl 1.3.7 语法高亮12启用：syntax on禁用：syntax off 1.3.8 文件格式123启用windows格式：set fileformat=dos启用unix格式：set fileformat=unix简写 set ff=dos|unix 1.3.9 Tab 用空格代替123启用：set expandtab 默认为8个空格代替Tab禁用：set noexpandtab简写：set et 1.3.10 Tab用指定空格的个数代替12启用：set tabstop=# 指定#个空格代替Tab简写：set ts=4 1.3.11 设置缩进宽度1234#向右缩进 命令模式&gt;&gt;#向左缩进 命令模式&lt;&lt;#设置缩进为4个字符set shiftwidth=4 1.3.12 设置文本宽度12set textwidth=65 (vim only) #从左向右计数set wrapmargin=15 #从右到左计数 1.3.13 设置光标所在行的标识线12启用：set cursorline，简写 set cul禁用：set nocursorline 1.3.14 加密12启用： set key=password禁用： set key= 1.4 命令模式1.4.1 退出VIM12ZZ 保存退出ZQ 不保存退出 1.4.2 光标跳转字符间跳转 12345h: 左L: 右j: 下k: 上#COMMAND：跳转由#指定的个数的字符 单词间跳转 1234w：下一个单词的词首e：当前或下一单词的词尾b：当前或前一个单词的词首#COMMAND：由#指定一次跳转的单词数 当前页跳转 123456H：页首 M：页中间行 L：页底zt：将光标所在当前行移到屏幕顶端zz：将光标所在当前行移到屏幕中间zb：将光标所在当前行移到屏幕底端 行首行尾跳转 123^ 跳转至行首的第一个非空白字符0 跳转至行首$ 跳转至行尾 行间移动 1234#G 或者扩展命令模式下:# 跳转至由第#行G 最后一行1G, gg 第一行 句间移动 12) 下一句( 上一句 段落间移动 12&#125; 下一段&#123; 上一段 命令模式翻屏操作 1234Ctrl+f 向文件尾部翻一屏,相当于PagedownCtrl+b 向文件首部翻一屏,相当于PageupCtrl+d 向文件尾部翻半屏Ctrl+u 向文件首部翻半屏 1.4.3 字符编辑12345x 剪切光标处的字符#x 剪切光标处起始的#个字符xp 交换光标所在处的字符及其后面字符的位置~ 转换大小写J 删除当前行后的换行符 1.4.4 替换命令12r 只替换光标所在处的一个字符R 切换成REPLACE模式（在末行出现-- REPLACE -- 提示）,按ESC回到命令模式 1.4.5 删除命令1234567891011d 删除命令，可结合光标跳转字符，实现范围删除d$ 删除到行尾d^ 删除到非空行首d0 删除到行首dwdedb#COMMANDdd： 剪切光标所在的行#dd 多行删除D：从当前光标位置一直删除到行尾，等同于d$ 1.4.6 复制命令1234567891011y 复制，行为相似于d命令y$y0y^yeywyb#COMMANDyy：复制行#yy 复制多行Y：复制整行 1.4.7 粘贴命令12p 缓冲区存的如果为整行，则粘贴当前光标所在行的下方；否则，则粘贴至当前光标所在处的后面P 缓冲区存的如果为整行，则粘贴当前光标所在行的上方；否则，则粘贴至当前光标所在处的前面 1.4.8 改变命令命令 c 删除后切换成插入模式 12345678910c$c^c0cbcecw#COMMANDcc #删除当前行并输入新内容，相当于S#cc C #删除当前光标到行尾，并切换成插入模式,相当于c$ 1.4.9 查找1234/PATTERN：从当前光标所在处向文件尾部查找?PATTERN：从当前光标所在处向文件首部查找n：与命令同方向N：与命令反方向 1.4.10 撤消更改123456u 撤销最近的更改，相当于windows中ctrl+z#u 撤销之前多次更改U 撤消光标落在这行后所有此行的更改Ctrl-r 重做最后的“撤消”更改，相当于windows中crtl+y. 重复前一个操作#. 重复前一个操作#次 1.4.11 高级用法格式 12345678910111213&lt;start position&gt;&lt;command&gt;&lt;end positi#常见Commandy 复制、d 删除、gU 变大写、gu 变小写0y$ 命令0 → 先到行头y → 从这里开始拷贝$ → 拷贝到本行最后一个字符di&quot; 光标在” “之间，则删除” “之间的内容yi( 光标在()之间，则复制()之间的内容vi[ 光标在[]之间，则选中[]之间的内容dtx 删除字符直到遇见光标之后的第一个 x 字符ytx 复制字符直到遇见光标之后的第一个 x 字符 范例：粘贴“wang”100次 1100iwang [ESC] 1.5 可视化模式在末行有”– VISUAL – “指示，表示在可视化模式 允许选择的文本块 v 面向字符，– VISUAL – V 面向整行，– VISUAL LINE – ctrl-v 面向块，– VISUAL BLOCK – 可视化键可用于与移动键结合使用 w ) } 箭头等突出显示的文字可被删除，复制，变更，过滤，搜索，替换等 范例：在文件指定行的行首插入# 1234561、先将光标移动到指定的第一行的行首2、输入ctrl+v 进入可视化模式3、向下移动光标，选中希望操作的每一行的第一个字符4、输入大写字母 I 切换至插入模式5、输入 #6、按 ESC 键 范例：在指定的块位置插入相同的内容 123451、光标定位到要操作的地方2、CTRL+v 进入“可视块”模式，选取这一列操作多少行3、SHIFT+i(I)4、输入要插入的内容5、按 ESC 键 1.6 多文件模式12345678vim FILE1 FILE2 FILE3 ...:next 下一个:prev 前一个:first 第一个:last 最后一个:wall 保存所有:qall 不保存退出所有:wqall保存退出所有 1.7 多窗口模式1.7.1 多文件分割12345vim -o|-O FILE1 FILE2 ...-o: 水平或上下分割-O: 垂直或左右分割（vim only）在窗口间切换：Ctrl+w, Arrow：wqall：退出 1.7.2 单文件窗口分割12345Ctrl+w,s：split, 水平分割，上下分屏Ctrl+w,v：vertical, 垂直分割，左右分屏ctrl+w,q：取消相邻窗口ctrl+w,o：取消全部窗口:wqall 退出 1.8 文件恢复当我们正在编辑vim文件时，操作被中断，如死机、断电等等，我们编辑的文件就会丢失，可以看到目录中的文件，多出了一个swp文件，这个文件就保存了我们没有保存的信息，还有另一种情况也可能出现这种情况：其他用户正在编辑这个文件，可以通知其他用户，停止编辑，swp文件就会消失。 12345O #只读方式打开E #直接编辑，不会载入暂存文件，这样做可能会导致多个用户操作冲突R #恢复文件，即使使用R选项恢复了文件，swp文件也不会自动删除，需要我们手动进行删除，否则下次编辑还会出现Q #离开Vim不做任何操作A #忽略这个编辑行为，回到命令行，几乎和Q的作用相同 2 文本内容处理命令2.1 内容直接查看2.1.1 cat格式 1234567cat [option] file-A 显示所有控制符-E：显示行结束符$-n 加行号-b 非空行加行号-s 压缩连续的空行为一行 范例 123456789101112cat /proc/cpuinfo #显示CPU info的信息cat /proc/interrupts #显示中断cat /proc/meminfo #校验内存使用cat /proc/swaps #显示哪些swap被使用cat /proc/version #显示内核的版本cat /proc/net/dev #显示网络适配器及统计cat /proc/mounts #显示已加载的文件系统cat /proc/cpuinfo |grep &quot;cpu cores&quot; |uniq #查看核数cat /proc/cpuinfo |grep &quot;processor&quot; |wc -l #查看逻辑cpu个数cat /proc/cpuinfo |grep &quot;physical id&quot; |sort |uniq |wc -l #查看物理CPU个数cat /sys/class/fc_host/host1/supported_speeds #查看HBA卡cat /proc/cpuinfo |grep MHz|uniq #查看CPU的主频 2.1.2 nl显示行号，相当于cat -b 123456789[root@centos8 ~]#nl /data/f1.txt 1 a 2 b 3 c 4 d 5 e 6 f 7 g 8 h 2.1.3 tac逆向显示文本内容 123456789101112[root@centos8 ~]#cat /data/fa.txt12345[root@centos8 ~]#tac /data/fa.txt54321 2.1.4 rev将同一行的内容逆向显示 123456789101112[root@centos8 ~]#cat /data/fa.txt1 2 3 4 5a b c[root@centos8 ~]#tac /data/fa.txta b c1 2 3 4 5[root@centos8 ~]#rev /data/fa.txt5 4 3 2 1c b a[root@centos8 ~]#echo &#123;1..10&#125; |rev01 9 8 7 6 5 4 3 2 1 2.1.5 hexdump查看非文本文件内容 123-C 每个字节显示为16进制和相应的ASCII字符-n 只显示前n个长度的字符-s 从偏移量开始输出 范例 12hexdump -C -n 512 /dev/sda00000000 eb 63 90 10 8e d0 bc 00 b0 b8 00 00 8e d8 8e c0 |.c..............| 范例：跳过前735个字节,观察后面30个字节 1234[root@centos8 ~]#hexdump -s 735 -Cn 30 /bin/ls000002df 00 05 6d da 3f 1b 77 91 91 63 a7 de 55 63 a2 b9 |..m.?.w..c..Uc..|000002ef d9 d2 45 55 4c 00 00 00 00 03 00 00 00 7d |..EUL........&#125;|000002fd 2.2 内容分页查看2.2.1 more可以实现分页查看文件，可以配合管道实现输出信息的分页 123456more [OPTIONS...] FILE...-d: 显示翻页及退出提示空格键 一页一页往下翻q 退出（或者翻到底自动退出）Ctrl b 往回翻页 2.2.2 less可配合管道实现输出信息的分页 翻到底不会自动退出 12/文本 #搜索文本n/N #跳到下一个 或 上一个匹配 2.3 内容分行查看2.3.1 head可配合管道，可以显示文件或标准输入的前面行（默认前十行） 12345head [OPTION]... [FILE]...-c # 指定获取前#字节-n # 指定获取前#行,#如果为负数,表示不要倒数#行，即文件头取到倒数第#-1行-# 同上 2.3.2 tail可配合管道，可以显示文件或标准输入的倒数行 1234567tail [OPTION]... [FILE]...-c # 指定获取后#字节-n # 指定获取后#行,如果#为正数，显示不要前#-1行，即第#行取到文件尾-# 同上-f 跟踪显示文件fd新追加的内容,常用日志监控(/var/log/messages)，当文件删除再新建同名文件,将无法继续跟踪文件-F 跟踪文件名，当文件删除再新建同名文件,将可以继续跟踪文件 范例：查看最新发生的日志 12[root@centos8 ~]#tail -fn0 /var/log/messages[root@centos8 ~]#tail -0f /var/log/messages 2.4 合并多个文件内容2.4.1 cat123456789101112131415161718192021222324252627282930cat file1 file2（竖着合并）[root@centos8 ~]#cat alpha.logabcdefgh[root@centos8 ~]#cat seq.log12345[root@centos8 ~]#cat alpha.log seq.logabcdefgh12345 2.4.2 pastepaste 合并多个文件同行号的列到一行 1234paste [OPTION]... [FILE]...-d #分隔符：指定分隔符，默认用TAB-s #所有行合成一行显示 范例 123456789101112131415161718192021222324252627282930[root@centos8 ~]#paste alpha.log seq.loga 1b 2c 3d 4e 5fgh[root@centos8 ~]#paste -d&quot;:&quot; alpha.log seq.loga:1b:2c:3d:4e:5f:g:h:[root@centos8 ~]#paste -s seq.log1 2 3 4 5[root@centos8 ~]#paste -s alpha.loga b c d e f g h[root@centos8 ~]#paste -s alpha.log seq.loga b c d e f g h1 2 3 4 5[root@centos8 ~]#paste -s -d: f1.log f2.log1:2:3:4:5:6:7:8:9:10a:b:c:d:e:f:g:h:i:j 范例: 批量修改密码 12345678910[root@centos8 ~]#cat user.txtwangmage[root@centos8 ~]#cat pass.txt123456magedu[root@centos8 ~]#paste -d: user.txt pass.txtwang:123456mage:magedu[root@centos8 ~]#paste -d: user.txt pass.txt|chpasswd 2.5 分析文本2.5.1 收集文本统计数据 wc统计文件的行总数，单词总数，字节总数和字符总数 1234-l #只计行数-w #只计单词数-c #只计字节数-m #只计字符数 2.5.2 文本排序 sort文本排序，不改变原始文件 1234567sort [options] file(s)-r 反方向排序-n 按数字大小排序-t c 使用c作为分隔符-k # 按照使用c作为分隔符分隔的#列排序 -u 合并重复项，去重 范例：统计日志访问量 12[root@centos8 data]#cut -d&quot; &quot; -f1 /var/log/nginx/access_log |sort -u|wc -l201 范例：统计分区利用率 1[root@centos8 ~]#df | tr -s &quot; &quot; %|cut -d% -f5|tr -d &#x27;[:alpha:]&#x27; | sort -n 2.5.3 uniq 去重123-c 显示每行重复的次数-d 显示重复过的行-u 显示不曾重复过的行 范例：统计日志访问量最多的请求 12345678[root@centos8 data]#cut -d&quot; &quot; -f1 access_log |sort |uniq -c|sort -nr |head -3 4870 172.20.116.228 3429 172.20.116.208 2834 172.20.0.222[root@centos8 data]#lastb -f btmp-34 | tr -s &#x27; &#x27; |cut -d &#x27; &#x27; -f3|sort |uniq -c |sort -nr | head -3 86294 58.218.92.37 43148 58.218.92.26 18036 112.85.42.201 范例：并发连接最多的远程主机IP 123[root@centos8 ~]#ss -nt|tail -n+2 |tr -s &#x27; &#x27; : |cut -d: -f6|sort|uniq -c|sort -nr |head -n2 7 10.0.0.1 2 10.0.0.7 范例：取两个文件的相同和不同的行 1234567891011121314151617181920212223[root@centos8 data]#cat test1.txtab1c[root@centos8 data]#cat test2.txtbefc12#取文件的共同行[root@centos8 data]#cat test1.txt test2.txt | sort |uniq -d1bc#取文件的不同行[root@centos8 data]#cat test1.txt test2.txt | sort |uniq -u2aef 2.6 比较不同文件内容2.6.1 diff比较两个文件的区别 范例：知道一个文件怎么操作可以和另一个文件相同（-u） 123456789101112131415161718192021222324252627282930313233343536373839404142[20:01:23 root@10 data[]#cat a.txtabcd[20:02:03 root@10 data[]#cat b.txtaefdgh[20:02:08 root@10 data[]#diff a.txt b.txt2,3c2,3&lt; b&lt; c---&gt; e&gt; f4a5,6&gt; g&gt; h#&lt;代表a.txt ；&gt;代表b.txt ；2,3;5,6表示第二行到第三行，第五行到第六行不一样[20:02:18 root@10 data[]#diff -u a.txt b.txt--- a.txt 2023-08-10 20:01:15.394938187 +0800+++ b.txt 2023-08-10 20:02:03.802934670 +0800@@ -1,4 +1,6 @@ a-b-c+e+f d+g+h#-1,4表示a.txt的第一行到第四行；+1,6表示b.txt的第一行到第六行#a；d：表示两文件都有a，d#-b，-c，+e，+f，+g，+h：表示a.txt减b和c，加efgh后可以和b.txt一样 2.6.2 patch复制在其它文件中进行的改变（要谨慎使用） 1-b 选项来自动备份改变了的文件 范例：找回文件 前提 和diff相结合，可以将两个不同的文件直接用来生成一个新的被误删除的文件（ab.log），如果不小心把b.txt删了，可以结合a.txt和ab.txt把b.txt找回来 找回来后b.txt的内容会覆盖到a.txt上，加一个-b，可以给a.txt做备份，所以最后a.txt是原来b.txt的内容，a.txt.orig是原来a.txt的内容 123456789101112131415161718[20:06:59 root@10 data[]#diff -u a.txt b.txt &gt; ab.log[20:18:57 root@10 data[]#rm -f b.txt[20:21:25 root@10 data[]#patch -b a.txt ab.logpatching file a.txt[20:21:30 root@10 data[]#cat a.txtaefdgh[20:21:37 root@10 data[]#lsab.log a.txt a.txt.orig[20:21:40 root@10 data[]#cat a.txt.orig abcd 2.6.3 vimdiff相当于 vim -d 更加直观看到两个文件的区别，也可以去改 1vimdiff file1 file2 2.6.4 cmp范例：查看二进制文件的不同 12[root@centos8 ~]#cmp /bin/dir /bin/ls/bin/dir /bin/ls differ: byte 737, line 2 2.7 按列抽取文本cut12345678910cut [OPTION]... [FILE]...-d DELIMITER: 指明分隔符，默认tab-f FILEDS: #: 第#个字段,例如:3 #,#[,#]：离散的多个字段，例如:1,3,6 #-#：连续的多个字段, 例如:1-6 混合使用：1-3,7-c 按字符切割--output-delimiter=STRING指定输出分隔符 范例：以 : 为分隔符，取第1和第3列 1[root@centos8 ~]#cut -d: -f1,3 /etc/passwd 范例: 取分区利用率 123[root@centos8 ~]#df | cut -c 44-46|tail -n +2[root@centos8 ~]#df | tail -n +2|tr -s &#x27; &#x27; % |cut -d% -f5[root@centos8 ~]#df | tail -n +2|tr -s &#x27; &#x27; |cut -d&#x27; &#x27; -f5 |tr -d % 2.8 转换或删除字符tr它支持标准输入，它默认读取键盘输入的内容，做一些相关处理的转换，它可以读取标准输入，做转换，压缩，删除等等（一一对应），处理完还会在屏幕上继续把结果输出出来 12345678910111213141516171819202122232425262728293031tr [OPTION]... SET1 [SET2]#常用选项-d #删除-s #压缩-c #用SET2替换SET1中没有包含的字符-t #将SET1用SET2替换，SET2中不够的，就不处理#常用通配符[:alnum:] #字母和数字[:alpha:] #字母[:digit:] #数字[:lower:] #小写字母[:upper:] #大写字母[:space:] #空白字符[:print:] #可打印字符[:punct:] #标点符号[:graph:] #图形字符[:cntrl:] #控制（非打印）字符[:xdigit:] #十六进制字符#常用符号\\NNN #具有八进制值 NNN 的字符（1 到 3 个八进制数字）\\\\ #反斜杠\\a #发出警告声\\b #退格键\\f #form feed\\n #换行\\r #回车，即光标移至行首，但不换行\\t #插入tab\\v #垂直tab 范例 1234#把/etc/issue中的小写字符都转换成大写字符tr ‘a-z’ ‘A-Z’&lt; /etc/issue#删除fstab文件中的所有abc中任意字符tr –d abc &lt; /etc/fstab 范例 12345678910tr &#x27;a-z&#x27; &#x27;A-Z&#x27;（把a-z小写转换为A-Z大写）tr &#x27;abc&#x27; &#x27;xyz&#x27;（a转x，b转y，c转z）tr -d &#x27;\\n&#x27; &#x27; &#x27;（将换行转换为一个空格）tr -d &#x27;abc&#x27;（把abc删除）tr -d &#x27;\\n&#x27;（把换行删除）tr -s &#x27;abc&#x27;（把abc压缩，压缩只压相邻的）tr -s &#x27; &#x27;（压缩成为只有一个空格）tr -s &#x27; &#x27; +（压缩成为只有一个空格后，将空格转换为+） 范例 123456789#非123就替换成x[root@ubuntu2204 ~]# tr -c 123 x1357913xxxx#非2-5的内容替换成 x[root@ubuntu2204 ~]# tr -c &#x27;2-5&#x27; x123456789x2345xxxxx 范例：与管道符相应用 1234echo hallboc | tr &#x27;abc&#x27; &#x27;xyz&#x27;（输出结果为hxllyoz)echo hallboc | tr -d &#x27;abc&#x27;（输出结果为hllo)echo aaabbbccccc | tr -s &#x27;abc&#x27;（输出结果为abc)tr &#x27;\\n&#x27; &#x27; &#x27; | tr &#x27; &#x27; &#x27;\\n&#x27;（开始是换行转空格，接着空格转换行） 范例 1234567891011121314151617181920212223242526[lsc@localhost ~]$ cat test.shI love cats.I love rabbits.I love camels.#SET1多出来的字符映射为SET2最后一个字符[lsc@localhost ~]$ tr cat do &lt; test.shI love doos.I love robbios.I love domels.#SET1中有重复字符 —— 以SET2最后一次映射的为准[lsc@localhost ~]$ tr caa dog &lt; test.shI love dgts.I love rgbbits.I love dgmels.#-c选项的SET2只有最后一个字符是有效的，会将文本中除了SET1的所有字符替换为SET2的最后一个字符，这里包括文本最后的一个换行符，所以会发现Linux控制台前缀会出现在输出结果的后面，而不是另起一行[lsc@localhost ~]$ tr -c cat dog &lt; test.shgggggggcatgggggggggggagggtggggggggggcagggggg[lsc@localhost ~]$#[:alnum:]是SET1，即需要包含的内容，因为没有SET2，有个-d，所以就把不包含在SER1中的内容删掉[root@ubuntu2004 ~]#tr -dc dog &lt; test.txtooo[root@ubuntu2004 ~]#[root@ubuntu2004 ~]#tr -dc &#x27;[:alnum:]&#x27; &lt; test.txtIlovecatsIloverabbitsIlovecamels 2.9 双向输出tee利用 tee 命令可以重定向到多个目标，经常配合管道符一起使用 12命令1 | tee [-a ] 文件名 | 命令2 #把命令1的STDOUT保存在文件中，做为命令2的输入-a 追加，不会覆盖原来的内容 功能： 保存不同阶段的输出 复杂管道的故障排除 同时查看和记录输出 范例 123456789[root@centos7 ~]#tee c.logaabb^C[root@centos7 ~]#cat c.log ab 范例 123456789101112131415[root@centos7 ~]#echo -e &#x27;line1\\nline2\\nline3&#x27; | tee d.logline1line2line3[root@centos7 ~]#cat d.logline1line2line3[root@centos8 ~]#cat &lt;&lt;EOF | tee /etc/motd&gt; welcome to magedu&gt; happy new year&gt; EOFwelcome to mageduhappy new year","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"文件管理工具","slug":"Linux/basics/file-command","date":"2025-08-21T02:44:22.000Z","updated":"2025-09-06T10:55:56.702Z","comments":true,"path":"Linux/basics/file-command/","permalink":"https://aquapluto.github.io/Linux/basics/file-command/","excerpt":"","text":"1 文件通配符匹配文件名的字符串 用来匹配符合条件的多个文件，方便批量管理文件 通配符采用特定的符号，表示特定的含义，此特定符号称为元 meta 字符 常见通配符 12345* #匹配零个或多个字符，不匹配 “.” 开头的文件，即隐藏文件？ #匹配任何单个字符，一个汉字也算一个字符~ #当前用户家目录** #匹配后代所有子目录[] #只会匹配[]中的一个字符，不可以为空 范例 12ls *.conf #把所有以.conf结尾的文件都输出ls * con* #把中间包含con的文件都输出 范例 123ls ??? #显示三个字符的文件，但是会显示文件夹）ls ??? -d #显示三个字符的文件，不会显示文件夹）ls ???.conf #显示三个字符，以.conf结尾的文件） 范例 12ls ~ #显示家目录ls ~wu #显示wu家（别人家）目录，如果都是隐藏文件，后面加个-a，即ls ~wu -a 范例 123456#这样子写会认为是c.* g.* f.*[root@centos7 etc]#ls [cfg].*ls: cannot access [cfg].*: No such file or directory[root@centos7 etc]#ls [cgf]*.*chrony.conf chrony.keys cron.deny csh.cshrc csh.login favicon.png fstab.log fuse.conf grub2.cfg 范例：匹配etc目录及其所有子目录下的所有以c开头的文件 1[root@centos7 ~]#ls /etc/**/c* [] 匹配字符 123456789101112131415161718[0-9] #匹配数字范围[a-z] #一个小写字母[A-Z] #一个大写字母[wang] #匹配列表中的任何的一个字符[^wang] #匹配列表中的所有字符以外的字符[^a-z] #匹配列表中的所有字符以外的字符[:digit:] #任意数字，相当于0-9[:lower:] #任意小写字母,表示 a-z[:upper:] #任意大写字母,表示 A-Z[:alpha:] #任意大小写字母[:alnum:] #任意数字或字母[:blank:] #水平空白字符[:space:] #水平或垂直空白字符[:punct:] #标点符号[:print:] #可打印字符[:cntrl:] #控制（非打印）字符[:graph:] #图形字符[:xdigit:] #十六进制字符 特殊字符之 “-” 1234567#减号用在字符集“[…]”里表示一组字符“[1-3]” —— 表示1到5中的任意一个字符，所以“&lt;H[1-3]&gt;”表示“&lt;H1&gt;”、“&lt;H2&gt;”或者“&lt;H3&gt;”“[d-g]” —— 表示“d”、“e”、“f”或者“g”#如果不是用在字符集“[…]”里，就是普通的字符，即减号“1-[1-3]” —— 表示“1-1”、“1-2”或者“1-3”#但是，即使在字符集“[…]”里，却非连续字符之间，也失去了特殊含义“1[-1]” —— 表示“1-”或者“11” 范例： 1234567891011121314#先创建一个文件 touch /data/&#123;a..z&#125;.txt ls /data/[wang].txt #输出w,a,n,g字符开头的文件ls /data/[c-h].txt #输出c,d,e,f,g,h字符开头的文件ls /data/[ ^wang].txt #输出除了w,a,n,g字符开头的所有文件#然后接着创建touch /data/&#123;0..9&#125;.txtls /data/[wang1-3].txt #输出1,2,3,w,a,n,g字符开头的文件#最后创建touch /data/&#123;A..Z&#125;.txtls /data/[a-d].txt #输出a,A,b,B,c,C,d字符开头的文件ls /data/[[:lower:]].txt #输出所有小写字母开头的文件ls /data/[[:upper:]].txt #输出所有大写字母开头的文件ls /data/[[:alpha:]].txt #输出所有大小写字母开头的文件ls /data/[[:alnum:]].txt #输出所有字母和数字开头的文件 范例：比较有无 * 的功能区别 12345ls * #等同于 ls 列出当前目录内容，不包括 . 和 ..ls -a * #等同于 ls 列出当前目录内容，不包括 . 和 ..ls -a #列出当前目录所有内容，包括 . 和 ..ls .* #列出当前目录和上级目录的内容， .* 包括 .. 也就是上级目录 ls -d .* #只显示当前目录的内容，有预定的别名 l. 范例 123456789101112[root@centos7 data]#ls -a . .. a2.txt ab.log a.txt.orig backup2023-08-16 scripts[root@centos7 data]#ls -a *a2.txt ab.log a.txt.origbackup2023-08-16:. .. etcscripts:. chess.sh date.sh hello.sh ip.sh nginx.sh num_sort.sh sum.sh text3.sh t.sh user.sh.. color.sh first.sh hosts.txt nginx_install.sh num_add.sh ping.sh text2.sh text.sh useradd.sh var.sh 范例 123456[root@centos8 data]#touch file*.log[root@centos8 data]#touch file1.log[root@centos8 data]#ls file*.logfile1.log &#x27;file*.log&#x27;[root@centos8 data]#ls &#x27;file*.log&#x27;&#x27;file*.log&#x27; 2 CRUD文件和目录2.1 目录操作2.1.1 显示当前工作目录每个shell和系统进程都有一个当前的工作目录 CWD：current work directory 显示当前shell CWD的绝对路径 pwd命令: printing working directory -P 显示真实物理路径 -L 显示链接路径（默认） 12345#当前目录pwd#上一次目录oldpwd 2.1.2 绝对和相对路径（1）绝对路径 以&#x2F;即根目录开始，打出完整的文件位置路径 完整的文件的位置路径 可用于任何想指定一个文件名的时候 12[root@centos7 ~]#ls /etc/sysconfig/network-scripts/ifcfg-eth0/etc/sysconfig/network-scripts/ifcfg-eth0 （2）相对路径 不以&#x2F;开头，先以cd切换到了一个目录，再想访问其目录下的子目录的话，可以直接打出文件名，相对于当前目录的路径 123[root@centos7 ~]#cd /etc/sysconfig/[root@centos7 sysconfig]#ls network-scripts/ifcfg-eth0network-scripts/ifcfg-eth0 （3）basename 只取文件名不要路径 12[root@centos7 sysconfig]#basename /etc/sysconfig/network-scripts/ifcfg-eth0ifcfg-eth0 （4）dirname 只取路径不要文件名 12[root@centos7 sysconfig]#dirname /etc/sysconfig/network-scripts/ifcfg-eth0/etc/sysconfig/network-scripts 2.1.3 更改目录12345678910cd [-L|[-P [-e]] [-@]] [dir]#常用选项-L #切换至链接目录，默认选项-P #切换至真实目录，而非链接目录cd .. #切换至根目录cd - #切换到上一个目录cd | cd ~ #切换至当前用户家目录cd ~username #切换至指定用户家目录cd path #切换到指定目录 2.1.4 列出目录内容1234567891011121314151617181920ls （简略列出）-l #显示除文件名外的大小属性权限等，相当于ll-a #显示隐藏文件 -d #查看目录属性-S #按从大到小排序-R #目录递归-t #按mtime排序，时间新的靠前-u #配合-t选项，显示并按atime从新到旧排序-U #按目录存放顺序显示-X #按文件后缀排序-F #对不同类型文件显示时附加不同的符号：*/=&gt;@|-C #文件多时，以多列的方式显示文件，默认是一列（标准输出）-l --time=atime #atime是指access(使用) time，读取文件内容就会更新这个时间-l --time=ctime #ctime是指change(改变) time，元数据发生改变就会更改这个时间-l --time=mtime #mtime是指modify(修改) time，改变文件内容或者数据就会更改这个时间ll （详细列出）-a #显示隐藏文件 -d #查看目录属性!* #快速打上一条命令的后面部分 说明： ls 查看不同后缀文件时的颜色由 &#x2F;etc&#x2F;DIR_COLORS 和@LS_COLORS变量定义 ls -l 看到文件的大小，不一定是实际文件真正占用空间的大小 ll 是 ls命令的一个别名，在centos 和 ubuntu 系统中，该别名的参数不一样 12345[root@rocky86 ~]# alias llalias ll=&#x27;ls -l --color=auto&#x27;root@ubuntu20:~# alias llalias ll=&#x27;ls -alF&#x27; 范例 12345678910111213141516[root@centos8 ~]#vim /etc/DIR_COLORS.jpg 01;31 #修改此行[root@centos8 ~]#exit[root@centos8 ~]#echo $LS_COLORS....#ubuntu 中无此文件[root@ubuntu2204 ~]# ls /etc/DIR_COLORSls: cannot access &#x27;/etc/DIR_COLORS&#x27;: No such file or directory#ubuntu 中的颜色是在 dircolors 命令中定义的[root@ubuntu2204 ~]# dircolors -p...#在ubuntu 中定义颜色，类似于别名定义，加在 .bashrc 中即可 2.1.5 查看目录结构12345678910111213141516171819tree [-acdfghilnpqrstuvxACDFJQNSUX] [-H baseHREF] [-T title ]-a #显示所有，包括隐藏目录和文件-d #只显示目录-f #显示所有内容的完整路径-F #在执行文件，目录，Socket，符号链接，管道文件，各自加上&quot;*&quot;,&quot;/&quot;,&quot;=&quot;,&quot;@&quot;,&quot;|&quot;号-g #显示文件属组，没找到组名，以gid代替-u #显示文件属主，没找到用户名，以uid代替-p #显示内容权限-s #显示内容大小-i #不以层级结构显示-n #不显示颜色-t #显示时用修改时间排序-r #以默认显示顺序的反向规则显示，默认以数字，首字母的顺序规则来显示-D #显示内容修改时间-C #显示色彩-L n #只显示n层目录-P pattern #只显示由指定wild-card pattern匹配到的路径-o filename #将显示的内容输出到指定文件中 范例 1234567#显示当前目录的树[root@ubuntu2204 ~]# tree[root@ubuntu2204 ~]# tree dir1/#仅显示两层目录[root@ubuntu2204 ~]# tree -d -L 2 / 2.1.6 创建目录123456mkdir [OPTION]... DIRECTORY...#常用选项-m|--mode #目录权限属性-p|--parents #如果要创建的目录父级目录不存在，则一起创建，是递归的意思-v|--verbose #显示创建过程 范例 12345678#创建data下的文件夹dir1[root@ubuntu2204 ~]#mkdir /data/dir1 #这样子创建会失败，因为dir2还没有，dir3是不可能创建的，要想一键创建，需要-p[root@ubuntu2204 ~]#mkdir /data/dir1/dir2/dir3/dir4 #指定权限[root@ubuntu2204 ~]# mkdir -m=777 dirb 2.1.7 删除空目录rmdir只能删除空目录，如果想删除非空目录，可以使用rm -r 命令，递归删除目录树 123456rmdir [OPTION]... DIRECTORY...#常用选项--ignore-fail-on-non-empty #忽略非空错误提示-p|--parents #连着父目录一起删除-v|--verbose #显示删除过程 范例：从外层开始创建，从里层开始删除 1234567891011[root@ubuntu2204 ~]# mkdir -pv a/b/c/dmkdir: created directory &#x27;a&#x27;mkdir: created directory &#x27;a/b&#x27;mkdir: created directory &#x27;a/b/c&#x27;mkdir: created directory &#x27;a/b/c/d&#x27;[root@ubuntu2204 ~]# rmdir -pv a/b/c/drmdir: removing directory, &#x27;a/b/c/d&#x27;rmdir: removing directory, &#x27;a/b/c&#x27;rmdir: removing directory, &#x27;a/b&#x27;rmdir: removing directory, &#x27;a&#x27; 范例 12#rmdir具有局限性，只能删空目录，所以此命令只能删掉dir4，因为只有dir4是空的[root@ubuntu2204 ~]#rmdir /data/dir1/dir2/dir3/dir4 2.2 文件操作2.2.1 查看文件状态一个文件有两部份信息：元数据和具体内容 查看文件元数据 1234567stat [OPTION]... FILE...可以一键全部显示atime ctime mtime#常用选项-t|--terse #使用简洁格式显示-f|--file-system #显示相关的文件系统信息，所谓文件系统，对应的就是windows下面的硬盘分区-c|--format #使用特定格式输出 范例 1234567891011#查看文件所在分区的信息[root@ubuntu2204 ~]# stat -f /etc/fstabFile: &quot;/etc/fstab&quot;ID: 55a3efb585efc96c Namelen: 255 Type: ext2/ext3Block size: 4096 Fundamental block size: 4096Blocks: Total: 25397502 Free: 23933766 Available: 22632109Inodes: Total: 6488064 Free: 6395997#权限-inode-文件名[root@ubuntu2204 ~]# stat -c &quot;%a-%i-%n&quot; /etc/fstab644-3277488-/etc/fstab 2.2.2 判断文件是什么类型文件可以包含多种类型的数据，使用file命令检查文件的类型，然后确定适当的打开命令或应用程序使用 1234567file [options] &lt;filename&gt;...#常用选项-b|--brief #只显示结果，不显示文件名-f|--files-from FILE #从指定文件中获取要处理的文件名-F|--separator STRING #指定分割符-L|--dereference #跟随软链接 范例：windows的文本格式和Linux的文本格式的区别 1234567891011121314151617181920212223242526272829[root@ubuntu2204 ~]# cat linux.txtabc[root@ubuntu2204 ~]# cat win.txtabc[root@ubuntu2204 ~]# file linux.txt win.txtlinux.txt: ASCII textwin.txt: ASCII text, with CRLF line terminators#安装转换工具 [root@ubuntu2204 ~]# apt install dos2unix#将Windows的文本格式转换成的Linux文本格式[root@ubuntu2204 ~]# dos2unix win.txtdos2unix: converting file win.txt to Unix format...[root@ubuntu2204 ~]# file win.txtwin.txt: ASCII text#将Linux的文本格式转换成Windows的文本格式[root@ubuntu2204 ~]# unix2dos win.txtunix2dos: converting file win.txt to DOS format...[root@ubuntu2204 ~]# file win.txtwin.txt: ASCII text, with CRLF line terminators 范例：转换文件字符集编码 12345678910111213141516171819202122232425262728#显示支持字符集编码列表[root@centos8 ~]#iconv -l#windows7上文本默认的编码ANSI（GB2312）[root@centos8 data]#file windows.txtwindows.txt: ISO-8859 text, with no line terminators[root@centos8 data]#echo $LANGen_US.UTF-8#默认在linux无法正常显示文本内容[root@centos8 data]#cat windows.txt#将windows7上文本默认的编码ANSI（GB2312）转换成UTF-8[root@centos8 data]#iconv -f gb2312 windows.txt -o windows1.txt[root@centos8 data]#cat windows1.txt[root@centos8 data]#ll windows1.txt-rw-r--r-- 1 root root 12 Mar 23 10:13 windows1.txt[root@centos8 data]#file windows1.txtwindows1.txt: UTF-8 Unicode text, with no line terminators#将UTF-8转换成windows10上文本默认的编码ANSI（GB2312）[root@centos8 data]#iconv -f utf8 -t gb2312 windows1.txt -o windows2.txt[root@centos8 data]#file windows2.txtwindows2.txt: ISO-8859 text, with no line terminators 2.2.3 创建空文件和刷新时间touch命令可以用来创建空文件或刷新文件的时间，如果创建的文件已经存在，那么将会刷新三个时间戳 123456touch [OPTION]... FILE...-a #仅改变 atime和ctime-m #仅改变 mtime和ctime-t [[CC]YY]MMDDhhmm[.ss] #指定atime和mtime的时间戳-c #如果文件不存在，则不予创建 范例 1234567[root@centos8 data]#touch `date -d &quot;-1 day&quot; +%F_%T`.log[root@centos8 data]#ls2019-12-12_16:11:48.log[root@centos8 data]#touch $(date -d &quot;1 year&quot; +%F_%T).log[root@centos8 data]#ls2019-12-12_16:11:48.log 2020-12-13_16:13:11.log 2.2.4 复制文件和目录123456789101112131415cp [OPTION]... source... directorycp [OPTION]... -t DIRECTORY SOURCE...-i #如果目标已存在，覆盖前提示是否覆盖-n #不覆盖，注意两者顺序-a #保留文件所有属性，递归复制目录及内部的所有内容-b #目标存在，覆盖前先备份，默认形式为 filename~ ,只保留最近的一个备份-r #递归复制目录及内部的所有内容-v #显示拷贝后的路径描述-d #不复制原文件，只复制链接名-f #强行复制文件或目录，不论是否存在-u #只复制源比目标更新文件或目标不存在的文件，即当源文件比目标文件新时，才执行复制操作--backup=numbered #目标存在，覆盖前先备份加数字后缀，形式为 filename.~#~ ，可以保留多个版本--preserve[=ATTR_LIST] #选择属性，默认为 mode,ownership,timestamps-p #等同--preserve=mode,ownership,timestamp 1234567mode #权限ownership #属主属组timestamp #时间戳links #保留链接xattr #保留自定义属性context #保留selinux属性all #所有属性 源目标 不存在 存在且为文件 存在且为目录 一个文件 新建DEST，并将SRC中内容填充至DEST中 将SRC中的内容覆盖至DEST中，注意数据丢失风险！ 建议用 –i 选项 在DEST下新建与原文件同名的文件，并将SRC中内容填充至新文件中 多个文件 提示错误 提示错误 在DEST下新建与原文件同名的文件，并将原文件内容复制进新文件中 目录须使用-r选项 创建指定DEST同名目录，复制SRC目录中所有文件至DEST下 提示错误 在DEST下新建与原目录同名的目录，并将SRC中内容复制至新目录中 范例 12345678910111213141516171819202122232425262728#保留属主属组，时间戳[root@ubuntu2204 ~]# cp -p ~wang/issue /data/issue_wang2.bak#cp 整个目录[root@ubuntu2204 ~]# cp /etc/sysconfig/ /data/cp: -r not specified; omitting directory &#x27;/etc/sysconfig/&#x27;[root@ubuntu2204 ~]# cp -r /etc/sysconfig/ /data/#特殊文件复制，不能直接cp ,要加 a 选项[root@ubuntu2204 ~]# cp /dev/zero&#123;,.bak&#125;#保留多版本备份[root@ubuntu2204 0508]# cp --backup=numbered x.log y.log[root@ubuntu2204 0508]# lsx.log y.log y.log.~1~[root@ubuntu2204 0508]# cp --backup=numbered x.log y.log[root@ubuntu2204 0508]# lsx.log y.log y.log.~1~ y.log.~2~#如果复制目录的目录名已存在，将已复制目录的命令命名并成为该目录的子目录[root@ubuntu2004 ~]#cp -r /etc/ /opt/bak[root@ubuntu2004 ~]#ls /opt/bak[root@ubuntu2004 ~]#cp -r /etc/ /opt/bak[root@ubuntu2004 ~]#ls /opt/bak/ | grep etcetc[root@ubuntu2004 ~]#ls /opt/bak/etc...... 2.2.5 移动文件和重命名文件mv 命令可以实现文件或目录的移动和改名 同一分区移动数据，速度很快，数据位置没有变化 不同分区移动数据，速度相对慢，数据位置发生了变化 123456789101112mv [OPTION]... [-T] SOURCE DESTmv [OPTION]... SOURCE... DIRECTORYmv [OPTION]... -t DIRECTORY SOURCE...#常用选项-b #如果目标存在，则先备份-n #如果目标文件己存在，则跳过此文件移动-i #如果目标文件己存在，则提示是否覆盖-u #当源文件比目标文件新时，才执行移动操作-v #显示移动过程-f #强制-t distory #取反。当不知道要把什么文件移动时，可以加-t，就可以保证不知名文件移动到distory下 范例 1234567891011121314151617#将0.txt移动到opt下mv 0.txt /opt/ #将1.txt改名为11.txtmv 1.txt 11.txt #修改了ls的后缀（Linux对后缀不敏感）mv ls ls.mp3#将 abc目录放入 abcd 目录中，如果 abcd 目录不存在，则将 abc 目录改名为 abcd 目录[root@rocky86 ~]# mv abc/ abcd#将 abc 目录下所有内容移动到当前目录[root@rocky86 ~]# mv abc/* .#移动多个文件到目录[root@rocky86 ~]# mv a b c d abcd/ 利用 rename 可以批量修改文件名 1234567rename [options] &lt;expression&gt; &lt;replacement&gt; &lt;file&gt;...#常用选项-v|--verbose #显示过程-s|--symlink #如果目标是链接文件，则是重命名其指向-n|--no-act #不做任何改变-o|--no-overwrite #不覆盖己存在文件 范例 1234567891011121314#将所有txt后缀的文件改为txt.orig后缀 （*.txt表示在txt文件下修改）[root@rocky86 ~]#rename txt txt.orig *.txt#为所有的f开头包含conf的文件加上.bak后缀：[root@rocky86 ~]#rename &#x27;conf&#x27; &#x27;conf.bak&#x27; f*#去掉所有的bak后缀：[root@rocky86 ~]#rename &#x27;.bak&#x27; &#x27;&#x27; *.bak#将当前目录下 以.txt结尾的文件，批量改名成 .log结尾[root@rocky86 ~]# rename -v txt log *.txt#将 abc.link 指向的文件由 abc 改成 xyz[root@rocky86 ~]# rename -s abc xyz abc.link 2.2.6 删除文件使用 rm 命令可以删除文件和目录 注意：此命令非常危险，慎重使用，建议使用 mv 代替 rm 12345678rm [OPTION]... [FILE]...#常用选项-i #删除前确认-f #不确认直接删除-r|-R #递归删除-d #删除空目录--no-preserve-root #强删根目录 由于rm命令比较危险，可以设置别名，将删除的东西放进垃圾箱 1alias rm=&#x27;DIR=/data/backup`date +%F%T`;mkdir $DIR;mv -t $DIR&#x27; 或者用trash-put命令执行删除，删除的文件可以恢复，删除的文件默认在 ~&#x2F;.local&#x2F;share&#x2F;Trash&#x2F;files 范例：删除特殊文件 1234567891011121314151617181920212223242526272829303132333435[root@ubuntu2204 0508]# ls-f#删不掉[root@ubuntu2204 0508]# rm -f[root@ubuntu2204 0508]# ls-f#删不掉[root@ubuntu2204 0508]# rm -rf -f[root@ubuntu2204 0508]# ls-f#删不掉[root@ubuntu2204 0508]# rm -rf *[root@ubuntu2204 0508]# ls-f[root@ubuntu2204 0508]#rm -rf ./-f[root@ubuntu2204 0508]# ls[root@ubuntu2204 0508]# rm -- -f[root@ubuntu2204 0508]# ls[root@ubuntu2204 0508]# touch &#x27;~&#x27;[root@ubuntu2204 0508]# ls&#x27;~&#x27;[root@ubuntu2204 0508]# rm -f ~rm: cannot remove &#x27;/root&#x27;: Is a directory[root@ubuntu2204 0508]# rm -- ~rm: cannot remove &#x27;/root&#x27;: Is a directory[root@ubuntu2204 0508]# rm -f ./~ 范例：删除大文件 1cat /dev/null &gt; /var/log/huge.log 2.2.7 读取文件12345tail [OPTION]... [FILE]...-f 循环读取-q 不显示处理信息-v 显示详细的处理信息 2.2.8 快速删除一个大文件在大多数文件系统中，当你使用 rm 命令删除一个文件时，实际上被删除的是文件的目录项和对文件数据块的引用计数。也就是说，文件系统会移除指向该文件的数据块的指针，并将这些数据块标记为可用空间。然而，实际的数据块内容并没有立刻从磁盘上物理地擦除。 由于数据块只是被标记为“可重用”，而没有真正清零或覆盖，因此如果新的数据尚未写入这些块，那么原先的数据仍然存在于磁盘上。这意味着，在一定条件下（例如通过专门的数据恢复工具），这些数据是可以被恢复的。 有时，文件特别特别的大，或者磁盘IO繁忙，你直接用 rm -rf 删除的话会非常慢，进而影响其他进程的正常运行。如何处理呢？思路就在于inode块上，inode中存着文件的元数据（例如，文件大小、所有者ID、组ID、文件权限、时间戳等） 1truncate -s 0 /path/to/your/large/file truncate命令工作时，会直接修改inode中记录的文件大小，将其设置为0，而并不会去触及到文件内容所在的数据块。所以磁盘IO消耗极小，速度极快。设为0之后，你再用rm命令把这这个空文件删掉就可以了 3 文件查找和压缩3.1 文件查找工具3.1.1 locate locate 查询系统上预建的文件索引数据库 &#x2F;var&#x2F;lib&#x2F;mlocate&#x2F;mlocate.db 索引的构建是在系统较为空闲时自动进行(周期性任务)，执行updatedb可以更新数据库 索引构建过程需要遍历整个根文件系统，很消耗资源 locate和updatedb命令来自于mlocate包 搜索之前先用updatedb更新一下数据库 工作特点: 查找速度快 模糊查找 非实时查找，适合查找静止的文件 搜索的是文件的全路径，不仅仅是文件名 可能只搜索用户具备读取和执行权限的目录 12345678910locate [OPTION]... [PATTERN]...-i #不区分大小写的搜索-n N #只列举前N个匹配项目-r #使用基本正则表达式--regex #使用扩展正则表达式-A #输出所有能匹配到的文件名，不管文件是否存在-b #仅匹配文件名部份，而不匹配路径中的内容-c #只输出找到的数量-d #指定数据库 范例 12345678#搜索名称或路径中包含“conf&quot;的文件locate conf#使用Regex来搜索以“.conf&quot;结尾的文件locate -r &#x27;\\.conf$&#x27;#搜索ect目录中以a开头的文件或目录locate /etc/a#指定数据库locate -d /tmp/nofile conf 3.1.2 findfind 是实时查找工具，通过遍历指定路径完成文件查找 工作特点： 查找速度略慢 精确查找 实时查找 查找条件丰富 可能只搜索用户具备读取和执行权限的目录 1find [OPTION]... [查找路径] [查找条件] [处理动作] 查找路径：指定具体目标路径；默认为当前目录 查找条件：指定的查找标准，可以文件名、大小、类型、权限等标准进行；默认为找出指定路径下的所有文件 处理动作：对符合条件的文件做操作，默认输出至屏幕 3.1.2.1 指定搜索目录层级12-maxdepth level 最大搜索目录深度,指定目录下的文件为第1级-mindepth level 最小搜索目录深度 范例 123456#最大搜索深度[root@ubuntu2204 ~]# find /etc/ -maxdepth 2#最小搜索深度[root@ubuntu2204 ~]# find /etc/ -mindepth 2#仅搜索第二层目录[root@ubuntu2204 ~]# find /etc/ -maxdepth 2 -mindepth 2 3.1.2.2 对每个目录先处理目录内的文件，再处理目录本身1-depth 范例 12345678910111213141516[root@centos8 data]#find /data/test/data/test/data/test/f1.txt/data/test/f2.txt/data/test/test2/data/test/test2/test3/data/test/test2/test3/f3.txt/data/test/test2/test3/f4.txt[root@centos8 data]#find /data/test -depth/data/test/f1.txt/data/test/f2.txt/data/test/test2/test3/f3.txt/data/test/test2/test3/f4.txt/data/test/test2/test3/data/test/test2/data/test 3.1.2.3 根据文件名和inode查找123456-name &quot;文件名称&quot; #支持使用glob，如：*, ?, [], [^],通配符要加双引号引起来-iname &quot;文件名称&quot; #不区分字母大小写-inum n #按inode号查找-samefile name #相同inode号的文件-links n #链接数为n的文件-regex “PATTERN&quot; #以PATTERN匹配整个文件路径，而非文件名称 范例 12345678910111213141516171819find -name snow.pngfind -iname snow.pngfind / -name &quot;.txt&quot;find /var –name &quot;log*&quot;[root@centos8 data]#find -regex &quot;.*\\.txt$&quot;./scripts/b.txt./f1.txt[root@ubuntu2204 ~]# find -regex &quot;.*test-[a-z].*&quot;./test-a.log./test-a.txt./test-b.log./test-b.txt[root@ubuntu2204 ~]# find -regex &quot;.*dir3.*&quot;./dir1/dir2/dir3./dir1/dir2/dir3/fx./dir1/dir2/dir3/fy[root@ubuntu2204 ~]# find -regex &quot;.*dir3$&quot;./dir1/dir2/dir3 范例 1234find / -name file1 #从 &#x27;/&#x27; 开始进入根文件系统搜索文件和目录find /home/user1 -name \\*.bin #在目录 &#x27;/ home/user1&#x27; 中搜索带有&#x27;.bin&#x27; 结尾的文件find / -name \\*.rpm -exec chmod 755 &#x27;&#123;&#125;&#x27; \\; #搜索以 &#x27;.rpm&#x27; 结尾的文件并定义其权限find / -xdev -name \\*.rpm #搜索以 &#x27;.rpm&#x27; 结尾的文件，忽略光驱、捷盘等可移动设备 3.1.2.4 根据属主、属组查找123456-user USERNAME #查找属主为指定用户(UID)的文件-group GRPNAME #查找属组为指定组(GID)的文件-uid UserID #查找属主为指定的UID号的文件-gid GroupID #查找属组为指定的GID号的文件-nouser #查找没有属主的文件-nogroup #查找没有属组的文件 3.1.2.5 根据文件类型查找123456789-type TYPETYPE可以是以下形式：f: 普通文件d: 目录文件l: 符号链接文件s：套接字文件b: 块设备文件c: 字符设备文件p: 管道文件 范例 12#查看/home的目录find /home –type d -ls 3.1.2.6 空文件或目录1-empty 范例 1[root@centos8 ~]#find /app -type d -empty 3.1.2.7 组合条件123与：-a ，默认多个条件是与关系，所以可以省略-a或：-o非：-not ! 范例 12345678[root@centos8 ~]#find /etc/ -type d -o -type l |wc -l307#此处 ls 只列出了后一个条件的匹配[root@centos8 ~]#find /etc/ -type d -o -type l -ls |wc -l101#把条件括起来才表示全部[root@centos8 ~]#find /etc/ \\( -type d -o -type l \\) -ls |wc -l307 德·摩根定律 (非 A) 且 (非 B) &#x3D; 非(A 或 B) (非 A) 或 (非 B) &#x3D; 非(A 且 B) 示例 12!A -a !B = !(A -o B)!A -o !B = !(A -a B) 范例 123456789[root@centos8 data]#find ! \\( -type d -a -empty \\)| wc -l56[root@centos8 data]#find ! -type d -o ! -empty |wc -l56[mage@centos8 data]$find ! -user wang ! -user mage#找出/tmp目录下，属主不是root，且文件名不以f开头的文件find /tmp \\( -not -user root -a -not -name &#x27;f*&#x27; \\) -lsfind /tmp -not \\( -user root -o -name &#x27;f*&#x27; \\) –ls 3.1.2.8 排除目录1-prune #跳过，排除指定目录,必须配合 -path使用 范例 12345678#查找/etc/下，除/etc/security目录的其它所有.conf后缀的文件find /etc -path &#x27;/etc/security&#x27; -a -prune -o -name &quot;*.conf&quot;#查找/etc/下，除/etc/security和/etc/systemd,/etc/dbus-1三个目录的所有.conf后缀的文件find /etc \\( -path &quot;/etc/security&quot; -o -path &quot;/etc/systemd&quot; -o -path &quot;/etc/dbus-1&quot; \\) -a -prune -o -name &quot;*.conf&quot;#排除/proc和/sys目录find / \\( -path &quot;/sys&quot; -o -path &quot;/proc&quot; \\) -a -prune -o -type f -a -mmin -1 3.1.2.9 根据文件大小来查找1234-size [+|-]#UNIT #UNIT常用单位：k, M, G，c（byte）,注意大小写敏感#UNIT: #表示(#-1, #],如：6k 表示(5k,6k]-#UNIT #表示[0,#-1],如：-6k 表示[0,5k]+#UNIT #表示(#,∞),如：+6k 表示(6k,∞) 范例 1234567891011#大于2K，小于或等于3K的文件[root@ubuntu2204 ~]# find /var/log/ -size 3k -ls#小于或等于2k[root@ubuntu2204 ~]# find /var/log/ -size -3k -name &quot;*log&quot; -ls#大于2k[root@ubuntu2204 ~]# find /var/log/ -size +2k -name &quot;*log&quot; -ls#大于2k,小于或等于9k[root@ubuntu2204 ~]# find /var/log/ -size +2k -size -10k -name &quot;*log&quot; -ls 3.1.2.10 根据时间戳1234567891011121314#以“天&quot;为单位-atime [+|-]## #表示[#,#+1)+# #表示[#+1,∞]-# #表示[0,#)-mtime-ctime#以“分钟&quot;为单位-amin-mmin-cmin 范例 12find /usr/bin -type f -atime -100 #搜索在过去100天内未被使用过的执行文件find /usr/bin -type f -mtime -10 #搜索在10天内被创建或者修改过的文件 3.1.2.11 根据权限查找123456-perm [/|-]MODEMODE #精确权限匹配/MODE #任何一类(u,g,o)对象的权限中只要有一位匹配即可，或关系，+ 从CentOS 7开始淘汰-MODE #每一类对象都必须同时拥有指定权限，与关系0 表示不关注 说明： find -perm 755 会匹配权限模式恰好是755的文件 只要当任意人有写权限时，find -perm &#x2F;222就会匹配 只有当每个人都有写权限时，find -perm -222才会匹配 只有当其它人（other）有写权限时，find -perm -002才会匹配 3.1.2.12 正则表达式12-regextype type #正则表达式类型，emacs|posix-awk|posix-basic|posix-egrep|posix-extended-regex pattern #正则表达式 范例 1find /you/find/dir -regextype posix-extended -regex &quot;regex&quot; 3.1.2.13 处理动作1234567-print：默认的处理动作，显示至屏幕-ls：类似于对查找到的文件执行&quot;ls -dils&quot;命令格式输出-fls file：查找到的所有文件的长格式信息保存至指定文件中，相当于 -ls &gt; file-delete：删除查找到的文件，慎用！-ok COMMAND &#123;&#125; \\; 对查找到的每个文件执行由COMMAND指定的命令，对于每个文件执行命令之前，都会交互式要求用户确认-exec COMMAND &#123;&#125; \\; 对查找到的每个文件执行由COMMAND指定的命令&#123;&#125;: 用于引用查找到的文件名称自身 关于 &#123;&#125; \\; 1https://askubuntu.com/questions/339015/what-does-mean-in-the-find-command 范例 1234567891011#备份配置文件，添加.orig这个扩展名find -name &quot;.conf&quot; -exec cp &#123;&#125; &#123;&#125;.orig \\;#提示删除存在时间超过３天以上的joe的临时文件find /tmp -ctime +3 -user joe -ok rm &#123;&#125; \\;#在主目录中寻找可被其它用户写入的文件find ~ -perm -002 -exec chmod o-w &#123;&#125; \\;#查找/data下的权限为644，后缀为sh的普通文件，增加执行权限find /data –type f -perm 644 -name &quot;*.sh&quot; –exec chmod 755 &#123;&#125; \\; 3.1.2.14 参数替换xargs由于很多命令不支持标准输入，xargs就可以用于产生某个命令的参数，读取标准输入的数据，把这个数据以空格或换行符将数据分割成参数，然后传递给某个命令，作为这个命令的参数 另外，许多命令不能接受过多参数，命令执行可能会失败，xargs 可以解决 注意：文件名或者是其他意义的名词内含有空格符的情况 12345xargs cmdxargs 默认所有元素都在一行xargs -n1 一行一个元素xargs -n2 一行两个元素 find 经常和 xargs 命令进行组合，形式如下： 1find | xargs COMMAND 范例 12345678910111213141516171819[root@centos8 ~]#seq 10 | xargs1 2 3 4 5 6 7 8 9 10[root@centos8 data]#echo &#123;1..10&#125; |xargs -n112345678910[root@centos8 data]#echo &#123;1..10&#125; |xargs -n21 23 45 67 89 10 范例 1234567891011121314151617181920212223#删除当前目录下的大量文件ls | xargs rm#批量创建和删除用户echo user&#123;1..10&#125; |xargs -n1 useraddecho user&#123;1..100&#125; | xargs -n1 userdel -r#批量创建文件echo f-&#123;1..130752&#125;.txt | xargs -n 10000 touch#查找有特殊权限的文件，并排序find /bin/ -perm /7000 | xargs ls -Sl #此命令和上面有何区别？find /bin/ -perm -7000 | xargs ls -Sl #以字符nul分隔find -type f -name &quot;*.txt&quot; -print0 | xargs -0 rm#并发执行多个进程seq 100 |xargs -i -P10 wget -P /data http://10.0.0.8/&#123;&#125;.html#并行下载bilibili视频yum install python3-pip -ypip3 install you-get seq 60 | xargs -i -P3 you-get https://www.bilibili.com/video/BV14K411W7UF?p=&#123;&#125; 范例：有一种文件名为&#39;c d.log&#39;，这种文件名系统会认为 c 和 d.log 都是文件，不会认为 c d.log 是完整的文件，一文件变成两文件，是空格导致的，利用xargs移动或删除这个文件名的时候会失败，这个时候就要用字符nul分隔，nul不会成为文件名，因为在Linux文件命令规范中nul和斜线是不能成为文件名的，也就是 -print0 1234567891011121314151617[14:09:46 root@10 ~[]#touch a.log[14:10:07 root@10 ~[]#touch &#x27;a b&#x27;[14:10:40 root@10 ~[]#touch a.conf[14:10:47 root@10 ~[]#touch &#x27;c d.log&#x27;[14:12:01 root@10 ~[]#ls&#x27;a b&#x27; a.conf a.log anaconda-ks.cfg &#x27;c d.log&#x27; c.txt data initial-setup-ks.cfg[14:12:34 root@10 ~[]#find -name &#x27;*.log&#x27; | xargs mv -t datamv: 无法获取&#x27;./c&#x27; 的文件状态(stat): 没有那个文件或目录mv: 无法获取&#x27;d.log&#x27; 的文件状态(stat): 没有那个文件或目录[14:12:43 root@10 ~[]#ls&#x27;a b&#x27; a.conf anaconda-ks.cfg &#x27;c d.log&#x27; c.txt data initial-setup-ks.cfg[14:13:32 root@10 ~[]#find -name &#x27;*.log&#x27; -print0 | xargs -0 rm[14:13:40 root@10 ~[]#ls&#x27;a b&#x27; a.conf anaconda-ks.cfg c.txt data initial-setup-ks.cfg 3.2 文件压缩和解压缩（主要针对某个文件压缩，非目录）3.2.1 compress和uncompress此工具来自于ncompress包,此工具目前已经很少使用 对应的文件是 .Z 后缀 123456compress Options [file ...]uncompress file.Z #解压缩-d 解压缩，相当于uncompress-c 结果输出至标准输出,不删除原文件-v 显示详情 3.2.2 gzip和gunzip来自于 gzip 包 对应的文件是 .gz 后缀 123456gzip [OPTION]... FILE ...-k keep, 保留原文件,CentOS 8 新特性-d 解压缩，相当于gunzip-c 结果输出至标准输出，保留原文件不改变-# 指定压缩比，#取值为1-9，值越大压缩比越大 范例 12345#解压缩gunzip file.gz #不显式解压缩的前提下查看文本文件内容zcat file.gz 范例 1234gzip -c messages &gt;messages.gzgzip -c -d messages.gz &gt; messageszcat messages.gz &gt; messagescat messages | gzip &gt; m.gz 3.2.3 bzip2和bunzip2来自于 bzip2 包 对应的文件是 .bz2 后缀 123456bzip2 [OPTION]... FILE ...-k keep, 保留原文件-d 解压缩-c 结果输出至标准输出，保留原文件不改变-# 1-9，压缩比，默认为9 范例 12bunzip2 file.bz2 #解压缩bzcat file.bz2 #不显式解压缩的前提下查看文本文件内容 3.2.4 xz和unxz来自于 xz 包 对应的文件是 .xz 后缀 123456xz [OPTION]... FILE ...-k keep, 保留原文件-d 解压缩-c 结果输出至标准输出，保留原文件不改变-# 压缩比，取值1-9，默认为6 范例 12unxz file.xz #解压缩xzcat file.xz #不显式解压缩的前提下查看文本文件内容 3.2.5 zip和unzipzip 可以实现打包目录和多个文件成一个文件并压缩，但可能会丢失文件属性信息，如：所有者和组信息，一般建议使用 tar 代替 分别来自于 zip 和 unzip 包 对应的文件是 .zip 后缀 范例 123456789101112131415#打包并压缩zip -r /backup/sysconfig.zip /etc/sysconfig/#不包括目录本身，只打包目录内的文件和子目录cd /etc/sysconfig; zip -r /root/sysconfig.zip *#默认解压缩至当前目录unzip /backup/sysconfig.zip #解压缩至指定目录,如果指定目录不存在，会在其父目录（必须事先存在）下自动生成unzip /backup/sysconfig.zip -d /tmp/config cat /var/log/messages | zip messages -#-p 表示管道unzip -p message.zip &gt; message 加密 12-e file.zip #压缩后的文件交互式加密-P passed file.zip #非交互式加密 范例 1zip -r -P 123456 etc.zip /etc/ 3.3 文件打包和解包3.3.1 tar可以实现打包目录和多个文件成一个文件并压缩，生成一个归档文件，保留文件属性不变，用于备份功能 对应的文件是.tar 后缀 1234567891011121314-t #列出归档内容-c #创建一个新归档-x #从归档中解出文件-v #详细地列出处理的文件-f #指定 (将要创建或已存在的) 归档文件名-r #追加文件至归档结尾-p #保留原文件的访问权限-z #gzip压缩-j #bzip2压缩-J #xz压缩--exclude #排除文件-T #选项指定输入文件-X #选项指定包含要排除的文件列表-C #指定目录 范例 12345678910111213141516171819202122232425262728#打包文件和目录，将文件file1、file2和目录dir1打包成一个名为archive.tar的归档文件，并在终端输出打包过程的详细信息tar cvf archive.tar file1 file2 dir1#解压归档文件，将解压归档文件archive.tar，并将其中的文件和目录恢复到当前目录。tar xvf archive.tar#将文件file1、file2和目录dir1打包成一个名为archive.tar.gz的归档文件，并使用gzip进行压缩tar czvf archive.tar.gz file1 file2 dir1#将解压归档文件archive.tar，并将其中的文件和目录恢复到当前目录，并保留它们的权限信息tar xvpf archive.tar#将文件newfile添加到归档文件archive.tar中，保持归档文件的完整性tar rvf archive.tar newfile#将显示归档文件archive.tar中的文件和目录列表，并输出它们的详细信息，而不解压缩归档文件tar tvf archive.tar#创建归档，保留权限 tar -cpvf /path/file.tar file#追加文件至归档，不支持对压缩文件追加tar -rf /path/file.tar file#解开归档#不加-C默认解开到当前目录，加-C解开到指定目录tar xf /path/file.tartar xf /path/file.tar -C /path/ 范例 1234567[root@centos8 ~]#tar zcvf etc.tar.gz /etc/[root@centos8 ~]#tar jcvf etc.tar.bz2 /etc/[root@centos8 ~]#tar Jcvf etc.tar.xz /etc/[root@centos8 ~]#ll etc.tar.*-rw-r--r-- 1 root root 3645926 Dec 20 22:00 etc.tar.bz2-rw-r--r-- 1 root root 5105347 Dec 20 21:59 etc.tar.gz-rw-r--r-- 1 root root 3101616 Dec 20 22:00 etc.tar.xz 范例: 只打包目录内的文件，不所括目录本身 123456#方法1[root@centos8 ~]#cd /etc[root@centos8 etc]#tar zcvf /root/etc.tar.gz ./#方法2[root@centos8 ~]#tar -C /etc -zcf etc.tar.gz ./ 范例 12tar zcvf /root/a.tgz --exclude=/app/host1 --exclude=/app/host2 /apptar zcvf mybackup.tgz -T /root/includefilelist -X /root/excludefilelist 3.3.2 split可以分割一个文件为多个文件 1split -b size 原文件 保存的文件 范例 12345678#分割大的 tar 文件为多份小文件split -b Size –d tar-file-name prefix-name示例:split -b 1M mybackup.tgz mybackup-parts#切换成的多个小分文件使用数字后缀split -b 1M –d mybackup.tgz mybackup-parts 切割后的多个文件合并成一个文件 1cat 保存的文件* &gt; 原文件 范例 1cat etc-split* &gt; etc.tar（*代表所有的文件）","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"物理网络架构全链路","slug":"ComputerBasic/network/network-architecture","date":"2025-08-21T02:43:39.000Z","updated":"2025-08-28T12:33:05.388Z","comments":true,"path":"ComputerBasic/network/network-architecture/","permalink":"https://aquapluto.github.io/ComputerBasic/network/network-architecture/","excerpt":"","text":"流量类型 南北流量：内网机器与外网通信 东西流量：内网机器在局域网内彼此之间进行通信 跨数据中心的流量：跨数据中心之间的通信 家庭组网流程 购买路由器 向运营商付费开通宽带服务，并将运营商提供的网线一端连接其设备，另一端接入路由器，路由器会有两个IP地址：一个是内部私网IP，一个是与运营商设备连接的外网IP 通过网线将电脑等设备直连路由器 路由器提供无线功能，手机也能连接路由器 所有设备都在一个局域网内，要访问外网，都要经过路由转发，出去的时候，数据包的源地址都换成了路由器对外的IP地址 在家庭网络中，路由器的作用是什么？ 将多个设备连接到一起，并共享一个外网IP地址。它可以兼具交换机和路由器功能，帮助家庭内多台设备通过一个公共的外网IP地址访问互联网。此外，路由器还提供无线功能，允许手机等设备连接到网络 如何构建上万台服务器的集群？ 单个交换机的接口数有限，不能满足上万台服务器的连接需求，且广播包的大小受限，通过分层组织解决痛点： 接入层: 负责连接服务器，形成一个个局域网 汇聚层: 将接入层的交换机汇聚在一起，形成一个二层网络。 核心层: 汇聚多个汇聚层，形成统一的网络结构 网络出口用防火墙还是路由器？ 中小型网络使用防火墙出口较多 特定行业&#x2F;网络出口必须使用路由器 大型网络路由器与防火墙并存 网络通信的应用程序主要有两种架构： CS架构（Client-Server 架构）：客户端与服务器之间直接进行通信。 BS架构（Browser-Server 架构）：使用浏览器和前端代码（如HTML、CSS、JS）与服务器进行通信 集群能够把多个设备连在一起提供服务，有效避免单点故障，即一个设备坏了导致整个系统都挂了，南北流量和东西流量是把使用地理方向作比喻，大型集群通过分层架构解决交换机性能瓶颈，集群需关注时间同步与网络性能以此保证稳定性","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"应用层","slug":"ComputerBasic/network/application-layer","date":"2025-08-21T02:43:32.000Z","updated":"2025-08-28T12:33:05.381Z","comments":true,"path":"ComputerBasic/network/application-layer/","permalink":"https://aquapluto.github.io/ComputerBasic/network/application-layer/","excerpt":"","text":"一、DNS域名解析1.1 DNS介绍当前TCP&#x2F;IP网络中的设备之间进行通信，是利用和依赖于IP地址实现的。但数字形式的IP地址是很难记忆的。当网络设备众多，想要记住每个设备的IP地址，可以说是”不可能完成的任务”。那么如何解决这一难题呢？我们可以给每个网络设备起一个友好的名称，如： www.baidu.com，这种由文字组成的名称，显而易见要更容易记忆。但是计算机不会理解这种名称的，我们可以利用一种名字解析服务将名称转化成（解析）成IP地址。从而我们就可以利用名称来直接访问网络中设备了。除此之外还有一个重要功能，利用名称解析服务可以实现主机和IP的解耦，即：业务不需要固定依赖具体的IP，当主机IP变化时，只需要修改名称服务即可，用户仍可以通过原有的名称进行访问而不受影响。 DNS：应用层协议， 是互联网的一项服务。它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网，基于C&#x2F;S架构，服务器端：53&#x2F;udp, 53&#x2F;tcp，TCP 的53端口主要用于主从DNS之间的数据同步 域名的组成： 根域: 全球根服务器节点只有13个,10个在美国，1个荷兰，1个瑞典，1个日本，“.” 表示 一级域名：Top Level Domain: tld三类：组织域、国家域(.cn, .ca, .hk, .tw)、反向域com, edu, mil, gov, net, org, int,arpa 二级域名：baidu.com 三级域名：study.baidu.com 最多可达到127级域名 域名，又称网域，顾名思义，是一个域的名称。 是一串用点号分隔的字符，可以用来标识网络中某台主机或某个节点，由DNS服务维护域名和主机IP地址之间的映射关系，当我们在网络中访问某个域名时，实际上访问的是该域名对应的IP地址所标识的主机。 FQDN：全称域名 域名是一个域的名称，一个网域或一个节点，可以有多台主机，所以为了精确表示域里面的某台主机，我们在使用域名时，还需要加上主机名，FQDN指的就是同时带有主机名和域名的名称。 1234567891011全称域名=域名www.baidu.org.blog.baidu.org.study.baidu.org.www.sina.com.cn.主机名（www）+域名（baidu.org.） 实际上有三种域名baidu #二级域名org #一级域名， #根域 1.2 DNS服务工作原理 当客户端主机决定访问 https://www.baidu.com 这个域名时，首先会查询本机缓存； 如果本机缓存没有解析记录，则会向其配置的DNS服务器发起解析请求； DNS代理解析服务器会先查询其缓存是否有这条解析记录，如果有，则直接返回，如果没有，则继续向上解析； DNS代理解析服务器会向根域名服务器发起解析请求，根域名服务器返回 com 域名的DNS地址； DNS代理解析服务器继续向 com 域名服务器发起解析请求，com 域名服务器返回 baidu.com域名服务器DNS地址； DNS代理解析服务器继续向 baidu.com 域名服务器发起解析请求，baidu.com 域名服务器返回 www.baidu.com 主机的IP； DNS代理解析服务器将 www.baidu.com 的IP地址存入本机缓存，再读取缓存，将 IP地址发送给客户端主机； 客户端主机通过IP地址顺利访问 https://www.baidu.com； DNS服务只负责域名解析，也就是说DNS服务只负责返回与域名对应的IP地址，但该IP地址在网络上是否是可达的，并不由DNS决定。每个域名都有专门的服务器负责 1.3 DNS查询类型递归查询 是指DNS服务器在收到用户发起的请求时，必须向用户返回一个准确的查询结果。如果DNS服务器本地没有存储与之对应的信息，则该服务器需要询问其他服务器，并将返回的查询结构提交给用户。形象来讲你向一个人借钱，那个人即使没钱，都直接答应你了，自己去借高利贷等途径给你钱。 一般客户机和本地DNS服务器之间属于递归查询，即当客户机向DNS服务器发出请求后,若DNS服务器本身不能解析，则会向另外的DNS服务器发出查询请求，得到最终的肯定或否定的结果后转交给客户机。此查询的源和目标保持不变,为了查询结果只需要发起一次查询 递归算法：客户端向LocalDNS发起域名查询 –&gt; localDNS不知道域名对应的IP –&gt; 但它知道谁知道 –&gt; 他代为帮客户端去查找 –&gt; 最后再返回最终结果 迭代查询 是指DNS服务器在收到用户发起的请求时，并不直接回复查询结果，而是告诉另一台DNS服务器的地址，用户再向这台DNS服务器提交请求，这样依次反复，直到返回查询结果。形象来讲你向一个人借钱，那个人没钱，但是他认识有钱人，让你去找那个有钱人借钱。 一般情况下(有例外)本地的DNS服务器向其它DNS服务器的查询属于迭代查询,如：若对方不能返回权威的结果，则它会向下一个DNS服务器(参考前一个DNS服务器返回的结果)再次发起进行查询，直到返回查询的结果为止。此查询的源不变,但查询的目标不断变化,为查询结果一般需要发起多次查询。 迭代算法：客户端向LocalDNS发起域名查询 –&gt; localDNS不知道域名对应的IP –&gt; 但它知道谁知道并推荐客户端应该找谁 –&gt; 客户端自己去找它。 DNS缓存 DNS缓存是将解析数据存储在靠近发起请求的客户端的位置，也可以说DNS数据是可以缓存在任意位置，最终目的是以此减少递归查询过程，可以更快的让用户获得请求结果。 Windows系统 12345#显示DNS缓存C:\\Users\\44301&gt;ipconfig/displaydns#清除DNS缓存C:\\Users\\44301&gt;ipconfig/flushdns CentOS系统 12345#查看DNS缓存[root@rocky86 ~]# nscd -g#清除DNS缓存[root@rocky86 ~]# nscd -i hosts Ubuntu系统 12345#查看DNS缓存[root@ubuntu ~]# resolvectl statistics#清除DNS缓存[root@ubuntu ~]# resolvectl reset-statistics 二、HTTP详解2.1 什么是http协议1、全称Hyper Text Transfer Protocol（超文本传输协议） 普通文本：文件内存放的是一些人类认识的文字符号(汉字、英语、阿拉伯数字) 超级文本：除了普通文本内容之外，还有视频、图片、语音、超链接 http协议都能传输上述内容，所以说http协议是专用于传输超文本的协议 2、http主要用于B&#x2F;S架构 3、http是基于tcp协议的 强调：基于http协议发包之前，必须先建立tcp协议的双向通路 2.2 http协议的发展史2.2.1 http0.9请求方法：只支持GET方法 请求头：不支持 响应信息：只支持纯文本，不支持图片 无连接&#x2F;短连接&#x2F;非持久连接：利用完 tcp 连接之后会立即回收，所以无连接指的不是说没有连接，而是说没有持久连接&#x2F;长连接的http协议通信，先建立tcp连接，然后客户端发请求包，服务端收到后发送响应包，服务端一旦发送完响应包之后，服务端会立即主动断开tcp连接，下次http通信还需要重新建立tcp连接。 同一个用户在短期内访问多次服务端，那大量的时候都会消耗在重复创建tcp连接上，在高并发场景下，对服务端是非常大的消耗，客户端的访问速度也会非常的慢 无状态（一个http协议的请求无法标识自己的身份）：http无法保存状态，比如登录状态，那意味着每次请求都需要重新输出一次账号密码来认证 2.2.2 http1.0请求方法：支持PUT(增)、DELETE(删)、POST(改)、GET(查) 请求头：支持 响应信息：支持超文本 支持缓存 同一个用户在短期内访问多次服务端，不要重复建立tcp连接，而是能够共用一个tcp连接解决方案：支持持久连接&#x2F;长连接 keep-alive 前提：发送完http响应包之后，服务端立即断tcp连接，这是服务端的默认行为，要改变这种默认行为，要客户端通知服务端才行 实现： 客户端在发送http的请求时，需要再请求头里带上connection: keep-alive这个参数 服务端的keepalive timeout设置要大于0 服务端收到后读取该参数，服务端会保持与这一个客户端tcp连接一段时间，响应时也会在响应头里放connection:keep-alive这个参数 该tcp会保持一段时间直到达到服务端设置的keepalive_timeout时间 补充: 在http1.0协议还需要你发请求时你自己加上connection: keep-alive这个参数 在http1.1协议里所有的请求都会自动加上connection: keep-alive，也就是说在htp1.1客户端默认就开启了长连接支持，配套的服务端也要开启(服务端的keepalive_timeout设置要大于0) 服务端要客户端有状态(让客户端每次发请求的时候都能标识自身的唯一性)：解决方案是cookie、session、jwt 2.2.3 http1.1默认所有请求都启用长连接，请求与响应头里都带着connection:keep-alive，对应服务端需要设置keepalive timeout大于0 Pipelining(请求流水线化&#x2F;管道化) 分块传输编码chunked 2.2.4 http2.0引入头信息压缩机制：头信息使用gzip或compress压缩后再发送 允许服务器有新数据时未经请求，主动向客户端发送资源，而无需客户端拉取，即服务器推送（server push） 2.3 http协议的格式2.3.1 URI和URLURI：统一资源标识符 URL：统一资源定位服务，是URI的一种具体实现http://192.168.71.10:8080/a/b/1.txt?x=1&amp;y=2&amp;page=10#_label5 http:&#x2F;&#x2F; —&gt; 协议部分。不写协议，默认http协议 192.168.71.10:8080 —&gt; ip+port部分，不写端口默认服务端的端口是80 &#x2F;a&#x2F;b&#x2F;1.txt —&gt; 路径部分，不写路径，默认加一个&#x2F;结尾 ?x&#x3D;1&amp;y&#x3D;2&amp;page&#x3D;10 —&gt; 请求参数部分 #_label5 —&gt; 锚 URN：也是URI的一种具体实现，例如：mailto:java-net@java.sun.com. 2.3.2 请求request包含四部分： 请求首行：请求方法 请求的路径部分及后续部分 http协议版本 GET &#x2F;a.txt HTTP&#x2F;1.1 请求头：都是 key:value 格式，用来定制一些参数 空行 请求体数据 请求方法: GET(查) —-&gt; 请求的数据可以放在URL地址的?号后 POST(改) —-&gt; 携带请求体数据 DELETE(删) PUT(增) HEAD：类似GET请求，不一样的是不会获取响应的数据，但是会获取响应头，而响应头包含着状态码，状态码代表着本次访问是否成功，所以HEAD主要用来检测某个资源是否可以正常访问 OPTIONS：一般用作预检请求，在发真正请求之前先发个options请求预检一下服务端支持哪些http方法、跨域检测等 GET和POST的区别: 携带数据的方式不同 携带数据的话post更安全 传输数据大小 GET与POST这两个方法本身没有限制 但因为GET方法的数据都放在URL地址中，而URL地址的长度在一些浏览器中是有限制 所以如果要传一些比较大的数据，不能用GET方法，应该使用POST方法把数据放入请求里传输 2.3.3 响应response也是包含四部分： 响应首行：协议 状态码 HTTP&#x2F;1.1 200 OK 响应头 set-cookie：要求浏览器把cookie信息存入本地 cache-control：要求浏览器把一些文件缓存到本地 connection:keep-alive：要求浏览器保持长连接 Content-Type:text&#x2F;html：告诉浏览器本次返回内容的格式 text&#x2F;plain 告诉浏览器本次返回的内容格式是普通文本 空行 响应体 状态码 2xx：代表访问成功 3xx：本次请求被重定向 4xx：客户端错误 404：客户端访问的资源不存在 403：客户端没有对目标资源的访问权限 5xx：服务端错误 503：服务端故障 2.4 http协议完整的请求与响应流程浏览器访问一个URL地址：http://baidu.com:80/a/b/1.html 浏览器会先问本地DNS把域名baidu.com解析为ip地址 浏览器作为客户端会与目 ip:port 建立TCP三次握手 浏览器会基于HTTP协议封装请求包(OSI七层的封包流程) 服务端收到包(OSI七层的解包流程)，拿到一个HTTP协议的请求包，按照HTTP协议来解析请求会拿到请求路径部分 &#x2F;a&#x2F;b&#x2F;1.html，服务端会打开该文件(对一个文件描述符)把文件内容从硬盘读入内存，然后服务端程序会基于HTTP协议封装读入内存的数据，形成一个响应包，发给客户端浏览器 浏览器收到HTTP协议的响应包之后，先解析响应头，看到响应的状态码，知道本次是否成功，在解析响应头时，可以拿到Content-Type就知道该用什么数据格式来解析内容，如果值为 text&#x2F;html 就会按照html代码的方式来解析返回的内容，再读取内容部分，当成html代码来解析 在解析html代码的过程中，有可能遇到css、jss、图片、视频等资源，会发起二次、三次….请求，直到把整个页面都渲染完毕 三、HTTPS详解3.1 为什么使用HTTPShttp协议：明文传输，可能会遭到窃听、改、冒充&#x2F;挟持，因此使用HTTP协议传输隐私信息非常不安全。https：http协议 + ssl协议，密文传输，可以防止窃听、算改，并且有ca权威机构认证服务端身份，可以防冒充&#x2F;劫持 3.2 SSL协议SSL是一种加密协议，对http协议通信的加强，可以防窃听、改、伪装&#x2F;冒充 数字签名：防止篡改&#x2F;丢失 把包的内容做hash校验得到的hash值称之为摘要 —&gt; digest 服务端用自己的私钥对digest进行加密 —&gt; 得到东西叫数字签名signature 客户端收到包之后先用公钥解开得到digest —&gt; 重新hash，验证是否被篡改 数字证书：防止伪装&#x2F;冒充 CA中心：公认的权威认证中心，找一个证书中心为自己的公钥做认证。 CA中心用自己的私钥，对服务端的公钥和其他相关信息一起加密，生成“数字证书”。 数字证书就是加了密的服务端公钥。 非对称加密 两个密码(公钥、私钥)：公钥加密用私钥解密，私钥加密用公钥解密 优点： 公私分明，公钥任何人都可以获取，而私钥只有服务端自己手里有 安全性更高一些，只要私钥不泄露，就没法解开包 缺点：非对称加密的速度慢，不适合大规模数据加解密 对称加密 只有一个私钥，加解密用的都是同一套私钥 优点：加解密效率高 缺点：私钥的泄露几率高，一旦某一方泄露私钥，则加密信息就无法得到安全保障问题 SSL协议通信过程中即用了非对称加密，又用了对称加密 客户端先通过ssl通信的第二次握手获得服务端数字证书（内含公钥） 然后使用非对称加密传输对称加密的密钥（更安全），后面的数据传输就使用对称性加密（更高效）。 3.3 TLS协议SSL是一种技术，您的应用程序或浏览器可能使用该技术在任何网络上创建安全的加密通信通道。但是SSL是一种较老的技术，包含一些安全漏洞。传输层安全性协议(TLS)是SSL 的升级版本，用于修复现有 SSL 漏洞。TLS 可以更高效地进行身份验证，并继续支持加密的通信通道。 TLS协议就是一个升级版的SSL，由于SSL这一术语更为常用，所以我们通常仍将我们的安全证书称作SSL 3.4 HTTPS通信过程 客户端发起HTTPS请求 用户在浏览器里输入一个https网址，然后连接到服务器的443端口 服务端的配置 采用HTTPS协议的服务器必须要有一套数字证书，可以自己制作，也可以向组织申请。区别就是自己颁发的证书需要客户端验证通过，才可以继续访问，而使用受信任的公司申请的证书则不会弹出提示页面。这套证书其实就是一对公钥和私钥 传送服务器的证书给客户端 证书里其实就是公钥，并且还包含了很多信息，如证书的颁发机构，过期时间等等 客户端解析验证服务器证书 这部分工作是由客户端的TLS来完成的，首先会验证公钥是否有效，比如：颁发机构，过期时间等等，如果发现异常，则会弹出一个警告框，提示证书存在问题。如果证书没有问题，那么就生成一个随机值。然后用证书中的公钥对该随机值进行非对称加密 客户端将加密信息传送服务器 这部分传送的是用证书加密后的随机值，目的就是让服务端得到这个随机值，以后客户端和服务端的通信就可以通过这个随机值来进行加密解密了 服务端解密信息 服务端将客户端发送过来的加密后的随机值用服务器私钥解密后，得到了客户端传过来的随机值 服务器加密信息并发送信息 服务器将数据利用随机值进行对称加密,再发送给客户端 客户端接收并解密信息 客户端用之前生成的随机值解密服务段传过来的数据，于是获取了解密后的内容 四、跨域和Cookiecookie机制： 访问一个站点，服务端返回的响应头会设置set-cookie: k1&#x3D;v1;k2&#x3D;v2 浏览器收到后，会根据set-cookie来设置存入本地的cookie值 下次请求该网站，浏览器会从本地cookie里取出cookie值，放到http请求的cookie字段里，发往服务端 特点: cookie是浏览器的功能，是放在客户端的 cookie内存放的内容是可以被客户端篡改的 cookie机制+session机制： 访问一个站点，输入自己的账号密码进程认证，服务端收到请求之后认证通过，会产生一些标识用身份的数据 这些数据 —&gt; value 把这些数据关联一个 —&gt; key key:value key给客户端，存入cookie value放在服务端，称之为session 服务端会把key放入cookie放入set-cookie里，返回给客户端 客户端收到响应后，会把key存入本地的cookie 下次请求该网站，会带着该key去到目标站点，目标站点收到后，会根据key取出value，value里放着本次请求的身份 特点：把保密数据放在服务端，称之为session数据，然后针对session数据生成一个key值存入客户端的cookie中 可以防止篡改 在集群的场景下，需要做会话共享(session存入共享的地方) 通过会将session数据存入redis redis作为一个大家依赖的共享点，会影响集群的扩展性 总结cookie和session： 单用cookie来存放状态信息 优点:服务端不需要做会话共享 缺点:客户端可以算改状态信息，不安全 cookie+session 优点:状态信息即session数据是存放在服务端的，状态不会被改 缺点:服务端需要做会话共享，增加了集群的耦合性 JWT(json web token)： 服务端会将状态信息进行加密，然后把加密数据放入客户的cookie中，这个加密的数据称之为token —&gt; 篡改的问题解决了 下次请求会从cookie中取出token带上一起发送给服务端，服务端收到后用加密算法解密 —&gt; 不需要再做会话共享&#x2F;保持 追求：服务端不保存状态 优点：不需要做会话共享、又能很安全 缺点：最大的缺点就是无法做到主动废弃掉某个token，一个token一旦下发之后，就只能等着该token，服务端无法做到主动废弃该token —&gt; 想要做到随时都能主动废弃掉某个token，就需要开发额外的代码来支持","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"传输层","slug":"ComputerBasic/network/transport-layer","date":"2025-08-21T02:43:27.000Z","updated":"2025-08-28T12:33:05.402Z","comments":true,"path":"ComputerBasic/network/transport-layer/","permalink":"https://aquapluto.github.io/ComputerBasic/network/transport-layer/","excerpt":"","text":"一、TCP和UDP协议tcp协议：可靠传输，TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。 之所以可靠，不是因为有双向通路，而是因为有确认机制 udp协议：不可靠传输，报头部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。 没有确定机制，所以不可靠 相对tcp协议来说数据传输效率高 二、TCP三次握手和四次挥手SYN：在建立连接时使用，用来同步序号。当SYN&#x3D;1，ACK&#x3D;0时，表示这是一个请求建立连接的报文段；当SYN&#x3D;1，ACK&#x3D;1时，表示对方同意建立连接 ACK：表示是否前面确认序列号字段是否有效。只有当ACK&#x3D;1时，前面的确认序列号字段才有效 seq：序列号 保证接收端数据有序接收； 可以根据序号判断是否以前接收过该数据，用于去除重复； 判断数据的合法性； 序号机制结合 ACK 可以完成数据重传 ack：确认号（由对方序列号+1组成），表示希望对方发送什么样的东西给它 TIME_WAIT状态说明 主动关闭方，等待足够的时间确保最后一个ACK到达对端，也确保所有仍在网络中的旧报文都有足够的时间被丢弃或到达目的地，防止旧报文干扰新连接 一般来说，服务器是主动挥手的一方，因为服务器发完数据后，为了不占用自身的资源，就会断开连接，不过频繁的建立和关闭连接也会增加负担。在web服务中，HTTP&#x2F;1.1默认使用了长连接，服务器通常是被动关闭的一方。 服务器是否主动关闭连接，取决于是否启用 keep-alive 和负载策略，在高并发环境下，保持连接复用比频繁建立、关闭连接更高效 如果服务器中TIME_WAIT状态比较多，可能说明在高并发中短连接请求数量多，或者被攻击，如syn洪水攻击 CLOSE_WAIT状态说明 被动关闭方，表示对方已经关闭连接，本端需要关闭，等待本地应用调用 close() 关闭连接 如果服务器中CLOSE_WAIT状态比较多，说明应用层没有正确关闭 socket，或者是客户端频繁断开，而服务端未及时释放资源 总结：长连接中主动挥手通常是客户端。短连接中主动挥手通常是服务端。但实际谁主动关闭取决于应用层协议、业务逻辑和配置。例如长连接中，服务端也可以检测客户端断开失败而主动关闭；短连接中，客户端也可能因为异常中断被动断开。 为什么要三次握手？ 确认双方的发送和接收能力：通过3次握手，客户端和服务器可以确认彼此都具备发送和接收数据的能力。这是建立可靠连接的基础 同步初始序列号：TCP协议通过序列号来标识发送的数据包，确保数据的顺序性和完整性。在3次握手过程中，双方会交换初始序列号，以便后续的数据传输能够正确地进行 避免已失效的连接请求报文段突然又传送到了服务端：这种情况可能发生在网络拥堵或者延迟较大的情况下。通过3次握手，服务端可以确认客户端的请求是有效的，而不是一个过时的请求 为什么不是四次握手？ 三次握手已满足可靠性要求，额外步骤会增加延迟且无必要。例如：若第三次握手后服务器再发送ACK，客户端需第四次确认，形成冗余循环 为什么要四次挥手？ 在关闭连接时，需要确保双方都完成了数据的传输和接收，以防止数据丢失或错误。如果只有三次挥手，可能会导致一些问题。 假设只有三次挥手，当客户端发送结束请求后，服务器收到后会发送确认，表示已收到客户端的结束请求。但是在此过程中，服务器可能还有未发送完的数据，如果直接关闭连接，那么这些数据就会丢失。因此，引入第三次挥手，服务器在发送结束请求前，先发送所有未发送完的数据，并等待客户端的确认。客户端接收到服务器的结束请求后，会确认并处理完未接收的数据，然后发送确认，表示自己已准备好关闭连接。 通过四次挥手，可以确保双方都能正确地结束连接，并处理未发送和未接收的数据，保证数据的完整性和可靠性。因此，关闭连接需要四次挥手。 为什么 TIME_WAIT 等待的时间是 2MSL？ 网络中可能存在来自发送方的数据包。当这些数据包被接收方处理后，它会向对方发送响应，因此往返需要等待2倍的时间。就是确保最后一个ACK被服务端接收到了，如果没有接收到也要给足时间让服务器端的第三次挥手的FIN重新传过来。例如被动关闭方没有收到断开连接的最后一个ACK报文，就会触发超时重发FIN报文。另一方收到FIN报文后，会重发ACK给被动关闭方，这样来回就需要2个MSL的时间 三、半连接池和syn洪水攻击半连接池：操作系统维护的一个队列，做完第一次握手后，客户端的tcp连接是进入到半连接池，随后操作系统从半连接池取出连接进行第二次握手，接着第三次握手，连接建立后从半连接池中移除。 SYN洪水攻击 建立大量的连接，将半连接池占满，即客户端变为ESTABLISHED状态，但不回复ack，服务端无法从SYN_RECD变为ESTABLISHED 一般来说，在服务器很少会看见SYN_RECD状态，如果看到很多，说明有可能遭受了攻击 其他攻击 DDOS：分布拒绝服务攻击 DOS：拒绝服务攻击 查看半连接池的数量：/proc/sys/net/ipv4/tcp_max_syn_backlog 可以提高半连接池的数量，可以适当防止SYN洪水攻击或高并发情况，但仅仅只是辅助手段，如果有大量的连接，不仅服务器自身资源消耗殆尽，也会导致网卡（过载，带宽限制），交换机（转发能力不足，MAC地址表溢出），路由器（路由表溢出）等硬件设备的性能瓶颈","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"网络层","slug":"ComputerBasic/network/network-layer","date":"2025-08-21T02:43:22.000Z","updated":"2025-08-28T12:33:05.389Z","comments":true,"path":"ComputerBasic/network/network-layer/","permalink":"https://aquapluto.github.io/ComputerBasic/network/network-layer/","excerpt":"","text":"一、网络基础OSI 七层&#x3D;&#x3D;第7层 应用层&#x3D;&#x3D; 应用层（Application Layer）提供为应用软件而设的接口，以设置与另一应用软件之间的通信。例如:HTTP、HTTPS、FTP、TELNET、SSH、SMTP、POP3、MySQL等 &#x3D;&#x3D;第6层 表示层&#x3D;&#x3D; 主条目：表示层（Presentation Layer）把数据转换为能与接收者的系统格式兼容并适合传输的格式 &#x3D;&#x3D;第5层 会话层&#x3D;&#x3D; 会话层（Session Layer）负责在数据传输中设置和维护电脑网络中两台电脑之间的通信连接。 &#x3D;&#x3D;第4层 传输层&#x3D;&#x3D; 传输层（Transport Layer）把传输表头（TH）加至数据以形成数据包。传输表头包含了所使用的协议等发送信息。例如:传输控制协议（TCP）等。 &#x3D;&#x3D;第3层 网络层&#x3D;&#x3D; 网络层（Network Layer）决定数据的路径选择和转寄，将网络表头（NH）加至数据包，以形成报文。网络表头包含了网络数据。例如:互联网协议（IP）等。 &#x3D;&#x3D;第2层 数据链接层&#x3D;&#x3D; 数据链路层（Data Link Layer）负责网络寻址、错误侦测和改错。当表头和表尾被加至数据包时，会形成信息框（Data Frame）。数据链表头（DLH）是包含了物理地址和错误侦测及改错的方法。数据链表尾（DLT）是一串指示数据包末端的字符串。例如以太网、无线局域网（Wi-Fi）和通用分组无线服务（GPRS）等。分为两个子层：逻辑链路控制（logical link control，LLC）子层和介质访问控制（Mediaaccess control，MAC）子层 &#x3D;&#x3D;第1层 物理层&#x3D;&#x3D; 物理层（Physical Layer）在局部局域网上传送数据帧（Data Frame），它负责管理电脑通信设备和网络媒体之间的互通。包括了针脚、电压、线缆规范、集线器、中继器、主机接口卡等 TCP&#x2F;IP五层协议&#x3D;&#x3D;第一层：物理层&#x3D;&#x3D; 数据称为bit，负责传输电信号，对其进行分组 高电压对应数字1，低电压对应数字0 &#x3D;&#x3D;第二层：数据链路层&#x3D;&#x3D; 使用的是以太网协议传输，即MAC地址 MAC地址是网卡上的地址，唯一的，在局域网内部用的地址，出不了局域网 在局域网中是基于以太网协议的广播方式，使用MAC地址去找到目标机器 一组电信号成为数据帧，每一个数据帧由两部分构成 head头部：长度固定18字节。源地址，目标地址和数据描述信息各占6字节（地址里用的是MAC地址） data：数据，默认最长的是1500字节 &#x3D;&#x3D;第三层：网络层&#x3D;&#x3D; 网络层由来：世界范围的互联网是由一个个彼此隔离的小的局域网组成的，如果所有的通信都采用以太网的广播方式，那么一台机器发送的包全世界都会收到，不仅效率低，也是安全问题 使用的是ipv4协议，用于跨网段通信，数据被称为数据包，由两部分构成 head：源ip地址和目标ip地址 data：数据，最长65515个字节 MAC地址标识局域网内唯一的服务器，ip地址标识唯一的局域网，ip+mac标识全世界唯一的服务器 ip地址要是既可以标识唯一的局域网，又可以标识该局域网内的某台机器的mac地址，就要用到了子网掩码 ip地址&#x3D;网络地址+主机地址 ip地址与子网掩码做与运算，得到网络地址 ARP协议：负责将ip地址解析成mac地址 一台主机通过arp协议获取另外一台主机的mac地址 所以网络通信用的是mac地址，注意不是ip地址 ip相关地址计算工具：https://ipjisuanqi.com/ &#x3D;&#x3D;第四层：传输层&#x3D;&#x3D; 传输层的由来：我们通过ip和mac找到了一台特定的主机，如何标识这台主机上的应用程序？端口，即应用程序与网卡关联的编号。 TCP&#x2F;UDP协议，基于端口工作，数据称之为数据段，负责建立端口到端口的通信 基于tcp协议或者udp协议工作的应用程序，都会拥有一个唯一的端口号（在一个操作系统里端口的范围0-65535，0-1023为系统占用端口） ip+port: 全世界范围内唯一的基于网络通信应用程序 &#x3D;&#x3D;第五层：应用层&#x3D;&#x3D; 应用程序是自己开发，所以应用层用用什么协议应用程序自己定义就好，负责规定好数据的组织形式 http协议、ssl协议 &#x3D;&#x3D;补充：socket抽象层&#x3D;&#x3D; 位于传输层与应用层之间 socket层是对传输层及其以下的封装，封装完之后提供了一系列简单的功能给上次应用程序去调用 基于网络通信的应用程序基本上都是基于socket开发的，所以又称之为套接字程序 网络通信流程网络通信必备的四个地址 不跨网段：ip地址+mac地址 跨网段：网关的ip地址+mac地址+子网掩码 如果有域名：还需要再加一个DNS服务器的地址 网络流量流向 东西流量：服务器集群机器之间进行通信 南北流量：与外网进行通信 网络通信流程 打开浏览器输入一个url地址：http:&#x2F;&#x2F; egonlin.com:80 计算机会请求本地dns把域名egonlin.com解析为ip地址：http:&#x2F;&#x2F; 1.1.1.1:80 应用层封包：用http协议把数据封装一下，假设为4960字节 传输层：用tcp协议封装一下，打上tcp的头（源端口，目标端口），tcp数据包为20字节，嵌入HTTP数据包，总长度为4980字节（TCP三次握手） 网络层：用ip协议封装一下，打上ip头（源ip，目标ip），ip数据包也为20字节，嵌入TCP数据包，总长度为5000字节 数据链路层：用以太网协议封装一下，打上ethernet协议的头（源mac、目标mac） 注意：以太网协议的数据部分默认最大传输单元是1500字节 —-》MTU 数据包需要分片发送，以1500个字节为单位，5000/1500=3.3，所以IP数据包必须分割成四个包 服务器端响应：服务器收到四个以太网数据包，拼起来取出完整的TCP数据包，读出里面的HTTP请求，做出HTTP响应，再用TCP协议发回来 同局域网通信前提条件： 发送端 IP：192.168.71.7/24 发送端 MAC：FFFFFFFFFFFF 接收端 IP：192.168.71.8/24 通信之前，ARP协议开始工作 判断是否在同一局域网 在一个局域网，ARP协议获取目标MAC地址（mac+广播） 发送端广播一个ARP请求帧，里面包含了发送端IP，MAC和接收端IP，局域网内的所有主机都会收到这个广播请求 192.168.71.8的接收端返回自己的mac给发送端 发送端将目标MAC地址缓存到本地ARP缓存表中 真正通信开始，封装数据帧进行传输，包含发送端的mac，接收端的mac，192.168.71.7/24，192.168.71.8/24和数据部分 跨局域网通信前提条件： 发送端 IP：192.168.71.7/24 发送端 MAC：FFFFFFFFFFFF 接收端 IP：172.16.202.8/24 通信之前，ARP协议开始工作 判断是否在同一局域网 不在一个局域网，跨网段通信靠路由器，接下来ARP协议获取路由器（默认网关）的MAC地址（mac+广播） FFFFFFFFFFFF 192.168.71.7 192.168.71.1 192.168.71.1网关返回自己的mac给发送端，发送端手里有网关mac，就可以把信息送给网关 真正通信开始，封装数据帧进行传输，包含发送端自己的mac地址网关的mac地址，192.168.71.7/24，172.16.202.8/24和数据部分 数据封装和数据解封数据封装：数据包在网络中传输时，为了更高效、准确的到达目的地，需要对其进行拆分和打包，比如在所发数据包上附加本地以及目标地址、加纠错字节、以及加密处理等。这些操作就是数据封装。 数据解封：是数据封装的逆过程，就是将发送方发过来的信息经过拆解协议包进而获得业务数据的过程。 网络通讯三种通讯模式unicast: 单播,目标设备是一个 broadcast: 广播,目标设备是所有 multicast: 多播,组播,目标设备是多个 冲突域和广播域冲突域：两个网络设备同时发送数据,如果发生了冲突,则两个设备处于同一个冲突域,反之,则各自处于不同的冲突域 广播域：一个网络设备发送广播，另一个设备收到了,则两个设备处于同一个广播域,反之,则各自处于不同的广播域 三种通讯机制 单工通信：只有一个方向的通信,比如: 收音机 半双工通信：通信双方都可以发送和接收信息，但不能同时发送，也不能同时接收,比如:对讲机 全双工通信：通信双方可以同时发送和同时接收,比如: 手机 范例：查看双工和速度 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#如果mii-tool不支持，则可以使用 ethtool 命令#FD全双工，100千兆，base基带传输，T双绞线，link ok表示网络状态正常[root@localhost ~]# mii-tool eth0eth0: negotiated 100baseTx-FD, link ok[root@localhost ~]# mii-tool -v eth0eth0: negotiated 100baseTx-FD, link ok product info: vendor 00:07:32, model 17 rev 5 basic mode: autonegotiation enabled basic status: autonegotiation complete, link ok capabilities: 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD advertising: 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD flow-control link partner: 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD[root@ubuntu2004 ~]#ethtool eth0Settings for eth0: Supported ports: [ TP ] #网卡接口支持的类型 TP 表示双绞线 Supported link modes: 10baseT/Half 10baseT/Full #支持的工作模式 100baseT/Half 100baseT/Full 1000baseT/Full Supported pause frame use: No Supports auto-negotiation: Yes #是否支持自动协商 Supported FEC modes: Not reported Advertised link modes: 10baseT/Half 10baseT/Full #通告模式 100baseT/Half 100baseT/Full 1000baseT/Full Advertised pause frame use: No Advertised auto-negotiation: Yes#通告是否使用自动协商 Advertised FEC modes: Not reported Speed: 1000Mb/s #当前速度 Duplex: Full #工作模式，全双工 Port: Twisted Pair #接口类型,Twisted Pair是双绞线,FIBRE是光纤 PHYAD: 0 Transceiver: internal Auto-negotiation: on #自动协商是否打开 MDI-X: off (auto) Supports Wake-on: d #是否支持Wake On LAN,d不支持，g支持 Wake-on: d #Wake On LAN是否启用 d禁用, g启用 Current message level: 0x00000007 (7) drv probe link Link detected: yes #是否连接到网络,可以判断网线有无断开 [root@ubuntu2004 ~]#ethtool -i eth0driver: e1000version: 7.3.21-k8-NAPI #网卡驱动firmware-version: expansion-rom-version: bus-info: 0000:02:01.0supports-statistics: yessupports-test: yessupports-eeprom-access: yessupports-register-dump: yessupports-priv-flags: no#网络断开的状态[root@centos8 ~]#mii-tool -v eth1eth1: no link product info: Yukon 88E1011 rev 3 basic mode: autonegotiation enabled basic status: no link capabilities: 1000baseT-FD 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD advertising: 1000baseT-FD 100baseTx-FD 100baseTx-HD 10baseT-FD 10baseT-HD 二、局域网 Local Area Network网络为一个单位所拥有，地理范围和站点数目均有限，用于资源共享和数据通信，能方便地共享昂贵的外部设备、主机以及软件、数据。从一个站点可以访问全网，便于系统的扩展和逐渐演变，各设备的位置可灵活的调整和改变，提高系统的可靠性、可用性和易用性 交换机和网桥网桥和交换机都工作在数据链路层，根据 MAC 地址 转发数据帧， 传统网桥通常只有2个端口，用于连接两个网段 交换机本质上是一个具有多个端口的高速网桥，每个端口可以连接一台设备或一个网段，形成“端口到端口”的独立冲突域 交换机工作流程 从源 MAC 学习设备位置 查表转发目标帧 若目标未知 → 泛洪（广播到所有端口） 交换机会造成广播风暴：交换机会将广播帧（如ARP请求、DHCP请求）转发到所有端口，当网络中存在大量广播流量或存在环路时，可能引发广播风暴，占用大量带宽，导致网络拥塞甚至瘫痪 使用 VLAN 划分广播域，缩小广播范围 启用生成树协议（STP） 防止环路 路由器路由器：跨网络通信 工作层次：OSI 模型 网络层（第3层） 核心功能：将数据包从一个网络转发到另一个网络，实现跨网通信 关键机制：依靠路由表（基于IP地址）决定到达目标网络的最佳路径 功能：每个接口属于不同广播域，有效抑制广播风暴，分隔广播域和冲突域 路由器工作流程 接收数据包 → 查看目标 IP 地址 查询路由表：匹配目的网络 选择最佳路径 → 转发到下一跳（Next Hop） 若无匹配项 → 丢弃或转发至默认路由 虚拟局域网 VLAN因为交换机上各主机都属于同一网段，每个接口组合成了一个广播域，当有一个接口发起通讯时，在同一网段的其他接口都可以接收到，所以这样并不安全，所以利用了VLAN技术可以用交换机来进行网络隔离，要求交换机具有网络管理功能。有了VLAN划分局域网，交换机收到广播帧后，只转发到属于同一VLAN的其他端口 可以不用通过路由器来隔离不同广播域 可以突破地理位置的限制，在逻辑上划分出不同的广播域 基于端口的 VLAN这种模式中，在交换机上创建若干个VLAN，在将若干端口放在每个VLAN 中。每个端口在某一时刻只能属于一个VLAN。一个 VLAN 可以包含所有端口，或者部分端口。每个端口有个PVID （port VLAN identifier)。这种模式下，一个端口上收到的 frame 是 untagged frame，因此它不包含任何有关 VLAN 的信息。VLAN 的关系只能从端口的 PVID 上看出来。交换机在转发 frame 时，只将它转发到相同 PVID 的端口。 如上图所示，连接两个交换机的同一个 VLAN 中的两个计算机需要通信的话，需要在两个交换机之间连两根线： 一根从 Switch A 端口4 到 Switch B 端口 4 （VLAN 1） 一根从 Switch A 端口8 到 Switch B 端口 8 （VLAN 2） 带 VLAN 的交换机的端口分为两类： Access port 这些端口被打上了 VLAN Tag。离开交换机的 Access port 进入计算机的以太帧中没有 VLAN Tag，这意味着连接到 access ports 的机器不会觉察到 VLAN 的存在。离开计算机进入这些端口的数据帧被打上了 VLAN Tag。 Trunk port 有多个交换机时，组A中的部分机器连接到 switch 1，另一部分机器连接到 switch 2。要使得这些机器能够相互访问，你需要连接两台交换机。 要避免使用一根电缆连接每个 VLAN 的两个端口，我们可以在每个交换机上配置一个 VLAN trunk port。Trunk port 发出和收到的数据包都带有 VLAN header，该 header 表明了该数据包属于那个 VLAN。因此，只需要分别连接两个交换机的一个 trunk port 就可以转发所有的数据包了。通常来讲，只使用 trunk port 连接两个交换机，而不是用来连接机器和交换机，因为机器不想看到它们收到的数据包带有 VLAN Header。 Tagged VLANs （数据帧中带有 VLAN tag）这种模式下，frame 的VLAN 关系是它自己携带的信息中保存的，这种信息叫 a tag or tagged header。当交换机收到一个带 VLAN tag 的帧，它只将它转发给具有同样 VID 的端口。一个能够接收或者转发 tagged frame 的端口被称为 a tagged port。所有连接到这种端口的网络设备必须是 802.1Q 协议兼容的。这种设备必须能处理 tagged frame，以及添加 tag 到其转发的 frame。 上图中，两个交换机上的端口8 支持 VLAN 1 和 2， 因此一根线就可以了实现跨交换机的同VLAN 内的计算机互相通信了。 VLAN的不足VLAN 使用 12-bit 的 VLAN ID，所以 VLAN 的第一个不足之处就是它最多只支持 4096 个 VLAN 网络（当然这还要除去几个预留的），对于大型数据中心的来说，这个数量是远远不够的。 VLAN 操作需手工介入较多，这对于管理成千上万台机器的管理员来说是难以接受的。 三、Internet层地址解析协议ARP作用：将IP地址解析成MAC地址 主机发送信息时将包含目标IP地址的ARP请求广播到局域网络上的所有主机，并接收返回消息，以此确定目标的物理地址；收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。地址解析协议是建立在网络中各个主机互相信任的基础上的，局域网络上的主机可以自主发送ARP应答消息，其他主机收到应答报文时不会检测该报文的真实性就会将其记入本机ARP缓存 同网段的ARP 跨网段的ARP 范例：ARP 表 12345678910111213141516171819202122232425262728293031[root@baidu ~]#ip neigh192.168.1.110 dev eth0 lladdr 60:02:b4:e3:8a:c0 STALE192.168.1.156 dev eth0 lladdr 50:01:d9:8a:1d:3f STALE192.168.1.114 dev eth0 lladdr 40:8d:5c:e1:97:34 STALE192.168.1.118 dev eth0 lladdr 94:65:2d:38:44:82 STALE#查看ARP缓存[root@baidu ~]#arp -nAddress HWtype HWaddress Flags Mask Iface192.168.1.110 ether 60:02:b4:e3:8a:c0 C eth0192.168.1.156 ether 50:01:d9:8a:1d:3f C eth0192.168.1.114 ether 40:8d:5c:e1:97:34 C eth0192.168.1.118 ether 94:65:2d:38:44:82 C eth0#ARP只对局域网有效，要是跨网段就实现不了[root@centos7 ~]#arp -nAddress HWtype HWaddress Flags Mask Iface10.0.0.1 ether 00:50:56:c0:00:08 C eth010.0.0.2 ether 00:50:56:ea:d3:07 C eth0[root@centos7 ~]#ping www.baidu.comPING www.a.shifen.com (183.2.172.185) 56(84) bytes of data.64 bytes from 183.2.172.185 (183.2.172.185): icmp_seq=1 ttl=128 time=19.7 ms[root@centos7 ~]#arp -nAddress HWtype HWaddress Flags Mask Iface10.0.0.1 ether 00:50:56:c0:00:08 C eth010.0.0.2 ether 00:50:56:ea:d3:07 C eth0#删除记录[root@centos7 ~]#arp -d [ipaddr] 范例：ARP静态绑定可以防止ARP欺骗 123456[root@centos8 ~]#arp -s 10.0.0.6 00:0c:29:32:80:38[root@centos8 ~]#arp -nAddress HWtype HWaddress Flags Mask Iface10.0.0.6 ether 00:0c:29:32:80:38 CM eth010.0.0.7 ether 00:0c:29:32:80:38 C eth010.0.0.1 ether 00:50:56:c0:00:08 C eth0 Gratuitous ARPGratuitous ARP也称为免费ARP，无故ARP。Gratuitous ARP 不同于一般的ARP请求，它并非期待得到ip对应的mac地址，而是当主机启动的时候，将发送一个Gratuitous arp请求，即请求自己的ip地址的mac地址 免费ARP可以有两个方面的作用： 验证IP是否冲突：一个主机可以通过它来确定另一个主机是否设置了相同的 IP地址 更换物理网卡：如果发送ARP的主机正好改变了物理地址（如更换物理网卡），可以使用此方法通知网络中其它主机及时更新ARP缓存 判断IP地址有没有地址冲突 1234567arping ipaddr如果同一IP地址出现两个MAC地址，说明地址冲突[root@LVS ~]#arping 10.0.0.100ARPING 10.0.0.10060 bytes from 00:0c:29:28:40:00 (10.0.0.100): index=0 time=219.480 usec60 bytes from 00:0c:29:fe:8e:b4 (10.0.0.100): index=1 time=297.546 usec 反向地址解析协议RARP作用：将MAC转换成IP IP地址IP地址组成它们可唯一标识 IP 网络中的每台设备 ，每台主机（计算机、网络设备、外围设备）必须具有唯一的地址 MAC 属于物理地址，mac 地址不可变，一出厂就写死，标识惟一设备 IP 属于逻辑地址，逻辑地址可修改，人为赋予，可以修改，使用灵活，便于管理 IP地址由两部分组成 网络 ID：标识网络，每个网段分配一个网络ID，处于高位（表示处于哪个网段） 主机 ID：标识单个主机，由组织分配给各设备，处于低位 IPv4地址格式：点分十进制记法 范例：IP地址转十进制INT 12345678#二进制转十进制[root@ubuntu2204 ~]# echo &quot;ibase=2;00001010000000000000000010011110&quot;|bc167772318[root@ubuntu2204 ~]# ping 167772318PING 167772318 (10.0.0.158) 56(84) bytes of data.64 bytes from 10.0.0.158: icmp_seq=1 ttl=64 time=0.973 ms64 bytes from 10.0.0.158: icmp_seq=2 ttl=64 time=0.823 ms 范例：遍历检测当前网段内的主机 12345678910111210.0.0.1 = 00001010 00000000 00000000 0000000110.0.0.254 = 00001010 00000000 00000000 11111110 254 = 128+64+32+16+8+4+2[root@ubuntu2204 ~]# echo &quot;ibase=2;00001010000000000000000000000001&quot;|bc167772161[root@ubuntu2204 ~]# echo &quot;ibase=2;00001010000000000000000011111110&quot;|bc167772414[root@ubuntu2204 ~]# for i in &#123;167772161..167772414&#125;;do ping -c1 -W1 $i &amp;&amp; echo the $i is up || echo the $i is down;done;[root@ubuntu2204 ~]# net=10.0.0.1;for i in &#123;1..254&#125;;do ping -c1 -W1 $net.$i &amp;&gt;/dev/null &amp;&amp; echo the $net.$i is up || echo the $net.$i is down;done; IP地址分类 类别 首字节范围（十进制） 二进制前缀 网络ID位数 主机ID位数 网络数量 每网络最大主机数 默认子网掩码 私有地址范围 示例 A类 1–126① 0xxxxxxx 8位 24位 126 &#x3D; 2⁷ - 2 16777214 &#x3D; 2²⁴ - 2 255.0.0.0 10.0.0.0 – 10.255.255.255 114.114.114.114, 8.8.8.8, 1.1.1.1 B类 128–191 10xxxxxx 16位 16位 16384 &#x3D; 2¹⁴ 65534 &#x3D; 2¹⁶ - 2 255.255.0.0 172.16.0.0 – 172.31.255.255 180.76.76.76, 172.16.0.1 C类 192–223 110xxxxx 24位 8位 2097152 &#x3D; 2²¹ 254 &#x3D; 2⁸ - 2 255.255.255.0 192.168.0.0 – 192.168.255.255 223.6.6.6, 223.5.5.5 D类 224–239 1110xxxx ——（多播） —— —— —— —— —— 多播（组播）地址 E类 240–255 1111xxxx ——（保留） —— —— —— —— 公共和私有IP地址公共IP地址：互联网上设备拥有的唯一地址 私有IP地址：不直接用于互联网，通常在局域网中使用 1234567891011A类私有：10.0.0.0 -- 10.255.255.255公有：1.0.0 -- 9.255.255.255; 1.0.0.0 -- 126.255.255.255B类私有：172.16.0.0 -- 172.31.255.255公有：128.0.0.0 -- 172.15.255.255; 172.32.0.0 -- 191.255.255.255C类私有：192.168.0.0 -- 192.168.255.255公有：192.0.0.0 -- 192.167.255.255; 192.169.0.0 -- 223.255.255.255 特殊地址 IP 地址 &#x2F; 范围 类型 含义与用途 是否可路由 备注 0.0.0.0 特殊地址 - 表示“任意地址”或“未知网络&#x2F;主机”- 常用于默认路由（如 0.0.0.0/0）- 在服务器绑定中表示监听所有网络接口 ❌ 不用于通信 不是有效主机地址，仅用于配置或通配 255.255.255.255 限制广播地址 - 本网段内的广播地址- 发送到该地址的数据包会被本广播域内所有主机接收- 路由器不会转发此广播 ❌ 仅限本地广播域 又称“有限广播地址” 127.0.0.1 ~ 127.255.255.254（即 127.0.0.0/8） 回环地址（Loopback） - 用于本机内部通信- 测试 TCP&#x2F;IP 协议栈是否正常- 本地服务间通信（如数据库、Web 服务）- 数据包不会离开主机，不经过物理网卡 ❌ 仅限本机 - 常用 127.0.0.1 ping 测试网络协议- 虚拟接口，永不宕机 ::1（IPv6） IPv6 回环地址 IPv6 中的回环地址，功能同 127.0.0.1 ❌ 仅限本机 零压缩表示为 ::1 224.0.0.0 ~ 239.255.255.255 组播（Multicast）地址 - 用于一点对多点的数据传输- 常见于视频会议、流媒体、路由协议等 ✅ 可跨网络（需组播路由支持） D类地址，也称“多播地址” 224.0.0.1 组播地址 所有支持组播的主机 ✅ 本地子网范围 224.0.0.2 组播地址 所有支持组播的路由器 ✅ 本地子网范围 224.0.0.5 组播地址 OSPF 路由协议使用的组播地址 ✅ 用于 OSPF 路由器间通信 169.254.x.x（169.254.0.0/16） 链路本地地址（Link-Local） - 当主机使用 DHCP 但无法获取 IP 时，Windows 自动分配此地址- 用于本地链路通信（仅限同一网段）- 表示“网络连接异常” ❌ 不能跨路由器 又称“自动私有 IP 地址”（APIPA） 概念 说明 回环地址工作原理 数据包在本机网络层处理，不经过物理接口。用于服务自测、本地通信，如 localhost。 组播地址特点 不同于广播（发给所有人），组播是“有选择的广播”——只有加入特定组的主机才会接收。 APIPA（169.254.x.x） 是 Windows 的容错机制。若看到此地址，说明 DHCP 故障或网络不通。 0.0.0.0 的常见用途 - 路由表中的默认路由：0.0.0.0/0- 服务器监听所有接口：0.0.0.0:80 保留地址在一个IP地址中，如果主机ID全为0，或主机ID全为1，则该地址是保留地址 如在B类127.16 网段中，172.16.0.0 和 172.16.255.255 为保留地址 子网掩码CIDR：无类域间路由，目前的网络已不再按A，B，C类划分网段，可以任意指定网段的范围 CIDR 无类域间路由表示法：IP&#x2F;网络ID位数，如：172.16.0.100&#x2F;16 子网掩码的八位 12345678900000000 010000000 12811000000 19211100000 22411110000 24011111000 24811111100 25211111110 25411111111 255 范例 123IP地址 172.16.0.0子网掩码 11111111.11111111.00000000.00000000子网掩码十进制表示 255.255.0.0 相关公式： 一个网络的最多的主机数 ＝2 ^ 主机ID位数 - 2 网络（段）数 &#x3D; 2 ^ 网络ID中可变的位数 网络ID &#x3D; IP 与 netmask 与运算 1234560与0=00与1=01与0=01与1=1任何数与0都为0任何数与1保留原值 判断对方主机是否在同一个网段： 用自已的子网掩码分别和自已的IP及对方的IP相与，比较结果，相同则同一网络，不同则不同网段 范例：判断A和B是否在网一个网段? 123456789101112A:10.0.1.1/16 B:10.0.2.2/24 如果A访问B，则A和A自已的netmask相与 10.0.0.0/16B和A的netmask相与 10.0.0.0/16两个结果相比较,结果相同如果B访问A，则B和B自已的netmask相与 10.0.2.0/24A和B的netmask相与 10.0.1.0/24两个结果相比较,结果不相同，不在同一个网络 范例：一个主机：220.100.199.0&#x2F;22，求主机数量，最小IP，最大IP 123456789101112131415#网络ID11011100.01100100.11000111.00000000 IP11111111.11111111.11111100.00000000 network11011100.01100100.11000100.00000000 网络ID 220.100.196.0#主机数量2^(32-22)-2=2^10-2=1024-2=1022 32-22 表示32位减去22位网络ID，剩下主机位ID数#最小IP和最大IP220.100.196.1220.100.196196=128+64+4=11000100前22位属于网络ID 220.100.110001 00.00000000则最大是 220.100.110001 11.11111111 = 220.100.199.254 划分子网划分子网：将一个大的网络（主机数多）划分成多个小的网络（主机数少），主机ID位数变少，网络ID位数变多，网络ID位向主机ID位借n位,将划分2^n个子网 子网划分核心：就是控制子网掩码的长短来让ip地址分散到不同的网段&#x2F;子网里 划分子网数&#x3D;2^(网络ID向主机ID借的位数) 123456789101112131415161718192021222324252627282930313233343536#10.0.0.0/8#主机数 2^24-2=16777214#10.0.0.1 -- 10.255.255.254#网络ID向主机ID借一位，能划分成两个子网10.0 0000000.00000000.00000000 10.0.0.0/9 10.1 0000000.00000000.00000000 10.128.0.0/9#网络ID向主机ID借两位，能划分成四个子网10.00 000000.00000000.00000000 10.0.0.0/1010.01 000000.00000000.00000000 10.64.0.0/1010.10 000000.00000000.00000000 10.128.0.0/1010.11 000000.00000000.00000000 10.192.0.0/10#划分成32个子网 2^5=32，所以借5位10.00000 000.00000000.00000000 10.0.0.0/13......10.11111 000.00000000.00000000 10.248.0.0/1311000000.10101000.01000111.00000000 ---&gt; 192.168.71.011000000.10101000.01000111.00000001 ---&gt; 192.168.71.111000000.10101000.01000111.00000011 ---&gt; 192.168.71.2........11000000.10101000.01000111.01111111 ---&gt; 192.168.71.127 一直到127为止计算出的网络地址都是192.168.71.011111111.11111111.11111111.10000000 ---&gt; 25位子网掩码11000000.10101000.01000111.00000000 ---&gt; 192.168.71.0 11000000.10101000.01000111.10000000 ---&gt; 192.168.71.128 11000000.10101000.01000111.10000001 ---&gt; 192.168.71.129 11000000.10101000.01000111.10000010 ---&gt; 192.168.71.130........11000000.10101000.01000111.11111111 ---&gt; 192.168.71.255 一直到127为止计算出的网络地址都是192.168.71.12811111111.11111111.11111111.10000000 ---&gt; 25位子网掩码11000000.10101000.01000111.10000000 ---&gt; 192.168.71.128 在ip层面来说，他们是属于不同的网段，但是在物理层面，交换机是认为他们在同一网段的。假如两台服务器连接到同一个交换机，即使属于不同网段，192.168.71.2发的广播包，192.168.71.129也是会收到的 那么如何使交换机也可以隔离呢？VLAN技术 虚拟局域网，通过虚拟技术把一个交换机分成多个广播域去用，一个广播域就是一个小的局域网 一个局域网内的广播包与另外一个局域网是隔离 物理端口 Access端口：连接终端设备，仅允许单个VLAN流量通过 Trunk端口：连接交换机或路由器，允许多个VLAN流量通过 优化IP地址分配合并超网：将多个小网络合并成一个大网，主机ID位向网络ID位借位，主要实现路由聚合功能 1234567891011121314151617181920#8个C类网段220.78.168.0/24220.78.169.0/24220.78.170.0/24220.78.171.0/24220.78.172.0/24220.78.173.0/24220.78.174.0/24220.78.175.0/24#主机ID向网络ID借3位，网络ID变成21位 168=128+32+8220.78.10101 000.0 220.78.168.0/24220.78.10101 001.0 220.78.169.0/24220.78.10101 010.0 220.78.170.0/24......220.78.10101 110.0 220.78.174.0/24220.78.10101 111.0 220.78.175.0/24#合并成一个大网220.78.168.0/21 四、GRE和VXLAN基于vlan模式来构建虚拟机跨宿主机通信的网络有两点问题 vlan id采用12bit位标识，意味着最多也就4000多个网络，会限制网络的规模 物理节点必须处于一个二层网络 基于gre与vxlan技术就是用来突破上述两点限制的，其实gre与vxlan两种协议的基本原理都是利用已有协议来封装自己新的东西 vxlan特点： 基于udp协议进行分发，所以vxlan是无状态的 支持组播 gre特点： 通过是基于ip协议就信息分发，所以gre是有状态的 不支持组播 每新增与一个节点，所有相关节点都需要与其建立GRE隧道链接 GRE技术本身还是存在一些不足之处： （1）Tunnel 的数量问题 GRE 是一种点对点（point to point）标准。Neutron 中，所有计算和网络节点之间都会建立 GRE Tunnel。当节点不多的时候，这种组网方法没什么问题。但是，当你在你的很大的数据中心中有 40000 个节点的时候，又会是怎样一种情形呢？使用标准 GRE的话，将会有 780 millions 个 tunnels。 （2）扩大的广播域 GRE 不支持组播，因此一个网络（同一个 GRE Tunnel ID）中的一个虚机发出一个广播帧后，GRE 会将其广播到所有与该节点有隧道连接的节点。 （3）GRE 封装的IP包的过滤和负载均衡问题 目前还是有很多的防火墙和三层网络设备无法解析 GRE Header，因此它们无法对 GRE 封装包做合适的过滤和负载均衡。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"IO模型","slug":"ComputerBasic/operating-system/IO-model","date":"2025-08-21T02:42:26.000Z","updated":"2025-08-28T12:33:05.406Z","comments":true,"path":"ComputerBasic/operating-system/IO-model/","permalink":"https://aquapluto.github.io/ComputerBasic/operating-system/IO-model/","excerpt":"","text":"一、IO的分类1.1 磁盘IO和网络IO磁盘I&#x2F;O处理过程 进程向内核发起系统调用，请求磁盘上的某个资源比如是html 文件或者图片 然后内核通过相应的驱动程序将目标文件加载到内核的内存空间 加载完成之后把数据从内核内存再复制给进程内存，如果是比较大的数据也需要等待时间。 网络I&#x2F;O 处理过程 获取请求数据，客户端与服务器建立连接发出请求，服务器接受请求 构建响应，当服务器接收完请求，并在用户空间处理客户端的请求，直到构建响应完成 返回数据，服务器将已构建好的响应再通过内核空间的网络 I&#x2F;O 发还给客户端 不论磁盘和网络I&#x2F;O，每次I&#x2F;O，都要经由两个阶段： 第一步：将数据从文件先加载至内核内存空间（缓冲区），等待数据准备完成，时间较长 第二步：将数据从内核缓冲区复制到用户空间的进程的内存中，时间较短 1.2 同步IO和异步IO函数或方法被调用的时候，调用者是否得到最终结果的。 直接得到最终结果结果的，就是同步调用； 不直接得到最终结果的，就是异步调用。 1.3 阻塞IO和非阻塞IO函数或方法调用的时候，是否立刻返回。 立即返回就是非阻塞调用； 不立即返回就是阻塞调用。 1.3.1 阻塞I&#x2F;O (Blocking I&#x2F;O)当一个进程发起一个I&#x2F;O请求时，它会暂停执行，直到I&#x2F;O操作完成。 这是最常见的I&#x2F;O模型，例如当你打开一个文件进行读取时。 1.3.2 非阻塞I&#x2F;O (Non-blocking I&#x2F;O)在非阻塞模式下，当进程尝试发起I&#x2F;O操作时，如果操作不能立即完成，则会立即返回一个错误或特定值，而不是等待。 进程需要不断地轮询检查I&#x2F;O操作的状态，这称为“忙等”(busy-waiting)。 1.4 联系同步阻塞，我啥事不干，就等你打饭打给我。打到饭是结果，而且我啥事不干一直等，同步加阻塞。 同步非阻塞，我等着你打饭给我，饭没好，我不等，但是我无事可做，反复看饭好了没有。打饭是结果，但是我不一直等。 异步阻塞，我要打饭，你说等叫号，并没有返回饭给我，我啥事不干，就干等着饭好了你叫我。例如，取了号什么不干就等叫自己的号。 异步非阻塞，我要打饭，你给我号，你说等叫号，并没有返回饭给我，我去看电视、玩手机，饭打好了叫我。 1.5 总结同步与异步：任务的启动&#x2F;调用方式 同步：多个任务是同步执行的指的是启动一个任务之后，必须在原地等待该任务运行完毕之后，才能启动下一个任务并且运行 异步：提交完一个任务之后，不用在原地等待该任务运行完毕，就能立即提交下一个任务执行 并发&#x2F;并行与串行：指的是任务给人展现出的运行的效果 并发&#x2F;并行：多个任务是”同时“运行的 串行：一个任务运行完毕，才能运行下一个 阻塞与非阻塞：任务在操作系统中的运行状态 会引起阻塞的事项 硬盘IO 网络IO sleep read命令 二、同步IO模型2.1 阻塞IO模型进程等待（阻塞），直到读写完成。（全程等待） read() write() recv() send() accept() connect() 等…都是阻塞IO 当用户调用了read()，kernel 就开始了IO的第一个阶段：准备数据。对于网络IO来说，很多时候数据在一开始还没有达到（比如，还没有收到一个完整的数据包），这个时候kernel 就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel 一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来 所以，阻塞IO的特点就是在IO执行的两个阶段（等待数据和拷贝数据两个阶段）都被阻塞了。这导致用户在发起IO请求时，不能做任何事情，对CPU的资源利用率不够 2.2 非阻塞IO模型进程调用recvfrom操作，如果IO设备没有准备好，立即返回ERROR，进程不阻塞。用户可以再次发起系统调用（可以轮询），如果内核已经准备好，就阻塞，然后复制数据到用户空间。可防止进程阻塞在IO操作上，需要轮询。 第一阶段数据没有准备好，可以先忙别的，等会再来看看。检查数据是否准备好了的过程是非阻塞的。 第二阶段是阻塞的，即内核空间和用户空间之间复制数据是阻塞的。 淘米、蒸饭我不阻塞等，反复来询问，一直没有拿到饭。盛饭过程我等着你装好饭，但是要等到盛好饭才算完事，这是同步的，结果就是盛好饭。 所以，在非阻塞式 IO 中，用户进程其实是需要不断的主动询问（轮询） kernel 数据准备好了没有。 轮询的时间不好把握，这里是要猜多久之后数据才能到。等待时间设的太长，程序响应延迟就过大；设的太短，就会造成过于频繁的重试，干耗CPU而已，是比较浪费CPU的方式。 2.3 多路复用IO模型以前一个应用程序需要对多个网络连接进行处理，传统的方法是使用多线程或多进程模型，为每个连接创建一个线程或进程进行处理。这种方法存在一些问题，例如线程或进程的创建和销毁需要消耗大量的系统资源，且容易导致线程或进程的数量过多，进而导致系统崩溃或运行缓慢。所以便出现了IO多路复用。 多路复用思想是将操作的所有文件描述符保存在一张文件描述符表中，然后将文件描述符表交给内核，让内核检测当前是否有准备就绪的文件描述符（例如有数据可读或可写），如果有则通知应用程序，操作就绪的文件描述符。这种方式可以让一个进程处理多个并发的IO操作，而不需要为每个IO操作创建一个独立的线程或进程。 以select为例，当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 相比非阻塞IO 在非阻塞IO模型中，如果一个文件描述符还没有准备好进行读写操作，应用程序需要不断地轮询该描述符的状态（即不断调用read()或write()）。这会导致较高的CPU使用率，因为即使在没有数据的情况下，程序也需要不断地执行这些系统调用。 相比之下，使用IO多路复用（如select(), poll(), 或 epoll()），程序只需要在一个调用中指定多个文件描述符，然后等待至少有一个描述符准备就绪。这减少了CPU的轮询开销。 2.3.1 select这是最早的多路复用函数之一，它可以监控多个文件描述符，并且在任何一个描述符上有事件发生时返回。但是它的最大限制是文件描述符的数量受限于系统定义的最大值。 通过将已连接的Socket放入一个文件描述符集合中，然后调用select函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件产生。然而，select存在一些缺点，如每次调用select都需要将fd集合从用户态拷贝到内核态，且仅仅返回可读文件描述符的个数，具体哪个可读还需要用户自己以轮询的方式线性扫描，效率不高。 2.3.2 poll与select()类似，但没有文件描述符数量的限制。poll()使用链表来跟踪描述符的状态变化。 修复了select的一些问题，不再使用BitsMap来存储所关注的文件描述符，改用动态数组，以链表形式来组织，突破了select的文件描述符个数限制。然而，poll和select并没有太大的本质区别，都是使用线性结构存储进程关注的Socket集合，因此都需要遍历文件描述符集合来找到可读或可写的Socket。 2.3.3 epoll这是Linux系统提供的一个更为高效的多路复用接口，它可以高效地处理大量的文件描述符。epoll使用事件驱动的方式，只有当事件发生时才会通知应用程序。 它针对select和poll的缺点进行了优化。epoll在内核中保存一份文件描述符集合，无需用户每次都重新传入，只需告诉内核修改的部分。内核不再通过轮询的方式找到就绪的文件描述符，而是通过异步I0事件唤醒。这使得epoll在处理大量并发连接时具有更高的性能和效率。 2.4 信息驱动IO模型进程在IO访问时，先通过sigaction系统调用，提交一个信号处理函数，立即返回，进程不阻塞。当内核准备好数据后，产生一个SIGIO信号并投递给信号处理函数（由内核通知，发送信号）。可以在此函数中调用recvfrom函数操作数据从内核空间复制到用户空间，这段过程进程阻塞。 此模型的优势在于等待数据报到达期间进程不被阻塞。用户主程序可以继续执行，只要等待来自信号处理函数的通知。但是信号 I&#x2F;O 在大量 IO 操作时可能会因为信号队列溢出导致没法通知。 三、异步IO模型进程发起异步IO请求，立即返回。内核完成IO的两个阶段，内核给进程发一个信号。异步I&#x2F;O允许进程发起一个I&#x2F;O操作后继续执行其他任务，当I&#x2F;O操作完成时，操作系统会通知进程。所以IO两个阶段，进程都是非阻塞的。 异步I&#x2F;O 与 信号驱动I&#x2F;O最大区别在于，信号驱动是内核通知用户进程何时开始一个I&#x2F;O操作，而异步I&#x2F;O是由内核通知用户进程I&#x2F;O操作何时完成。 但是Linux的 aio 的系统调用，内核是从版本2.6开始支持，只用在磁盘IO读写操作，不用于网络IO。 四、IO模型总结","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://aquapluto.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"进程与线程","slug":"ComputerBasic/operating-system/processes-threads","date":"2025-08-21T02:42:21.000Z","updated":"2025-09-09T15:41:47.603Z","comments":true,"path":"ComputerBasic/operating-system/processes-threads/","permalink":"https://aquapluto.github.io/ComputerBasic/operating-system/processes-threads/","excerpt":"","text":"一、进程 程序：存放代码的文件 –&gt; 静态 进程：程序的运行过程 –&gt; 动态 详细的讲，进程是具有一定独立功能的程序在一个数据集上的一次动态执行的过程，是操作系统进行资源分配和调度的一个独立单位，是应用程序运行的载体。可以理解为是程序的实际执行实例，当程序在计算机上运行时，操作系统会为它创建一个进程，分配资源（如内存、CPU时间、文件描述符等），并在计算机上执行程序的指令。 1.1 进程分类1.1.1 操作系统分类协作式多任务：早期 windows 系统使用，即一个任务得到了 CPU 时间，除非它自己放弃使用CPU，否则将完全霸占 CPU ，所以任务之间需要协作——使用一段时间的 CPU ，主动放弃使用。 抢占式多任务：Linux内核，CPU的总控制权在操作系统手中，操作系统会轮流询问每一个任务是否需要使用 CPU ，需要使用的话就让它用，不过在一定时间后，操作系统会剥夺当前任务的 CPU使用权，把它排在询问队列的最后，再去询问下一个任务。 1.1.2 进程类型守护进程：daemon，在后台运行的特殊进程。这类进程通常并不通过交互式终端控制，而是在系统启动后自动运行，进行一些系统管理任务，和终端无关，通常不需要与用户交互（ps aux 命令中TTY为 ? 的） 前台进程：跟终端相关，通过终端启动的进程，可以直接与用户交互（占用终端资源） 注意：两者可相互转化 1.1.3 进程资源使用的分类CPU-Bound：CPU 密集型，非交互 IO-Bound：IO 密集型，交互 1.2 进程与进程之间的关系在操作系统中，不同进程之间可以有多种关系和交互方式。以下是一些常见的进程之间的关系： 父子进程关系 在某些操作系统中，程序运行时产生的第一个进程（父进程）通过调用 fork() 可以创建其他进程（子进程），子进程通常继承父进程的一些属性，如文件描述符、环境变量等。父子进程之间可以通过进程间通信机制来交换数据和信息。 兄弟进程关系 兄弟进程是指由同一个父进程创建的多个子进程。这些兄弟进程通常是独立运行的，但它们可以共享某些资源，如父进程创建的文件描述符或共享内存区域。 并发进程关系 并发进程是指在系统中同时运行的多个独立进程，它们可以在不同的处理器核心（比如多核服务器）上并行执行，以提高系统性能。 竞争条件和同步关系 当多个进程试图同时访问共享资源时，可能会发生竞争条件。为了避免数据损坏和不一致性，需要使用同步机制来确保进程之间的顺序执行或互斥访问共享资源。 进程间通信关系 进程之间可以通过进程间通信（Inter-Process Communication，IPC）机制来交换数据和信息。常见的 IPC 方式包括管道、消息队列、信号、套接字、共享内存等。 进程间协作关系 进程可以协作来完成复杂的任务。这种协作可以通过共享数据、互相通知、等待其他进程完成某些操作等方式来实现。 进程间客户端-服务器关系 在分布式系统中，进程之间可以扮演客户端和服务器的角色。客户端进程请求服务并与服务器进程通信，以实现分布式应用程序的功能。 1.3 进程的状态和转换1.3.1 状态分类创建状态：进程在创建时会首先完成资源分配。如果创建工作无法完成，比如资源无法满足，就无法被调度运行。在这个状态下，进程只是被初始化，但尚未分配 CPU 资源。 就绪状态：进程已准备好，已分配到所需资源，只要分配到CPU就能够立即运行，其实就是进入了任务队列，在就绪状态下，进程等待操作系统的调度以获得 CPU 时间片来执行。 运行状态：当操作系统调度进程并将其分配到 CPU 时，进程进入运行状态。在运行状态下，进程将会执行其指令和代码。 阻塞状态：如果一个进程在执行过程中需要等待某些事件（I&#x2F;O操作完成，申请缓存区失败，等待资源释放）而暂时无法运行，进程受到阻塞。在满足请求时进入就绪状态等待系统调用，在阻塞状态下，进程不会消耗CPU时间，直到等待的事件发生。 终止状态：进程结束，或出现错误，或被系统终止，进入终止状态。无法再执行，在终止状态下，进程的所有资源被释放。 睡眠态：进程需要等待某种资源而进入的状态，要等的资源可以是一个信号量Semaphore）, 或者是磁盘 I&#x2F;O，这个状态的进程会被放入到 wait queue 队列里。分为两种，可中断：interruptable，不可中断：uninterruptable 可中断睡眠态的进程在睡眠状态下等待特定事件发生，即使特定事件没有产生，也可以通过其它手段唤醒该进程，比如，发信号，释放某些资源等 不可中断睡眠态的进程在也是在睡眠状态下等待特定事件发生，但其只能被特定事件唤醒，发信号或其它方法都无法唤醒该进程。 停止态：stopped，暂停于内存，但不会被调度，除非手动启动 僵死态：zombie，僵尸态。父进程结束前，子进程关闭，杀死父进程可以关闭僵死态的子进程 1.3.2 状态转换运行——&gt;就绪 主要是进程占用CPU的时间过长，而系统分配给该进程占用CPU的时间是有限的；在采用抢先式优先级调度算法的系统中,当有更高优先级的进程要运行时，该进程就被迫让出CPU，该进程便由执行状态转变为就绪状态 就绪——&gt;运行 运行的进程的时间片用完，调度就转到就绪队列中选择合适的进程分配CPU 运行——&gt;阻塞 正在执行的进程因发生某等待事件而无法执行，则进程由执行状态变为阻塞状态，如发生了I&#x2F;O请求 阻塞——&gt;就绪 进程所等待的事件已经结束，就进入就绪队列 以下两种状态是不可能发生的： 阻塞——&gt;运行：即使给阻塞进程分配CPU，也无法执行，操作系统在进行调度时不会从阻塞队列进行挑选，而是从就绪队列中选取 就绪——&gt;阻塞：就绪态根本就没有执行，谈不上进入阻塞态 1.3.3 总结1.3.3.1 活着的状态运行着的 ——-》R 正在执行：手里拿着cpu正在运行 就绪（随时可以投入运行）：正在等待操作系统分配cpu，一旦分配到，就可以立即投入运行 阻塞的 —-》S或D S：可中断的睡眠 等待某个资源而进入的状态，执行的IO操作可以得到硬件设备的相应 可以用例如 ctrl+c, kill -9 pid号 命令来终止 类似python中执行 input 代码，shell语言中的 read -p &quot;请输入xxx：&quot; x D: 不可中断睡眠 执行的IO操作不可以得到硬件设备的相应（可能是因为存储设备太忙了响应不过来了） 因此导致内存中的数据无法及时刷入磁盘，所以不可以被中止（linux系统为了防止数据丢失的一种保护机制） 但是要注意的是，只有R和D才属于活跃的进程，一个进程内要做的事可以分为两大类 计算任务 —&gt; cpu负责运行 IO任务 —&gt; 磁盘、网卡负责处理 只要该进程正在被处理着，那它就属于活跃的进程，cpu在执行该进程的计算机任务，肯定属于活跃；磁盘在处理该进程的IO任务，那肯定也属于活跃，总之有事做就属于活跃，而S状态，在等待用户输入内容，而此时用户什么也没有输，即IO操作啥事也没做，计算任务也肯定没有，整个进程就是不活跃的 1.3.3.2 死了的状态僵尸进程 —–》Z 僵尸进程是linux操作系统的一种优化机制 一个进程死掉之后，会把其占用的cpu、内存资源都释放掉，但是会保留该进程的状态信息，例如pid号、存在过的一些运行信息，这些保留下来的信息都是操作系统给父进程准备的 每个进程死掉之前都会进入僵尸进程的状态 僵尸进程通常由父进程来回收 退出的进程 —》X几乎看不到 1.4 僵尸进程和孤儿进程1.4.1 僵尸进程&#x3D;&#x3D;1、什么是僵尸进程&#x3D;&#x3D; 操作系统负责管理进程，我们的应用程序若想开启子进程，都是在向操作系统发送系统调用，当一个子进程开启起来以后，它的运行与父进程是异步的，彼此互不影响，谁先死都不一定。 在linux操作系统的设计中规定父进程应该具备随时获取子进程状态的能力。如果子进程先于父进程运行完毕，此时若linux操作系统立刻把该子进程的所有资源全部释放掉，那么父进程来查看子进程状态时，会突然发现自己刚刚生了一个儿子，但是儿子没了，这就违背了linux操作系统的设计规定 所以linux系统出于好心，若子进程先于父进程运行完毕&#x2F;死掉，那么linux系统在清理子进程的时候，会将子进程占用的重型资源都释放掉(比如占用的内存空间、cpu资源、打开的文件等)，但是会保留一部分子进程的关键状态信息，比如进程号the process ID，退出状态the termination status of the process，运行时间the amount of CPU time taken by the process等，此时子进程就相当于死了但是没死干净，因而得名”僵尸进程”。 其实僵尸进程是linux操作系统出于好心，为父进程准备的一些子进程的状态数据，专门供父进程查阅，也就是说”僵尸进程”是linux系统的一种数据结构，所有的子进程结束后都会进入僵尸进程的状态。僵尸进程不占用系统资源，但在系统中存在时会占用进程号（PID）等资源，影响系统性能 &#x3D;&#x3D;2、那么问题来了，僵尸进程残存的那些数据不需要回收吗？&#x3D;&#x3D; 当然需要回收了，但是僵尸进程毕竟是linux系统出于好心，为父进程准备的数据，至于回收操作，应该是父进程觉得自己无需查看僵尸进程的数据了，父进程觉得留着僵尸进程的数据也没啥用了，然后由父进程发起一个系统调用 wait/waitpid 来通知linux操作系统可以把僵尸进程回收掉了，然后操作系统再清理掉僵尸进程的残余状态。两者配合的非常默契，但是，怕就怕在… &#x3D;&#x3D;3、分三种情况讨论&#x3D;&#x3D; 第一种：linux系统自带的一些优秀的开源软件，这些软件在开启子进程时，父进程内部都会及时调用 wait/waitpid 来通知操作系统回收僵尸进程，所以我们通常看不到优秀的开源软件堆积僵尸进程，因为很及时就回收了，与linux系统配合的很默契 第二种：水平良好的程序员开发的应用程序，知道父进程要对子进程负责，会在父进程内考虑调用 wait/waitpid 来通知操作系统回收僵尸进程，但是发起系统调用 wait/waitpid 的时间可能慢了些，于是我们可以在linux系统中通过命令 ps aux | grep [Z]+ 查看到僵尸进程状态 第三种：一些水平不好的程序员，只知道一直开子进程，父进程也不结束，不知道啥叫僵尸进程，至于 wait/waitpid的系统调用更是没听说过，这个时候操作系统中会堆积很多僵尸进程，此时我们的计算机会进入一个奇怪的现象，就是内存充足、硬盘充足、cpu空闲，但是启动新的软件就是无法启动起来，因为操作系统负责管理进程，每启动一个进程就会分配一个pid号，而pid号是有限的，正常情况下pid也用不完，但怕就怕堆积一堆僵尸进程，他吃不了多少内存，但能吃一堆pid &#x3D;&#x3D;4、如何清理僵尸进程&#x3D;&#x3D; 针对情况3，只有一种解决方案，就是杀死父进程，那么僵尸的子进程会被linux系统中pid为1的顶级进程（init或systemd）接管，顶级进程会定期发起系统调用 wait/waitpid 来通知操作系统清理僵尸 针对情况2，可以发送信号 kill -CHLD 父进程PID 给父进程，通知它快点发起系统调用 wait/waitpid 来清理僵尸的儿子 &#x3D;&#x3D;5、所以当应用程序运行在某个系统上速度特别慢，通过top命令发现系统中有几个僵尸进程存在，当前问题是否是僵尸进程的锅，我能否直接杀死僵尸进程来解决这个问题？&#x3D;&#x3D; 僵尸进程不是应用程序变慢的根本原因，僵尸进程已经死了，无法被进一步杀死，kill命令发送信号给进程，但僵尸进程已经被终止不会处理信号。运行速度特别慢应该用top命令查看 %wa 是否有I&#x2F;O等待，查看 %CPU 情况判断是否有进程长期占用cpu，查看 %MEM 判断是否因内存不足触发频繁交换。 系统调用 wait() 与 waitpid() 123wait()系统调用是一个阻塞的调用，也就是说，如果没有子进程是僵尸进程的话，这个调用就一直不会返回，那么整个进程就会被阻塞住，而不能去做别的事了。Linux还提供了一个类似的系统调用waitpid()，这个调用的参数更多。其中就有一个参数WNOHANG，它的含义就是如果在调用的时候没有僵尸进程，那么函数就马上返回了，而不会像wait()调用那样一直等待在那里。 1.4.2 孤儿进程父进程先死掉，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被进程号为1的顶级进程（init或systemd）所收养，并由顶级进程对它们完成状态收集工作。顶级进程会成为孤儿进程的新父进程，以避免它们变成僵尸进程。因此孤儿进程并不会有什么危害。 1.5 进程之间的通信&#x3D;&#x3D;管道（pipe）&#x3D;&#x3D; 单向传输，只能用于父子进程（有亲缘关系）之间的通信，允许一个进程将数据写入管道，另一个进程从管道中读取数据，随进程的创建而建立，随进程的结束而销毁。 在linux中，管道符 “|” 主要连接左右两个命令，左侧命令执行时是一个进程，他的标准输出（数据）写入管道，右边命令（进程）从管理读取数据，作为其标准输入 &#x3D;&#x3D;命名管道（FIFO）&#x3D;&#x3D; 允许无亲缘关系进程间的通信，不适合进程间频繁地交换数据。 &#x3D;&#x3D;消息队列（MessageQueue）&#x3D;&#x3D; 解决了FIFO的缺点。消息队列实际上是链表，链表的每个节点都是一条消息。每一条消息都有自己的消息类型，用整数来表示，而且必须大于 0，但不适合比较大的数据的传输。读取和写入的过程，都会发生用户态与内核态之间的消息拷贝过程。 &#x3D;&#x3D;共享内存（SharedMemory）&#x3D;&#x3D; 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。 每个进程都会维护一个从内存地址到虚拟内存页面之间的映射关系。尽管每个进程都有自己的内存地址，不同的进程可以同时将同一个内存页面映射到自己的地址空间中，从而达到共享内存的目的。 比如a把数据全都丢到共享内存里面，b去共享内存拿数据，而且b可以按需选择拿哪些数据。 &#x3D;&#x3D;信号（sinal）&#x3D;&#x3D; 用于通知接收进程某个事件已经发生。 &#x3D;&#x3D;信号量（Semaphore）&#x3D;&#x3D; 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 &#x3D;&#x3D;套接字（Socket）&#x3D;&#x3D; 套接字文件，双工通信，网络进程间通信，比如A与B要通信，需要A将数据发生给套接字文件，B从套接字文件接收，或者B发A收。 它既解决了管道只能在相关进程间单向通信的问题，又解决了网络上不同主机之间无法通信的问题。 1.6 进程优先级在Linux系统中，它是一个多用户多任务的操作系统，如何理解多用户和多任务？ 多用户： 即 Linux 系统允许多个用户同时访问和使用系统资源，每个用户有自己的用户账户（用于身份验证和授权），且每个用户账户通常有自己的家目录，用于存储个人文件和配置。 多任务： 多任务是指操作系统能够同时运行多个程序或进程，而不仅仅是一个程序在运行，这使得多个任务可以并发执行。在 Linux 系统中，每个任务通常是一个独立的进程或线程。这些任务可以是用户启动的应用程序、系统服务、后台进程等。所有的任务都放在一个队列中，操作系统通过分时调度算法来分配 CPU 时间片，以便在多个任务之间切换，从而实现并发执行。 对于单 CPU 多任务操作系统而言，CPU 通过分配时间片以并发的方式来执行多任务（串行执行——即在某个时间点只能执行一个任务）；而对于多 CPU 多任务操作系统而言，CPU 既可以通过分配时间片以并发的方式来执行多任务，也可以在某个时间点并行执行多个任务，这就是多核 CPU 比单核 CPU 处理性能高的原因。 那在队列中的这些任务谁先会被 CPU 执行呢？是随机的吗？于是就涉及到了进程优先级与 nice 值的相关概念了。在操作系统中，通常存在两种类型的进程优先级：动态优先级和静态优先级。这两种优先级概念用于进程调度和资源分配，但它们有不同的性质和影响。 静态优先级（Static Priority）： 静态优先级是在进程创建时分配的，并且在进程的整个生命周期内保持不变。 静态优先级通常由用户或程序员明确设置，以反映进程的重要性或性能需求。这通常通过操作系统提供的API或命令来实现。 进程的静态优先级在进程的整个生命周期中不会改变，除非明确修改。 动态优先级（Dynamic Priority）： 动态优先级是在运行时根据系统负载和其他因素动态调整的优先级。 动态优先级的调整可以根据进程的行为、CPU利用率、等待时间等因素进行，以便更好地响应系统的需求。 操作系统的调度算法通常会考虑动态优先级来决定哪个进程在给定时刻获得CPU时间。 linux中将任务优先级进行了一个划分，实时进程的静态优先级范围是0-99，而普通进程（非实时进程）的静态优先级范围是100-139 优先级范围 进程 描述 0-99 实时进程 对响应时间有严格要求的进程，需要在确定的时间内完成任务 100-139 非实时进程 对响应时间的要求相对宽松 不同优先级的值对应如下 系统优先级：0-139，数字越小优先级越高 realtime（实时）优先级：99-0，值最大，优先级最高 nice值：-20到19，对应系统优先级100-139，数值越小优先级越高 1.7 进程切换操作系统基于多道技术控制着cpu在多个任务&#x2F;进程之间切换，每种切换对效率的影响是什么？ （1）时间片耗尽 效率影响：公平性高，但频繁切换导致 上下文切换开销增大 （2）进程阻塞 效率影响：提升 CPU 利用率，频繁 I&#x2F;O 切换可能导致 吞吐量下降 （3）中断处理 效率影响：及时处理外部事件，频繁中断导致用户进程执行时间碎片化 （4）优先级调整 效率影响：关键任务响应及时，但可能打断长任务执行，增加调度延迟 二、线程在早期的操作系统中并没有线程的概念，后来，随着计算机的发展，对CPU的要求越来越高，进程之间的切换开销较大，已经无法满足越来越复杂的程序的要求了。于是就发明了线程。 线程是程序执行的最小单位，是进程内的一个执行单元，一个进程可以有一个或多个线程，各个线程之间共享进程的内存空间。 2.1 线程的状态和转换就绪(Ready)：线程能够运行，但在等待被调度。可能线程刚刚创建启动，或刚刚从阻塞中恢复，或者被其他线程抢占 运行(Running) ：线程正在运行 阻塞(Blocked) ：线程等待外部事件发生而无法运行，如I&#x2F;O等待操作 终止(Terminated)：线程完成，或退出，或被取消 2.2 processes-threads的区别&#x3D;&#x3D;在概念上&#x3D;&#x3D; 进程是操作系统分配资源的最小单位；线程是程序执行的最小单位；一个进程由一个或多个线程组成。 &#x3D;&#x3D;在资源分配上&#x3D;&#x3D; 进程是独立的资源拥有单位，每个进程都有独立的地址空间和系统资源，某进程内的线程在其它进程不可见；而线程是共享所属进程的资源，多个线程共享同一个地址空间和系统资源，包括内存、文件和其他系统资源。 &#x3D;&#x3D;在创建和销毁上&#x3D;&#x3D; 进程的开销通常比线程大，因为进程需要为其分配独立的内存空间和资源；而线程的创建和销毁开销相对较小，因为它们共享了进程的资源。 &#x3D;&#x3D;在通信上，也是在并发性上&#x3D;&#x3D; 线程之间的通信和同步相对容易，并发性较高，因为它们共享同一地址空间；而进程之间的通信和同步则需要额外的机制，如管道、消息队列等，并发性较低，因为它们通常是相互独立的。 三、多进程和多线程多进程：多进程是指在一个应用程序中同时运行多个进程，每个进程都有独立的地址空间和资源，可以较充分地利用多处理器。 多线程：顾名思义，多个线程，一个进程中如果有多个线程运行，就是多线程，实现一种并发。 3.1 并行和并发并行：指的是多个任务真正地在同一时刻执行，通常需要多个处理器核心来实现。 并发：指的是多个任务在同一时间段内交替执行，但并不一定是同时执行。操作系统通过时间片轮转等调度机制使得多个任务看起来像是同时进行的。（多道技术） 单核CPU：无论是多进程还是多线程，都是通过时间片轮转的方式实现并发，不能实现真正的并行执行。 多核CPU：可以实现真正的并行执行，即多个进程或线程可以在不同的核心上同时执行，从而提高系统的性能和效率。 3.2 python中的GIL锁GIL 保证CPython进程中，只有一个线程执行字节码。甚至是在多核CPU的情况下，也只允许同时只能有一个CPU核心上运行该进程的一个线程。所以python中的多线程是假并行。 3.3 CPU密集型和IO密集型CPU密集型任务： 指那些需要大量CPU计算时间的任务，例如视频编码、图像处理、科学计算等。这类任务主要消耗的是CPU资源。 适用模型：多进程 因为其主要依赖于CPU计算能力，用多进程可以充分利用多核CPU的优势，实现真正的并行计算；而使用多线程可能不会带来显著的性能提升，因为在Python中有GIL（全局解释器锁）。 IO密集型任务： 指那些花费大量时间等待外部资源响应的任务，例如读写文件、网络请求、数据库查询等。这类任务大部分时间都在等待I&#x2F;O操作完成，而非占用CPU资源。 适用模型：多线程或多协程 当一个线程执行 I&#x2F;O 操作（如网络请求、文件读写等）时，CPython 会检测到这是一个 I&#x2F;O 操作，并允许该线程暂时释放 GIL。这使得其他线程有机会获取 GIL 并执行它们的 Python 字节码。一旦 I&#x2F;O 操作完成，线程重新获取 GIL 并继续执行后续代码。这种机制对于 I&#x2F;O 密集型任务非常重要，因为它允许在等待 I&#x2F;O 的时候让出 CPU 给其他线程使用，从而提高程序的整体效率和响应速度。 四、协程是一种用户级别的轻量级线程，由程序员显式控制其执行流程。协程可以在特定点暂停（yield），然后在之后恢复执行，而且无需内核态与用户态之间的转换，这使得它非常适合于异步编程模型。例如可以使用Python中的asyncio模块来实现。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://aquapluto.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"操作系统的概念","slug":"ComputerBasic/operating-system/concept","date":"2025-08-21T02:42:14.000Z","updated":"2025-09-09T11:29:33.758Z","comments":true,"path":"ComputerBasic/operating-system/concept/","permalink":"https://aquapluto.github.io/ComputerBasic/operating-system/concept/","excerpt":"","text":"一、操作系统的概念操作系统是一个协调、管理、控制计算机硬件资源给应用程序使用的一种控制程序 计算机三层体系 应用程序：图形界面，命令解释器 操作系统：windows操作系统 linux 计算机硬件（cpu、内存、硬盘） 操作系统与应用程序的区别 为了方便上层的应用程序开发（操作系统把复杂的硬件控制的代码都给写好了，然后对上层提供简单的功能） 操作系统是负责控制硬件的程序，给上层应用程序来调用 应用程序是给用户使用的程序 进程与程序 程序：就是一个或者一系列代码文件—》静态 进程：就是一个程序的运行过程——–》动态 进程代表的是程序的运行过程，而负责运行整个过程是操作系统，所以进程是操作系统的最核心的概念 二、操作系统的构成操作系统由两部分构成 系统调用接口：为上层的应用程序提供的一系列的功能 内核：负责控制硬件的运行的 操作系统的两种工作状态 用户态：执行的是系统调用接口层的代码，负责跟上层的应用程序打交道 内核态：执行的是内核某部分代码，负责跟底层的硬件打交道 操作系统的整个运行过程会频繁发生用户态与内核态的切换 操作系统启动之后，应用程序又是如何启动&#x2F;运行的？ 操作系统接到启动程序的指令 操作系统会控制硬件来运行某个应用程序 操作系统控制硬盘把程序的代码文件读入内存 操作系统控制cpu去内存里读取程序的指令来运行 双击快捷方式——-》图形界面——-》windows系统————》硬件 执行某个命令——-》命令解释器bash——-》linux系统————》硬件 操作系统的功能 隐藏了丑陋的硬件调用接口，为应用程序员提供调用硬件资源的更好，更简单，更清晰的模型（系统调用接口）。应用程序员有了这些接口后，就不用再考虑操作硬件的细节，专心开发自己的应用程序即可。 操作系统提供了文件这个抽象概念，对文件的操作就是对磁盘的操作，有了文件我们无需再去考虑关于磁盘的读写控制（比如控制磁盘转动，移动磁头读写数据等细节） 将应用程序对硬件资源的竞态请求变得有序化 很多应用软件其实是共享一套计算机硬件，比方说有可能有三个应用程序同时需要申请打印机来输出内容，那么a程序竞争到了打印机资源就打印，然后可能是b竞争到打印机资源，也可能是c，这就导致了无序，打印机可能打印一段a的内容然后又去打印c…,操作系统的一个功能就是将这种无序变得有序 三、多道技术多道技术是操作系统的核心技术，控制多个&#x2F;多道程序看起来是同时运行，多道技术的实现是为了解决多个程序竞争或者说共享同一个资源（比如cpu）的有序调度问题，解决方式即多路复用，多路复用分为时间上的复用和空间上的复用。 空间上的复用 复用的是内存，指的是多个程序能够同时读入内存里 空间上的复用必须注意一个点：加载到内存中的多个程序所占用的内存空间必须是隔离的才行 时间上的复用 复用的是cpu的时间，指的是cpu在多个内存中的程序之间快速的切换 什么情况切换 遇到 IO 操作一定会切换 没有遇到 IO 操作，也要切换，因为要让cpu能够雨露均沾（Linux操作系统） 一个cpu同一时间只能做一件事 并发：多个任务看起来是同时运行的，只有单核也能实现并发 并行：多个任务是真正意义上的同时运行的，只能多核才能实现并行 四、操作系统的安装原理驱动程序：是硬件厂商专门为自己的某款硬件设备开发的，用于驱动该硬件运行的专项程序（必须遵循操作系统的标准） 安装的操作的核心原理简介：操作系统本质就是一种程序。从大的层面看安装程序的本质就是把这个程序的文件存入硬盘 操作系统的iso包（又称之为操作系统镜像）：iso的本质就是一个压缩包，里面放着一堆操作的代码文件 安装原理详解 用另外一台机器从网上下载一个iso镜像包，将该iso包存入移动硬盘、光盘、U盘中—》得到一个启动盘 把启动盘插入你的计算机中（接下来要做的事情，是把启动盘里的操作系统数据拷贝到你自己的电脑的硬盘里） 按下电源键，启动计算机，固定先启动bios程序（basic input output system） bios启动之后，会根据配置去某些地方加载真正的操作系统代码，bios的配置信息是存放于CMOS中的 找到启动盘后，bios会将启动盘里的操作系统读入内存，然后bios会控制cpu去内存中执行代码，然后真正的操作系统就运行起来 接下来负责掌管硬件运行的就是真正的操作系统了，bios就可以退出舞台 把启动盘里的操作系统拷贝到本地硬盘 五、操作系统的启动流程5.1 硬件启动BIOSBIOS：Basic Input and Output System（基本输入输出系统） 保存着有关计算机系统最重要的基本输入输出程序，系统信息设置，开机加电自检程序和系统启动自举程序等。BIOS 就是一个程序，其代码存储在主板的一颗ROM存储芯片上，ROM是只能读不能写的，这颗芯片上的BIOS程序，在主板出厂的时候，己经固化好了，所以不管断不断电，这个BIOS程序都会一直存储在这颗芯片上。当我们修改了BIOS里面的某些设置时，这个修改的数据是存储在另外一颗RAM存储芯片上，RAM掉电后数据就会消失，所以主板上有一颗纽扣电池来给这个RAM供电，当这颗纽扣电池没电了，BIOS里面的设置项，就又恢复成出厂设置了。 POST：Power-On-Self-Test 加电自检，是BIOS功能的一个主要部分。负责完成对CPU、主板、内存、硬盘子系统、显示子系统、串并行接口、键盘等硬件情况的检测 主板的ROM：Read-Only Memory （只读存储） 该存储器上的数据只能读出，不能写入，其存储的数据一般在硬件出厂时就写入固定下来了，所以即使切断电源，数据也不会丢失，所以又称为固定存储器。 主板的RAM：Random Access Memory （随机存取存） 这里的随机取存，是指通电后，随时可在任意位置单元存取数据信息，不过断电后内部信息也随之消失。 所以对于一台计算机来讲，通电后第一件事件就是运行BIOS程序，BIOS程序最先做的，就是对硬件执行POST（加电自检），如果硬件自检不通过，会显示相应的错误，还会有相应的蜂鸣音 系统的启动引导方式有两种，分别是BIOS模式和UEFI模式： 在BIOS模式下，Bootloader第一阶段的程序(代码)存储在硬盘0磁道0扇区的前446个字节的空间内，由第一阶段的程序来引导第二阶段的bootloader程序，在linux中像ntloader，LILO，GRUB等都是采用这种分段执行的方式。 在UEFI模式下，直接由EFI系统分区中的 .efi 引导程序来引导操作系统。 Win11中的EFI分区 5.2 程序启动加载器bootloaderbootloader是底层硬件与上层应用软件(操作系统)之间的一个中间接口软件，它不是BIOS中的功能，也不是操作系统中的功能，它是一个独立的软件，运行在BIOS之后，操作系统启动之前，主要作用是将磁盘中操作系统的内核程序加载到内存中去并让CPU去执行 它是一个独立软件，可以单独安装，但在一般情况下，安装操作系统时，也会一起安装Bootloader程序不同的操作系统，会安装不同的Bootloader程序 5.3 启动流程 按电源键，通电 先执行bios程序，由bios程序临时接管整个硬件的控制 bios会读取自己的配置项（CMOS），找到一个启动盘（存放有操作系统的硬盘、光盘、移动u盘） 找到启动盘之后，会先读取启动盘的第一个扇区的数据512Byte拉起bootloader程序（前446引导信息，64是分区信息，后2位是结束的标志位） 446字节的引导程序又称之为bootloader（grub程序是我们常用的一种bootloader） bootloader程序启动之后，负责把操作系统后续的代码都加载到内存中，然后运行起来 接下来就由真正的操作系统掌握整个计算机的运行 bios操作会去检查各个驱动程序、硬件是否正常，反馈给真正的操作系统，然后就可以退出舞台 六、内核态、用户态、内核空间、用户空间内核空间与用户空间、内核态与用户态是 Linux（及类 Unix）操作系统中内存隔离和程序运行权限的两组核心概念，它们紧密关联，共同实现了操作系统的安全性和稳定性。 6.1 内核空间和用户空间操作系统将内存划分为两个独立区域，本质是通过内存地址范围的隔离，防止用户程序直接访问核心系统资源 内核空间 是操作系统内核使用的内存区域，占据高端内存地址。这里存放了内核代码、设备驱动程序、以及所有需要直接访问硬件资源的数据结构等。内核空间允许直接操作硬件资源和管理整个系统的状态。 用户空间 指的是操作系统为用户程序分配的内存区域，占据低端内存地址。每个运行在用户空间的应用程序都有其独立的地址空间，这有助于保护系统免受恶意或错误代码的影响。用户空间的应用程序不能直接访问硬件资源或关键的操作系统数据结构。当程序以用户态运行时，它只能访问属于自己的那部分用户空间内存，而不能直接访问内核空间或其他进程的空间。 6.2 内核态和用户态除了在嵌入式系统中的非常简单的CPU之外，多数CPU执行程序时都有两种运行模式，决定了程序能访问的资源和执行的指令集 内核态： 当cpu在内核态运行时，cpu可以执行指令集中所有的指令，很明显，所有的指令中包含了使用硬件的所有功能，（操作系统在内核态下运行，从而可以访问整个硬件） 用户态： 用户程序在用户态下运行，仅仅只能执行cpu整个指令集的一个子集，该子集中不包含操作硬件功能的部分，因此，一般情况下，在用户态中有关I&#x2F;O和内存保护（操作系统占用的内存是受保护的，不能被别的程序占用），当然，在用户态下，将PSW中的模式设置成内核态也是禁止的。 内核态与用户态切换： 用户态下工作的软件不能操作硬件，但是我们的软件比如暴风影音，一定会有操作硬件的需求，比如从磁盘上读一个电影文件，那就必须经历从用户态切换到内核态的过程，为此，用户程序必须使用系统调用（system call），系统调用陷入内核并调用操作系统，TRAP指令把用户态切换成内核态，并启用操作系统从而获得服务。 为什么要区分用户态和内核态？ 在CPU的所有指令中，有一些指令是非常危险的，如果错用，将导致整个系统崩溃。比如：清内存、设置时钟等。 所以，CPU将指令分为特权指令和非特权指令，对于那些危险的指令，只允许操作系统及其相关模块使用，普通的应用程序只能使用那些不会造成灾难的指令。 如此设计的本质意义是进行权限保护。 限定用户的程序不能乱搞操作系统，如果人人都可以任意读写任意地址空间软件管理便会乱套。 6.3 四者的关联关系1、用户程序默认在 “用户态 + 用户空间” 运行 当用户程序需要访问内核管理的资源（如读写文件、网络通信、申请内存）时，由于权限不足，无法直接操作，必须发起 “系统调用”。 2、系统调用触发 “用户态→内核态” 切换 用户程序通过特殊指令（如int 0x80或syscall）触发 CPU 切换到内核态，此时内核代表用户程序执行操作（如调用内核中的文件系统驱动）。 3、内核在 “内核态 + 内核空间” 完成操作 内核态程序可以直接访问内核空间的资源和硬件，完成用户请求后，将结果返回给用户程序，并切换回用户态。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"操作系统","slug":"操作系统","permalink":"https://aquapluto.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}]},{"title":"硬盘","slug":"ComputerBasic/computer-composition/hard-disk","date":"2025-08-21T02:38:40.000Z","updated":"2025-08-28T12:33:05.365Z","comments":true,"path":"ComputerBasic/computer-composition/hard-disk/","permalink":"https://aquapluto.github.io/ComputerBasic/computer-composition/hard-disk/","excerpt":"","text":"一、概念磁盘里存放的是磁信号，固态硬盘里存放的电子，断电数据都不会丢失，相当于人把事物记录到本子上，肯定不会忘记了。程序运行过程中产生的数据一定是先存放于内存中的，若想永久保存，必须由内存刷如硬盘。硬盘上也有缓存芯片 磁盘上是一连串的2进制位（称为bit位），为了统计方法，8个bit称为一个字节bytes，1024bytes&#x3D;1k，1024k&#x3D;1M，1024M&#x3D;1G,所以我们平时所说的磁盘容量最终指的就是磁盘能写多少个2进制位。 1234567891B= 8bit1KB=2(10)B=1024B；括号中的数字为2的指数(即多少次方) 1MB=2(10)KB=1024KB=2(20)B； 1GB=2(10)MB=1024MB=2(30)B。 1TB=2(10)GB=1024GB=2(40)B 1PB=2(10)TB=1024TB=2(50)B 1EB=2(10)PB=1024PB=2(60)B 1ZB=2(10)EB=1024EB=2(70)B 1YB=2(10)ZB=1024ZB=2(80)B 虽然计算机上存放的都是一个个的bit位，但是计算机存取硬盘的单位都是一个扇区，一个扇区512个字节。数据都存放于一段一段的扇区，从磁盘读取一段数据需要经历寻道时间和延迟时间 平均寻道时间： 机械手臂上的磁头找到存储数据的那一圈磁道所花费的时间 平均延迟时间： 磁盘转半圈的速度 站在硬盘的角度最小读写单位是一个扇区；站在操作系统的角度最小的读写单位一个block块（默认是由8个扇区） 二、磁盘类型硬盘接口类型 IDE：133MB&#x2F;s，并行接口，早期家用电脑 SCSI：640MB&#x2F;s，并行接口，早期服务器 SCSI 是常用且重要的数据传输协议之一，它支撑着操作系统对外部设备的i&#x2F;o操作。实际应用场景中，在linux系统上添加一个新的 scsi 磁盘是不需要重启的，一般可以通过扫描发现的操作来识别就绪设备 SATA：6Gbps，SATA数据端口与电源端口是分开的，即需要两条线，一条数据线，一条电源线 SAS：6Gbps，SAS是一整条线，数据端口与电源端口是一体化的，SAS中是包含供电线的，而SATA中不包含供电线。SATA标准其实是SAS标准的一个子集，二者可兼容，SATA硬盘可以插入SAS主板上，反之不行 USB：480MB&#x2F;s M.2： 注意：速度不是由单纯的接口类型决定，支持Nvme协议硬盘速度是最快的 服务器硬盘大小 LFF：3.5寸，一般见到的那种台式机硬盘的大小 SFF：Small Form Factor 小形状因数，2.5寸，注意不同于2.5寸的笔记本硬盘 L、S分别是大、小的意思，目前服务器或者盘柜采用sff规格的硬盘主要是考内虑增大单位密度内的磁盘容量、增强散热、减小功耗 三、机械硬盘和固态硬盘机械硬盘（HDD）：Hard Disk Drive，即是传统普通硬盘，主要由：盘片，磁头，盘片转轴及控制电机，磁头控制器，数据转换器，接口，缓存等几个部分组成。机械硬盘中所有的盘片都装在一个旋转轴上，每张盘片之间是平行的，在每个盘片的存储面上有一个磁头，磁头与盘片之间的距离比头发丝的直径还小，所有的磁头联在一个磁头控制器上，由磁头控制器负责各个磁头的运动。磁头可沿盘片的半径方向运动，加上盘片每分钟几千转的高速旋转，磁头就可以定位在盘片的指定位置上进行数据的读写操作。数据通过磁头由电磁流来改变极性方式被电磁流写到磁盘上，也可以通过相反方式读取。硬盘为精密设备，进入硬盘的空气必须过滤 固态硬盘（SSD）：Solid State Drive，用固态电子存储芯片阵列而制成的硬盘，由控制单元和存储单元（FLASH芯片、DRAM芯片）组成。固态硬盘在接口的规范和定义、功能及使用方法上与普通硬盘的完全相同，在产品外形和尺寸上也与普通硬盘一致 相较于HDD，SSD在防震抗摔、传输速率、功耗、重量、噪音上有明显优势，SSD传输速率性能是HDD的2倍 相较于SSD，HDD在价格、容量占有绝对优势 硬盘有价，数据无价，目前SSD不能完全取代HHD 四、硬盘存储方式在计算机存储系统中，硬盘是主要的数据存储设备之一，数据被存储在硬盘的表面上，而硬盘表面被划分为许多扇区，每个扇区都是数据存储和访问的最小单位。但实际中操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是 4KB，即连续八个 sector 组成一个 block。以下是磁盘和扇区之间的关系的要点 硬盘存储术语 CHS 硬盘驱动器：硬盘驱动器是计算机中的存储设备，它包含一个或多个磁盘盘片。这些盘片上有一个或多个磁性涂层，用于存储数据。 磁头head：一个盘面对应一个磁头，磁头数&#x3D;盘面数，硬盘驱动器上有多个读&#x2F;写头，它们位于磁盘的顶部和底部，用于读取和写入数据。磁头可以移动到不同的磁道上，以读取或写入数据。 磁道track：盘面上的每一圈就是一个磁道，磁道&#x3D;柱面数。磁盘表面被划分为一个个同心圆环，每个环被称为一个磁道（track）。数据通常存储在这些磁道上。 扇区sector：把每个磁道按512bytes大小再进行划分，这就是扇区，每个磁道上的扇区数量是不一样的。每个磁道被分为若干个扇区（sector），扇区是存储数据的最小物理单位。通常，每个扇区的大小为512字节或4K字节。 柱面cylinder：磁头移动的时候，是一起移动的，如果是6个盘面，则6个磁头对应的磁道是一致的，这就是柱面，柱面 1柱面&#x3D;512 * sector数&#x2F;trackhead数&#x3D;51263*255&#x3D;7.84M。柱面（cylinder）是垂直堆叠在多个盘片上的同心圆磁道的集合。在磁盘驱动器上，读&#x2F;写头可以同时访问相同柱面上的多个磁道。 数据在磁盘上的存储和检索过程通常涉及到读&#x2F;写头移动到正确的柱面和磁道上，然后在相应的扇区上进行数据传输。操作系统和文件系统管理这些硬件细节，以便应用程序可以读取和写入文件，而无需关心底层硬件的物理结构。 在Linux系统中，CentOS 5 之前版本 Linux 以柱面的整数倍划分分区，CentOS 6之后可以支持以扇区划分分区，CentOS7之后，就只显示扇区信息了 五、磁盘上寻址的两种方式CHS CHS采用 24 bit位寻址 其中前10位表示cylinder，中间8位表示head，后面6位表示sector 最大寻址空间 8 GB LBA LBA是一个整数，通过转换成 CHS 格式完成磁盘具体寻址 ATA-1规范中定义了28位寻址模式，以每扇区512位组来计算，ATA-1所定义的28位LBA上限达到128 GiB。2002年ATA-6规范采用48位LBA，同样以每扇区512位组计算容量上限可达128Petabytes 由于CHS寻址方式的寻址空间在大概8GB以内，所以在磁盘容量小于大概8GB时，可以使用CHS寻址方式或是LBA寻址方式；在磁盘容量大于大概8GB时，则只能使用LBA寻址方式 六、文件的 I&#x2F;O 模式Buffer I&#x2F;O (标准 I&#x2F;O) 原理：数据先写入内核的缓冲区（页面缓存），内核异步将缓冲区的脏数据写入磁盘。 优点： 读操作：大多数情况下直接从 RAM 缓存读取数据，速度快。 写操作：页面缓存减少磁盘 I&#x2F;O 次数，提高写性能。 缺点： 可能存在数据丢失风险，如系统崩溃或断电。 缓存溢出可能导致性能问题。 适用场景：适合大多数场景，尤其是读操作较多的应用。 Direct I&#x2F;O (直接 I&#x2F;O) 原理：数据直接写入磁盘，绕过页面缓存。 优点：避免缓冲区数据丢失的风险。 缺点：性能较低，因为每次写操作都需要直接与磁盘交互。 适用场景：适合对数据安全性要求较高的应用，如数据库系统。 七、磁盘 I&#x2F;O 调度策略在 Linux 中，磁盘 I&#x2F;O 调度策略用于决定磁盘上的 I&#x2F;O 请求的执行顺序，以最大程度地提高系统性能和吞吐量。不同的磁盘 I&#x2F;O 调度策略采用不同的算法来管理 I&#x2F;O 请求队列。以下是一些常见的Linux磁盘I&#x2F;O调度策略： CFQ（Completely Fair Queuing） CFQ 调度器旨在提供公平性，确保每个进程都能公平地访问磁盘。它为每个进程维护一个 I&#x2F;O 请求队列，并使用时间片轮转的方式为各个队列提供服务。在 Linux 2.6 内核上默认采用的就是该调度策略。 Deadline Deadline 调度器着重于 I&#x2F;O 请求的响应时间。它将 I&#x2F;O 请求分为两类：读取请求和写入请求，然后按照截止时间来排序。读取请求通常具有较短的截止时间，以减少读取延迟，而写入请求通常具有较长的截止时间，以优化写入性能。其核心就是在于保证每个 I&#x2F;O 请求在一定时间内一定要被服务到，避免某个请求饥饿。在 Linux 3.x 内核上默认采用的就是该调度策略。 NOOP NOOP 调度器是一个简单的 FIFO（先进先出）队列，不执行任何排序或优化。它通常用于高性能存储设备，如固态硬盘（SSD），因为这些设备通常具有较低的访问延迟。 BFQ（Budget Fair Queueing） BFQ 调度器旨在提供更好的磁盘 I&#x2F;O 吞吐量和响应时间。它采用基于权重的算法，为每个进程分配预算，然后根据预算来调度I&#x2F;O请求。BFQ 适用于需要更好响应时间的多任务工作负载。 Kyber Kyber 调度器是一个新的 I&#x2F;O 调度器，旨在兼顾低延迟和高吞吐量。它使用一种称为 Kyber Deadline 的方式来管理 I&#x2F;O 请求，尽量减少延迟并提高性能。 MQ-DEADLINE MQ-DEADLINE 是一种多队列版本的 Deadline 调度器，它在多核系统中更有效地管理 I&#x2F;O 请求队列。 如何查看 Linux 的磁盘 I&#x2F;O 调度策略？ 12345678[root@rocky8 ~]#cat /sys/block/sda/queue/scheduler[mq-deadline] kyber bfq none[root@ubuntu2004 ~]#cat /sys/block/sda/queue/scheduler[mq-deadline] none[mq-deadline]：表示当前的默认策略（但当前未激活）kyber bfq none：表示可选策略 八、衡量硬盘性能的常见指标IOPS（每秒 I&#x2F;O 操作次数） 定义：每秒磁盘完成的读或写操作次数。 计算公式：IOPS &#x3D; 1000 ms &#x2F; (寻道时间 + 旋转延迟)。 BPS（每秒 I&#x2F;O 流量） 定义：一秒内磁盘写入和读出的数据总量，单位为 MB&#x2F;s。 计算公式：BPS &#x3D; IOPS × 数据块大小 IO 操作的基本单位 扇区：硬盘的最小读写单位，默认大小为 512Bytes。 12345678~# fdisk -l磁盘 /dev/sda: 21.5 GB, 21474836480 字节, 41943040 个扇区Units = 扇区 of 1 * 512 = 512 bytes扇区大小(逻辑/物理): 512 字节 / 512 字节I/O 大小(最小/最佳): 512 字节 / 512 字节磁盘标签类型: dos磁盘标识符: 0x0009ecfa 块：操作系统的最小读写单位，通常指文件系统的逻辑块。(4096Bytes) 123456789~# stat /dev/sda3文件: &quot;/dev/sda3&quot;大小: 0 块: 0 IO 块: 4096 块特殊文件设备: 5h/5d Inode: 10432 硬链接: 1 设备类型: 8,3权限: (0660/brw-rw----) Uid: ( 0/ root) Gid: ( 6/ disk)最近访问: 2025-01-30 22:29:32.161000073 +0800最近更改: 2025-01-30 22:29:32.161000073 +0800最近改动: 2025-01-30 22:29:32.161000073 +0800创建时间: - 九、实际读写硬盘涉及的因素在实际读写硬盘的过程中，性能和行为会受到多个因素的影响，包括应用程序、操作系统和硬盘本身的特性。 应用程序并发任务数 定义：应用程序同时发起的 I&#x2F;O 请求数量。 影响： 并发任务数越高，磁盘的负载越大。 如果并发任务数超过磁盘的最大 IOPS 能力，会导致性能下降，甚至出现 I&#x2F;O 队列积压。 提交方式 同步提交 定义：应用程序提交一个 I&#x2F;O 请求后，会等待该请求完成后再提交下一个请求。 影响： 适合顺序读写操作。 性能较低，因为每次 I&#x2F;O 操作都需要等待完成。 异步提交 定义：应用程序提交一个 I&#x2F;O 请求后，不需要等待该请求完成，可以直接提交下一个请求。 影响： 适合高并发场景。 性能较高，因为可以充分利用磁盘的并行处理能力。 操作系统调度算法：操作系统使用的调度算法会影响 I&#x2F;O 请求的处理顺序和效率。 缓存机制：操作系统中的缓存机制（如 page cache）可以提高读写性能，但也可能导致数据一致性问题。 文件系统：不同的文件系统（如 ext4、xfs 等）有不同的性能特性和优化策略。 文件 IO 模式：选择合适的 IO 模式可以平衡性能和数据安全性。 硬盘本身类型：不同类型的硬盘（如 HDD、SSD）有不同的性能特点。HDD 通常具有较高的寻道时间和旋转延迟，而 SSD 则具有较低的延迟和更高的 IOPS。 容量：硬盘的容量大小也会影响其性能。大容量硬盘可能需要更长的时间来寻道和旋转。 接口：硬盘接口（如 SATA、SAS、NVMe）也会影响其传输速率和延迟。 磁盘访问方式 随机访问 定义：每次 I&#x2F;O 操作访问的逻辑块地址不连续。 影响： 性能较低，因为每次访问都需要经历平均延迟和平均寻道时间。 适合需要频繁随机读写的应用，如数据库查询。 顺序访问 定义：每次 I&#x2F;O 操作访问的逻辑块地址连续。 影响： 性能较高，因为只需要经历一次平均延迟和平均寻道时间，后续的块可以连续读取。 适合需要大量顺序读写的应用，如文件传输。 十、RAID硬盘分区痛点：一个硬盘损坏，其下所有分区全坏，解决需要用到RAID技术，优化磁盘文件系统 独立硬盘冗余阵列，旧称廉价磁盘冗余阵列，简称磁盘阵列。利用虚拟化存储技术把多个硬盘组合起来，成为一个或多个硬盘阵列组，目的为提升性能或数据冗余，或是两者同时提升。 RAID存储通过将数据重复或重新创建，并将其存储在附加的驱动器上来防止磁盘驱动器数据的完全丢失，这个过程也被称为数据冗余。 提供数据丢失保护的配置被称为“容错”配置，这意味着即使磁盘驱动器发生故障，阵列仍然可以成功运行并提供可恢复的数据。 磁盘冗余阵列（RAID）是一种用于提高数据存储可靠性和性能的技术。RAID 将多个硬盘驱动器组合在一起，以创建一个单一的逻辑存储单元，数据会分散存储在这些驱动器上，不同的 RAID 级别提供不同级别的冗余和性能。 RAID 层级不同，数据会以多种模式分散于各个硬盘，RAID 层级的命名会以 RAID 开头并带数字，例如：RAID 0、RAID 1、RAID 5、RAID 6、RAID 7、RAID 01、RAID 10、RAID 50、RAID 60。每种等级都有其理论上的优缺点，不同的等级在两个目标间获取平衡，分别是增加数据可靠性以及增加存储器（群）读写性能。 简单来说，RAID把多个硬盘组合成为一个逻辑硬盘，因此，操作系统只会把它当作一个实体硬盘。RAID常被用在服务器电脑上，并且常使用完全相同的硬盘作为组合。由于硬盘价格的不断下降与RAID功能更加有效地与主板集成，它也成为普通用户的一个选择，特别是需要大容量存储空间的工作，如：视频与音频制作。 条带化阵列是一种将数据分割并跨多个磁盘存储的技术，用以提高数据的读写性能。条带化（Striping）是RAID（Redundant Array of Independent Disks，独立磁盘冗余阵列）技术中的一种，它通过将连续的数据分割成相同大小的数据块，并将这些数据块分别写入到阵列中的不同磁盘上的方法来操作。条带化的主要目的是实现I&#x2F;O（输入&#x2F;输出）负载均衡和提高数据传输速度 RAID功能实现 提高IO能力,磁盘并行读写 提高耐用性,磁盘冗余算法来实现 RAID实现的方式 外接式磁盘阵列：通过扩展卡提供适配能力 内接式RAID：主板集成RAID控制器，安装OS前在BIOS里配置 软件RAID：通过OS实现，比如：群晖的NAS RAID-0RAID 0 也称为条带化（striping）阵列，它将数据块分成多个条带并将这些条带分散存储在多个硬盘上。RAID 0 提供了更高的性能，因为数据可以并行读取和写入多个驱动器。然而，RAID 0 没有冗余，如果一个驱动器故障，所有数据都会丢失。因此，如果你的程序读写数据频繁、且对数据安全性一般的话，就可采用 RAID 0 以 chunk 单位（大小可以指定）读写数据，因为读写时可以并行处理，所以在所有的级别中，RAID 0的速度是最快的。但是RAID 0既没有冗余功能，也不具备容错能力，如果一个磁盘（物理）损坏，所有数据都会丢失 每块硬盘的大小要一样，不管多少块硬盘，Linux上会认为是一个硬盘(&#x2F;dev&#x2F;sda)；比如现在有100M的文件，chunk&#x3D;64K，那么100M的文件就会分成若干个64K，第一个chunk放在A1，第二个放在A2…..均匀存放（n块硬盘，各有1&#x2F;n） 读、写性能提升 可用空间：N*min(S1,S2,…) 无容错能力 最少磁盘数：1+ RAID-1RAID 1 是镜像（mirroring）阵列，它将相同的数据复制到两个或更多硬盘上。因此 RAID 1 提供了数据冗余，如果一个驱动器故障，数据仍然可用。RAID 1 通常用于重要数据的备份。RAID 1 需要至少两个硬盘驱动器。 也称为镜像, 两组以上的N个磁盘相互作镜像，在一些多线程操作系统中能有很好的读取速度，理论上读取速度等于硬盘数量的倍数，与 RAID-0相同。另外写入速度有微小的降低，但解决了硬盘损坏时出现的问题 读性能提升、写性能略有下降 可用空间：1*min(S1,S2,…) 磁盘利用率 50%（因为两块硬盘一块是存放数据，一块是备份数据） 有冗余能力（可防止磁盘损坏带来的数据丢失，防止不了人为的删除，因为两边是同时操作的，不能代替传统的备份） 最少磁盘数：2+ RAID-4100M的文件，chunk&#x3D;64K，第一个放在A1，第二个放在A2，第三个放在A3，然后A1+A2+A3&#x3D;Ap（校验位），以此类推，好处就是假如其中一个硬盘坏了，可以通过校验位找回来（不能坏两块以上），但是万一放校验位的硬盘坏了，就都没有了，所以RAID-5解决这个问题 多块数据盘异或运算值存于专用校验盘（校验位都在一个硬盘） 磁盘利用率 (N-1)&#x2F;N 有冗余能力 至少3块硬盘才可以实现 RAID-5RAID 5 使用条带化和奇偶校验来提供数据冗余和性能。数据被分成多个条带，并且奇偶校验信息分布在不同的驱动器上。如果一个驱动器故障，可以通过奇偶校验信息恢复数据。RAID 5 需要至少三个硬盘驱动器。 每个硬盘都有校验位 读、写性能提升 可用空间：(N-1)*min(S1,S2,…) 有容错能力：允许最多1块磁盘损坏 最少磁盘数：3, 3+ RAID-6RAID 6 类似于 RAID 5，但提供更高级别的冗余。它使用两组奇偶校验信息来保护数据，因此可以容忍两个驱动器的故障。RAID 6 需要至少四个硬盘驱动器。 双份校验位,算法更复杂 读、写性能提升 可用空间：(N-2)*min(S1,S2,…) 有容错能力：允许最多2块磁盘损坏 最少磁盘数：4, 4+ RAID-10RAID 10 是RAID 1+0 的组合，它将数据复制到多个驱动器上，并使用条带化提供更高的性能。RAID 10提供了很高的性能和冗余，但需要至少四个硬盘驱动器。 读、写性能提升 可用空间：N*min(S1,S2,…)&#x2F;2 有容错能力：每组镜像最多只能坏一块 最少磁盘数：4, 4+ 容错性失败几率1&#x2F;3 RAID-01多块磁盘先实现RAID0,再组合成RAID1 容错性失败几率2&#x2F;3 RAID-50多块磁盘先实现RAID5,再组合成RAID0 RAID-60安全性高，提升数据安全 RAID 总结 RAID等级 最少硬盘 最大容错 可用容量 读取性能 写入性能 安全性 目的 应用场景 0 1 0 n n n 一个硬盘异常，全部硬盘都会异常 追求最大容量、速度 影片剪接，缓存用途 1 2 n-1 1 n 1 高，一个正常即可 追求最大安全性 个人、企业备份 5 3 1 n-1 n-1 n-1 高 追求最大容量、最小预算 个人、企业备份 6 4 2 n-2 n-2 n-2 安全性较RAID5高 同RAID 5，但较安全 个人、企业备份 10 4 高 综合RAID 0&#x2F;1优点，理论速度较快 大型数据库、服务器 50 6 高 提升数据安全 60 8 高 提升数据安全 要在 Linux 上设置 RAID，通常需要硬件支持或使用软件 RAID。软件 RAID 使用 Linux 内核中的软件驱动程序来管理 RAID 阵列，而硬件 RAID 依赖于专用 RAID 控制器。在 Linux 中，可以使用工具如 mdadm（用于软件 RAID 管理）或硬件 RAID 控制器的管理工具来创建、管理和监控 RAID 阵列。配置 RAID 阵列时，请确保备份重要数据，因为 RAID 虽然提供了一定程度的数据冗余，但并不是绝对可靠的备份解决方案。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}]},{"title":"内存","slug":"ComputerBasic/computer-composition/memory","date":"2025-08-21T02:38:36.000Z","updated":"2025-09-08T12:52:15.546Z","comments":true,"path":"ComputerBasic/computer-composition/memory/","permalink":"https://aquapluto.github.io/ComputerBasic/computer-composition/memory/","excerpt":"","text":"一、概念内存里存放的都是电信号，断电数据则丢失，相当于人脑失去记忆 cpu是从内存中取出指令来运行的，运行指令产生的数据也会放入内存中，所以内存又称之为主存，因为程序运行过程中产生的数据都是先存放于内存中 在计算机内存管理中，“页（Page）” 是虚拟内存和物理内存管理的基本单位（大小固定的块）。页的大小通常是固定的（如 4KB、2MB 等，由硬件和操作系统共同决定，x86 架构默认多为 4KB）。 二、物理内存与虚拟内存2.1 概念物理内存：是实际的硬件存储介质（如内存条），其大小通常小于或等于物理地址空间（例如，物理地址空间为 4GB 时，物理内存可能是 2GB、8GB 等）。物理内存中的每个存储单元都对应一个唯一的物理地址，属于物理地址空间的一部分。 虚拟内存：是一种 “逻辑存储技术”，通过将进程的虚拟地址空间映射到物理内存和磁盘（交换分区 &#x2F; 文件），让进程 “感觉” 自己拥有连续的大内存。虚拟内存的实际数据可能部分在物理内存中，部分在磁盘中（需要时由内核调入物理内存）。 虚拟内存是为了满足物理内存不足而提出的策略，通过将内存地址空间划分为固定大小的块（通常称为页或页帧），并在需要时将这些页面从磁盘交换到物理内存中来工作。这使得每个进程都有自己的独立地址空间，而不需要直接管理物理内存的分配。虚拟内存机制可以使计算机可以运行大于物理内存的程序，方法是将正在使用的程序放入内存取执行，而暂时不需要执行的程序放到磁盘的某块地方，这块地方成为虚拟内存，这种机制的核心在于快速地映射内存地址，整个过程计算机的速度被降低，但是保证不崩溃，由cpu中的一个部件负责，成为存储器管理单元。在linux中利用swap分区扩展内存的技术就是 “虚拟内存” 机制的一部分 要注意的是，虚拟内存并不是用来虚拟物理内存的，而是暂存数据的。所以虚拟内存不是代替物理内存来运行程序的。例如当运行某个大程序、大游戏，需要的内存超过空闲内存但小于物理内存总量时，会暂时把内存里这些数据放到磁盘上的虚拟内存里，空出物理内存运行游戏。等退出游戏后，又会把虚拟内存里的东西读出来，放回物理内存 2.2 页面缺失和页面调度页面缺失（Page Fault，也译为 “缺页异常”）和页面调度（Page Scheduling，也称为 “页面置换”）是虚拟内存管理中的核心机制，用于解决 “内存不足时如何高效使用有限物理内存” 的问题。两者紧密关联：页面缺失是触发页面调度的前提，而页面调度是处理页面缺失的关键手段 例如程序要读虚拟内存中的某个页面数据时，而恰好这个页面数据位于 Swap Space 中。那此时的流程就是：交换空间（Swap Space）中的数据在被读取时通常会首先被交换到物理内存，然后才能被程序访问。 2.2.1 页面缺失页面缺失的发生场景 页面被换出到磁盘：当程序尝试访问一个在物理内存中不存在的内存页时，会触发一个页面缺失。这可能是因为该页已经被交换到了交换空间中 程序首次访问该页：进程启动时，代码、数据等虚拟页面不会一次性全部加载到物理内存（“按需加载” 策略），首次访问时必然触发缺页 非法访问：若进程访问的虚拟地址未被分配（如越界访问），会触发 “非法缺页”，导致进程崩溃（如 Segmentation Fault）。 处理流程 进程访问虚拟地址，MMU（内存管理单元）检查页表，发现该页面未在物理内存（页表项标记为 “未有效”）。 CPU 触发缺页异常，暂停进程，转由内核的缺页异常处理程序处理。 内核判断缺页类型： 若为非法访问，发送信号终止进程（如 SIGSEGV）。 若为合法但未加载的页面，执行 “页面调入”： 若页面在磁盘文件中（如程序代码、数据文件），从文件加载到物理内存。 若页面曾被换出到 Swap 分区，从 Swap 加载回物理内存。 更新页表，将虚拟页映射到物理页，恢复进程执行。 2.2.2 页面调度当物理内存不足，无法容纳新调入的页面时，内核需要从物理内存中 “换出” 一部分页面，为新页面腾出空间，这一选择和换出的过程称为 “页面调度”。操作系统负责页面调度，在有限的物理内存中，尽可能保留 “最可能被再次访问” 的页面，减少后续缺页次数（降低磁盘 I&#x2F;O 开销），利用以下常用的调度算法决定哪些页面从交换空间加载到物理内存中以满足程序的需求。 LRU（最近最少使用）：优先换出 “最近一段时间内访问次数最少” 的页面（基于 “局部性原理”，最近少用的页面未来也可能少用）。 LFU（最不经常使用**）**：优先换出 “总访问次数最少” 的页面（适合长期运行的进程）。 FIFO（先进先出）：按页面进入内存的顺序换出最早的页面（简单但可能换出常用页面，效率较低）。 Clock 算法（改进的 FIFO）：给页面设置 “访问位”，遍历页面时，若访问位为 0 则换出，为 1 则置 0 并继续（近似 LRU，开销更低）。 Linux 中的 CLOCK-Pro：结合页面的访问频率和脏页（是否被修改）状态，优先保留频繁访问的干净页，减少换出脏页的磁盘写开销。 一旦数据加载到物理内存中，操作系统会更新进程的页表，以指示这些页面现在位于物理内存中，程序可以访问它们。页表是一个数据结构，用于映射虚拟地址到物理地址。 三、物理地址空间和虚拟地址空间物理地址空间 是物理内存中所有 “物理地址” 的集合，范围由硬件（如 CPU 地址总线宽度）决定。例如，32 位 CPU 的物理地址空间最大为 4GB（地址从 0 到 0xFFFFFFFF），对应物理内存的可寻址范围 在硬件中，扩展物理地址空间时插内存条按照奇数或偶数的插，不要按着1234的顺序插。 与物理内存的关系是：物理地址空间是物理内存的 “地址范围”，物理内存是该空间内实际被硬件实现的存储区域 虚拟地址空间 是进程 “看到” 的虚拟地址的集合，范围由操作系统和 CPU 决定（通常与 CPU 位数相关，如 64 位进程的虚拟地址空间可达 2⁶⁴字节）。每个进程独立拥有一套虚拟地址空间，彼此隔离。 与虚拟内存的关系是：虚拟地址空间是虚拟内存的 “地址范围”，虚拟内存是该空间对应的 “逻辑存储区域”（由物理内存和磁盘共同支撑）。进程通过访问虚拟地址空间中的地址，间接操作虚拟内存中的数据（由内核完成虚拟地址到物理地址的映射）。 3.1 MMU我们知道，CPU是通过寻址来访问内存的。32位CPU的寻址宽度是 0~0xFFFFFFFF ，16^8 计算后得到的大小是4G，也就是说可支持的物理内存最大是4G。但在实践过程中，碰到了这样的问题，程序需要使用4G内存，而可用物理内存小于4G，导致程序不得不降低内存占用。为了解决此类问题，现代CPU引入了 MMU（Memory Management Unit 内存管理单元） MMU 的核心思想是利用虚拟地址替代物理地址，即CPU寻址时使用虚址，由 MMU 负责将虚址映射为物理地址。MMU的引入，解决了对物理内存的限制，对程序来说，就像自己在使用4G内存一样。 内存分页(Paging)是在使用MMU的基础上，提出的一种内存管理机制。它将虚拟地址和物理地址按固定大小（4K）分割成页(page)和页帧(page frame)，并保证页与页帧的大小相同。这种机制从数据结构上，保证了访问内存的高效，并使OS能支持非连续性的内存分配。在程序内存不够用时，还可以将不常用的物理内存页转移到其他存储设备上，比如磁盘，即上文的虚拟内存。 虚拟地址与物理地址需要通过映射，才能使CPU正常工作。而映射就需要存储映射表。在现代CPU架构中，映射关系通常被存储在物理内存上一个被称之为页表(page table)的地方。页表是被存储在内存中的 3.2 TLB我们知道CPU通过总线访问内存，肯定慢于直接访问寄存器的。为了进一步优化性能，现代CPU架构引入了TLB，用来缓存一部分经常访问的页表内容，让 CPU 在转换地址时优先查询 TLB，若命中（找到映射）则直接完成转换，避免访问内存中的页表 工作流程 当 CPU 需要将虚拟地址转换为物理地址时，首先查询 TLB，看是否缓存了该虚拟地址对应的页表项 命中（TLB Hit）：直接从 TLB 中获取物理地址，快速完成转换 未命中（TLB Miss）：CPU 需访问内存中的页表查询映射，同时将查询到的页表项存入 TLB（替换旧的不常用项），供后续使用 3.3 大页内存HugepagesizeTLB是有限的，这点毫无疑问。当超出TLB的存储极限时，也会发生 TLB miss，之后，OS就会命令CPU去访问内存上的页表。如果频繁的出现TLB miss，程序的性能会下降地很快。为了让TLB可以存储更多的页地址映射关系，我们的做法是调大内存分页大小，如果一个页4M，对比一个页4K，前者可以让TLB多存储1000个页地址映射关系，性能的提升是比较可观的。 在linux中，需要2.6内核以上才能支持，可以通过下列命令来确认是否支持 1234# cat /proc/meminfo | grep HugeHugePages_Total: 0HugePages_Free: 0Hugepagesize: 2048 kB # 默认的大内存页size，相关配置参考 https://www.jianshu.com/p/b9470fc331dd 我们将OS共享内存最大值和大内存页数量的内核参数的值调大，以 Java 程序为例 12345# 共享内存段最大值，建议这个值大于Java Heap size，这个例子里设置了4G内存echo 4294967295 &gt; /proc/sys/kernel/shmmax# 大内存页数量，这个值一般是Java进程占用最大内存/单个页的大小 ，比如java设置1.5G，单个页10M，那么数量为 1536/10 = 154echo 154 &gt; /proc/sys/vm/nr_hugepages 注意：/proc 是一种基于内存的文件系统，并非存储在磁盘上，为了不让你的设置在重启后被冲掉，建议写个脚本放到 init 阶段(rc.local) 四、NUMANUMA（非统一内存访问）是一种关于多个CPU如何访问内存的架构模型，它是多处理器系统（如多 CPU 或多核 CPU）的一种内存架构，现在的CPU基本都是NUMA架构，linux内核2.5开始支持NUMA。 在 NUMA 架构下，多个处理器被划分到不同节点（Node）上，且每个 Node 都拥有自己的本地内存空间。而同一个 Node 内部的内存空间，实际上又可以进一步分为不同的内存域（Zone），比如直接内存访问区（DMA）、普通内存区（NORMAL）、伪内存区（MOVABLE）等，如下图所示： NUMA架构简单点儿说就是，一个物理cpu（一般包含多个逻辑cpu或者说多个核心）构成一个node，这个node不仅包括cpu，还包括一组内存插槽，也就是说一个物理cpu以及一块内存构成了一个node。每个cpu可以访问自己node下的内存，也可以访问其他node的内存，但是访问速度是不一样的，自己node下的更快。numactl –hardware 命令可以查看node状况。 通过numactl启动程序，可以指定node绑定规则和内存使用规则。可以通过cpunodebind参数使进程使用固定node上的cpu，使用localalloc参数指定进程只使用cpu所在node上分配的内存。如果分配的node上的内存足够用，这样可以减少抖动，提供性能。如果内存紧张，则应该使用interleave参数，否则进程会因为只能使用部分内存而out of memory或者使用swap分区造成性能下降。 NUMA的内存分配策略有localalloc、preferred、membind、interleave。 localalloc 规定进程从当前node上请求分配内存。 preferred 比较宽松地指定了一个推荐的node来获取内存，如果被推荐的node上没有足够内存，进程可以尝试别的node。 membind 可以指定若干个node，进程只能从这些指定的node上请求分配内存。 interleave 规定进程从指定的若干个node上以RR（Round Robin 轮询调度）算法交织地请求分配内存。 五、页高速缓存与页写回机制页高速缓存（Page Cache）和页写回机制（Page Writeback）是 Linux 内核中用于优化磁盘 I&#x2F;O 性能的核心机制，涉及内存与磁盘之间的数据交互策略 5.1 页高速缓存在当今的计算机系统中，处理器的运行速度是非常快的，但 RAM 和磁盘并没有质的飞跃（尤其是磁盘读写速度），这就导致了系统整体性能并没有因为处理器速度的提升而提升。于是就使用到了缓存技术（其实就是内存缓存的技术），通过缓存机制解决了处理器和磁盘直接速度的不平衡。 页高速缓存通常以页面的单位来存储数据，因此被称为”页”高速缓存。页高速缓存是操作系统在物理内存中维护的一个缓存，用于存储磁盘上的文件数据的副本。Linux 系统中当一个文件的数据（内容）被读取时，操作系统将数据从磁盘读取到页高速缓存中，以便后续的读取操作可以直接去内存中读取数据，更快速地访问数据。 因此页高速缓存提高了文件的读取性能，因为它允许频繁访问的数据保留在快速的内存中，而不是每次都从慢速的磁盘中读取。 5.2 页写回机制脏数据 (Dirty Pages)：缓冲区中未被写入磁盘的数据，称为脏数据。用于提高写性能，减少磁盘 I&#x2F;O 次数。 页写回机制是一种优化技术，用于减少文件写入操作对性能的影响，提高磁盘I&#x2F;O的性能。当文件数据被修改并需要写回磁盘时，操作系统通常不会立即将数据写回磁盘，而是将数据标记为”脏”，并将其保留在页高速缓存中。操作系统通过一种策略，例如延迟写回或按需写回，决定何时将脏数据写回磁盘。这允许操作系统将多个写操作合并，以减少磁盘写入的次数，提高性能。 页写回机制可以防止频繁的磁盘写入操作对系统性能造成明显的影响，因为它允许系统在更高效的时间进行磁盘写入，而不是在每个写操作之后立即进行。 linux中的脏数据落盘机制 落盘工具：Linux 内核中的 kworker&#x2F;flush 线程负责将脏数据写入磁盘。 落盘时机： 当脏数据比例超过 vm.dirty_background_ratio 时，开始异步写入。 当脏数据比例超过 vm.dirty_ratio 时，同步阻塞写入。 脏数据存活时间超过 vm.dirty_expire_centisecs 时，会强制落盘。 内核定期 (vm_dirty_writeback_centisecs 时间间隔) 唤醒 flush 线程处理脏数据。 六、进程使用内存问题6.1 内存泄漏：Memory Leak指程序中用malloc或new申请了一块内存，但是没有用free或delete将内存释放，导致这块内存一直处于占用状态 6.2 内存溢出：Memory Overflow指程序申请了10M的空间，但是在这个空间写入10M以上字节的数据，就是溢出,类似红杏出墙，比如10M的空间，我存放20M的数据，那么就会有10M存放到别的内存空间，而别的内存空间是由其他用户运行的，如果那个20M的数据是恶意代码，就会破坏其他用户的内存空间 6.3 内存不足：OOMOOM 即 Out Of Memory，“内存用完了”，系统会选一个进程将之杀死 OOM详解","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}]},{"title":"CPU","slug":"ComputerBasic/computer-composition/CPU","date":"2025-08-21T02:38:31.000Z","updated":"2025-09-09T10:49:35.310Z","comments":true,"path":"ComputerBasic/computer-composition/CPU/","permalink":"https://aquapluto.github.io/ComputerBasic/computer-composition/CPU/","excerpt":"","text":"一、概念中央处理器CPU由控制器 + 运算器组成，从内存中取指令-&gt;解码-&gt;执行，周而复始，直至整个程序被执行完成。所以CPU读取的数据都是从内存中来的 CPU读取的数据都是从主存储器（内存）来的！主存储器内的数据则是从输入单元所传输进来！而CPU处理完毕的数据也必须先写回主存储器中，最后数据才从主存储器传输到输出单元。 “物理核心” 是硬件层面的真实处理单元，决定了 CPU 的基础算力，在每个物理核心上会模拟出的 2 个逻辑线程（多线程） 2核4线程 &#x3D;》真2核假4核 4核 &#x3D;》真4核 寄存器是CPU 内部集成的高速存储单元，容量极小（通常以字节或字为单位，比如 32 位 CPU 的寄存器多为 32 位，64 位 CPU 多为 64 位），但访问速度极快（与 CPU 主频同步，纳秒级） 临时存储 CPU 正在处理的数据、指令或地址。例如，运算时，数据会先从内存加载到寄存器，再由 CPU 的运算单元（如 ALU）处理，结果也会暂存在寄存器中。 存放指令执行过程中的状态（如标志寄存器记录运算结果是否为零、是否溢出等）。 作为 CPU 与内存、外设之间的数据缓冲，减少 CPU 等待时间（因为内存访问速度远慢于 CPU）。 二、X86架构和64位x86架构：是针对cpu的型号或者说架构的一种统称，指明cpu的指令集是复杂指令集 64位：表示cpu一次性能够从内存中取出多少位二进制数(bit)，64bit指的是一次性能从内存中取出64位二进制指令，64位的cpu可以运行64位、32位的程序；而32位的cpu只能运行32位的程序 三、CPU的使用率和负载CPU时间片： 我们现在所使用的Windows、Linux、Mac OS都是“多任务操作系统”，就是说他们可以“同时”运行多个程序，比如一边打开Chrome浏览器浏览网页还能一边听音乐。 但是，实际上一个CPU内核在同一时刻只能干一件事，那操作系统是如何实现“多任务”的呢？大概的方法是让多个进程轮流使用CPU一小段时间，由于这个“一小段时间”很短(在linux上为5ms-800ms之间)，用户感觉不到，就好像是几个程序同时在运行了。上面提到的“一小段时间”就是我们所说的CPU时间片，CPU的现代分时多任务操作系统对CPU都是分时间片使用的。 CPU利用率： 就是程序对CPU时间片的占用情况，即CPU使用率 &#x3D; CPU时间片被程序使用的时间 &#x2F; 总时间。比如A进程占用10ms，然后B进程占用30ms，然后空闲60ms，再又是A进程占10ms，B进程占30ms，空闲60ms，如果在一段时间内都是如此，那么这段时间内的CPU占用率为40%。CPU利用率显示的是程序在运行期间实时占用的CPU百分比。 CPU负载： 指的是一段时间内正在使用和等待使用CPU的任务数。简单理解，CPU利用率是CPU的实时使用情况，CPU负载是CPU的当前以及未来一段时间的使用情况。举例来说：如果我有一个程序它需要一直使用CPU的运算功能，那么此时CPU的使用率可能达到100%，但是CPU的工作负载则是趋近于“1”，因为CPU仅负责一个工作嘛！如果同时执行这样的程序两个呢？CPU的使用率还是100%，但是工作负载则变成2了。所以也就是说，CPU的工作负载越大，代表CPU必须要在不同的工作之间进行频繁的工作切换。无论CPU的利用率是高是低，跟后面有多少任务在排队(CPU负载)没有必然关系。 总结： load average平均负载（负载指的是几个活跃的活要干）：在一段时间内，处于R状态的进程数+不可中断D睡眠的进程数，衡量系统的繁忙程度，假设有4个CPU 平均负载为3，代有个3个活跃进程，—》低负载 平均负载为4，代有个4个活跃进程，—》满负载 平均负载&gt;4， —》超负载 CPU的使用率：反应的是CPU的使用情况 如果一个进程（包括用户态us和内核态sy）的cpu使用率为100%，说明占了一个cpu核心 如果为200%，说明占了两个cpu核心","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}]},{"title":"计算机硬件组成","slug":"ComputerBasic/computer-composition/hardware-composition","date":"2025-08-21T02:38:26.000Z","updated":"2025-09-08T05:23:23.787Z","comments":true,"path":"ComputerBasic/computer-composition/hardware-composition/","permalink":"https://aquapluto.github.io/ComputerBasic/computer-composition/hardware-composition/","excerpt":"","text":"1 计算机硬件组成计算机硬件的核心由五大部分组成 控制器：是计算机的指挥系统，负责控制所有其他硬件的运行 运算器：数学运算+逻辑运算 存储器：是计算机用来存放所有数据和程序的记忆部件。按指定的地址写入或者读取信息。 内存&#x2F;主存 RAM：基于电信号来存储数据 优点：存取速度都快 缺点：没有办法持久存储，断电数据就全部丢失 外存：机械磁盘基于磁信号来存储数据 优点：可以持久保存数据 缺点：存储速度都慢 CMOS：与内存一样断电数据就丢，但特点是耗电量非常低，由主板上的电池负责供电 输入设备 键盘、鼠标 输出设备 显示器、音响、打印机 站在计算机硬件的角度：一个程序在计算机中是怎么运行起来的？ 在运行程序之前：程序最先一定是先存放于硬盘中的（程序的安装本质也就是把一堆代码文件放到硬盘的各个位置） 程序开始运行分两个阶段 加载阶段&#x2F;启动阶段：把程序的指令或数据从硬盘读入内存 执行阶段：cpu从内存中取出指令来运行 2 总线总线是计算机硬件组件（如 CPU、内存、外设等）之间传输数据、地址和控制信号的公共通信线路，相当于 “硬件之间的高速公路”。 数据总线：传输实际的数据（如内存与 CPU 之间交换的数值），宽度决定了一次能传输的数据量（如 64 位数据总线一次可传 64 位数据）。 地址总线：传输内存或外设的地址（如 CPU 要访问内存的某个位置，需通过地址总线指定地址），宽度决定了系统可寻址的最大范围（如 32 位地址总线最大支持 4GB 内存）。 控制总线：传输控制信号（如 “读 &#x2F; 写命令”“设备就绪信号” 等），协调各组件的操作（如 CPU 通过控制总线告诉内存 “现在要读取数据”）。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"}]},{"title":"脚本相关工具","slug":"Linux/shell/script-tools","date":"2025-08-20T10:29:56.000Z","updated":"2025-09-09T09:45:21.129Z","comments":true,"path":"Linux/shell/script-tools/","permalink":"https://aquapluto.github.io/Linux/shell/script-tools/","excerpt":"","text":"1 信号捕捉traptrap 命令可以捕捉信号,修改信号原来的功能,实现自定义功能 1234567891011121314151617#查看信号trap -l#进程收到系统发出的指定信号后，将执行自定义指令，而不会执行原操作trap &#x27;触发指令&#x27; 信号#忽略信号的操作trap &#x27;&#x27; 信号#恢复原信号的操作trap &#x27;-&#x27; 信号#列出自定义信号操作trap -p#当脚本退出时，执行finish函数trap finish EXIT 信号的三种表示方法 123453) SIGQUIT3 #信号IDSIGQUIT #完整写法，大小写都支持QUIT #简短写法，大小写都支持 范例 123456789101112131415161718192021#!/bin/bashtrap &quot;echo &#x27;Press ctrl+c or ctrl+\\ &#x27;&quot; int quittrap -pfor((i=0;i&lt;=10;i++));do sleep 1 echo $idonetrap &#x27;&#x27; inttrap -pfor((i=11;i&lt;=20;i++));do sleep 1 echo $idonetrap &#x27;-&#x27; inttrap -pfor((i=21;i&lt;=30;i++)); do sleep 1 echo $idone 范例: 当脚本正常或异常退出时,也会执行finish函数 12345678finish()&#123;echo finish| tee -a /root/finish.log&#125;trap finish exitwhile true ;doecho runningsleep 1done 2 创建临时文件mktempmktemp 命令用于创建并显示临时文件，可避免冲突 格式 12345mktemp [OPTION]... [TEMPLATE]说明：TEMPLATE: filenameXXX，X至少要出现三个，X会被替换成随机串-d #创建临时目录-p DIR或--tmpdir=DIR #指明临时文件所存放目录位置 范例 12345678910111213141516171819202122[root@ubuntu2204 ~]# mktemp/tmp/tmp.YQXxc3bHdI[root@ubuntu2204 ~]# mktemp XXXXUYtW#如果指定了文件名，则后面一定要有X，至少要3个，X会被替换成随机串[root@ubuntu2204 ~]# mktemp testmktemp: too few X&#x27;s in template ‘test’[root@ubuntu2204 ~]# mktemp testXXXtest19C#创建并赋值给变量[root@ubuntu2204 ~]# tmp=`mktemp`[root@ubuntu2204 ~]# echo $tmp/tmp/tmp.wQAdrqn1UR#创建目录[root@ubuntu2204 ~]# mktemp -d/tmp/tmp.2iLac1ruHt[root@ubuntu2204 ~]# ll /tmp/tmp.2iLac1ruHt/total 0 范例：实现文件垃圾箱 12345678910#方法1:脚本实现[root@centos8 ~]#cat /data/scripts/rm.sh#!/bin/bashDIR=`mktemp -d /tmp/trash-$(date +%F_%H-%M-%S)XXXXXX`mv $* $DIRecho $* is move to $DIR[root@centos8 ~]#alias rm=/data/scripts/rm.sh#方法2:函数实现[root@centos8 ~]#function rm () &#123; local trash=`mktemp -d /tmp/trashXXXX`;mv $* $trash; &#125; 3 安装复制文件installinstall 功能相当于cp，chmod，chown，chgrp ,mkdir 等相关工具的集合，默认加执行权限 格式 1234567891011121314151617install [OPTION]... [-T] SOURCE DEST 单文件install [OPTION]... SOURCE... DIRECTORYinstall [OPTION]... -t DIRECTORY SOURCE...install [OPTION]... -d DIRECTORY... #创建空目录-m MODE，默认755-o OWNER-g GROUP-d DIRNAME 目录#复制并默认加执行权限[root@centos8 ~]#install /etc/hosts /data/hosts#复制后并修改权限和所有者，所属组[root@centos8 ~]#install -m 600 -o wu -g bin /etc/hosts /data/hosts#创建文件夹[root@centos8 ~]#install -d /data/etc 4 交互式转化批处理工具expect主要应用于自动化交互式操作的场景，借助 expect 处理交互的命令，可以将交互过程如：ssh登录，ftp登录等写在一个脚本上，使之自动化完成。尤其适用于需要对多台服务器执行相同操作的环境中，可以大大提高系统管理人员的工作效率 1234expect [选项] [ -c cmds ] [ [ -[f|b] ] cmdfile ] [ args ]-c：从命令行执行expect脚本，默认expect是交互地执行的-d：可以调试信息 expect的子命令 spawn 启动新的进程，后面跟一个执行命令 expect 从进程接收字符串，如果有符合的字符串立即返回，否则就等待超时时间后返回 send 用于向进程发送字符串 interact 允许用户交互，在目标用户保持交互状态，不会退回原用户 expect eof 交互结束，退回到原用户 set timeout 20 设置超时时间，默认为10秒，若为-1则不限制超时时间 exp_continue 匹配多个字符串在执行动作后加此命令，附加于某个expect判断项之后，可以使该项被匹配后，还能继续匹配该expect-判断语句内的其他项。exp_continue类似于控制语句中的continue 语句。表示允许expect继续向下执行指令 单一分支模式语法： 123456[root@centos8 test]#expectexpect1.1&gt; expect &quot;hi&quot; &#123;send &quot;You said hi\\n&quot;&#125;hahahixixiYou said hi#匹配到hi后，会输出“you said hi”，并换行 多分支模式语法： 123456789101112131415161718192021222324252627282930[root@centos8 ~]#expectexpect1.1&gt; expect &quot;hi&quot; &#123; send &quot;You said hi\\n&quot; &#125; &quot;hehe&quot; &#123; send &quot;Hehe yourself\\n&quot;&#125; &quot;bye&quot; &#123; send &quot;Good bye\\n&quot; &#125;heheHehe yourselfexpect1.2&gt; expect &quot;hi&quot; &#123; send &quot;You said hi\\n&quot; &#125; &quot;hehe&quot; &#123; send &quot;Hehe yourself\\n&quot;&#125; &quot;bye&quot; &#123; send &quot;Good bye\\n&quot; &#125;byeGood byeexpect1.3&gt; expect &quot;hi&quot; &#123; send &quot;You said hi\\n&quot; &#125; &quot;hehe&quot; &#123; send &quot;Hehe yourself\\n&quot;&#125; &quot;bye&quot; &#123; send &quot;Good bye\\n&quot; &#125;hiYou said hiexpect1.4&gt;匹配hi,hello,bye任意字符串时，执行相应输出。等同如下：expect &#123; &quot;hi&quot; &#123; send &quot;You said hi\\n&quot;&#125; &quot;hehe&quot; &#123; send &quot;Hehe yourself\\n&quot;&#125; &quot;bye&quot; &#123; send &quot; Good bye\\n&quot;&#125;&#125;[root@centos8 ~]#expectexpect1.1&gt; expect &#123;+&gt; &quot;hi&quot; &#123; send &quot;You said hi\\n&quot;&#125;+&gt; &quot;hehe&quot; &#123; send &quot;Hehe yourself\\n&quot;&#125;+&gt; &quot;bye&quot; &#123; send &quot; Good bye\\n&quot;&#125;+&gt; &#125;byeGood byeexpect1.2&gt; expect脚本 12345命名：expect1/2/3...开头段：#!/usr/bin/expect 执行：先加执行权限，后执行chmod +x expect1./expect1 范例1：非交互式复制文件 1234567#!/usr/bin/expect spawn scp /etc/redhat-release 10.0.0.7:/dataexpect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;magedu\\n&quot; &#125;&#125;expect eof 范例2：自动登录 1234567#!/usr/bin/expectspawn ssh 10.0.0.7expect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;magedu\\n&quot; &#125;&#125;interact 范例3：expect 变量 1234567891011#!/usr/bin/expectset ip 10.0.0.7 #set设置变量set user rootset password mageduset timeout 10spawn ssh $user@$ipexpect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;$password\\n&quot; &#125;&#125;interact 范例4：expect 位置参数 123456789101112131415161718[root@centos8 ~]#cat expect4#!/usr/bin/expectset ip [lindex $argv 0] #[lindex $argv 0]相当于$1set user [lindex $argv 1]set password [lindex $argv 2]spawn ssh $user@$ipexpect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;$password\\n&quot; &#125;&#125;interact[root@centos8 ~]#./expect4 10.0.0.7 root mageduspawn ssh root@10.0.0.7root@10.0.0.7&#x27;s password:Last login: Wed Apr 29 15:34:14 2020 from 10.0.0.8[root@centos7 ~]#exitlogoutConnection to 10.0.0.7 closed. 范例5：expect 执行多个命令 123456789101112131415#!/usr/bin/expectset ip [lindex $argv 0]set user [lindex $argv 1]set password [lindex $argv 2]set timeout 10spawn ssh $user@$ipexpect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; #如果没有exp_continue，就不会判断yes/no，去判断password &quot;password&quot; &#123; send &quot;$password\\n&quot; &#125;&#125;expect &quot;]#&quot; &#123; send &quot;useradd haha\\n&quot; &#125; #远程登陆成功后创建用户hahaexpect &quot;]#&quot; &#123; send &quot;echo magedu |passwd --stdin haha\\n&quot; &#125; #修改密码send &quot;exit\\n&quot; #执行完之后退出expect eof#./ssh4.exp 10.0.0.7 root magedu 范例6：shell脚本调用 expect 1234567891011121314151617#!/bin/baship=$1user=$2password=$3 expect &lt;&lt;EOF #expect是交互式命令，所以可以用重定向 set timeout 20 spawn ssh $user@$ip expect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;$password\\n&quot; &#125; &#125; expect &quot;]#&quot; &#123; send &quot;useradd hehe\\n&quot; &#125; expect &quot;]#&quot; &#123; send &quot;echo magedu |passwd --stdin hehe\\n&quot; &#125; expect &quot;]#&quot; &#123; send &quot;exit\\n&quot; &#125; expect eof EOF#./ssh5.sh 192.168.8.10 root magedu 范例7: shell脚本利用循环调用expect在CentOS和Ubuntu上批量创建用户 1234567891011121314151617181920212223#!/bin/bashNET=10.0.0user=rootpassword=mageduIPLIST=&quot;718101&quot;for ID in $IPLIST;doip=$NET.$IDexpect &lt;&lt;EOFset timeout 20spawn ssh $user@$ipexpect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;$password\\n&quot; &#125;&#125;expect &quot;#&quot; &#123; send &quot;useradd test\\n&quot; &#125;expect &quot;#&quot; &#123; send &quot;exit\\n&quot; &#125;expect eofEOFdone 范例8：实现多个主机初始化 1234567891011121314151617181920212223#!/bin/bashNET=10.0.0user=rootpassword=mageduIPLIST=&quot;718&quot;for ID in $IPLIST ;doip=$NET.$IDexpect &lt;&lt;EOFset timeout 20spawn ssh $user@$ipexpect &#123; &quot;yes/no&quot; &#123; send &quot;yes\\n&quot;;exp_continue &#125; &quot;password&quot; &#123; send &quot;$password\\n&quot; &#125;&#125;expect &quot;#&quot; &#123; send &quot;sed -i &#x27;s/^SELINUX=enforcing/SELINUX=disabled/&#x27;/etc/selinux/config\\n&quot; &#125;expect &quot;#&quot; &#123; send &quot;setenforce 0\\n&quot; &#125; #永久关闭SELINUXexpect &quot;#&quot; &#123; send &quot;exit\\n&quot; &#125;expect eofEOFdone 5 随机生成mkpassed来自expect包 1234567[root@centos8 ~]#yum -y install expect#随机生成[root@centos8 ~]#mkpasswd#随机生成15位字符，其中3个数字，5个大写字母，2个小写字母，剩下的随意[root@centos8 ~]#mkpasswd -l 15 -d 3 -C 5 -c 2","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"字符串","slug":"Linux/shell/string","date":"2025-08-20T10:29:48.000Z","updated":"2025-09-09T09:45:39.145Z","comments":true,"path":"Linux/shell/string/","permalink":"https://aquapluto.github.io/Linux/shell/string/","excerpt":"","text":"1 字符串切片1.1 基于偏移量取字符串1234567891011121314151617#返回字符串变量var的字符的长度,一个汉字算一个字符$&#123;#var&#125;#返回字符串变量var中从第offset个字符后（不包括第offset个字符）的字符开始，到最后的部分，offset的取值在0 到 $&#123;#var&#125;-1 之间(bash4.2后，允许为负值)$&#123;var:offset&#125;#返回字符串变量var中从第offset个字符后（不包括第offset个字符）的字符开始，长度为number的部分$&#123;var:offset:number&#125;#取字符串的最右侧几个字符,取字符串的最右侧几个字符, 注意：冒号后必须有一空白字符$&#123;var: -length&#125;#从最左侧跳过offset字符，一直向右取到距离最右侧lengh个字符之前的内容,即:掐头去尾$&#123;var:offset:-length&#125;#先从最右侧向左取到length个字符开始，再向右取到距离最右侧offset个字符之间的内容,注意：-length前空格,并且length必须大于offset$&#123;var: -length:-offset&#125; 范例 1234567891011121314151617181920212223[root@centos8 script40]#str=abcdef我你他[root@centos8 script40]#echo $&#123;#str&#125;9[root@centos8 script40]#echo $&#123;str:2&#125;cdef我你他[root@centos8 script40]#echo $&#123;str:2:3&#125;cde[root@centos8 script40]#echo $&#123;str:-3&#125;abcdef我你他[root@centos8 script40]#echo $&#123;str: -3&#125;我你他[root@centos8 script40]#echo $&#123;str:2:-3&#125;cdef[root@centos8 script40]#echo $&#123;str: -2:-3&#125;-bash: -3: substring expression &lt; 0[root@centos8 script40]#echo $&#123;str: -3:-2&#125;我[root@centos8 script40]#echo $&#123;str:-3:-2&#125;abcdef我你他[root@centos8 script40]#echo $&#123;str: -3:-2&#125;我[root@centos8 script40]#echo $&#123;str: -5:-2&#125;ef我 1.2 基于模式取子串123456789#其中word可以是指定的任意字符,自左而右，查找var变量所存储的字符串中，第一次出现的word, 删除字符串开头至第一次出现word字符串（含）之间的所有字符,即懒惰模式,以第一个word为界删左留右$&#123;var#*word&#125;#从var变量的值中删除以word开头的部分$&#123;var#word&#125;#同上，贪婪模式，不同的是，删除的是字符串开头至最后一次由word指定的字符之间的所有内容,即贪婪模式,以最后一个word为界删左留右$&#123;var##*word&#125;$&#123;var##word&#125; 范例 123456789101112[root@centos8 ~]#file=&quot;var/log/messages&quot;[root@centos8 ~]#echo $&#123;file#*/&#125;log/messages[root@centos8 ~]#echo $&#123;file##*/&#125;messages#其中word可以是指定的任意字符,功能：自右而左，查找var变量所存储的字符串中，第一次出现的word,删除字符串最后一个字符向左至第一次出现word字符串（含）之间的所有字符,即懒惰模式,以从右向左的第一个word为界删右留左$&#123;var%word*&#125;$&#123;var%word&#125;#同上，只不过删除字符串最右侧的字符向左至最后一次出现word字符之间的所有字符,即贪婪模式,以从右向左的最后一个word为界删右留左$&#123;var%%word*&#125;$&#123;var%%word&#125; 范例 1234567891011[root@centos8 ~]#file=&quot;var/log/messages&quot;[root@centos8 ~]#echo $&#123;file%/*&#125;var/log[root@centos8 ~]#echo $&#123;file%%/*&#125;var[root@centos8 ~]#url=http://www.magedu.com:8080[root@centos8 ~]#echo $&#123;url##*:&#125;8080[root@centos8 ~]#echo $&#123;url%%:*&#125;http 2 查找替换1234567891011#查找var所表示的字符串中，第一次被pattern所匹配到的字符串，以substr替换之$&#123;var/pattern/substr&#125;#查找var所表示的字符串中，所有能被pattern所匹配到的字符串，以substr替换之$&#123;var//pattern/substr&#125;#查找var所表示的字符串中，行首被pattern所匹配到的字符串，以substr替换之$&#123;var/#pattern/substr&#125;#查找var所表示的字符串中，行尾被pattern所匹配到的字符串，以substr替换之$&#123;var/%pattern/substr&#125; 3 查找并删除1234567891011#删除var表示的字符串中第一次被pattern匹配到的字符串$&#123;var/pattern&#125;#删除var表示的字符串中所有被pattern匹配到的字符串$&#123;var//pattern&#125;#删除var表示的字符串中所有以pattern为行首匹配到的字符串$&#123;var/#pattern&#125;#删除var所表示的字符串中所有以pattern为行尾所匹配到的字符串$&#123;var/%pattern&#125; 4 字符大小写切换12345#把var中的所有小写字母转换为大写$&#123;var^^&#125;#把var中的所有大写字母转换为小写$&#123;var,,&#125; 5 变量扩展123#扩展以所有prefix开头的变量$&#123;!prefix*&#125;$&#123;!prefix@&#125; 范例 1234567[root@centos8 ~]#file1=a.txt[root@centos8 ~]#file2=b.txt[root@centos8 ~]#file3=c.txt[root@centos8 ~]#echo $&#123;!file*&#125;file1 file2 file3[root@centos8 ~]#echo $&#123;!file@&#125;file1 file2 file3","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"数组","slug":"Linux/shell/array","date":"2025-08-20T10:29:40.000Z","updated":"2025-09-09T09:27:11.147Z","comments":true,"path":"Linux/shell/array/","permalink":"https://aquapluto.github.io/Linux/shell/array/","excerpt":"","text":"1 声明数组1234#普通数组可以不事先声明,直接使用（索引的编号从0开始，属于数值索引:title[0]）declare -a ARRAY_NAME#关联数组必须先声明,再使用（索引可支持使用自定义的格式:title[num]）declare -A ARRAY_NAME 注意：两者不可相互转换 2 数组赋值数组元素的赋值 (1) 一次只赋值一个元素 1234ARRAY_NAME[INDEX]=VALUEweekdays[0]=&quot;Sunday&quot;weekdays[4]=&quot;Thursday&quot; (2) 一次赋值全部元素 123456ARRAY_NAME=(&quot;VAL1&quot; &quot;VAL2&quot; &quot;VAL3&quot; ...)title=(&quot;ceo&quot; &quot;coo&quot; &quot;cto&quot;)num=(&#123;0..10&#125;)alpha=(&#123;a..g&#125;)file=( *.sh ) (3) 只赋值特定元素 1ARRAY_NAME=([0]=&quot;VAL1&quot; [3]=&quot;VAL2&quot; ...) (4) 交互式数组值对赋值 1read -a ARRAY 范例: 123456789101112131415161718[root@centos8 ~]#declare -A course[root@centos8 ~]#declare -a course-bash: declare: course: cannot convert associative to indexed array[root@centos8 ~]#file=( *.sh )[root@centos8 ~]#declare -A file-bash: declare: file: cannot convert indexed to associative array[root@ubuntu1804 ~]#i=a[root@ubuntu1804 ~]#j=1[root@ubuntu1804 ~]#declare -A arr[root@ubuntu1804 ~]#arr[$i$j]=mage[root@ubuntu1804 ~]#j=2[root@ubuntu1804 ~]#arr[$i$j]=wang[root@ubuntu1804 ~]#echo $&#123;arr[*]&#125;wang mage[root@ubuntu1804 ~]#echo $&#123;arr[a1]&#125;mage[root@ubuntu1804 ~]#echo $&#123;arr[a2]&#125;wang 3 显示所有数组1declare -a 4 引用数组引用特定的数组元素 1234567891011$&#123;ARRAY_NAME[INDEX]&#125;#如果省略[INDEX]表示引用下标为0的元素[root@centos8 ~]#declare -a title=([0]=&quot;ceo&quot; [1]=&quot;coo&quot; [2]=&quot;cto&quot;)[root@centos8 ~]#echo $&#123;title[1]&#125;coo[root@centos8 ~]#echo $&#123;title&#125;ceo[root@centos8 ~]#echo $&#123;title[2]&#125;cto[root@centos8 ~]#echo $&#123;title[3]&#125; 引用数组所有元素 1234567$&#123;ARRAY_NAME[*]&#125;$&#123;ARRAY_NAME[@]&#125;[root@centos8 ~]#echo $&#123;title[@]&#125;ceo coo cto[root@centos8 ~]#echo $&#123;title[*]&#125;ceo coo cto 数组的长度，即数组中元素的个数 12345$&#123;#ARRAY_NAME[*]&#125;$&#123;#ARRAY_NAME[@]&#125;[root@centos8 ~]#echo $&#123;#title[*]&#125;3 数组的所有下标 12345678$&#123;!ARRAY_NAME[*]&#125;$&#123;!ARRAY_NAME[@]&#125;[root@centos8 ~]#declare -a title=([0]=&quot;ceo&quot; [1]=&quot;coo&quot; [2]=&quot;cto&quot;)[root@centos8 ~]#echo $&#123;!title[@]&#125;0 1 2[root@centos8 ~]#echo $&#123;!title[*]&#125;0 1 2 5 删除数组删除数组中的某元素，会导致稀疏格式 1234567unset ARRAY[INDEX][root@centos8 ~]#echo $&#123;title[*]&#125;ceo coo cto[root@centos8 ~]#unset title[1][root@centos8 ~]#echo $&#123;title[*]&#125;ceo cto 删除整个数组 12345unset ARRAY[root@centos8 ~]#unset title[root@centos8 ~]#echo $&#123;title[*]&#125;[root@centos8 ~]# 6 数组数据处理数组切片： 12345678910111213$&#123;ARRAY[@]:offset:number&#125;$&#123;ARRAY[*]:offset:number&#125;offset #要跳过的元素个数number #要取出的元素个数#取偏移量之后的所有元素&#123;ARRAY[@]:offset&#125;&#123;ARRAY[*]:offset&#125;[root@centos8 ~]#num=(&#123;0..10&#125;)[root@centos8 ~]#echo $&#123;num[*]:2:3&#125;2 3 4[root@centos8 ~]#echo $&#123;num[*]:6&#125;6 7 8 9 10 向数组中追加元素： 12345678ARRAY[$&#123;#ARRAY[*]&#125;]=valueARRAY[$&#123;#ARRAY[@]&#125;]=value[root@centos8 ~]#num[$&#123;#num[@]&#125;]=11[root@centos8 ~]#echo $&#123;#num[@]&#125;12[root@centos8 ~]#echo $&#123;num[@]&#125;0 1 2 3 4 5 6 7 8 9 10 11 7 关联数组12declare -A ARRAY_NAMEARRAY_NAME=([idx_name1]=&#x27;val1&#x27; [idx_name2]=&#x27;val2‘...) 注意：关联数组必须先声明再调用 范例: 1234567891011121314151617181920212223242526[root@centos8 ~]#name[ceo]=mage[root@centos8 ~]#name[cto]=wang[root@centos8 ~]#name[coo]=zhang[root@centos8 ~]#echo $&#123;name[ceo]&#125;zhang[root@centos8 ~]#echo $&#123;name[cto]&#125;zhang[root@centos8 ~]#echo $&#123;name[coo]&#125;zhang[root@centos8 ~]#echo $&#123;name&#125;zhang[root@centos8 ~]#declare -A name-bash: declare: name: cannot convert indexed to associative array[root@centos8 ~]#unset name[root@centos8 ~]#declare -A name[root@centos8 ~]#name[ceo]=mage[root@centos8 ~]#name[cto]=wang[root@centos8 ~]#name[coo]=zhang[root@centos8 ~]#echo $&#123;name[coo]&#125;zhang[root@centos8 ~]#echo $&#123;name[ceo]&#125;mage[root@centos8 ~]#echo $&#123;name[cto]&#125;wang[root@centos8 ~]#echo $&#123;name[*]&#125;mage wang zhang 8 范例生成10个随机数保存于数组中，并找出其最大值和最小值 12345678910111213#!/bin/bashdeclare -i min max #声明min和max两个变量，i表示数字declare -a numsfor ((i=0;i&lt;10;i++));do nums[$i]=$RANDOM [ $i -eq 0 ] &amp;&amp; min=$&#123;nums[0]&#125; &amp;&amp; max=$&#123;nums[0]&#125;&amp;&amp; continue [ $&#123;nums[$i]&#125; -gt $max ] &amp;&amp; max=$&#123;nums[$i]&#125; &amp;&amp; continue [ $&#123;nums[$i]&#125; -lt $min ] &amp;&amp; min=$&#123;nums[$i]&#125;doneecho &quot;All numbers are $&#123;nums[*]&#125;&quot;echo Max is $maxecho Min is $min 编写脚本，定义一个数组，数组中的元素对应的值是&#x2F;var&#x2F;log目录下所有以.log结尾的文件；统计出其下标为偶数的文件中的行数之和 1234567891011#!/bin/bashdeclare -a filesfiles=(/var/log/*.log)declare -i lines=0for i in $(seq 0 $[$&#123;#files[*]&#125;-1]); do if [ $[$i%2] -eq 0 ];thenlet lines+=$(wc -l $&#123;files[$i]&#125; | cut -d&#x27; &#x27; -f1) fidoneecho &quot;Lines: $lines&quot;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"函数","slug":"Linux/shell/function","date":"2025-08-20T10:29:36.000Z","updated":"2025-09-09T09:27:47.522Z","comments":true,"path":"Linux/shell/function/","permalink":"https://aquapluto.github.io/Linux/shell/function/","excerpt":"","text":"1 定义函数1234567891011121314#语法一：func_name （）&#123;...函数体...&#125;#语法二：function func_name &#123;...函数体...&#125;#语法三：function func_name （） &#123;...函数体...&#125; 范例： 12345678[root@centos scripts]#cat hello.sh hello() &#123; echo &quot;hello&quot;&#125;hello[root@centos scripts]#bash hello.sh hello 2 查看函数1234567891011#查看当前已定义的函数名declare -F#查看当前已定义的函数定义declare -f#查看指定当前已定义的函数名declare -f func_name#查看当前已定义的函数名定义declare -F func_name 3 删除函数1unset func_name 4 函数调用函数的调用方式 可在交互式环境下定义函数 可将函数放在脚本文件中作为它的一部分 可放在只包含函数的单独文件中 调用：函数只有被调用才会执行，通过给定函数名调用函数，函数名出现的地方，会被自动替换为函数代码 函数的生命周期：被调用时创建，返回时终止 4.1 交互式环境调用1234567891011121314[root@centos8 ~]#dir() &#123;&gt; ls -l&gt; &#125;[root@centos8 ~]#dirtotal 4-rw-------. 1 root root 1559 Nov 7 19:33 anaconda-ks.cfg #范例:实现判断CentOS的主版本[root@centos8 ~]#centos_version() &#123;&gt; sed -rn &#x27;s#^.* +([0-9]+)\\..*#\\1#p&#x27; /etc/redhat-release&gt; &#125;[root@centos8 ~]#centos_version8 4.2 在脚本中定义及使用函数在使用前必须定义，因此应将函数定义放在脚本开始部分，直至shell首次发现它后才能使用，调用函数仅使用其函数名即可 1234567891011121314[root@centos8 ~]#cat func1.sh#!/bin/bash#name:func1hello()&#123;echo &quot;Hello there today&#x27;s date is `date +%F`&quot;&#125;echo &quot;now going to the function hello&quot;helloecho &quot;back from the function&quot;[root@centos8 ~]#./func1.shnow going to the function helloHello there today&#x27;s date is 2019-12-18back from the function 范例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/bin/bashdisable_selinux()&#123; sed -i.bak &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config echo &quot;SElinux已禁用,重新启动后才可生效&quot;&#125;disable_firewall()&#123; systemctl disable --now firewalld &amp;&gt; /dev/null echo &quot;防火墙已禁用&quot;&#125;set_ps1() &#123; echo &quot;PS1=&#x27;\\[\\e[1;35m\\][\\u@\\h \\W]\\\\$\\[\\e[0m\\]&#x27;&quot; &gt; /etc/profile.d/reset.sh echo &quot;提示符已修改成功,请重新登录生效&quot;&#125;set_eth()&#123; sed -i.bak &#x27;/GRUB_CMDLINE_LINUX=/s#&quot;$# net.ifnames=0&quot;#&#x27; /etc/default/grub grub2-mkconfig -o /boot/grub2/grub.cfg &amp;&gt; /dev/null echo &quot;网络名称已修改成功,请重新启动才能生效&quot;&#125;PS3=&quot;请选择相应的编号(1-6): &quot;MENU=&#x27;禁用SELinux关防火墙修改提示符修改网卡名以上全实现退出&#x27;select M in $MENU ;docase $REPLY in1) disable_selinux ;;2) disable_firewall ;;3) set_ps1 ;;4) set_eth ;;5) disable_selinux disable_firewall set_ps1 set_eth ;;6) break ;;*) echo &quot;请输入正确的数字&quot;esacdone 4.3 使用函数文件可以将经常使用的函数存入一个单独的函数文件，然后将函数文件载入shell，再进行调用函数 函数文件名可任意选取，但最好与相关任务有某种联系，例如：functions 一旦函数文件载入shell，就可以在命令行或脚本中调用函数。可以使用delcare -f 或set 命令查看所有定义的函数，其输出列表包括已经载入shell的所有函数 若要改动函数，首先用unset命令从shell中删除函数。改动完毕后，再重新载入此文件 实现函数文件的过程： 创建函数文件，只存放函数的定义 在shell脚本或交互式shell中调用函数文件，格式如下： 12. filenamesource filename 范例 1234567891011121314[root@centos8 ~]#cat functions#!/bin/bash#functionshello()&#123; echo Run hello Function&#125;hello2()&#123; echo Run hello2 Function&#125;[root@centos8 ~]#. functions[root@centos8 ~]#helloRun hello Function[root@centos8 ~]#hello2Run hello2 Function 5 函数返回值函数的执行结果返回值： 使用echo等命令进行输出 函数体中调用命令的输出结果 函数的退出状态码： 默认取决于函数中执行的最后一条命令的退出状态码 自定义退出状态码，其格式为： return 从函数中返回，用最后状态命令决定返回值 return 0 无错误返回 return 1-255 有错误返回 6 环境函数类拟于环境变量，也可以定义环境函数，使子进程也可使用父进程定义的函数 定义环境函数： 12export -f function_namedeclare -xf function_name 查看环境函数： 12export -fdeclare -xf 7 函数参数函数可以接受参数： 传递参数给函数：在函数名后面以空白分隔给定参数列表即可，如：testfunc arg1 arg2 … 在函数体中当中，可使用1, 2, …调用这些参数；还可以使用@, *, $#等特殊变量 范例：阶乘 1234567891011#!/bin/bash#fact() &#123; if [ $1 -eq 1 ]; thenecho 1 elseecho $[$1*$(fact $[$1-1])] fi&#125;fact 100 #100对应的就是$1【fact $1】[root@centos8 ~]#bash bact.sh 100 范例：实现进度条功能 12345678910111213141516171819202122#!/bin/bashfunction print_chars()&#123; # 传入的第一个参数指定要打印的字符串 local char=&quot;$1&quot; # 传入的第二个参数指定要打印多少次指定的字符串 local number=&quot;$2&quot; local c for ((c = 0; c &lt; number; ++c)); do printf &quot;$char&quot; done&#125;COLOR=32declare -i end=50for ((i = 1; i &lt;= end; ++i)); do printf &quot;\\e[1;$&#123;COLOR&#125;m\\e[80D[&quot; print_chars &quot;#&quot; $i print_chars &quot; &quot; $((end - i)) printf &quot;] %3d%%\\e[0m&quot; $((i * 2)) sleep 0.1sdoneecho 8 函数变量变量作用域： 普通变量：只在当前shell进程有效，为执行脚本会启动专用子shell进程；因此，本地变量的作用范围是当前shell脚本程序文件，包括脚本中的函数 环境变量：当前shell和子shell有效 本地变量：函数的生命周期；函数结束时变量被自动销毁 注意： 如果函数中定义了普通变量，且名称和局部变量相同，则使用本地变量 由于普通变量和局部变量会冲突，建议在函数中只使用本地变量 在函数中定义本地变量的方法 1234567891011local NAME=VALUE#不会改变函数体外的变量，只改变函数内的变量[root@centos scripts]#name=wang;hello() &#123; name=wu;echo name=$name; &#125;;helloname=wu[root@centos scripts]#echo $namewu[root@centos scripts]#name=wang;hello() &#123; local name=wu;echo name=$name; &#125;;helloname=wu[root@centos scripts]#echo $namewang 9 函数递归函数递归：函数直接或间接调用自身，注意递归层数，可能会陷入死循环 递归特点: 函数内部自已调用自已 必须有结束函数的出口语句,防止死循环 范例: 无出口的递归函数调用 12[root@centos8 ~]#func () &#123; echo $i;echo &quot;run func&quot;;let i++; func; &#125;[root@centos8 ~]#func 范例：无限打印hello 123456789[root@centos scripts]#cat hello.sh #!/bin/bashhello() &#123; echo &quot;hello&quot; hello&#125;hello 范例：阶乘 123456789#!/bin/bashfact() &#123; if [ $1 -eq 1 ]; thenecho 1 elseecho $[$1*$(fact $[$1-1])] #一直递归fact函数，实现100*99*98...*1 fi&#125;fact 100 #100对应的就是$1 fork 炸弹是一种恶意程序，它的内部是一个不断在 fork 进程的无限循环，实质是一个简单的递归程序。由于程序是递归的，如果没有任何限制，这会导致这个简单的程序迅速耗尽系统里面的所有资源 12345678#函数实现:()&#123; :|:&amp; &#125;;:bomb() &#123; bomb | bomb &amp; &#125;; bomb#脚本实现cat Bomb.sh#!/bin/bash./$0|./$0&amp;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"进程锁","slug":"Linux/shell/process-lock","date":"2025-08-20T10:29:31.000Z","updated":"2025-09-10T02:33:03.305Z","comments":true,"path":"Linux/shell/process-lock/","permalink":"https://aquapluto.github.io/Linux/shell/process-lock/","excerpt":"","text":"1 进程锁的概念进程锁（Process Lock） 是一种在 Shell 脚本中实现防止多个实例同时运行的常见技术。它通过文件系统中的一个特殊文件（通常称为 lock file，锁文件）来实现互斥。防止多个实例同时运行，避免同一脚本或程序被并发执行，导致资源竞争、数据混乱或重复处理。 锁文件（Lock File）：是一个普通文件，通常位于 /var/run/ 或 /tmp/ 目录下，如 my_script.lock。 作用：当脚本开始执行时，尝试创建这个文件；如果创建成功，表示获得“锁”，可以继续执行；如果文件已存在，说明另一个实例正在运行，当前脚本应退出或等待。 内容：锁文件中通常写入当前进程的 PID（$$），便于排查和清理。 2 实现方式2.1 文件锁通过创建一个临时锁文件，利用文件系统的特性实现互斥： 进程启动时检查锁文件是否存在，不存在则创建并持有锁 存在则说明已有进程在运行，当前进程退出或等待 进程结束时删除锁文件释放锁 123456789101112131415161718192021222324[root@aliyun ~]# cat lock.sh #!/bin/bashlock_file=/tmp/echo1.lock #判断进程是否正在运行if [ -f $lock_file ];then pid=`cat $lock_file` ps $pid &amp;&gt;/dev/null [ $? -eq 0 ] &amp;&amp; echo &quot;Script1 is running...&quot; &amp;&amp; exit 1 #if [ $? -eq 0 ];then # echo &quot;Script1 is running...&quot; # exit 1 #fifi #创建锁echo $$ &gt; $lock_file echo &quot;lock1 begin...&quot;sleep 500echo &quot;lock1 end&quot; #释放锁rm -rf $lock_file 通过判断锁文件（/tmp/echo1.lock）是否存在，以及锁文件中记录的进程 ID（$$ 表示当前进程 ID）对应的进程是否存活，来检测是否有其他实例在运行。 若锁文件存在且对应进程正在运行，则当前脚本退出，避免并发执行。 若锁文件不存在或对应进程已结束，则创建锁文件并写入当前进程 ID，执行核心逻辑（sleep 500 模拟操作），完成后删除锁文件释放资源。 2.2 flock命令使用系统提供的 flock 命令（本质也是基于文件锁机制） 12345678910111213141516171819202122#!/bin/bashLOCKFILE=&quot;/tmp/my_script.lock&quot;# 尝试创建锁文件，使用重定向和文件描述符exec 200&gt;&quot;$LOCKFILE&quot;# 判断是否获取锁（即文件是否已被其他进程占用）if ! flock -n 200; then echo &quot;脚本已在运行，退出。&quot; exit 1fi# 程序关键部分开始echo &quot;脚本开始执行，PID: $$&quot;echo &quot;$$&quot; &gt; &quot;$LOCKFILE&quot; # 写入当前PID# 模拟工作sleep 30# 脚本结束，锁会自动释放（文件描述符关闭）echo &quot;脚本执行完毕。&quot; 说明：flock -n 200 表示对文件描述符 200（对应锁文件）加非阻塞排他锁。若失败（已被其他进程锁定），则立即返回错误。 3 注意事项如果系统崩溃或脚本被 kill -9，锁文件可能不会自动删除。可在启动时检查 PID 是否仍存在 12345678910if [ -f &quot;$LOCKFILE&quot; ]; then PID=$(cat &quot;$LOCKFILE&quot;) if kill -0 &quot;$PID&quot; 2&gt;/dev/null; then echo &quot;仍在运行，退出&quot; exit 1 else echo &quot;旧锁文件，清理中...&quot; rm -f &quot;$LOCKFILE&quot; fifi 如果锁文件在/tmp 目录下可能在重启后清空，而 /var/run 更适合长期服务，可以使用 trap 清理 1trap &#x27;rm -f &quot;$LOCKFILE&quot;; exit&#x27; EXIT","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"shell队列实现线程并发控制","slug":"Linux/shell/queue","date":"2025-08-20T10:29:31.000Z","updated":"2025-09-10T02:41:39.332Z","comments":true,"path":"Linux/shell/queue/","permalink":"https://aquapluto.github.io/Linux/shell/queue/","excerpt":"","text":"shell队列实现线程并发控制","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"流程控制","slug":"Linux/shell/process-control","date":"2025-08-20T10:29:31.000Z","updated":"2025-09-09T09:44:55.401Z","comments":true,"path":"Linux/shell/process-control/","permalink":"https://aquapluto.github.io/Linux/shell/process-control/","excerpt":"","text":"1 条件选择1.1 if语句格式： 1if COMMANDS; then COMMANDS; [ elif COMMANDS; then COMMANDS; ]... [ else COMMANDS; ] fi 单分支： 123if 判断条件;then 条件为真的分支代码fi 双分支： 12345if 判断条件; then 条件为真的分支代码else 条件为假的分支代码fi 多分支： 12345678910if 判断条件1; then 条件1为真的分支代码elif 判断条件2; then 条件2为真的分支代码elif 判断条件3; then 条件3为真的分支代码...else 以上条件都为假的分支代码fi 范例：身体质量指数 (BMI） 1234567891011121314151617181920#!/bin/bashread -p &quot;请输入身高(m为单位): &quot; HIGHif [[ ! &quot;$HIGH&quot; =~ ^[0-2](\\.[0-9]&#123;,2&#125;)?$ ]];then echo &quot;输入错误的身高!&quot; exit 1firead -p &quot;请输入体重(kg为单位): &quot; WEIGHTif [[ ! &quot;$WEIGHT&quot; =~ ^[0-9]&#123;1,3&#125;$ ]];then echo &quot;输入错误的体重!&quot;; exit 2; fiBMI=`echo $WEIGHT/$HIGH^2|bc`if [ $BMI -le 18 ] ;then echo &quot;太瘦了,多吃点!&quot;elif [ $BMI -lt 24 ] ;then echo &quot;身材很棒!&quot;else echo &quot;太胖了,注意节食,加强运动!&quot;fi 1.2 case语句格式： 1234567891011121314case WORD in [PATTERN [| PATTERN]...) COMMANDS ;;]... esaccase 变量引用 inPAT1)分支1;;PAT2)分支2;;...*)默认分支;;esac case支持glob风格的通配符 1234* 任意长度任意字符? 任意单个字符[] 指定范围内的任意单个字符| 或者，如: a|b 范例 123456789101112131415161718192021222324252627#!/bin/bashread -p &quot;Do you agree(yes/no)? &quot; INPUTINPUT=`echo $INPUT | tr &#x27;A-Z&#x27; &#x27;a-z&#x27;`case $INPUT iny|yes) echo &quot;You input is YES&quot; ;;n|no) echo &quot;You input is NO&quot; ;;*) echo &quot;Input fales,please input yes or no!&quot;esac#!/bin/bashread -p &quot;Do you agree(yes/no)? &quot; INPUTcase $INPUT in[yY]|[Yy][Ee][Ss]) echo &quot;You input is YES&quot; ;;[Nn]|[Nn][Oo]) echo &quot;You input is NO&quot; ;;*) echo &quot;Input fales,please input yes or no!&quot; esac 范例: 文件后缀处理 1234567891011121314151617181920212223#!/bin/bashread -p &quot;please input a file: &quot; FILESUFFIX=`echo $FILE | grep -Eo &quot;[^.]+$&quot;`case $SUFFIX ingz) echo gzip ;;bz2) echo bzip2 ;;xz) echo xz ;;Z) echo compress ;;zip) echo zip ;;*) echo other ;;esac 范例：运维菜单实现版本2 1234567891011121314151617181920212223242526272829303132#!/bin/bashecho -en &quot;\\E[$[RANDOM%7+31];1m&quot;cat &lt;&lt;EOF请选择：1）备份数据库2）清理日志3）软件升级4）软件回滚5）删库跑路EOFecho -en &#x27;\\E[0m&#x27;read -p &quot;请输入上面数字1-5: &quot; MENUcase $MENU in1) echo &quot;执行备份数据库&quot; #./backup.sh ;;2) echo &quot;清理日志&quot; ;;3) echo &quot;软件升级&quot; ;;4) echo &quot;软件回滚&quot; ;;5) echo &quot;删库跑路&quot; ;;*) echo &quot;INPUT FALSE!&quot;esac 2 循环2.1 for循环格式1 123456789101112for NAME [in WORDS ... ] ; do COMMANDS; done#方式1for 变量名 in 列表;do 循环体done#方式2for 变量名 in 列表do 循环体done for 循环列表生成方式 直接给出列表 整数列表 12&#123;start..end&#125;$(seq [start [step]] end) 返回列表的命令 1$(COMMAND) 使用glob，如：***.**sh 变量引用，如：@，，$# 范例: 批量创建用户 123456#!/bin/bash[ $# -eq 0 ] &amp;&amp; &#123; echo &quot;Usage: createuser.sh USERNAME ...&quot; ;exit 1 ; &#125;for user ;do id $user &amp;&gt; /dev/null &amp;&amp; echo $user is exist || &#123; useradd $user ; echo $useris created; &#125;done 范例: 批量创建用户和并设置随机密码 12345678#!/bin/bashfor i in &#123;1..10&#125;;do useradd user$i PASS=`cat /dev/urandom | tr -dc &#x27;[:alnum:]&#x27; |head -c12` echo $PASS | passwd --stdin user$i &amp;&gt; /dev/null echo user$i:$PASS &gt;&gt; /data/user.log echo &quot;user$i is created&quot;done 范例: 九九乘法表 1234567891011121314#!/bin/bashfor i in &#123;1..9&#125;;do for j in `seq $i`;do echo -e &quot;$&#123;j&#125;x$&#123;i&#125;=$[j*i]\\t\\c&quot; done echodonefor((i=1;i&lt;=9;i++));do for((j=1;j&lt;=i;j++));do printf &quot;\\E[1;$[RANDOM%7+31]m$&#123;i&#125;x$&#123;j&#125;=$[i*j]\\E[0m\\t&quot; done printf &quot;\\n&quot;done 范例: 倒状的九九乘法表 1234567#!/bin/bashfor i in &#123;1..9&#125;;do for j in $(seq `echo $[10-$i]`);do echo -ne &quot;$&#123;j&#125;x`echo $[10-i]`=$(((10-i)*j))\\t&quot; done echodone 生产案例：将指定目录下的文件所有文件的后缀改名为 bak 后缀 1234567891011#!/bin/bashDIR=/data/testcd $DIR || &#123; echo 无法进入 $DIR;exit 1; &#125;for FILE in * ;doPRE=`echo $FILE|grep -Eo &quot;.*\\.&quot;` mv $FILE $&#123;PRE&#125;bak# PRE=`echo $FILE|rev|cut -d. -f 2-|rev`# PRE=`echo $FILE | sed -nr &#x27;s/(.*)\\.([^.]+)$/\\1/p&#x27;# SUFFIX=`echo $FILE | sed -nr &#x27;s/(.*)\\.([^.]+)$/\\2/p&#x27;`# mv $FILE $PRE.bakdone 格式2 123456for ((: for (( exp1; exp2; exp3 )); do COMMANDS; donefor ((控制变量初始化;条件判断表达式;控制变量的修正表达式))do 循环体done 范例 123456789#!/bin/bashfor((sum=0,i=1;i&lt;=100;i++));do let sum+=idoneecho sum=$sumfor((sum=0,i=1;i&lt;=100;sum+=i,i++));do truedoneecho sum=$sum 范例：等腰三角形 1234567891011#!/bin/bashread -p &quot;请输入三角形的行数: &quot; linefor((i=1;i&lt;=line;i++));do for((k=0;k&lt;=line-i;k++));do echo -e &#x27; \\c&#x27; done for((j=1;j&lt;=2*i-1;j++));do echo -e &#x27;*\\c&#x27; done echodone 范例：生成进度 12[root@centos8 ~]#for ((i = 0; i &lt;= 100; ++i)); do printf &quot;\\e[4D%3d%%&quot; $i;sleep 0.1s; done100%[root@centos8 ~]# 范例 12345678[root@centos8 ~]#for((;;));do echo for;sleep 1;doneforforforforforforfor 2.2 while循环格式： 12345while COMMANDS; do COMMANDS; donewhile CONDITION; do 循环体done 无限循环 1234567while true; do 循环体donewhile : ; do 循环体done 范例 12[root@centos8 ~]#sum=0;i=1;while ((i&lt;=100));do let sum+=i;let i++;done;echo $sum5050 范例 1234567891011121314151617181920#配置发邮件的邮箱[root@centos8 ~]#cat .mailrcset from=29308620@qq.comset smtp=smtp.qq.comset smtp-auth-user=29308620@qq.comset smtp-auth-password=esvnhbnqocirbicfset smtp-auth=loginset ssl-verify=ignore[root@centos8 ~]#cat while_diskcheck.sh#!/bin/bashWARNING=80while :;do USE=`df | sed -rn &#x27;/^\\/dev\\/sd/s#.* ([0-9]+)%.*#\\1#p&#x27; |sort -nr|head -n1` if [ $USE -gt $WARNING ];then echo Disk will be full from `hostname -I` | mail -s &quot;disk warning&quot;29308620@qq.com fi sleep 10done 范例: 防止Dos攻击的脚本 1234567891011121314#!/bin/bashWARNING=10touch deny_hosts.txtwhile true;do ss -nt | sed -nr &#x27;1!s#.* ([0-9.]+):[0-9]+ *#\\1#p&#x27;|sort |uniq -c|sort |while read count ip;do if [ $count -gt $WARNING ];then echo $ip is deny grep -q &quot;$ip&quot; deny_hosts.txt || &#123; echo $ip &gt;&gt; deny_hosts.txt;iptables -A INPUT -s $ip -j REJECT; &#125; fi done sleep 10done 特殊用法：while read 遍历文件或文本的每一行 格式： 123while read line; do 循环体done &lt; /PATH/FROM/SOMEFILE 说明：依次读取&#x2F;PATH&#x2F;FROM&#x2F;SOMEFILE文件中的每一行，且将行赋值给变量line 范例 12345678910[root@centos8 ~]#echo magedu | while read X ; do echo $X;donemagedu[root@centos8 ~]#echo magedu | &#123; read X ; echo $X; &#125;magedu[root@centos8 ~]#echo magedu | ( read X ; echo $X )magedu[root@centos8 ~]#echo mage wang zhang | ( read X Y Z; echo $X $Y $Z )mage wang zhang[root@centos8 ~]#echo mage wang zhang | while read X Y Z; do echo $X $Y $Z;donemage wang zhang 范例:磁盘检测 12345678#!/bin/bashWARNING=80MAIL=root@wangxiaochun.comdf |sed -nr &quot;/^\\/dev\\/sd/s#^([^ ]+) .* ([0-9]+)%.*#\\1 \\2#p&quot;|while read DEVICE USE;do if [ $USE -gt $WARNING ] ;then echo &quot;$DEVICE will be full,use:$USE&quot; | mail -s &quot;DISK WARNING&quot; $MAIL fidone 范例:防止Dos攻击 12345678#!/bin/bashMAX=3lastb | sed -rn &#x27;/ssh:/s@.* ([0-9.]&#123;1,3&#125;&#123;3&#125;[0-9]&#123;1,3&#125;) .*@\\1@p&#x27;|sort |uniq -c|while read count ip ;do if [ $count -gt $MAX ];then iptables -A INPUT -s $ip -j REJECT fidone 范例：查看&#x2F;sbin&#x2F;nologin的shell类型的用户名和UID 1234567#!/bin/bashwhile read line ;do if [[ &quot;$line&quot; =~ /sbin/nologin$ ]] ;then echo $line | cut -d: -f1,3 fi done &lt; /etc/passwd 2.3 until循环12345until COMMANDS; do COMMANDS; doneuntil CONDITION; do 循环体done 范例 12[root@centos8 ~]#sum=0;i=1;until ((i&gt;100));do let sum+=i;let i++;done;echo $sum5050 2.4 循环控制continuecontinue [N]：提前结束第N层的本轮循环，而直接进入下一轮判断；最内层为第1层 123456789while CONDITION1; do CMD1 ... if CONDITION2; then continue fi CMDn ...done 2.5 循环控制breakbreak [N]：提前结束第N层整个循环，最内层为第1层 123456789while CONDITION1; do CMD1 ... if CONDITION2; then break fi CMDn ...done 范例 12345678910111213141516171819202122232425262728293031323334353637383940#!/bin/bashsum=0COLOR=&#x27;echo -e \\033[1;31m&#x27;COLOR2=&#x27;echo -e \\033[1;32m&#x27;END=&quot;\\033[0m&quot;while true;doecho -e &quot;\\033[33;1m\\c&quot;cat &lt;&lt;-EOF1) 鲍鱼2) 满汉全席3) 龙虾4) 燕窝5) 帝王蟹6) 点菜结束,结帐EOFecho -e &quot;\\033[0m&quot;read -p &quot;请点菜(1-6): &quot; MENUcase $MENU in1|4) $COLOR&#x27;菜价: $10&#x27;$END let sum+=10 ;;3|5) $COLOR&#x27;菜价: $20&#x27;$END let sum+=20 ;;2) $COLOR&#x27;菜价: $200000&#x27;$END let sum+=200000 ;;6) $COLOR2&quot;你点的菜总价格是 \\$$sum&quot;$END break ;;*) echo &quot;点错了,没有这道菜&quot; ;;esac$COLOR2&quot;你点的菜总价格是 \\$$sum&quot;$ENDdone 范例 123456789101112#!/bin/bashNUM=$[RANDOM%10]while read -p &quot;输入 0-9 之间的数字: &quot; INPUT ;do if [ $INPUT -eq $NUM ];then echo &quot;恭喜你猜对了!&quot; break elif [ $INPUT -gt $NUM ];then echo &quot;数字太大了,重新猜!&quot; else echo &quot;数字太小了,重新猜!&quot; fidone 2.6 循环控制shiftshift [n] 用于将参量列表 list 左移指定次数，缺省为左移一次。 参量列表 list 一旦被移动，最左端的那个参数就从列表中删除。while 循环遍历位置参量列表时，常用到 shift 123456789while [ &quot;$1&quot; ] ;do #判断$1是否为空 useradd $1 &amp;&amp; echo $1 is created shiftdone[root@centos scripts]#bash useradd.sh a b ca is createdb is createdc is created 范例 1234567891011121314151617181920212223242526#!/bin/bashif [ $# -eq 0 ];then echo &quot;Usage: `basename $0` user1 user2 ...&quot; exitfi while [ &quot;$1&quot; ];do if id $1 &amp;&gt; /dev/null;then echo $1 is exist else useradd $1 echo &quot;$1 is created&quot; fi shiftdoneecho &quot;All user is created&quot;[root@centos8 script40]#bash shift_batch_user.shUsage: shift_batch_user.sh user1 user2 ...[root@centos8 script40]#bash shift_batch_user.sh tom alice jacktom is existalice is existjack is createdAll user is created 2.7 循环与菜单select格式： 12345select NAME [in WORDS ... ;] do COMMANDS; doneselect NAME in list ;do 循环体命令done 说明： select 循环主要用于创建菜单，按数字顺序排列的菜单项显示在标准错误上，并显示 PS3 提示符，等待用户输入 用户输入菜单列表中的某个数字，执行相应的命令 用户输入菜单列表中的某个数字，会将对应的WORD值赋值给NAME变量 用户输入被保存在内置变量 REPLY 中 select 是个无限循环，因此要用 break 命令退出循环，或用 exit 命令终止脚本。也可以按 ctrl+c退出循环 select 经常和 case 联合使用 与 for 循环类似，可以省略 in list，此时使用位置参量 范例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@rocky8 ~]#cat select.sh#!/bin/bashsum=0PS3=&quot;请点菜(1-6): &quot;select MENU in 北京烤鸭 佛跳墙 小龙虾 羊蝎子 火锅 点菜结束;docase $REPLY in1)echo $MENU 价格是 100let sum+=100;;2)echo $MENU 价格是 88let sum+=88;;3)echo $MENU价格是 66let sum+=66;;4)echo $MENU 价格是 166let sum+=166;;5)echo $MENU 价格是 200let sum+=200;;6)echo &quot;点菜结束,退出&quot;break;;*)echo &quot;点菜错误，重新选择&quot;;;esacdoneecho &quot;总价格是: $sum&quot;[root@rocky8 ~]#bash select.sh 1) 北京烤鸭2) 佛跳墙3) 小龙虾4) 羊蝎子5) 火锅6) 点菜结束请点菜(1-6):","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"邮件服务","slug":"Linux/service-manage/mail","date":"2025-08-20T10:29:24.000Z","updated":"2025-09-09T09:44:02.307Z","comments":true,"path":"Linux/service-manage/mail/","permalink":"https://aquapluto.github.io/Linux/service-manage/mail/","excerpt":"","text":"1 邮件协议邮件服务的运作基于多个标准协议，不同组件分工处理不同协议： 1、SMTP（Simple Mail Transfer Protocol） 用于邮件的发送和转发，端口通常为 25（明文）、465（SSL 加密）、587（TLS 加密）。 2、POP3（Post Office Protocol v3） 用于客户端从服务器下载邮件，端口 110（明文）、995（SSL 加密），特点是下载后可删除服务器上的邮件。 3、IMAP（Internet Message Access Protocol） 更灵活的邮件接收协议，端口 143（明文）、993（SSL 加密），支持在服务器上管理邮件（如分类、标记），客户端操作会同步到服务器。 4、MDA（Mail Delivery Agent） 邮件投递代理，负责将 SMTP 接收的邮件保存到本地用户的邮件目录（如/var/spool/mail/）。 5、MUA（Mail User Agent） 邮件用户代理，即客户端工具（如mutt、thunderbird、网页邮箱），用于用户编写、发送和查看邮件。 2 实现邮件服务mailx 是客户端工具，通过配置将邮件发往 SMTP 服务器（如 163、QQ 邮箱） 1yum -y install mailx 申请QQ邮箱的授权信息 （1）登陆邮箱 （2）“设置”——“账户”——“POP3&#x2F;IMAP&#x2F;SMTP&#x2F;Exchange&#x2F;CardDAV&#x2F;CalDAV服务”——设置开启——生成授权码 通过配置文件（如 /etc/mail.rc 或用户家目录的 .mailrc）设置默认发件人、SMTP 服务器 12345$ vim /etc/mail.rcset from=xxx@qq.comset smtp=smtp.qq.comset smtp-auth-user=xxx@qq.comset smtp-auth-password=授权码 发送邮件 12345# test为邮件内容，hello为主题echo test | mail -s hello xxxxxxx@qq.com# 发送带附件的邮件（-a 指定附件路径）mailx -s &quot;带附件的邮件&quot; -a /path/to/file.txt xxxxxxx@qq.com &lt; message.txt 接收和查看邮件 本地系统用户的邮件默认存储在 /var/spool/mail/[用户名] 1234# 直接输入 mailx 进入交互模式，查看当前用户的邮件mailx# 在交互模式中，输入邮件编号（如 1）查看对应邮件，输入 d 删除邮件，输入 q 退出 3 示例：监控磁盘空间并告警123USE=`df | grep &#x27;^/dev&#x27; | sed -rn &#x27;s/.* ([0-9]+)%.*/\\1/p&#x27; | sort -nr | head -1`WARNING=80[ $USE -gt $WARNING] &amp;&amp; echo &quot;Disk well be full&quot; | mail -s &quot;Disk Warning&quot; xxx@qq.com","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"运算和条件测试","slug":"Linux/shell/operations-conditional","date":"2025-08-20T10:29:24.000Z","updated":"2025-09-09T09:44:10.901Z","comments":true,"path":"Linux/shell/operations-conditional/","permalink":"https://aquapluto.github.io/Linux/shell/operations-conditional/","excerpt":"","text":"1 算术运算bash 只支持整数，不支持小数 乘法符号有些场景中需要转义 123456789101112131415161718+ - * / % i++ i-- ++i --i = *= /= %= += -= &lt;&lt;= &gt;&gt;= &amp;= ^= |= - + ! ~ ** &lt;&lt; &gt;&gt; &lt;= &gt;= &lt; &gt; == != &amp; | ^ &amp;&amp; || expr?expr:expr expr1 , expr2 实现算术运算 1234567(1) let var=算术表达式(2) ((var=算术表达式)) 和上面等价(3) var=$[算术表达式](4) var=$((算术表达式))(5) var=$(expr arg1 arg2 arg3 ...)(6) declare -i var = 数值(7) echo &#x27;算术表达式&#x27; | bc 内建的随机数生成器变量 1$RANDOM 取值范围：0-32767 范例 12345#生成 0 - 49 之间随机数echo $[$RANDOM%50]#随机字体颜色[root@centos8 ~]#echo -e &quot;\\033[1;$[RANDOM%7+31]mhello\\033[0m&quot;magedu 2 逻辑运算与：&amp; 和0相与结果为0，和1相与结果保留原值, 一假则假,全真才真 12340 与 0 = 00 与 1 = 01 与 0 = 01 与 1 = 1 或：| 和1相或结果为1，和0相或结果保留原值,一真则真,全假才假 12340 或 0 = 00 或 1 = 11 或 0 = 11 或 1 = 1 非：！ 12! 1 = 0 ! true! 0 = 1 ! false 异或：^ 异或的两个值，相同为假，不同为真。两个数字X,Y异或得到结果Z，Z再和任意两者之一X异或，将得出另一个值Y 12340 ^ 0 = 00 ^ 1 = 11 ^ 0 = 11 ^ 1 = 0 范例: 变量互换 1234[root@centos8 ~]#x=10;y=20;temp=$x;x=$y;y=$temp;echo x=$x,y=$yx=20,y=10[root@centos8 ~]#x=10;y=20;x=$[x^y];y=$[x^y];x=$[x^y];echo x=$x,y=$yx=20,y=10 3 短路运算短路与 &amp;&amp; 1234CMD1 &amp;&amp; CMD2第一个CMD1结果为真(1)，第二个CMD2必须要参与运算，才能得到最终的结果第一个CMD1结果为假(0)，总的结果必定为0，因此不需要执行CMD2 短路或 || 1234CMD1 || CMD2第一个CMD1结果为真(1)，总的结果必定为1，因此不需要执行CMD2第一个CMD1结果为假(0)，第二个CMD2 必须要参与运算,才能得到最终的结果 短路与和或组合 123456CMD1 &amp;&amp; CMD2 || CMD3当CMD1执行成功时,会执行CMD2当CMD1执行失败时,会执行CMD3注意： CMD1 || CMD2 &amp;&amp; CMD3 逻辑不通,不使用 4 条件测试和组合4.1 条件测试判断某需求是否满足，需要由测试机制来实现，专用的测试表达式需要由测试命令辅助完成 测试过程，实现评估布尔声明，以便用在条件性环境下进行执行 若真，则状态码变量 $? 返回0 若假，则状态码变量 $? 返回1 条件测试命令 test EXPRESSION [ EXPRESSION ] 和test 等价，建议使用 [ ] [[ EXPRESSION ]] 相关于增强版的 [ ], 支持[]的用法,且支持扩展正则表达式和通配符 注意：EXPRESSION前后必须有空白字符 范例 123456[root@centos scripts]#n=80[root@centos scripts]#[$n -gt 60]bash: [80: command not found[root@centos scripts]#[ $n -gt 60 ][root@centos scripts]#echo $?0 范例 123456[root@centos scripts]#name=txt.sh;[[ $name =~ \\.sh$ ]][root@centos scripts]#echo $?0[root@centos scripts]#name=txt.ch;[[ $name =~ \\.sh$ ]][root@centos scripts]#echo $?1 4.2 变量测试12#判断 NAME 变量是否定义[ -v NAME ] 范例 12345678910111213141516171819[root@centos8 ~]#unset x[root@centos8 ~]#test -v x[root@centos8 ~]#echo $?1[root@centos8 ~]#x=10[root@centos8 ~]#test -v x[root@centos8 ~]#echo $?0[root@centos8 ~]#y=[root@centos8 ~]#test -v y[root@centos8 ~]#echo $?0#注意 [ ] 中需要空格，否则会报下面错误[root@centos8 ~]#[-v name]-bash: [-v: command not found[root@centos8 ~]#[ -v name ][root@centos8 ~]#echo $?0 4.3 数值测试123456-eq 是否等于（equal）-ne 是否不等于（not equal）-gt 是否大于（greater than）-ge 是否大于等于（greater than or equal）-lt 是否小于（less than）-le 是否小于等于（less than or equal） 范例 12345678[root@centos8 ~]#i=10[root@centos8 ~]#j=8[root@centos8 ~]#[ $i -lt $j ][root@centos8 ~]#echo $?1[root@centos8 ~]#[ $i -gt $j ][root@centos8 ~]#echo $?0 算术表达式比较 123456== != &lt;=&gt;=&lt;&gt; 范例 123456789101112[root@centos8 ~]#x=10;y=10;(( x == y ));echo $?0[root@centos8 ~]#x=10;y=20;(( x == y ));echo $?1[root@centos8 ~]#x=10;y=20;(( x != y ));echo $?0[root@centos8 ~]#x=10;y=10;(( x != y ));echo $?1[root@centos8 ~]#x=10;y=20;(( x &gt; y ));echo $?1[root@centos8 ~]#x=10;y=20;(( x &lt; y ));echo $?0 4.4 字符串测试test和 [ ] 字符串测试用法 1234567-z STRING 字符串是否为空，没定义或空为真，不空为假，-n STRING 字符串是否不空，不空为真，空为假 STRING 同上STRING1 = STRING2 是否等于，注意 = 前后有空格STRING1 != STRING2 是否不等于&gt; ascii码是否大于ascii码&lt; 是否小于 [[]] 字符串测试用法 12345[[ expression ]] 用法== 左侧字符串是否和右侧的PATTERN相同 注意:此表达式用于[[ ]]中，PATTERN为通配符=~ 左侧字符串是否能够被右侧的正则表达式的PATTERN所匹配 注意: 此表达式用于[[ ]]中为扩展的正则表达式 建议：当使用正则表达式或通配符使用 [[ ]]，其它情况一般使用 [ ] 范例：使用 [ ] 123456789101112131415161718192021222324252627282930313233343536373839[root@centos8 ~]#unset str[root@centos8 ~]#[ -z &quot;$str&quot; ][root@centos8 ~]#echo $?0[root@centos8 ~]#str=&quot;&quot;[root@centos8 ~]#[ -z &quot;$str&quot; ][root@centos8 ~]#echo $?0[root@centos8 ~]#str=&quot; &quot;[root@centos8 ~]#[ -z &quot;$str&quot; ][root@centos8 ~]#echo $?1[root@centos8 ~]#[ -n &quot;$str&quot; ][root@centos8 ~]#echo $?0[root@centos8 ~]#unset str[root@centos8 ~]#[ -n &quot;$str&quot; ][root@centos8 ~]#echo $?1[root@centos8 ~]#[ &quot;$str&quot; ][root@centos8 ~]#echo $?1[root@centos8 ~]#str=magedu[root@centos8 ~]#[ &quot;$str&quot; ][root@centos8 ~]#echo $?0[root@centos8 ~]#str=magedu[root@centos8 ~]#[ &quot;$str&quot; ][root@centos8 ~]#echo $?0[root@centos8 ~]#str1=magedu[root@centos8 ~]#str2=mage[root@centos8 ~]#[ $str1 = $str2 ][root@centos8 ~]#echo $?1[root@centos8 ~]#str2=magedu[root@centos8 ~]#[ $str1 = $str2 ][root@centos8 ~]#echo $?0 范例：在比较字符串时，建议变量放在 “ ” 中 123456789[root@centos8 ~]#[ &quot;$NAME&quot; ][root@centos8 ~]#NAME=&quot;I love linux&quot;[root@centos8 ~]#[ $NAME ]-bash: [: love: binary operator expected[root@centos8 ~]#[ &quot;$NAME&quot; ][root@centos8 ~]#echo $?0[root@centos8 ~]#[ I love linux ]-bash: [: love: binary operator expected 范例: [[ ]] 和通配符 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@centos8 ~]#FILE=&quot;a*&quot;[root@centos8 ~]#echo $FILEa*[root@centos8 ~]#[[ $FILE == a* ]][root@centos8 ~]#echo $?0[root@centos8 ~]#FILE=&quot;ab&quot;[root@centos8 ~]#[[ $FILE == a* ]][root@centos8 ~]#echo $?0[root@centos8 ~]#FILE=&quot;a*&quot;#[[]]中如果不想使用通配符*,只想表达*本身,可以用&quot; &quot;引起来[root@centos8 ~]#[[ $FILE == a&quot;*&quot; ]][root@centos8 ~]#echo $?0[root@centos8 ~]#FILE=&quot;ab&quot;[root@centos8 ~]#[[ $FILE == a&quot;*&quot; ]][root@centos8 ~]#echo $?1#[[]]中如果不想使用通配符*,只想表达*本身,也可以使用转义符[root@centos8 ~]#[[ $FILE == a\\* ]][root@centos8 ~]#echo $?1[root@centos8 ~]#FILE=&quot;a\\b&quot;[root@centos8 ~t]#[[ $FILE == a\\* ]][root@centos8 ~]#echo $?1[root@centos8 ~]#FILE=&quot;a*&quot;[root@centos8 ~]#[[ $FILE == a\\* ]][root@centos8 ~]#echo $?0#通配符?[root@centos8 script]#FILE=abc[root@centos8 script]#[[ $FILE == ??? ]][root@centos8 script]#echo $?0[root@centos8 script]#FILE=abcd[root@centos8 script]#[[ $FILE == ??? ]][root@centos8 script]#echo $?1#通配符[root@centos8 ~]#NAME=&quot;linux1&quot;[root@centos8 ~]#[[ &quot;$NAME&quot; == linux* ]][root@centos8 ~]#echo $?0[root@centos8 ~]#[[ &quot;$NAME&quot; == &quot;linux*&quot; ]][root@centos8 ~]#echo $?1[root@centos8 ~]#NAME=&quot;linux*&quot;[root@centos8 ~]#[[ &quot;$NAME&quot; == &quot;linux*&quot; ]][root@centos8 ~]#echo $?0#结论：[[ == ]] == 右侧的 * 做为通配符，不要加“”，只想做为*符号使用时, 需要加 “” 或转义 范例: 判断合理的考试成绩 123456789101112131415[root@centos8 script]#SCORE=101[root@centos8 script]#[[ $SCORE =~ 100|[0-9]&#123;1,2&#125; ]][root@centos8 script]#echo $?0[root@centos8 script]#[[ $SCORE =~ ^(100|[0-9]&#123;1,2&#125;)$ ]][root@centos8 script]#echo $?1[root@centos8 script]#SCORE=10[root@centos8 script]#[[ $SCORE =~ ^(100|[0-9]&#123;1,2&#125;)$ ]][root@centos8 script]#echo $?0[root@centos8 script]#SCORE=abc[root@centos8 script]#[[ $SCORE =~ ^(100|[0-9]&#123;1,2&#125;)$ ]][root@centos8 script]#echo $?1 范例：使用 [[ ]] 判断文件后缀 123456789101112131415161718192021#通配符[root@centos8 ~]#FILE=test.log[root@centos8 ~]#[[ &quot;$FILE&quot; == *.log ]][root@centos8 ~]#echo $?0[root@centos8 ~]#FILE=test.txt[root@centos8 ~]#[[ &quot;$FILE&quot; == *.log ]][root@centos8 ~]#echo $?1[root@centos8 ~]#[[ &quot;$FILE&quot; != *.log ]][root@centos8 ~]#echo $?0#正则表达式[root@centos8 ~]#[[ &quot;$FILE&quot; =~ \\.log$ ]][root@centos8 ~]#echo $?1[root@centos8 ~]#FILE=test.log[root@centos8 ~]#[[ &quot;$FILE&quot; =~ \\.log$ ]][root@centos8 ~]#echo $?0 范例: 判断合法的非负整数 12345678[root@centos8 ~]#N=100[root@centos8 ~]#[[ &quot;$N&quot; =~ ^[0-9]+$ ]][root@centos8 ~]#echo $?0[root@centos8 ~]#N=Magedu10[root@centos8 ~]#[[ &quot;$N&quot; =~ ^[0-9]+$ ]][root@centos8 ~]#echo $?1 范例: 判断合法IP 123456789101112[root@centos8 ~]#IP=1.2.3.4[root@centos8 ~]#[[ &quot;$IP&quot; =~ ^([0-9]&#123;1,3&#125;\\.)&#123;3&#125;[0-9]&#123;1,3&#125;$ ]][root@centos8 ~]#echo $?0[root@centos8 ~]#IP=1.2.3.4567[root@centos8 ~]#[[ &quot;$IP&quot; =~ ^([0-9]&#123;1,3&#125;.)&#123;3&#125;[0-9]&#123;1,3&#125;$ ]][root@centos8 ~]#echo $?1[root@centos8 ~]#[[ $IP =~ ^(([1-9]?[0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\\.)&#123;3&#125;([1-9]?[0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])$ ]][root@centos8 ~]#echo $?1 范例 1234[root@centos7 ~]#cat check_ip.sh#!/bin/bashIP=$1[[ $IP =~ ^(([1-9]?[0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\\.)&#123;3&#125;([1-9]?[0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])$ ]] &amp;&amp; echo $IP is valid || echo $IP is invalid 4.5 文件测试存在性测试 123456789-a FILE：同 -e-e FILE: 文件存在性测试，存在为真，否则为假-b FILE：是否存在且为块设备文件-c FILE：是否存在且为字符设备文件-d FILE：是否存在且为目录文件-f FILE：是否存在且为普通文件-h FILE 或 -L FILE：存在且为符号链接文件-p FILE：是否存在且为命名管道文件-S FILE：是否存在且为套接字文件 范例： -e和-a 表示判断文件的存在性，建议使用-e 12345678910111213141516171819202122232425262728293031323334353637383940414243#文件是否不存在[root@centos8 ~]#[ -a /etc/nologin ][root@centos8 ~]#echo $?1[root@centos8 ~]#! [ -a /etc/nologin ][root@centos8 ~]#echo $?0#文件是否存在[root@centos8 ~]# [ -a /etc/issue ][root@centos8 ~]#echo $?0#取反后结果却没有变化[root@centos8 ~]# [ ! -a /etc/issue ][root@centos8 ~]#echo $?0[root@centos8 ~]#! [ -a /etc/issue ][root@centos8 ~]#echo $?1#文件是否存在[root@centos8 ~]#! [ -e /etc/issue ][root@centos8 ~]#echo $?1#此为推荐写法[root@centos8 ~]#[ ! -e /etc/issue ][root@centos8 ~]#echo $?1[root@centos8 ~]#[ -d /etc ][root@centos8 ~]#echo $?0[root@centos8 ~]#[ -d /etc/issue ][root@centos8 ~]#echo $?1[root@centos8 ~]#[ -L /bin ][root@centos8 ~]#echo $?0[root@centos8 ~]#[ -L /bin/ ][root@centos8 ~]#echo $?1 文件权限测试 123456-r FILE：是否存在且可读-w FILE: 是否存在且可写-x FILE: 是否存在且可执行-u FILE：是否存在且拥有suid权限-g FILE：是否存在且拥有sgid权限-k FILE：是否存在且拥有sticky权限 注意：最终结果由用户对文件的实际权限决定，而非文件属性决定 范例 12345678910111213141516171819[root@centos8 ~]#[ -w /etc/shadow ][root@centos8 ~]#echo $?0[root@centos8 ~]#[ -x /etc/shadow ][root@centos8 ~]#echo $?1[root@centos8 ~]#[ -w test.txt ][root@centos8 ~]#echo $?0[root@centos8 ~]#chattr +i test.txt[root@centos8 ~]#lsattr test.txt----i-------------- nianling.txt[root@centos8 ~]#[ -w test.txt ][root@centos8 ~]#echo $?1[root@centos8 ~]#chattr -i test.txt[root@centos8 ~]#[ -w test.txt ][root@centos8 ~]#echo $?0 文件属性测试 12345678-s FILE #是否存在且非空-t fd #fd 文件描述符是否在某终端已经打开-N FILE #文件自从上一次被读取之后是否被修改过-O FILE #当前有效用户是否为文件属主-G FILE #当前有效用户是否为文件属组FILE1 -ef FILE2 #FILE1是否是FILE2的硬链接FILE1 -nt FILE2 #FILE1是否新于FILE2（mtime）FILE1 -ot FILE2 #FILE1是否旧于FILE2 4.6 组合测试条件第一种方式 12345[ EXPRESSION1 -a EXPRESSION2 ] #并且，EXPRESSION1和EXPRESSION2都是真，结果才为真[ EXPRESSION1 -o EXPRESSION2 ] #或者，EXPRESSION1和EXPRESSION2只要有一个真，结果就为真[ ! EXPRESSION ] #取反说明： -a 和 -o 需要使用测试命令进行，[[ ]] 不支持 范例 1234567891011121314151617181920212223242526272829303132[root@centos8 ~]#FILE=&quot;/data/scrips/test.sh&quot;[root@centos8 ~]#ll /data/scrips/test.sh-rw-r--r-- 1 root root 382 Dec 23 09:32 /data/scripts/test.sh[root@centos8 ~]#[ -f $FILE -a -x $FILE ][root@centos8 ~]#echo $?1[root@centos8 ~]#chmod +x /data/scripts/test.sh[root@centos8 ~]#ll /data/scripts/test.sh-rwxr-xr-x 1 root root 382 Dec 23 09:32 /data/script40/test.sh#并且[root@centos8 ~]#[ -f $FILE -a -x $FILE ][root@centos8 ~]#echo $?0[root@centos8 ~]#chmod -x /data/scripts/test.sh[root@centos8 ~]#ll /data/scripts/test.sh-rw-r--r-- 1 root root 382 Dec 23 09:32 /data/scripts/test.sh#或者[root@centos8 ~]#[ -f $FILE -o -x $FILE ][root@centos8 ~]#echo $?0[root@centos8 ~]#[ -x $FILE ][root@centos8 ~]#echo $?1#取反[root@centos8 ~]#[ ! -x $FILE ][root@centos8 ~]#echo $?0[root@centos8 ~]#! [ -x $FILE ]0 第二种方式 12345678910COMMAND1 &amp;&amp; COMMAND2 #并且，短路与，代表条件性的AND THEN如果COMMAND1 成功,将执行COMMAND2,否则,将不执行COMMAND2COMMAND1 || COMMAND2 #或者，短路或，代表条件性的OR ELSE如果COMMAND1 成功,将不执行COMMAND2,否则,将执行COMMAND2COMMAND1 &amp;&amp; COMMAND2 || COMMAND3如果COMMAND1 成功,将执行COMMAND2，否则,将执行COMMAND3! COMMAND #非,取反 范例：&amp;&amp; 和 || 组合使用 1234567891011121314151617[root@centos8 ~]#NAME=wang; id $NAME &amp;&gt; /dev/null &amp;&amp; echo &quot;$NAME is exist&quot;wang is exist[root@centos8 ~]#NAME=wange; id $NAME &amp;&gt; /dev/null || echo &quot;$NAME is notexist&quot;wange is not exist[root@centos8 ~]#NAME=wange; id $NAME &amp;&gt; /dev/null &amp;&amp; echo &quot;$NAME is exist&quot; ||echo &quot;$NAME is not exist&quot;wange is not exist[root@centos8 ~]#NAME=wang; id $NAME &amp;&gt; /dev/null &amp;&amp; echo &quot;$NAME is exist&quot; ||echo &quot;$NAME is not exist&quot;wang is exist[root@centos8 ~]#NAME=wang; id $NAME &amp;&gt; /dev/null &amp;&amp; echo &quot;$NAME is exist&quot; ||echo &quot;$NAME is not exist&quot;wang is exist[root@centos8 ~]#NAME=wang; id $NAME &amp;&gt; /dev/null || echo &quot;$NAME is not exist&quot;&amp;&amp; echo &quot;$NAME is exist&quot;wang is exist[root@centos8 ~]#NAME=wange; id $NAME &amp;&gt; /dev/null || echo &quot;$NAME is notexist&quot; &amp;&amp; echo &quot;$NAME is exist&quot;wange is not existwange is exist#结论：如果&amp;&amp; 和 || 混合使用，&amp;&amp; 要在前，|| 放在后 范例：网络状态判断 123456789[root@centos8 ~]#cat /data/scripts/ping.sh#!/bin/bashIP=172.16.0.1ping -c1 -W1 $IP &amp;&gt; /dev/null &amp;&amp; echo &quot;$IP is up&quot; || &#123; echo &quot;$IP is unreachable&quot;; exit; &#125;echo &quot;Script is finished&quot;[root@centos8 ~]#bash /data/scripts/ping.sh172.16.0.1 is upScript is finished 范例：磁盘空间的判断 12345[root@centos8 ~]#cat /data/script/disk_check.sh#!/bin/bashWARNING=80SPACE_USED=`df|grep &#x27;^/dev/sd&#x27;|tr -s &#x27; &#x27; %|cut -d% -f5|sort -nr|head -1`[ &quot;$SPACE_USED&quot; -ge $WARNING ] &amp;&amp; echo &quot;disk used is $SPACE_USED,will be full&quot; | mail -s diskwaring root 范例：磁盘空间和Inode号的检查脚本 123456[root@centos8 scripts]#cat disk_check.sh#!/bin/bashWARNING=80SPACE_USED=`df | grep &#x27;^/dev/sd&#x27;|grep -oE &#x27;[0-9]+%&#x27;|tr -d %| sort -nr|head -1`INODE_USED=`df -i | grep &#x27;^/dev/sd&#x27;|grep -oE &#x27;[0-9]+%&#x27;|tr -d %| sort -nr|head-1`[ &quot;$SPACE_USED&quot; -gt $WARNING -o &quot;$INODE_USED&quot; -gt $WARNING ] &amp;&amp; echo &quot;DISKUSED:$SPACE_USED%, INODE_USED:$INODE_USED,will be full&quot; | mail -s &quot;DISK Warning&quot;root@wangxiaochun.com 5 关于 () 和 {}(CMD1;CMD2;...)和 &#123; CMD1;CMD2;...; &#125; 都可以将多个命令组合在一起，批量执行 (list) 会开启子shell,并且list中变量赋值及内部命令是临时性的，执行后将不再影响后续的环境 12345678910[root@centos scripts]#name=wujunlin[root@centos scripts]#(name=magedu;echo $name)magedu[root@centos scripts]#echo $namewujunlin[root@centos scripts]#name=wu;(echo $name;name=mage;echo $name);echo $namewumagewu &#123; list; &#125; 不会开启子shell, 在当前shell中运行,会影响当前shell环境 1234[root@centos scripts]#name=wu;&#123; echo $name;name=mage;echo $name; &#125;;echo $namewumagemage 6 使用read命令接受输入使用read来把输入值分配给一个或多个shell变量，read从标准输入中读取值，给每个单词分配一个变量，所有剩余单词都被分配给最后一个变量，如果变量名没有指定，默认标准输入的值赋值给系统内置变量REPLY 格式 1234567read [options] [name ...]-p 指定要显示的提示-s 静默输入，一般用于密码-n N 指定输入的字符长度N-d &#x27;字符&#x27; 输入结束符-t N TIMEOUT为N秒 范例 1234567891011121314151617181920212223[root@centos8 ~]#readwangxiaochun[root@centos8 ~]#echo $REPLYwangxiaochun[root@centos8 ~]#read NAME TITLEwang cto[root@centos8 ~]#echo $NAMEwang[root@centos8 ~]#echo $TITLEcto[root@Rocky scripts]#read -p &quot;Are you ok?&quot; ANWNERAre you ok?OK[root@Rocky scripts]#echo hello | &#123; read name ; echo $name; &#125;hello[root@centos8 ~]#read x y z &lt;&lt;&lt; &quot;I love you&quot;[root@centos8 ~]#echo $xI[root@centos8 ~]#echo $ylove[root@centos8 ~]#echo $zyou 范例：判断用户输入的是否为 YES 123456789101112131415161718#第一种#!/bin/bashread -p &quot;Are you rich?yes or no: &quot; ANSWER[[ $ANSWER =~ ^([Yy]|[Yy][Ee][Ss])$ ]] &amp;&amp; echo &quot;You are rich&quot; || echo &quot;Good Good Study,Day Day Up!&quot;#第二种#!/bin/bashread -p &quot;Please input yes or no: &quot; inputanswer=`echo $input| tr &#x27;A-Z&#x27; &#x27;a-z&#x27;`[ $answer = &#x27;yes&#x27; -o $answer = &#x27;y&#x27; ] &amp;&amp; echo YES[ $answer = &#x27;no&#x27; -o $answer = &#x27;n&#x27; ] &amp;&amp; echo NO#第三种#!/bin/bashread -p &quot;Please input yes or no: &quot; input[[ $input =~ ^([Yy][Ee][Ss]|[Yy])$ ]] &amp;&amp; echo YES[[ $input =~ ^([Nn][Oo]|[Nn])$ ]] &amp;&amp; echo NO 范例: 检查主机的网络状态 123#!/bin/bashread -p &quot;Please input a IP: &quot; IP[[ &quot;$IP&quot; =~ ^(([1-9]?[0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\\.)&#123;3&#125;([1-9]?[0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])$ ]] || &#123; echo &quot;IP is invalid&quot;;exit; &#125;ping -c1 -W1 $IP &amp;&gt; /dev/null &amp;&amp; echo $IP is up || echo $IP is down 范例：实现运维工作菜单 123456789101112131415161718#!/bin/bash. /etc/init.d/functionsecho -en &quot;\\E[$[RANDOM%7+31];1m&quot;cat &lt;&lt;EOF请选择：1）备份数据库2）清理日志3）软件升级4）软件回滚5）删库跑路EOFecho -en &#x27;\\E[0m&#x27;read -p &quot;请选择上面项对应的数字1-5: &quot; MENU[ $MENU -eq 1 ] &amp;&amp; ./backup.sh[ $MENU -eq 2 ] &amp;&amp; action &quot;清理日志&quot;[ $MENU -eq 3 ] &amp;&amp; action &quot;软件升级&quot;[ $MENU -eq 4 ] &amp;&amp; action &quot;软件回滚&quot;[ $MENU -eq 5 ] &amp;&amp; action &quot;删库跑路&quot;","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"变量","slug":"Linux/shell/variable","date":"2025-08-20T10:29:12.000Z","updated":"2025-09-09T10:03:41.732Z","comments":true,"path":"Linux/shell/variable/","permalink":"https://aquapluto.github.io/Linux/shell/variable/","excerpt":"","text":"1 变量类型变量类型： 内置变量，如：PS1，PATH，UID，HOSTNAME，$，BASHPID，PPID，?，HISTSIZE 用户自定义变量 不同的变量存放的数据不同，决定了以下 数据存储方式 参与的运算 表示的数据范围 变量数据类型： 字符 数值：整型、浮点型,bash 不支持浮点数 2 变量命令规则2.1 命名要求 区分大小写 不能使程序中的保留字和内置变量：如：if, for 只能使用数字、字母及下划线，且不能以数字开头，注意：不支持短横线 “ - ”，和主机名相反 2.2 命名习惯 见名知义，用英文单词命名，并体现出实际作用，不要用简写，如：ATM 变量名大写 局部变量小写 函数名小写 大驼峰StudentFirstName,由多个单词组成，且每个单词的首字母是大写，其它小写 小驼峰studentFirstName ,由多个单词组成，第一个单词的首字母小写，后续每个单词的首字母是大写，其它小写 下划线: student_name 3 变量的生效范围普通变量：生效范围为当前shell进程；对当前shell之外的其它shell进程，包括当前shell的子shell进程均无效 环境变量：生效范围为当前shell进程及其子进程 本地变量：生效范围为当前shell进程中某代码片断，通常指函数 示例说明 bash命令执行会开启一个子进程，进程编号就变了 . 执行不会开启一个子进程，还是在当前进程中 父进程上创建的变量是不能给子进程用的，要想继承，用环境变量 每一个变量的生效范围是在当前的进程内部有效，在下级进程是无效的 1234567891011121314151617181920212223#查看在当前哪个进程[root@Rocky scripts]#echo $BASHPID20456[root@Rocky scripts]#NAME=id_20456[root@Rocky scripts]#bashbash: 0a: 没有那个文件或目录[root@Rocky scripts]#echo $BASHPID26244#查看进程树[root@Rocky scripts]#pstree -pbash(20456)───bash(26244)[root@Rocky scripts]#echo $NAME#回到上一级进程[root@Rocky scripts]#exitexit#26244是20456的子进程，NAME是在20456进程中创建的，所以在26244进程下是输出不了NAME的 4 变量赋值1name=&#x27;value&#x27; value 可以是以下多种形式： 直接字串：name&#x3D;’root’ 变量引用：name&#x3D;”$USER” 命令引用：name&#x3D;COMMAND 或者 name&#x3D;$(COMMAND) 注意：变量赋值是临时生效，当退出终端后，变量会自动删除，无法持久保存，脚本中的变量会随着脚本结束，也会自动删除 5 变量引用12$name$&#123;name&#125; 弱引用和强引用 “$name” 弱引用，其中的变量引用会被替换为变量值 ‘$name’ 强引用，其中的变量引用不会被替换为变量值，而保持原字符串 范例：变量的各种赋值方式和引用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[root@centos8 ~]#TITLE=&#x27;cto&#x27;[root@centos8 ~]#echo $TITLEcto[root@centos8 ~]#echo I am $TITLEI am cto[root@centos8 ~]#echo &quot;I am $TITLE&quot;I am cto[root@centos8 ~]#echo &#x27;I am $TITLE&#x27;I am $TITLE[root@centos8 ~]#NAME=$USER[root@centos8 ~]#echo $NAMEroot[root@centos8 ~]#USER=`whoami`[root@centos8 ~]#echo $USERroot[root@centos8 ~]#FILE=`ls /run`[root@centos8 ~]#echo $FILEagetty.reload atd.pid auditd.pid autofs.fifo-misc autofs.fifo-net consolecron.reboot cryptsetup dbus faillock fsck initctl initramfs lock log mountNetworkManager plymouth rsyslogd.pid screen sepermit setrans sshd.pid sssd.pidsudo systemd tmpfiles.d tuned udev user utmp vmware[root@centos8 ~]#FILE=/etc/*[root@centos8 ~]#echo $FILE/etc/adjtime /etc/aliases /etc/alternatives /etc/anacrontab /etc/at.deny/etc/audit /etc/authselect /etc/autofs.conf /etc/autofs_ldap_auth.conf[root@centos8 ~]#seq 1012345678910[root@centos8 ~]#NUM=`seq 10`[root@centos8 ~]#echo $NUM1 2 3 4 5 6 7 8 9 10[root@centos8 ~]#echo &quot;$NUM&quot;12345678910[root@centos8 ~]#NAMES=&quot;wang&gt; zhang&gt; zhao&gt; li&quot;[root@centos8 ~]#echo $NAMESwang zhang zhao li[root@centos8 ~]#echo &quot;$NAMES&quot;wangzhangzhaoli 范例：变量引用 1234567891011121314[root@centos8 data]#NAME=mage[root@centos8 data]#AGE=20[root@centos8 data]#echo $NAMEmage[root@centos8 data]#echo $AGE20[root@centos8 data]#echo $NAME $AGEmage 20[root@centos8 data]#echo $NAME$AGEmage20[root@centos8 data]#echo $NAME_$AGE20[root@centos8 data]#echo $&#123;NAME&#125;_$AGEmage_20 范例：变量的间接赋值和引用 123456789101112[root@centos8 ~]#TITLE=cto[root@centos8 ~]#NAME=wang[root@centos8 ~]#TITLE=$NAME[root@centos8 ~]#echo $NAMEwang[root@centos8 ~]#echo $TITLEwang[root@centos8 ~]#NAME=mage[root@centos8 ~]#echo $NAMEmage[root@centos8 ~]#echo $TITLEwang 范例：变量追加值 1234[root@centos8 ~]#TITLE=CTO[root@centos8 ~]#TITLE+=:wang[root@centos8 ~]#echo $TITLECTO:wang 范例：利用变量实现动态命令 123[root@centos8 ~]#CMD=hostname[root@centos8 ~]#$CMDcentos8.wangxiaochun.com 范例：颜色变量 12345678910echo -e &quot;\\E[1;32mStarting backup...\\E[0m&quot;tar zcf /root/backup/data-`date +%F_%s`.tar.gz /data/ &amp;&gt; /dev/nullecho -e &quot;\\E[1;32mBackup is finished\\E[0m&quot;COLOR=&#x27;echo -e \\E[1;32m&#x27;END=&#x27;\\E[0m&#x27;$COLOR&quot;Backup is starting...&quot;$ENDtar zcf /root/backup/data-`date +%F_%s`.tar.gz /data/ &amp;&gt; /dev/null$COLOR&quot;Backup is finished!&quot;$END 6 显示已定义的所有变量1set 7 删除变量1unset name 范例 123456[root@centos8 ~]#NAME=mage[root@centos8 ~]#TITLE=ceo[root@centos8 ~]#echo $NAME $TITLEmage ceo[root@centos8 ~]#unset NAME TITLE[root@centos8 ~]#echo $NAME $TITLE 8 环境变量环境变量说明 可以使子进程（包括孙子进程）继承父进程的变量，但是无法让父进程使用子进程的变量 一旦子进程修改从父进程继承的变量，将会新的值传递给孙子进程 一般只在系统配置文件中使用，在脚本中较少使用 指在操作系统中用来指定操作系统运行环境的一些参数 环境变量的主要用途： 身份认证 动态库查找 保存工作路径(pwd) 特定路径查找 保存特定变量值 环境变量分类 按生命周期分： 永久的：在环境变量脚本文件中配置，用户每次登录时会自动执行这些脚本，相当于永久生效。 临时的：用户利用export命令，在当前终端下声明环境变量，关闭Shell终端失效。 按作用域分： 系统环境变量：公共的，对全部的用户都生效。 用户环境变量：用户私有的、自定义的个性化设置，只对该用户生效。 变量声明和赋值 1234567#声明并赋值export name=VALUEdeclare -x name=VALUE#或者分两步实现name=VALUEexport name 显示所有环境变量 1234envprintenvexportdeclare -x 查看指定进程的环境变量 1cat /proc/$PID/environ 范例: 查看进程的环境变量 1234567891011121314151617[root@centos8 ~]#cat /proc/1235/environUSER=rootLOGNAME=rootHOME=/rootPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/binSHELL=/bin/bashTERM=linuxSSH_AUTH_SOCK=/tmp/ssh-iIeuAxdLiY/agent.1234XDG_SESSION_ID=1XDG_RUNTIME_DIR=/run/user/0DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/0/busSSH_CLIENT=10.0.0.1 13258 22SSH_CONNECTION=10.0.0.1 13258 10.0.0.8 22SSH_TTY=/dev/pts/0[root@centos8 ~]#cat /proc/1235/environ |tr &#x27;\\0&#x27; &#x27;\\n&#x27;USER=rootLOGNAME=rootHOME=/rootPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/binSHELL=/bin/bashTERM=linuxSSH_AUTH_SOCK=/tmp/ssh-iIeuAxdLiY/agent.1234XDG_SESSION_ID=1XDG_RUNTIME_DIR=/run/user/0DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/0/busSSH_CLIENT=10.0.0.1 13258 22SSH_CONNECTION=10.0.0.1 13258 10.0.0.8 22SSH_TTY=/dev/pts/0 9 局部变量在shell脚本中，local是用于定义局部变量的关键字。定义局部变量后，该变量只能在当前函数或代码块中使用，其他函数或代码块无访问该变量。下面是一个使用local定义局部变量的例子 12345678910#!/bin/bashfunction test &#123; local name=&quot;liwenchao&quot; echo &quot;局部变量name的值为：$name&quot;&#125;name=&quot;global&quot;echo &quot;全局变量name的值为：$name&quot;testecho &quot;函数执行后全局变量name的值为：$name&quot; 在上面的例子中，我们定义了一个名为test的函数，在函数中使用local定义了一个名为name的局部变量。在函数外部，我们定义了一个名为name的全局变量。当我们执行test函数时，函数内部的name变量的值为liwenchao，而函数外部的name变量的值仍为global。这说明local定义的变量只在函数内部有效 10 只读变量只读变量：只能声明定义，但后续不能修改和删除，即常量 声明只读变量 12readonly namedeclare -r name 查看只读变量 12readonly [-p]declare -r 范例 1234567891011121314[root@centos8 ~]#readonly PI=3.14159[root@centos8 ~]#echo $PI3.14159[root@centos8 ~]#PI=3.14-bash: PI: readonly variable[root@centos8 ~]#unset PI-bash: unset: PI: cannot unset: readonly variable[root@centos8 ~]#echo $PI3.14159[root@centos8 ~]#exitlogout[root@centos8 ~]#echo $PI[root@centos8 ~]# 11 位置变量在bash shell中内置的变量, 在脚本代码中调用通过命令行传递给脚本的参数 1234567$1, $2, ... 对应第1个、第2个等参数，shift [n]换位置$0 命令本身,包括路径$* 传递给脚本的所有参数，全部参数合为一个字符串$@ 传递给脚本的所有参数，每个参数为独立字符串$# 传递给脚本的参数的个数注意：$@ $* 只在被双引号包起来的时候才会有差异 清空所有位置变量 1set -- 范例 12color.sh a b c $1=a $2=b $3=c 范例 12345678910111213141516171819#!bin/bashecho 1st is $1echo 2ed is $2echo 3rd is $3echo 10th is $&#123;10&#125;echo all args are $*echo all args are $@echo the number is $#echo the scriptsname is $0[root@centos scripts]#bash color.sh &#123;a..z&#125;1st is a2ed is b3rd is c10th is jall args are a b c d e f g h i j k l m n o p q r s t u v w x y zall args are a b c d e f g h i j k l m n o p q r s t u v w x y zthe number is 26the scriptsname is color.sh 范例：删库跑路之命令rm的安全实现 12345678910[root@centos8 ~]#cat /data/scripts/rm.shWARNING_COLOR=&quot;echo -e \\E[1;31m&quot;END=&quot;\\E[0m&quot;DIR=/tmp/`date +%F_%H-%M-%S`mkdir $DIRmv $* $DIR$&#123;WARNING_COLOR&#125;Move $* to $DIR $END[root@centos8 ~]#chmod a+x /data/scripts/rm.sh[root@centos8 ~]#alias rm=&#x27;/data/scripts/rm.sh&#x27; 范例 123456#!bin/bashcolor=$1echo -e &quot;\\E[1;$&#123;color&#125;m生日快乐\\E[0m&quot;（echo -e &quot;\\E[1;$1m生日快乐\\E[0m&quot;）[root@centos scripts]#bash color.sh 34生日快乐 范例：利用软链接实现同一个脚本不同功能 12345678910111213141516171819[root@centos7 dir]#vim text.sh#!bin/bashbasename $0[root@centos scripts]#ln -s text.sh text2.sh [root@centos scripts]#ln -s text.sh text3.sh [root@centos scripts]#chmod +x text.sh [root@centos scripts]#lltotal 8-rw-r--r--. 1 root root 62 Aug 19 20:12 color.shlrwxrwxrwx. 1 root root 7 Aug 19 20:37 text2.sh -&gt; text.shlrwxrwxrwx. 1 root root 7 Aug 19 20:37 text3.sh -&gt; text.sh-rwxr-xr-x. 1 root root 27 Aug 19 20:36 text.sh[root@centos7 dir]#./test2.sh test2.sh[root@centos7 dir]#./test3.sh test3.sh#起不同的软链接名，虽然脚本名是同一个，但是软链接名不一样，就可以做到功能不一样 12 退出状态码变量当我们浏览网页时，有时会看到下图所显示的数字，表示网页的错误信息，我们称为状态码，在shell脚本中也有相似的技术表示程序执行的相应状态 进程执行后，将使用变量 ? 保存状态码的相关数字，不同的值反应成功或失败，?取值范例 0-255 12$?的值为0 #代表成功$?的值是1到255 #代表失败 范例 123[root@centos8 ~]#curl -fs http://www.wangxiaochun.com &gt;/dev/null[root@centos8 ~]#echo $?0 用户可以在脚本中使用以下命令自定义退出状态码 1exit [n] 注意： 脚本中一旦遇到exit命令，脚本会立即终止；终止退出状态取决于exit命令后面的数字 如果exit后面无数字,终止退出状态取决于exit命令前面命令执行结果 如果没有exit命令, 即未给脚本指定退出状态码，整个脚本的退出状态码取决于脚本中执行的最后一条命令的状态码 13 展开命令行展开命令执行顺序 123456789把命令行分成单个命令词展开别名展开大括号的声明&#123;&#125;展开波浪符声明 ~命令替换$() 和 ``再次把命令行分成命令词展开文件通配符*、?、[abc]等等准备I/0重导向 &lt;、&gt;运行命令 防止扩展 1反斜线（\\）会使随后的字符按原意解释 范例 12345[root@centos8 ~]#echo Your cost: \\$5.00Your cost: $5.00[root@rocky8 ~]#echo &quot;The book&#x27;s price is \\$10&quot;The book&#x27;s price is $10 加引号来防止扩展 12单引号（’’）防止所有扩展双引号（”“）也可防止扩展，但是以下情况例外：$（美元符号） 变量扩展 123`` ： 反引号，命令替换\\：反斜线，禁止单个字符扩展!：叹号，历史命令替换 14 格式化输出printf1printf &quot;指定的格式&quot; &quot;文本1&quot; ”文本2“…… 常用格式替换符 替换符 功能 说明 %d，%i 十进制整数 %03d 表示3位宽度，不足前面用0补全，超出位数原样输出 %f 浮点 %.2f 中的2表示小数点后显示的小数位数 %c ASCII字符，即显示对应参数的第一个字符 %s 字符串 %#s 中的数字代表此替换符中的输出字符宽度，不足补空格，默认是右对齐，%-10s 表示10个字符宽，- 表示左对齐 %% 表示%本身 %b 相对应的参数中包含转义字符时，可以使用此替换符进行替换，对应的转义字符会被转义 %o 八进制值 %u 不带正负号的十进制值 %x，%X 十六进制值（a-f），（A-F） 常用转义字符 转义符 功能 \\a 警告字符，通常为ASCII的BEL字符 \\n 换行 \\t 水平制表符 \\v 垂直制表符 \\r 回车 \\ 表示\\本身 \\b 后退 \\f 换页 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@centos8 ~]#printf &quot;%s&quot; 1 2 3 41234[root@centos8 ~]#[root@centos8 ~]#printf &quot;%s\\n&quot; 1 2 3 41234[root@centos8 ~]#printf &quot;%f\\n&quot; 1 2 3 41.0000002.0000003.0000004.000000#.2f 表示保留两位小数[root@centos8 ~]#printf &quot;%.2f\\n&quot; 1 2 3 41.002.003.004.00[root@centos8 ~]#printf &quot;(%s)&quot; 1 2 3 4;echo(1)(2)(3)(4)[root@centos8 ~]#printf &quot; (%s) &quot; 1 2 3 4;echo &quot;&quot;(1) (2) (3) (4)[root@centos8 ~]#printf &quot;(%s)\\n&quot; 1 2 3 4(1)(2)(3)(4)[root@centos8 ~]#printf &quot;%s %s\\n&quot; 1 2 3 41 23 4[root@centos8 ~]#printf &quot;%s %s %s\\n&quot; 1 2 3 41 2 34 #%-10s 表示宽度10个字符，左对齐[root@centos8 ~]#printf &quot;%-10s %-10s %-4s %s \\n&quot; 姓名 性别 年龄 体重 小明 男 20 70 小红 女 18 50姓名 性别 年龄 体重小明 男 20 70小红 女 18 50#将十进制的17转换成16进制数[root@centos8 ~]#printf &quot;%X&quot; 1711[root@centos8 ~]##将十六进制C转换成十进制[root@centos8 ~]#printf &quot;%d\\n&quot; 0xC12[root@centos8 ~]#VAR=&quot;welcome to Magedu&quot;;printf &quot;\\033[1;32m%s\\033[0m\\n&quot; $VARwelcometoMagedu 15 高级变量15.1 高级变量赋值$str 为变量名，expr 为具体字符串 这些组合可以省掉一些 if，else 的判断代码 变量配置方式 str没有配置 str为空字符串 str己配置为非空字符串 var&#x3D;${str-expr} var&#x3D;expr var&#x3D; var&#x3D;$str var&#x3D;${str:expr} var&#x3D;expr var&#x3D;expr var&#x3D;$str var&#x3D;${str+expr} var&#x3D; var&#x3D;expr var&#x3D;expr var&#x3D;${str:+expr} var&#x3D; var&#x3D; var&#x3D;expr var&#x3D;${str&#x3D;expr} str&#x3D;expr; var&#x3D;expr str不变; var&#x3D; str不变; var&#x3D;$str var&#x3D;${str:&#x3D;expr} str&#x3D;expr; var&#x3D;expr str&#x3D;expr; var&#x3D;expr str不变; var&#x3D;$str var&#x3D;${str?expr} expr 输出至 stderr var&#x3D; var&#x3D;$str var&#x3D;${str:?expr} expr 输出至 stderr expr 输出至 stderr var&#x3D;$str 范例 12345678910111213141516171819202122232425262728[root@centos8 ~]#title=ceo[root@centos8 ~]#name=$&#123;title-mage&#125;[root@centos8 ~]#echo $nameceo[root@centos8 ~]#title=[root@centos8 ~]#name=$&#123;title-mage&#125;[root@centos8 ~]#echo $name[root@centos8 ~]#unset title[root@centos8 ~]#name=$&#123;title-mage&#125;[root@centos8 ~]#echo $namemage[root@centos8 ~]#title=ceo[root@centos8 ~]#name=$&#123;title:-mage&#125;[root@centos8 ~]#echo $nameceo[root@centos8 ~]#title=[root@centos8 ~]#name=$&#123;title:-mage&#125;[root@centos8 ~]#echo $namemage[root@centos8 ~]#unset title[root@centos8 ~]#name=$&#123;title:-mage&#125;[root@centos8 ~]#echo $namemage 15.2 高级变量用法-有类型变量Shell变量一般是无类型的，但是bash Shell提供了declare和typeset两个命令用于指定变量的类型，两个命令是等价的 12345678910111213141516declare [选项] 变量名#选项：-r #声明或显示只读变量-i #将变量定义为整型数-a #将变量定义为数组-A #将变量定义为关联数组-f #显示已定义的所有函数名及其内容-F #仅显示已定义的所有函数名-x #声明或显示环境变量和函数,相当于export-l #声明变量为小写字母 declare -l var=UPPER-u #声明变量为大写字母 declare -u var=lower-n #变量引用另外一个变量的值-p #显示每个变量的属性和值-t #声明或显示具有trace(追踪)属性的变量-x #显示环境变量和函数,相当于export 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#显示所有己定义的函数和函数体[root@ubuntu2204 ~]# declare -f#显示所有己定义的函数，仅函数名[root@ubuntu2204 ~]# declare -F#显示所有己定义的变量属性及值[root@ubuntu2204 ~]# declare -p#显示所有普通数组[root@ubuntu2204 ~]# declare -a#定义一个普通数组[root@ubuntu2204 ~]# declare -a arr1#显示所有关联数组[root@ubuntu2204 ~]# declare -A#定义一个关联数组[root@ubuntu2204 ~]# declare -A arr2#显示所有整型变量[root@ubuntu2204 ~]# declare -i#定义一个整型变量[root@ubuntu2204 ~]# declare -i int1[root@ubuntu2204 ~]# int1=abcd[root@ubuntu2204 ~]# echo $int10#显示所有小写变量[root@ubuntu2204 ~]# declare -l#定义一个小写变量[root@ubuntu2204 ~]# declare -l lower1[root@ubuntu2204 ~]# lower1=1234abcdABCD[root@ubuntu2204 ~]# echo $lower11234abcdabcd#变量引用[root@ubuntu2204 ~]# str1=1234[root@ubuntu2204 ~]# declare -n str2=str1[root@ubuntu2204 ~]# echo $str1 $str21234 1234#str2 会跟着改变[root@ubuntu2204 ~]# str1=abcd[root@ubuntu2204 ~]# echo $str1 $str2abcd abcd#str1 也会跟着改变[root@ubuntu2204 ~]# str2=xyz[root@ubuntu2204 ~]# echo $str1 $str2xyz xyz#显示所有只读变量[root@ubuntu2204 ~]# declare -r#只读变量在定义时就要赋值[root@ubuntu2204 ~]# declare -r r1[root@ubuntu2204 ~]# r1=123-bash: r1: readonly variable[root@ubuntu2204 ~]# declare -r r2=123[root@ubuntu2204 ~]# echo $r2123 15.3 变量间接引用15.3.1 eval命令eval命令将会首先扫描命令行进行所有的置换，然后再执行该命令。该命令适用于那些一次扫描无法实现其功能的变量,该命令对变量进行两次扫描 eval会对后面的命令进行两遍扫描，如果第一遍扫描后，命令是个普通命令，则执行此命令；如果命令中含有变量的间接引用，则保证间接引用的语义。也就是说，eval命令将会首先扫描命令行进行所有的置换，然后再执行该命令。因此，eval命令适用于那些一次扫描无法实现其功能的变量。 eval 执行以下两个步骤： 第一步，执行变量替换，类似与C语言的宏替代； 第二步，执行替换后的命令串 范例 1234567891011121314151617181920212223242526272829303132[root@centos8 ~]# CMD=whoami[root@centos8 ~]# echo $CMDwhoami[root@centos8 ~]# eval $CMDroot[root@centos8 ~]# n=10 [root@centos8 ~]# echo &#123;0..$n&#125; &#123;0..10&#125;[root@centos8 ~]# eval echo &#123;0..$n&#125;0 1 2 3 4 5 6 7 8 9 10[root@centos8 ~]#for i in `eval echo &#123;1..$n&#125;` ;do echo i=$i ;donei=1i=2i=3i=4i=5i=6i=7i=8i=9i=10[root@centos8 ~]#i=a[root@centos8 ~]#j=1[root@centos8 ~]#$i$j=hello-bash: a1=hello: command not found[root@centos8 ~]#eval $i$j=hello[root@centos8 ~]#echo $i$ja1[root@centos8 ~]#echo $a1hello 范例：执行含有带字符串的命令 我们可以新建一个文件test，将字符串 ”HelloWorld!&quot; 写入文件中，把 cat test 赋值给变量 WORD，如果我们 echo WORD 并不能得到 test 中的内容；然而 eval WORD 则能显示文件中的内容，因为 eval 命令对后面的命令进行了两次扫描，第一次将 WORD 替换为 cat test，第二次执行 cat test。这些需要进行两次扫描的变量有时被称为复杂变量。不过这些变量本身并不复杂。eval命令不仅可以回显复杂变量，也可以用于回显简单变量。 范例：eval命令还可以获取传给shell的最后一个参数 如果我们知道参数个数，我们想要查看最后一个参数的内容可以使用echo直接显示，如输入 first last 两个参数我们可以用 echo 2 来查看最后一个参数；但是，如果我们不知道参数个数还想查看最后一个参数内容该怎么办呢？这是我们就想到使用 # ，但显示的其实是参数个数，而使用 eval echo “$#” 才显示最后一个参数的内容。 范例：条件筛选 在file文件中写入两列数据，第一列对应KEY 、第二列为VALUE，使用eval命令将KEY与VALUE的值对应起来，从文件中读取： 注意： eval 不能获得函数处理结果。 eval 嵌套无意义，在其他语言中可以通过 eval(eval(“code”)) ，来执行（执行动态生成的 code 的返回），而由于shell 中 eval 将后面的 eval 命令简单当作命令字符串执行，失去了嵌套作用，嵌套被命令替换取代。 15.3.2 间接变量引用如果第一个变量的值是第二个变量的名字，从第一个变量引用第二个变量的值就称为间接变量引用variable1的值是variable2，而variable2又是变量名，variable2的值为value，间接变量引用是指通过variable1获得变量值value的行为 12345variable1=variable2variable2=value#示例:i=1$1=wang bash Shell提供了两种格式实现间接变量引用 123456789101112#方法1#变量赋值eval tempvar=\\$$variable1#显示值eval echo \\$$variable1eval echo &#x27;$&#x27;$variable1#方法2#变量赋值tempvar=$&#123;!variable1&#125;#显示值echo $&#123;!variable1&#125; 范例 1234567891011121314151617181920212223242526[root@centos8 ~]#ceo=name[root@centos8 ~]#name=mage[root@centos8 ~]#echo $ceoname[root@centos8 ~]#echo $$ceo33722ceo[root@centos8 ~]#echo $$33722[root@centos8 ~]#echo \\$$ceo$name[root@centos8 ~]#eval echo \\$$ceomage[root@centos8 ~]#eval tmp=\\$$ceo[root@centos8 ~]#echo $tmpmage[root@centos8 ~]#echo $&#123;!ceo&#125; #ceo=name,$&#123;!ceo&#125;相当于$name，因为!相当于$mage[root@server ~]# N1=N2[root@server ~]# N2=wangxiaochun[root@server ~]# eval NAME=\\$$N1[root@server ~]# echo $NAMEwangxiaochun[root@server ~]# NAME=$&#123;!N1&#125;[root@server ~]# echo $NAMEwangxiaochun 范例: 生成位置变量 1234567891011[root@centos7 ~]#cat test.sh#!/bin/bashfor i in &#123;1..3&#125;;do echo $&#123;!i&#125; #eval echo \\$$idone[root@centos7 ~]#bash test.sh a b cabc 范例: 批量创建用户 123456789101112#!/bin/bashn=$#[ $n -eq 0 ] &amp;&amp; &#123; echo &quot;Usage: `basename $0` username...&quot; ; exit 2; &#125;for i in `eval echo &#123;1..$n&#125;`;do user=$&#123;!i&#125; id $user &amp;&gt; /dev/null &amp;&amp; echo $user is exist || &#123; useradd $user; echo $user is created; &#125;done[root@centos8 ~]#bash create_user.sh hehe xixi hahahehe is createdxixi is createdhaha is created","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"系统优化","slug":"Linux/system-optimization","date":"2025-08-20T08:52:50.000Z","updated":"2025-09-08T13:51:53.487Z","comments":true,"path":"Linux/system-optimization/","permalink":"https://aquapluto.github.io/Linux/system-optimization/","excerpt":"","text":"1 文件描述符和进程系统正在经历着高并发访问，会涉及到打开很多文件，每个打开的文件都会对应一个文件描述符，而文件描述符是有上限的，达到上限将无法继续同时打开更多的文件，访问也就会受到限制，所以需要加大文件描述符与最大打开的进程数 硬限制（hard limit）一旦被设置以后就不能被非root用户修改 软限制（soft limit）可以增长达到硬限制（hard limit）。 硬限制（hard limit）一旦被设置以后就不能被非root用户修改，软限制（soft limit）可以增长达到硬限制 大型高并发项目可以设置为1000000，如果是一般中型项目几十万就可以了 1234567891011121314cat &gt;&gt;/etc/security/limits.conf&lt;&lt;EOF* soft nofile 102400* hard nofile 102400* soft nproc 102400* hard nproc 102400EOF&lt;domain&gt; &lt;type&gt; &lt;item&gt; value&gt;&lt;domain&gt; 表示要限制的用户&lt;type&gt; 设定类型&lt;item&gt; 表示可选的资源&lt;value&gt; 表示要限制的值/etc/security/limits.d/20-nproc.conf 优先级高于 /etc/security/limits.conf 2 调整Kernel pid max123456此文件的默认值32768将产生与早期内核相同的pid范围。在32位平台上，32768是pid最大值。在64位系统上，pid最大值可以设置为2^22等于4194304（pid最大值限制,大约400万）。[root@egon ~~]# cat /proc/sys/kernel/pid_max 131072 # 默认值echo &quot;kernel.pid_max= 4194303&quot; | tee -a /etc/sysctl.conf sysctl -p 3 其他内核优化1234567891011121314151617181920[root@egon ~]# cat &gt;&gt;/etc/sysctl.conf&lt;&lt;EOFnet.ipv4.tcp_fin_timeout = 2net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_keepalive_time = 600net.ipv4.ip_local_port_range = 4000 65000net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.route.gc_timeout = 100net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_synack_retries = 1net.core.somaxconn = 16384net.core.netdev_max_backlog = 16384net.ipv4.tcp_max_orphans = 16384net.ipv4.ip_forward = 1EOF#生效[root@egon ~]# sysctl -p syn cookies开启后用来防止syn洪水攻击，了解请看 更多参考：Linux内核参数优化","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"系统优化","slug":"系统优化","permalink":"https://aquapluto.github.io/tags/%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/"}]},{"title":"存储概论","slug":"Storage/conspect","date":"2025-08-20T08:52:20.000Z","updated":"2025-09-09T06:43:13.436Z","comments":true,"path":"Storage/conspect/","permalink":"https://aquapluto.github.io/Storage/conspect/","excerpt":"","text":"1 存储类型IT 基础设施有三个主要组成，分别是 计算，存储，网络 存储类型分为三种 直连式存储：Direct-Attached Storage，简称 DAS（本地存储） 存储区域网络：Storage Area Network，简称 SAN（远程存储，块，共享空间给远程用户，用户独占，可以分区，创建文件系统，挂载） 网络附加存储：Network-Attached Storage，简称 NAS（远程存储，文件，共享目录文件） 1.1 DASDAS 是指存储设备（硬盘、SSD、磁带库）直接通过物理接口（SATA、SAS、SCSI 或 USB）连接到单个服务器或计算机，不通过网络 由于是专用连接，延迟低，带宽高，性能最好 配置和管理相对简单，成本较低 通常只能被一台主机访问，数据共享困难 受限于主机的物理接口和空间 1.2 SANSAN 是一个专用的、高速的网络，通常使用光纤通道（Fibre Channel, FC）或基于以太网的 iSCSI&#x2F;FCoE 协议将多个存储设备（磁盘阵列、磁带库）连接到多个服务器，提供块级（Block-Level）存储访问。服务器将 SAN 存储视为本地磁盘，需要服务器进行文件系统管理 专用网络，延迟低，吞吐量高 可以轻松添加服务器和存储设备 存储资源集中管理，便于备份、快照、复制等操作 支持冗余路径和故障切换 需要专用的硬件（如光纤交换机、HBA卡），成本高，部署和维护复杂 1.3 NASNAS 是一个连接到现有网络（通常是 IP 网络）的专用存储设备，提供文件级（File-Level）存储服务 基于标准网络，配置简单，即插即用 多台客户端可以通过标准协议（如 NFS、SMB&#x2F;CIFS）同时访问文件 利用现有网络基础设施，硬件成本相对较低 NAS 设备自身管理文件系统，客户端直接访问文件和目录 性能依赖于网络带宽和负载，通常低于 DAS 和 SAN 大量 I&#x2F;O 操作会占用网络带宽 1.4 三种存储比较SAN与NAS的主要区别体现在文件系统所在的位置 特性 DAS NAS SAN 传输类型 SCSI、FC IP IP、FC、SAS 数据类型 数据块 文件 数据块 典型应用 任何 文件服务器 数据库应用 优点 磁盘与服务器分离，便于统一管理 不占用应用服务器资源广泛支持操作系统扩展较容易即插即用，安装简单方便 高扩展性高可用性数据集中，易管理 缺点 连接距离短数据分散，共享困难存储空间利用率不高扩展性有限 不适合存储量大的块级应用数据备份及恢复占用网络带宽 相比NAS成本较高安装和升级比NAS复杂 应用场景 DAS虽然比较古老了，但是还是很适用于那些数据量不大，对磁盘访问速度要求较高的中小企业 NAS多适用于文件服务器，用来存储非结构化数据，虽然受限于以太网的速度，但是部署灵活，成本低 SAN则适用于大型应用或数据库系统，缺点是成本高、较为复杂 2 存储方式块存储：像硬盘，高性能，挂载后用。 文件存储：像U盘&#x2F;网盘，有目录，直接读写文件，可共享。 对象存储：像网盘（如OSS&#x2F;S3），通过URL操作，无目录层级，适合大文件。 分布式存储：底层整合多台机器，可统一提供以上三种服务。 类型 核心特点 访问方式 优点 缺点 典型应用 块存储 原始“磁盘”，需格式化 + 挂载 通过设备（如 /dev/sdb）访问 ⚡ 读写快、低延迟 ❌ 不易扩展、难共享 虚拟机磁盘、数据库 文件存储 层次化目录结构（如文件夹） 标准文件接口（open/read/write） ✅ 易共享、易管理、POSIX 兼容 🐢 速度较慢 NAS、共享文件夹、Web 静态资源 对象存储 扁平结构，唯一 ID 标识对象 RESTful API &#x2F; SDK（如 S3） ☁️ 海量扩展、高可用、支持元数据 🔒 不支持随机读写、只能全量操作 图片&#x2F;视频存储、备份归档、云存储 分布式存储 多节点协同，统一资源池 可提供块、文件、对象三种接口 🌐 高扩展、高性能、高可靠 ⚙️ 架构复杂 云平台、大数据、AI 训练","categories":[{"name":"存储","slug":"存储","permalink":"https://aquapluto.github.io/categories/%E5%AD%98%E5%82%A8/"}],"tags":[]},{"title":"NFS数据共享服务","slug":"Storage/nfs","date":"2025-08-20T08:52:20.000Z","updated":"2025-09-09T09:25:12.983Z","comments":true,"path":"Storage/nfs/","permalink":"https://aquapluto.github.io/Storage/nfs/","excerpt":"","text":"1 NFS服务的工作原理NFS（Network File System，网络文件系统）是由 Sun 公司开发的基于内核的文件系统，其核心功能是允许用户和程序像访问本地文件一样访问远程系统上的文件。NFS 的实现依赖于 RPC（Remote Procedure Call Protocol，远程过程调用）协议 ——RPC 采用客户端 &#x2F; 服务器（C&#x2F;S）模式，客户端请求程序通过发送包含进程参数的调用信息到服务进程以获取服务，服务器进程则在接收请求后计算结果并返回应答，客户端再根据应答继续执行后续操作。 NFS 的显著优势在于节省本地存储空间：例如，可将常用数据（如/home目录）集中存储在 NFS 服务器上，本地终端通过网络访问即可，无需占用自身存储。一个简单的应用场景是：服务器 A 将/data/目录通过 NFS 共享，客户端 B 将其挂载到本地/tmp目录，此时 B 在/tmp中写入的数据实际存储在 A 的/data/中；同时，A 直接向/data/写入的数据，B 也可通过/tmp目录实时访问，实现了跨主机的文件共享与协同操作。 注册中心：假设Aserver对外提供web服务，它的地址通过容器的方法实现，地址自动分配，不固定，用户想访问没办法直接访问，所以Aserver启动后，就会把地址向注册中心进行注册，用户就可以向注册中心寻找自己想要的服务查找IP地址 由于 NFS 服务的端口不固定，其启动时会将自身端口号向 RPC 服务注册，而 RPC 服务的端口号固定为 111。因此，当用户需要访问 NFS 服务时，会先访问 111 端口的 RPC 服务，查询 NFS 当前的端口号，获取端口后即可直接与 NFS 服务建立连接并进行数据访问，这一过程类似 “注册中心” 的机制 —— 就像服务 A 启动后向注册中心注册地址，用户通过注册中心查询地址再访问服务一样，NFS 通过 RPC 完成端口的 “注册与查询”，解决了端口不固定导致的访问问题。 2 NFS软件包由于NFS 基于 C&#x2F;S 模式实现，所以有客户端软件和服务端软件。红帽系统和Ubuntu的NFS软件包名不一样，除了安装NFS软件包，还需要确保 rpcbind（必须）和 tcp_wrappers（非必须）已安装，通常它们可能作为依赖自动安装 红帽系统：nfs-utils，包括服务器和客户端相关工具 Ubuntu：nfs-server 服务器包名，nfs-common 客户端包名 NFS服务主要进程： rpc.nfsd：最主要的NFS进程，管理客户端是否可登录 rpc.mountd：挂载和卸载NFS文件系统，包括权限管理 rpc.lockd：非必要，管理文件锁，避免同时写出错 rpc.statd：非必要，检查文件一致性，可修复文件 NFS 服务的端口分为固定与动态两部分：其核心进程nfsd使用固定端口 2049（TCP&#x2F;UDP），而其他关键进程（如负责挂载管理的rpc.mountd、文件锁管理的rpc.lockd等）的端口是动态分配的。这些动态端口会在 NFS 服务启动时向rpcbind（端口 111，TCP&#x2F;UDP）注册，rpcbind作为 “端口注册中心”，负责记录并向客户端提供这些动态端口的信息。因此，客户端访问 NFS 时，需先通过 111 端口查询rpcbind获取动态端口，再结合固定的 2049 端口完成连接。 NFS相关文件 /etc/exports：共享规则配置文件（全局） /etc/exports.d/*.exports：按业务创建规则（局部），后缀必须为exports /var/lib/nfs/：日志文件 红帽系统安装NFS软件包 1234567[root@centos8 ~]#yum -y install nfs-utils#查看支持的NFS版本,注意:只有服务启动才能看此文件[root@centos8 ~]#cat /proc/fs/nfsd/versions-2 +3 +4 +4.1 +4.2[root@centos8 ~]#systemctl enable --now nfs-server.service Ubuntu 安装NFS软件包 12345678910#服务器[root@ubuntu2004 ~]#apt update &amp;&amp; apt -y install nfs-kernel-server[root@ubuntu2004 ~]#systemctl status nfs-server#查看支持的NFS版本,注意:只有服务启动才能看此文件[root@ubuntu2004 ~]#cat /proc/fs/nfsd/versions-2 +3 +4 +4.1 +4.2#客户端[root@ubuntu2004 ~]#apt -y install nfs-common 服务端依赖包，注册中心，NFS服务中有大量的组件，部份组件端口并不固定，需要依赖 rpcbind 发布端口 1234567891011121314151617181920212223242526272829303132333435363738[root@ubuntu ~]# dpkg -l rpcbindDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name Version Architecture Description+++-==============-=============-============-=====================================================ii rpcbind 1.2.6-2build1 amd64 converts RPC program numbers into universal addresses#查看服务端端口[root@ubuntu ~]# ss -tunlp | grep rpcudp UNCONN 0 0 127.0.0.1:603 0.0.0.0:* users:((&quot;rpc.statd&quot;,pid=14843,fd=5))udp UNCONN 0 0 0.0.0.0:38632 0.0.0.0:* users:((&quot;rpc.statd&quot;,pid=14843,fd=8))udp UNCONN 0 0 0.0.0.0:58332 0.0.0.0:* users:((&quot;rpc.mountd&quot;,pid=15464,fd=4))udp UNCONN 0 0 0.0.0.0:54239 0.0.0.0:* users:((&quot;rpc.mountd&quot;,pid=15464,fd=12))udp UNCONN 0 0 0.0.0.0:48141 0.0.0.0:* users:((&quot;rpc.mountd&quot;,pid=15464,fd=8))udp UNCONN 0 0 0.0.0.0:111 0.0.0.0:* users:((&quot;rpcbind&quot;,pid=14257,fd=5),(&quot;systemd&quot;,pid=1,fd=138))....#重启服务[root@ubuntu ~]# systemctl restart nfs-server.service#再次查看端口，端口发生了变化[root@ubuntu ~]# ss -tunlp | grep rpcudp UNCONN 0 0 0.0.0.0:36333 0.0.0.0:* users:((&quot;rpc.mountd&quot;,pid=15507,fd=12))udp UNCONN 0 0 127.0.0.1:603 0.0.0.0:* users:((&quot;rpc.statd&quot;,pid=14843,fd=5))udp UNCONN 0 0 0.0.0.0:38632 0.0.0.0:* users:((&quot;rpc.statd&quot;,pid=14843,fd=8))udp UNCONN 0 0 0.0.0.0:111 0.0.0.0:* users:((&quot;rpcbind&quot;,pid=14257,fd=5),(&quot;systemd&quot;,pid=1,fd=138))udp UNCONN 0 0 0.0.0.0:48272 0.0.0.0:* users:((&quot;rpc.mountd&quot;,pid=15507,fd=4))udp UNCONN 0 0 0.0.0.0:38052 0.0.0.0:* users:((&quot;rpc.mountd&quot;,pid=15507,fd=8))#由于服务端监听的端口会发生变化，所以需要依赖rpcbind对客户端提供注册服务，rpcbind端口不会发生变化#查看服务端使用的端口[root@ubuntu ~]# rpcinfo -p program vers proto port service 100000 4 tcp 111 portmapper #rpcbind，早期叫portmapper .... 3 NFS共享配置文件格式12345/dir 客户端主机1(opt1,opt2) 客户端主机2(opt1,opt2).../dir #NFS服务器上要被共享的目录路径客户端主机 #可以共享此目录的客户端主机opt #配置项 客户端主机格式 12345678910anonymous # *表示不限制，即任何主机都能访问单个主机 #可以写具体的IPV4,IPV6，FQDN，主机名IP networks（网段） #两种掩码格式均支持172.18.0.0/255.255.0.0172.18.0.0/16wildcards #主机名通配，例如:*.wang.org，IP不可以netgroups #NIS域的主机组，@group_namegss/krb5i #这种写法来限制对使用rpcsec_gss安全性的客户端的访问 opt配置项 12345678910111213141516171819202122232425262728293031323334默认选项：(ro,sync,root_squash,no_all_squash)# 访问权限控制ro # 只读rw # 读写# 同步/异步写入sync # 同步写入，数据在请求时立即写入共享存储磁盘（默认，性能低，安全性高）async # 异步写入，先写入缓冲区，过一段时间再写入磁盘（性能高，安全性低）# 用户映射与匿名控制root_squash # （默认）将远程客户端主机上的root用户映射为nfsnobody,UID为65534的用户，CentOS8为nobodyno_root_squash # 不映射远程root用户，保留其root权限all_squash # 所有远程客户端主机上的用户均映射为匿名用户（UID 65534），此项会覆盖no_root_squashno_all_squash # （默认）保留客户端用户的原始UID和GIDanonuid # 指定匿名用户的UID（配合all_squash使用），注意:目录需要给此用户权限,否则无法访问 anongid # 指定匿名用户的GID（配合all_squash使用）# 检查父目录权限subtree_check # 如果共享子目录，强制NFS检查父目录权限no_subtree_check # 不检查父目录权限，提升性能# 写入延迟控制wdelay # （默认）等待多个写操作合并后一起写入no_wdelay # 立即写入，不等待；使用async时自动启用# 目录可见性控制hide # 不共享NFS服务器上的子目录（NFSv4无效）no_hide # 共享NFS服务器上的子目录（NFSv4无效）# 安全与端口设置secure # 仅允许1024以下的特权端口连接insecure # 允许1024以上的非特权端口连接（常见于NFSv3客户端）insecure NFS # 服务通过1024以上的TCP/IP端口通信 范例：NFS配置示例 12345678910111213[root@centos8 ~]#vim /etc/exports/myshare server.example.com/myshare *.example.com/myshare server?.example.com/myshare server[0-20].example.com/myshare 172.25.11.10/myshare 172.25.0.0/16/myshare 2000:472:18:b51:c32:a21/myshare 2000:472:18:b51::/64/myshare *.example.com 172.25.0.0/16/myshare desktop.example.com(ro)/myshare desktop.example.com(ro) server[0-20].example.com(rw)/myshare diskless.example.com(rw,no_root_squash) 范例：默认挂载 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#服务器[root@ubuntu ~]# mkdir /data/dir&#123;a,b&#125;[root@ubuntu ~]# cp /etc/fstab /data/dira/[root@ubuntu ~]# cat /etc/exports/data/dira *#读取配置，生效[root@ubuntu ~]# exportfs -r#查看[root@ubuntu ~]# exportfs -v/data/dira &lt;world&gt;(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash)[root@ubuntu ~]# mount10.0.0.208:/data/dira on /data/dir1 type nfs4(ro,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.206,local_lock=none,addr=10.0.0.208)#在客户端#查看NFS服务器上有什么共享文件夹[root@ubuntu ~]# showmount -e 10.0.0.208Export list for 10.0.0.208:/data/dira *[root@ubuntu ~]# mkdir -pv /data/dir&#123;1,2&#125;mkdir: created directory &#x27;/data&#x27;mkdir: created directory &#x27;/data/dir1&#x27;mkdir: created directory &#x27;/data/dir2&#x27;#将远程主机上的 /data/dira 目录挂载到本机的 /data/dir1 上[root@ubuntu ~]# mount 10.0.0.208:/data/dira /data/dir1#查看[root@ubuntu ~]# df -h /data/dir1/Filesystem Size Used Avail Use% Mounted on10.0.0.208:/data/dira 97G 7.4G 85G 9% /data/dir1[root@ubuntu ~]# ls -l /data/dir1total 4-rw-r--r-- 1 root root 657 Jul 9 09:58 fstab#可读[root@ubuntu ~]# cat /data/dir1/fstab# /etc/fstab: static file system information.#......#不可写[root@ubuntu ~]# echo &quot;123&quot; &gt;&gt; /data/dir1/fstab-bash: /data/dir1/fstab: Read-only file system[root@ubuntu ~]# touch /data/dir1/testtouch: cannot touch &#x27;/data/dir1/test&#x27;: Read-only file system 范例：指定客户端主机 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475[root@ubuntu ~]# cat /etc/exports#10.0.0.206 可读写， 10.0.0.150 默认选项/data/dira 10.0.0.206(rw) 10.0.0.150#生效[root@ubuntu ~]# exportfs -r[root@ubuntu ~]# exportfs -v/data/dira 10.0.0.206(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,root_squash,no_all_squash)/data/dira 10.0.0.150(sync,wdelay,hide,no_subtree_check,sec=sys,ro,secure,root_squash,no_all_squash)#206 主机上测试，还是可读不可写[root@ubuntu ~]# ls -l /data/dir1/total 4-rw-r--r-- 1 root root 657 Jul 9 09:58 fstab[root@ubuntu ~]# cat /data/dir1/fstab# /etc/fstab: static file system information.[root@ubuntu ~]# touch /data/dir1/test-206touch: cannot touch &#x27;/data/dir1/test-206&#x27;: Permission denied #权限不够#查看服务器上共享目录的权限[root@ubuntu ~]# ll /data/dira/total 4drwxr-xr-x 1 root root 657 Jul 9 09:58 /data/dira#虽然root上w权限，但是仅限于服务器的root，206虽然是以root用户远程连接，但是属于其他[root@ubuntu ~]#chmod 777 /data/dira#成功[root@ubuntu ~]# touch /data/dir1/test-206#客户端去服务器访问数据的时候，如果是以root身份，将映射成服务器的nobody用户，所以解释了为什么206虽然是以root用户远程连接，但是属于其他用户，之前无权限创建时因为没有给nobody用户授权[root@ubuntu ~]# ll /data/dir1/total 4-rw-r--r-- 1 root root 657 Jul 9 09:58 fstab-rw-r--r-- 1 nobody nogroup 0 Jul 9 10:02 test-206#所以解决权限问题有三种办法#第一种[root@ubuntu ~]#chown nobody /data/dira/#第二种[root@ubuntu ~]#setfacl -m u:nobody:rwx /data/dira/#第三种[root@ubuntu ~]# cat /etc/exports/data/dira 10.0.0.206(rw,no_root_squash) 10.0.0.150#150 主机上测试[root@rocky ~]# mkdir /data/dir&#123;1,2&#125;[root@rocky ~]# showmount -e 10.0.0.208Export list for 10.0.0.208:/data/dira 10.0.0.150,10.0.0.206#挂载[root@rocky ~]# mount 10.0.0.208:/data/dira /data/dir1#查看[root@rocky ~]# df /data/dir1/Filesystem 1K-blocks Used Available Use% Mounted on10.0.0.208:/data/dira 101590016 7749888 88633344 9% /data/dir1[root@rocky ~]# mount10.0.0.208:/data/dira on /data/dir1 type nfs4(rw,relatime,vers=4.2,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=10.0.0.150,local_lock=none,addr=10.0.0.208)#查看，可读不可写[root@rocky ~]# ls -l /data/dir1total 4-rw-r--r-- 1 root root 657 Jul 9 09:58 fstab[root@rocky ~]# cat /data/dir1/fstab# /etc/fstab: static file system information.#[root@rocky ~]# touch /data/dir1/test-150touch: cannot touch &#x27;/data/dir1/test-150&#x27;: Read-only file system 范例：没有权限的主机无法挂载 12345678#10.0.0.210 主机[root@ubuntu ~]# showmount -e 10.0.0.208Export list for 10.0.0.208:/data/dira 10.0.0.150,10.0.0.206#当前主机不在NFS 服务器配置的客户端主机列表中，无法挂载[root@ubuntu ~]# mount 10.0.0.208:/data/dira /data/dir1mount.nfs: access denied by server while mounting 10.0.0.208:/data/dira 范例：永久挂载 123#在客户端主机添加如下配置[root@ubuntu ~]# cat /etc/fstab10.0.0.208:/data/dira /var/www/html nfs defaults,_netdev 0 0 4 NFS工具4.1 rpcinforpcinfo 可以访问指定的 RPC 服务器，显示其响应信息，从而查询出在该服务器上注册的RPC服务，如果不指定主机，则默认是当前主机 查看注册在指定主机的RPC程序 12rpcinfo -p hostnamerpcinfo -s hostname 范例 123456789101112131415#显示已注册到本机的所有RPC服务[root@ubuntu ~]# rpcinfo#显示已注册到本机的rpcbind V2 版本的 RPC 服务[root@ubuntu ~]# rpcinfo -p program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper #简短格式显示[root@ubuntu ~]# rpcinfo -s#查看远程主机[root@ubuntu ~]# rpcinfo -s 10.0.0.208 4.2 exportfsexportfs 命令用于管理本机 NFS 文件系统，默认配置文件是 /etc/exports 1234-v #查看本机所有NFS共享-r #重读配置文件，并共享目录-a #输出本机所有共享-au #停止本机所有共享 4.3 showmountshoumount 可以查看远程主机的共享设置 123456showmount [ -opt... ] [ host ]#常用选项-h|--help #显示帮助-a|--all #显示己连接的客户端-e|--exports #显示指定NFS服务器上的配置列表 范例 1234#查看远程主机的NFS共享配置[root@centos7 ~]#showmount -e 10.0.0.8Export list for 10.0.0.8:/data/wordpress * 4.4 mount.nfs客户端 NFS 服务挂载命令，将远程NFS服务器上共享出来的目录挂载到本机，可以直接写成 mount 12345678910111213141516171819202122mount [选项] &lt;远程主机:导出路径&gt; &lt;本地挂载点&gt;#常用选项-r #只读挂载-v #显示详细信息-V #显示版本-w #读写挂载-n #不更新/etc/fstab 文件，默认项-o #指定挂载参数，更多挂载选项：man 5 nfs fg #（默认）前台挂载 bg #后台挂载 hard #（默认）持续请求，如果挂不上，一直重试 soft #非持续请求 intr #是否可以强制中断，配合hard 选项，如果挂不上，可以ctrl+c 中断 rsize/wsize #指定一次读/写的最大字节数，值必须为1024的倍数，最大为1048576，最小为1024，如果小于1024会被替换成4096 nosuid #挂载的文件系统中，setuid 和 setgid 位将被忽略。用于安全目的，防止执行带有特权的程序。 noexec #禁止在挂载的文件系统上直接执行任何二进制程序。常用于提高安全性，如挂载用户上传目录。 _netdev #无网络服务时不挂载NFS资源 vers #指定版本，客户端centos8默认4.2 ，centos7默认4.1 centos6默认4.0，如果NFS 服务端不支持此版本，则无法挂载 提示：基于安全考虑，建议使用 nosuid , _netdev , noexec 挂载选项 范例：临时挂载NFS共享 1[root@ubuntu ~]# mount -o rw,fg,hard,intr 10.0.0.208:/testdir /mnt/nfs/ 12345678[root@centos7 ~]#mkdir /mnt/nfs[root@centos7 ~]#mount 10.0.0.8:/data/wordpress /mnt/nfs[root@centos7 ~]#ls /mnt/nfsindex.html[root@centos7 ~]#df -T /mnt/nfsFilesystem Type 1K-blocks Used Available Use% Mounted on10.0.0.8:/data/wordpress nfs4 52403200 398336 52004864 1% /mnt/nfs 范例：开机挂载 12#vim /etc/fstab 172.16.0.1:/public /mnt/nfs nfs defaults,_netdev 0 0 远程的 root 映射为NFS服务器的nobody用户 12345678910[root@centos6 ~]#grep nobody /etc/passwdnobody:x:99:99:Nobody:/:/sbin/nologinnfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin[root@centos7 ~]#grep nobody /etc/passwdnobody:x:99:99:Nobody:/:/sbin/nologinnfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin[root@centos8 ~]#grep nobody /etc/passwdnobody:x:65534:65534:Kernel Overflow User:/:/sbin/nologin 5 NFS共享实现5.1 目标将NFS的共享目录，做为远程主机用户的家目录 5.2 环境准备 123456共三台主机一台主机 nfs serverIP:10.0.0.8另两台当 nfs clientIP:10.0.0.7IP:10.0.0.6 5.3 步骤5.3.1 在服务端配置共享1234567#NFS服务器创建用户和相应的家目录，将用户wang的家目录共享[root@centos8 ~]#yum -y install nfs-utils[root@centos8 ~]#systemctl enable --now nfs-server[root@centos8 ~]#mkdir -pv /data/home/wang[root@centos8 ~]#Vim /etc/exports.d/test.exports/data/home/wang *(rw)[root@centos8 ~]#exportfs -r 5.3.2 客户端配置123456789101112131415161718192021222324252627282930#在第一台NFS客户端主机10.0.0.7上实现[root@centos7 ~]#yum -y install nfs-utils[root@centos7 ~]#useradd -u 2000 wang[root@centos7 ~]#vim /etc/fstab10.0.0.8:/data/home/wang /home/wang nfs _netdev 0 0[root@centos7 ~]#mount -a[root@centos7 ~]#su - wangLast login: Fri Jul 3 16:33:34 CST 2020 on pts/0[wang@centos7 ~]$pwd/home/wang[wang@centos7 ~]$df /home/wang -TFilesystem Type 1K-blocks Used Available Use% Mounted on10.0.0.8:/data/home/wang nfs4 52403200 398464 52004736 1% /home/wang#在第二台NFS客户端主机10.0.0.6上实现[root@centos6 ~]#yum -y install nfs-utils[root@centos6 ~]#useradd -u 2000 wang[root@centos6 ~]#vim /etc/fstab10.0.0.8:/data/home/wang /home/wang nfs _netdev 0 0[root@centos6 ~]#su - wang[wang@centos6 ~]$pwd/home/wang[wang@centos6 ~]$df -T /home/wangFilesystem Type 1K-blocks Used Available Use% Mounted on10.0.0.8:/data/home/wang nfs 52403200 398464 52004736 1% /home/wang 5.3.3 用户映射12345678910111213141516171819202122232425262728293031323334353637383940414243#普通用户不映射#在客户机上以普通用户创建文件[wang@centos7 ~]$ touch /home/wang/wang.txt#客户机上显示属主属组是wang[wang@centos7 ~]$ ls -l /home/wang/wang.txt-rw-rw-r-- 1 wang wang 0 Jul 9 00:31 /home/wang/wang.txt#NFS 服务器上查看，找不到对应的用户，直接显示UID[root@centos8 ~]#ls -l /data/home/wang/wang.txt-rw-rw-r-- 1 2000 2000 0 Jul 9 00:31 /data/home/wang/wang.txt#NFS 服务器上以相同UID 创建用户,再查看[root@centos8 ~]#useradd -d /data/home/wang -u 2000 wang[root@centos8 ~]#ls -l /data/home/wang/wang.txt-rw-rw-r-- 1 wang wang 0 Jul 9 00:31 /data/home/wang/wang.txt#再次修改规则，all_squash 项会覆盖 no_root_squash[root@centos8 ~]#cat /etc/exports/data/www *(rw,no_root_squash,all_squash)#生效[root@centos8 ~]# exportfs -r[root@centos8 ~]# exportfs -v/data/www &lt;world&gt;(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,all_squash)#在客户端创建文件[root@centos7 ~]#touch /home/wang/wang1.txt[root@centos7 ~]#su - wang [wang@centos7 ~]$ touch /home/wang/wang2.txt[wang@centos7 ~]$ exitlogout#在客户端查看[root@centos7 ~]# ll /home/wang/wang&#123;1,2&#125;.txt-rw-r--r-- 1 nobody nogroup 0 Jul 9 08:51 /home/wang/wang2.txt-rw-rw-r-- 1 nobody nogroup 0 Jul 9 08:51 /home/wang/wang1.txt#在NFS服务端查看，普通用户和 root 都被映射[root@centos8 ~]# ll /data/home/wang/wang&#123;1,2&#125;.txt-rw-r--r-- 1 nobody nogroup 0 Jul 9 08:51 /data/home/wang/wang2.txt-rw-rw-r-- 1 nobody nogroup 0 Jul 9 08:51 /data/home/wang/wang1.txt 5.3.4 统一用户映射123456789101112131415161718192021222324#NFS 服务端配置[root@centos8 ~]# groupadd -g 12345 www[root@centos8 ~]# useradd -u 12345 -g 12345 -s /sbin/nologin www#指定映射用户，对 root 和普通用户都生效[root@centos7 ~]# cat /etc/exports/data/www *(rw,all_squash,anongid=12345,anonuid=12345)[root@ubuntu ~]# exportfs -r[root@ubuntu ~]# exportfs -v/data/www &lt;world&gt;(sync,wdelay,hide,no_subtree_check,anonuid=12345,anongid=12345 ,sec=sys,rw,secure,root_squash,all_squash)#客户端创建文件[root@centos7 ~]# touch /home/wang/wang3.txt[root@centos7 ~]# su - wang[wang@centos7 ~]$ touch /home/wang/wang4.txt[wang@centos7 ~]$ exitlogout#客户端查看[root@centos7 ~]# ll /home/wang/wang&#123;3,4&#125;.txt-rw-r--r-- 1 12345 12345 0 Jul 9 09:37 /home/wang/wang4.txt-rw-rw-r-- 1 12345 12345 0 Jul 9 09:37 /home/wang/wang3.txt 统一客户端主机用户和权限的方法 保证客户端主机使用相同的用户(相同的用户ID)对共享目录进行读写 在NFS 服务器上统一映射 新建服务器使用 LDAP（轻量级目录访问协议） 服务进行集中的帐号管理 6 基于LAMP架构，通过NFS实现数据共享存储，并实时同步到备份服务器储备知识：数据实时同步 架构说明：两台web服务器（WordPress），将业务数据（如 WordPress 的文章、用户信息等）存储在mysql，静态文件资源存储在NFS上，两台web服务器共享，保证多 Web 节点的内容同步，并实时同步到备份服务器上 配置 说明 10.0.0.176（centos） DNS 10.0.0.177 MySQL 10.0.0.179（rocky） Web 10.0.0.180 Web 10.0.0.182（Ubuntu） NFS 10.0.0.183 NFS BACKUP 6.1 配置DNS12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@DNS ~]#yum -y install bind bind-utils[root@DNS ~]#systemctl enable --now named[root@DNS ~]#vim /etc/named.rfc1912.zoneszone &quot;wu.org&quot; IN &#123; type master; file &quot;wu.org.zone&quot;;&#125;;[root@DNS ~]#vim /etc/named.conf // listen-on port 53 &#123; 127.0.0.1; &#125;;// allow-query &#123; localhost; &#125;;[root@DNS ~]#vim /var/named/wu.org.zone$TTL 1D@ IN SOA wu.org. admin.wu.com. (1 3H 1M 1D 1W) NS master master A 10.0.0.176www A 10.0.0.179www A 10.0.0.180[root@DNS ~]#chmod 640 /var/named/wu.org.zone [root@DNS ~]#chgrp named /var/named/wu.org.zone[root@DNS ~]#named-checkconf[root@DNS ~]#named-checkzone wu.org /var/named/wu.org.zone[root@DNS ~]#rndc reload#测试[root@DNS ~]#host www.wu.org 127.0.0.1Using domain server:Name: 127.0.0.1Address: 127.0.0.1#53Aliases: www.wu.org has address 10.0.0.179www.wu.org has address 10.0.0.180C:\\Users\\Administrator&gt;nslookup&gt; server 10.0.0.176默认服务器: [10.0.0.176]Address: 10.0.0.176&gt; www.wu.org服务器: [10.0.0.176]Address: 10.0.0.176名称: www.wu.orgAddresses: 10.0.0.180 10.0.0.179 #Switchhosts软件10.0.0.179 www.wu.orgC:\\Users\\Administrator&gt;ping www.wu.org正在 Ping www.wu.org [10.0.0.179] 具有 32 字节的数据:来自 10.0.0.179 的回复: 字节=32 时间&lt;1ms TTL=64来自 10.0.0.179 的回复: 字节=32 时间&lt;1ms TTL=64 6.2 配置Web1和MySQL123456789101112131415161718192021222324252627[root@Web1 ~]#yum -y install httpd php php-mysqlnd php-json nfs-utils[root@Web1 ~]#systemctl enable --now httpd[root@Web1 ~]#wget https://cn.wordpress.org/latest-zh_CN.zip[root@Web1 ~]#unzip latest-zh_CN.zip[root@Web1 ~]#mv wordpress/* /var/www/html/[root@Web1 ~]#chown apache.apache /var/www/html/ -R[root@MySQL ~]#yum install -y mysql-server[root@MySQL ~]#systemctl enable --now mysqld#Ubuntu默认开启mysql，但是要改端口指向地址vim /etc/mysql/mysql.conf.d/mysqld.cnf#bind...#mysql...systemctl restart mysqlmysql&gt; create database wordpress;mysql&gt; create user wordpress@&#x27;10.0.0.%&#x27; identified by &#x27;123456&#x27;;mysql&gt; grant all on wordpress.* to wordpress@&#x27;10.0.0.%&#x27;;Windows访问：www.wu.org[root@Web1 ~]#cd /var/www/html/wp-content/[root@Web1 wp-content]#lsindex.php languages plugins themes uploads#说明themes:主题uploads：图片 6.3 配置NFS1234567891011121314151617181920212223242526272829303132333435363738394041[root@NFS ~]#apt update &amp;&amp; apt install -y nfs-server[root@NFS ~]#mkdir -p /data/www[root@NFS ~]#mkdir /etc/exports.d[root@NFS ~]#vim /etc/exports.d/wordpress.exports/data/www 10.0.0.0/24(rw)[root@NFS ~]#exportfs -r[root@NFS ~]#groupadd -g 48 apache[root@NFS ~]#useradd -u 48 -g 48 apache[root@NFS ~]#chown -R apache.apache /data/www/#说明如果web服务器为Ubuntu，就只需执行chown -R www-data.www-data /data/www/，因为Ubuntu共有www-data账号[root@Web1 ~]#rsync -av /var/www/html/ 10.0.0.182:/data/www/[root@NFS ~]#ll /data/www/total 244drwxr-xr-x 5 apache apache 4096 Dec 13 06:23 ./drwxr-xr-x 3 root root 4096 Dec 13 06:50 ../-rw-r--r-- 1 apache apache 228 Dec 13 06:23 .htaccess-rw-r--r-- 1 apache apache 405 Feb 6 2020 index.php-rw-r--r-- 1 apache apache 19915 Dec 6 18:09 license.txt-rw-r--r-- 1 apache apache 7399 Dec 6 18:09 readme.html-rw-r--r-- 1 apache apache 7211 May 12 2023 wp-activate.phpdrwxr-xr-x 9 apache apache 4096 Dec 6 18:00 wp-admin/-rw-r--r-- 1 apache apache 351 Feb 6 2020 wp-blog-header.php-rw-r--r-- 1 apache apache 2323 Jun 14 14:11 wp-comments-post.php-rw-rw-rw- 1 apache apache 3293 Dec 13 06:09 wp-config.php-rw-r--r-- 1 apache apache 3013 Dec 6 18:09 wp-config-sample.phpdrwxr-xr-x 6 apache apache 4096 Dec 13 06:28 wp-content/-rw-r--r-- 1 apache apache 5638 May 30 2023 wp-cron.phpdrwxr-xr-x 27 apache apache 12288 Dec 6 18:09 wp-includes/-rw-r--r-- 1 apache apache 2502 Nov 26 2022 wp-links-opml.php-rw-r--r-- 1 apache apache 3927 Jul 16 12:16 wp-load.php-rw-r--r-- 1 apache apache 50924 Sep 29 22:01 wp-login.php-rw-r--r-- 1 apache apache 8525 Sep 16 06:50 wp-mail.php-rw-r--r-- 1 apache apache 26409 Oct 10 14:05 wp-settings.php-rw-r--r-- 1 apache apache 34385 Jun 19 18:27 wp-signup.php-rw-r--r-- 1 apache apache 4885 Jun 22 14:36 wp-trackback.php-rw-r--r-- 1 apache apache 3154 Sep 30 07:39 xmlrpc.php 测试 123456789101112131415161718192021222324252627[root@Web1 ~]#showmount -e 10.0.0.182Export list for 10.0.0.182:/data/www 10.0.0.0/24[root@Web1 ~]#mv /var/www/html/* /opt/[root@Web1 ~]#vim /etc/fstab 10.0.0.182:/data/www /var/www/html nfs _netdev 0 0 [root@Web1 ~]#mount -a[root@Web1 ~]#dfFilesystem 1K-blocks Used Available Use% Mounted ondevtmpfs 894016 0 894016 0% /devtmpfs 914116 0 914116 0% /dev/shmtmpfs 914116 8936 905180 1% /runtmpfs 914116 0 914116 0% /sys/fs/cgroup/dev/mapper/rl-root 17811456 2600584 15210872 15% //dev/sda1 1038336 217304 821032 21% /boottmpfs 182820 0 182820 0% /run/user/010.0.0.182:/data/www 10219008 3314688 6363648 35% /var/www/html[root@Web1 ~]#ls /var/www/html/index.php wp-admin wp-config-sample.php wp-links-opml.php wp-settings.phplicense.txt wp-blog-header.php wp-content wp-load.php wp-signup.phpreadme.html wp-comments-post.php wp-cron.php wp-login.php wp-trackback.phpwp-activate.php wp-config.php wp-includes wp-mail.php xmlrpc.php#Windows访问正常 6.4 配置Web212345678910111213141516171819202122232425[root@Web2 ~]#yum -y install httpd php php-mysqlnd php-json nfs-utils[root@Web2 ~]#systemctl enable --now httpd[root@Web2 ~]#vim /etc/fstab 10.0.0.182:/data/www /var/www/html nfs _netdev 0 0 [root@Web2 ~]#mount -a[root@Web2 ~]#dfFilesystem 1K-blocks Used Available Use% Mounted ondevtmpfs 894016 0 894016 0% /devtmpfs 914116 0 914116 0% /dev/shmtmpfs 914116 8916 905200 1% /runtmpfs 914116 0 914116 0% /sys/fs/cgroup/dev/mapper/rl-root 17811456 2431172 15380284 14% //dev/sda1 1038336 217304 821032 21% /boottmpfs 182820 0 182820 0% /run/user/010.0.0.182:/data/www 10219008 3314688 6363648 35% /var/www/html[root@Web2 ~]#ls /var/www/html/index.php wp-admin wp-config-sample.php wp-links-opml.php wp-settings.phplicense.txt wp-blog-header.php wp-content wp-load.php wp-signup.phpreadme.html wp-comments-post.php wp-cron.php wp-login.php wp-trackback.phpwp-activate.php wp-config.php wp-includes wp-mail.php xmlrpc.php#Switchhosts软件10.0.0.180 www.wu.org 6.5 配置backup1234567891011121314151617181920212223242526272829303132333435363738394041[root@BACKUP ~]#vim /etc/rsyncd.confuid = rootgid = rootmax connections = 0ignore errorsexclude = lost+found/log file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidlock file = /var/run/rsyncd.lockreverse lookup = no[backup]path = /data/backup/comment = backup dirread only = noauth users = rsyncusersecrets file = /etc/rsync.pas[root@BACKUP ~]#mkdir -pv /data/backup[root@BACKUP ~]#echo &quot;rsyncuser:123456&quot; &gt; /etc/rsync.pas[root@BACKUP ~]#chmod 600 /etc/rsync.pas[root@BACKUP ~]#systemctl restart rsync.service[root@NFS ~]#tar xf sersync2.5.4_64bit_binary_stable_final.tar.gz -C /usr/local/[root@NFS ~]#cd /usr/local/[root@NFS local]#lsbin etc games GNU-Linux-x86 include lib man sbin share src[root@NFS local]#ln -s GNU-Linux-x86/ sersync[root@NFS local]#ln -s /usr/local/sersync/sersync2 /usr/local/bin/[root@NFS local]#cd sersync[root@NFS sersync]#rm -rf confxml.xml [root@NFS sersync]#rz[root@NFS sersync]#lsconfxml.xml sersync2[root@NFS sersync]#vim confxml.xml&lt;remote ip=&quot;10.0.0.183&quot; name=&quot;backup&quot;/&gt;[root@NFS sersync]#echo 123456 &gt; /etc/rsync.pas[root@NFS sersync]#chmod 600 /etc/rsync.pas 6.6 测试1234567891011121314151617181920212223[root@NFS ~]#sersync2 -dro /usr/local/sersync/confxml.xml后台执行：nohup sersync2 -dro /usr/local/sersync/confxml.xml &amp;&gt; /dev/null &amp;[root@BACKUP ~]#ls /data/backup/index.php wp-admin wp-config-sample.php wp-links-opml.php wp-settings.phplicense.txt wp-blog-header.php wp-content wp-load.php wp-signup.phpreadme.html wp-comments-post.php wp-cron.php wp-login.php wp-trackback.phpwp-activate.php wp-config.php wp-includes wp-mail.php xmlrpc.php[root@BACKUP ~]#tree /data/backup/wp-content/uploads//data/backup/wp-content/uploads/└── 2023 └── 12 └── image.png1#Windows发表新的文章[root@BACKUP ~]#tree /data/backup/wp-content/uploads//data/backup/wp-content/uploads/└── 2023 └── 12 └── image.png1 └── image.png2","categories":[{"name":"存储","slug":"存储","permalink":"https://aquapluto.github.io/categories/%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"NFS","slug":"NFS","permalink":"https://aquapluto.github.io/tags/NFS/"}]},{"title":"数据的实时同步","slug":"Linux/service-manage/data-synchronization","date":"2025-08-20T08:52:20.000Z","updated":"2025-09-09T09:05:05.967Z","comments":true,"path":"Linux/service-manage/data-synchronization/","permalink":"https://aquapluto.github.io/Linux/service-manage/data-synchronization/","excerpt":"","text":"1 数据的实时同步方式在生产环境，要将数据或文件进行实时同步，保证数据更新后其它节点能立即获得最新的数据，会需要两台主机的特定目录实现实时同步。比如，将NFS共享目录的数据文件，自动实时同步到备份服务器特定目录中 数据同步的两种方式 PULL：拉，使用定时任务的方式配合同步命令或脚本等，从指定服务器上将数据同步到本地，一般是周期性定时同步 PUSH：推，如果当前机器上的数据或文件发生更新了，立即推送到指定的节点上，可以做到实时同步 2 实现实时同步的方法2.1 inotify+rsyncinotify + rsync 方式实现数据同步，需自行编写脚本组合 inotify 和 rsync 实现 同步原理：利用监控服务（inotify），监控同步数据服务器目录中信息的变化，如果发现目录中数据产生变化，就利用rsync服务推送到备份服务器上 2.1.1 inotify服务inotify 是用于通知用户空间程序文件系统变化的服务，它通过异步的文件系统事件监控机制和事件驱动机制，而无须通过诸如cron等的轮询机制来获取事件。linux内核从2.6.13起支持 inotify，通过inotify监控文件系统中添加、删除，修改、移动等各种事件，在监听到文件系统发生变化后，会向相应的应用程序发送事件，立即让用户空间知道。项目地址 1234567891011[root@ubuntu2204 ~]#grep -i inotify /boot/config-5.15.0-52-genericCONFIG_INOTIFY_USER=y[root@centos8 ~]#grep -i inotify /boot/config-4.18.0-80.el8.x86_64CONFIG_INOTIFY_USER=y#列出下面的参数，也说明服务器内核支持inotify[root@centos8 ~]#ls -l /proc/sys/fs/inotify -rw-r--r-- 1 root root 0 Dec 7 10:10 max_queued_events-rw-r--r-- 1 root root 0 Dec 7 10:10 max_user_instances-rw-r--r-- 1 root root 0 Dec 6 05:54 max_user_watches 内核参数说明 参数名称 作用 默认值 调整场景 max_queued_events 限制单个 inotify 实例的事件队列最大长度（未处理事件的缓存上限） 16384 应用需处理高频事件（如监控频繁更新的日志目录），避免队列溢出导致事件丢失 max_user_instances 限制单个用户（UID）可创建的 inotify 实例最大数量 128 单个用户需运行多个独立监控程序（如多进程分别监控不同目录），默认数量不足时 max_user_watches 限制单个示例可监控的文件 &#x2F; 目录总数量（即 “watch” 的上限） 8192 监控大量文件（如包含数万文件的代码仓库、网站目录），避免 “监控数量超限” 错误 修改内核参数示例 1234567891011121314[root@ubuntu ~]# vim /etc/sysctl.conffs.inotify.max_queued_events=66666fs.inotify.max_user_instances=256fs.inotify.max_user_watches=100000[root@ubuntu ~]# sysctl -pfs.inotify.max_queued_events = 66666fs.inotify.max_user_instances = 256fs.inotify.max_user_watches = 100000[root@centos8 ~]#cat /proc/sys/fs/inotify/*66666256100000 2.1.2 inotify-tools工具由于 inotify 是内核中的功能模块，只能通过调用API接口的形式使用其功能，所以我们可以通过 inotify-tools 来对其进行操作 inotify-tools参考文档 安装inotify-tools：基于epel源 12345678[root@data-centos8 ~]# yum -y install inotify-tools[root@data-ubuntu2004]#apt -y install inotify-tools#主要工具/usr/bin/fsnotifywait #fsnotify监控工具，fsnotify 是 inotify 的新版本/usr/bin/fsnotifywatch #fsnotify统计工具/usr/bin/inotifywait #实时监控指定目录的所有事件，在被监控的文件或目录上等待特定事件发生(open,close,write..)/usr/bin/inotifywatch #收集被监控的文件系统使用的统计数据，指文件系统事件发生的次数统计 inotifywait 命令 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647inotifywait [ options ] file1 [ file2 ] [ file3 ] [ ... ]-m, --monitor #始终保持事件监听-d, --daemon #以守护进程方式执行，和-m相似，配合-o使用-r, --recursive #递归监控目录数据信息变化-q, --quiet #输出少量事件信息--exclude &lt;pattern&gt; #指定排除文件或目录，使用扩展的正则表达式匹配的模式实现--excludei &lt;pattern&gt; #和exclude相似，不区分大小写-o, --outfile &lt;file&gt; #打印事件到文件中，相当于标准正确输出，注意：使用绝对路径-s, --syslogOutput #发送错误到syslog相当于标准错误输出--timefmt &lt;fmt&gt; #指定时间输出格式 %Y #年份信息，包含世纪信息 %y #年份信息，不包括世纪信息 %m #显示月份，范围 01-12 %d #每月的第几天，范围是 01-31 %H #小时信息，使用 24小时制，范围 00-23 %M #分钟，范围 00-59 %S #秒，范例 0-60 例子： --timefmt &quot;%Y-%m-%d %H:%M:%S&quot; --format &lt;fmt&gt; #指定的输出格式；即实际监控输出内容 %T #输出时间格式中定义的时间格式信息，通过 --timefmt option 语法格式指定时间信息 %w #事件出现时，监控文件或目录的名称信息，相当于dirname %f #事件出现时，将显示监控目录下触发事件的文件或目录信息，否则为空，相当于basename %e #显示发生的事件信息，不同的事件默认用逗号分隔 %Xe #显示发生的事件信息，不同的事件指定用X进行分隔 例子： --format &quot;%T %w%f event: %;e&quot; --format &#x27;%T %w %f&#x27;-e #指定监听指定的事件，如果省略，表示所有事件都进行监听 create #文件或目录创建 delete #文件或目录被删除 modify #文件或目录内容被写入 attrib #文件或目录属性改变 close_write #文件或目录关闭，在写入模式打开之后关闭的 close_nowrite #文件或目录关闭，在只读模式打开之后关闭的 close #文件或目录关闭，不管读或是写模式 open #文件或目录被打开 lsdir #浏览目录内容 moved_to #文件或目录被移动到监控的目录中 moved_from #文件或目录从监控的目录中被移动 move #文件或目录不管移动到或是移出监控目录都触发事件 access #文件或目录内容被读取 delete_self #文件或目录被删除，目录本身被删除 unmount #取消挂载 例子： -e create,delete,moved_to,close_write,attrib，moved_from 范例：只监控一个事件 1234567891011121314151617181920[root@ubuntu ~]# tree /data//data/├── dir1└── dir22 directories, 0 files#开始监控[root@ubuntu ~]# inotifywait /data/Setting up watches.Watches established.#在另一个终端中执行[root@ubuntu ~]# ls /data/dir1 dir2#提示open事件并退出[root@ubuntu ~]# inotifywait /data/Setting up watches.Watches established./data/ OPEN,ISDIR 范例：持续监控 123456789101112131415161718192021222324252627282930313233[root@ubuntu ~]# inotifywait -m /data/Setting up watches.Watches established.#另一个终端操作[root@ubuntu ~]# ls /datadir1 dir2[root@ubuntu ~]# ls /data/dir1[root@ubuntu ~]# touch /data/test.txt#查看监控，一条命令可能有多个事件[root@ubuntu ~]# inotifywait -m /data/Setting up watches.Watches established./data/ OPEN,ISDIR/data/ ACCESS,ISDIR/data/ ACCESS,ISDIR/data/ CLOSE_NOWRITE,CLOSE,ISDIR/data/ OPEN,ISDIR dir1/data/ ACCESS,ISDIR dir1/data/ ACCESS,ISDIR dir1/data/ CLOSE_NOWRITE,CLOSE,ISDIR dir1/data/ CREATE test.txt/data/ OPEN test.txt/data/ ATTRIB test.txt #ATTRIB表示属性更改/data/ CLOSE_WRITE,CLOSE test.txt #CLOSE_WRITE表示有文件内容发生变化 #递归监控inotifywait -mrq /data/www --exclude=&quot;.*\\.swx|\\.swp&quot;/data/www/ OPEN f1.txt/data/www/ ACCESS f1.txt/data/www/ CLOSE_NOWRITE,CLOSE f1.txt 范例：持续后台监控，并指定输出格式 123456[root@ubuntu ~]# inotifywait -drq /data/ -o inotify.log --timefmt &quot;%Y-%m-%d %H:%M:%S&quot; --format &quot;%T %w%f event: %e&quot;[root@ubuntu ~]# cat inotify.log2023-07-11 14:45:53 /data/ event: OPEN,ISDIR2023-07-11 14:45:53 /data/ event: ACCESS,ISDIR2023-07-11 14:45:53 /data/ event: CLOSE_NOWRITE,CLOSE,ISDIR 范例：持续前台监控特定事件，指定输出格式 1234567891011121314[root@ubuntu ~]# ls /data/dir1 dir2 test.txt[root@ubuntu ~]# touch /data/test.log[root@ubuntu ~]# touch /data/test.log[root@ubuntu ~]# rm -f /data/test.log[root@ubuntu ~]# inotifywait -mrq /data/ --timefmt &quot;%F %H:%M:%S&quot; --format &quot;%T %w%f event:%;e&quot; -e create,delete,moved_to,close_write,attrib2023-07-11 14:58:03 /data/test.log event:CREATE2023-07-11 14:58:03 /data/test.log event:ATTRIB2023-07-11 14:58:03 /data/test.log event:CLOSE_WRITE;CLOSE2023-07-11 14:58:06 /data/test.log event:ATTRIB2023-07-11 14:58:06 /data/test.log event:CLOSE_WRITE;CLOSE2023-07-11 14:58:09 /data/test.log event:DELETE 范例：将结果输出到文件 123456789101112[root@ubuntu ~]# inotifywait -m -r /data/ -o inotify.txtSetting up watches. Beware: since -r was given, this may take a while!Watches established.[root@ubuntu ~]# ls /data/dir1 dir2 test.txt#查看结果文件[root@ubuntu ~]# cat inotify.txt/data/ OPEN,ISDIR/data/ ACCESS,ISDIR/data/ CLOSE_NOWRITE,CLOSE,ISDIR 范例：从文件中读取要监控的内容 123456[root@ubuntu ~]# cat a.txt/data/[root@ubuntu ~]# inotifywait -rm --fromfile a.txtSetting up watches. Beware: since -r was given, this may take a while!Watches established. 2.1.3 rsync服务rsync 常作为 Linux 系统下的数据镜像备份工具，支持本地复制、与其他 SSH 主机的远程同步以及增量备份；配合任务计划可实现定时或间隔同步，配合 inotify 或 sersync 则能实现触发式的实时数据同步。官方网站 软件包：rsync（Ubuntu20.04），rsync-daemon（CentOS 8） 服务文件：/usr/lib/systemd/system/rsyncd.service 配置文件：/etc/rsyncd.conf 端口：873&#x2F;tcp 123456789101112131415161718192021222324252627282930313233343536373839404142#Ubuntu默认提供了service文件[root@ubuntu2204 ~]#systemctl cat rsync.service# /lib/systemd/system/rsync.service[Unit]Description=fast remote file copy program daemonConditionPathExists=/etc/rsyncd.confAfter=network.targetDocumentation=man:rsync(1) man:rsyncd.conf(5)[Service]ExecStart=/usr/bin/rsync --daemon --no-detachRestartSec=1# Citing README.md:## [...] Using ssh is recommended for its security features.## Alternatively, rsync can run in `daemon&#x27; mode, listening on a socket.# This is generally used for public file distribution, [...]## So let&#x27;s assume some extra security is more than welcome here. We do full# system protection (which makes /usr, /boot, &amp; /etc read-only) and hide# devices. To override these defaults, it&#x27;s best to do so in the drop-in# directory, often done via `systemctl edit rsync.service`. The file needs# just the bare minimum of the right [heading] and override values.# See systemd.unit(5) and search for &quot;drop-in&quot; for full details.ProtectSystem=full#ProtectHome=on|off|read-onlyPrivateDevices=onNoNewPrivileges=on[Install]WantedBy=multi-user.target#红帽系统需要安装rsync-daemon包提供service文件[root@backup-centos8 ~]#dnf -y install rsync-daemon[root@backup-centos8 ~]#rpm -ql rsync-daemon/etc/rsyncd.conf/etc/sysconfig/rsyncd/usr/lib/systemd/system/rsyncd.service/usr/lib/systemd/system/rsyncd.socket/usr/lib/systemd/system/rsyncd@.service/usr/share/man/man5/rsyncd.conf.5.gz 2.1.3.1 rsync命令rsync 有三种工作方式 本地文件系统上实现同步 本地主机使用远程ssh和远程主机通信 本地主机通过网络套接字连接远程主机上的 rsync daemon 前两者的本质是通过本地或远程ssh，而第3种方式则是让远程主机上运行rsyncd服务，使其监听在一个端口上，等待客户端的连接 123456789101112#本地文件同步：代替cprsync [OPTION]... SRC [SRC]... DEST#远程主机需要开启ssh协议rsync [OPTION]... [USER@]HOST:SRC [DEST] #PULL 拉，将远程主机上的文件拉到本地rsync [OPTION]... SRC [SRC]... [USER@]HOST:DEST #PUSH 推，将本地文件推送到远程主机上#远程主机需要开启rsync协议，需要搭建rsync服务器rsync [OPTION]... [USER@]HOST::SRC [DEST] #PULL 拉，将远程主机上的文件拉到本地rsync [OPTION]... rsync://[USER@]HOST[:PORT]/SRC [DEST] #PULL 拉，将远程主机上的文件拉到本地rsync [OPTION]... SRC [SRC]... [USER@]HOST::DEST #PUSH 推，将本地文件推送到远程主机上rsync [OPTION]... SRC [SRC]... rsync://[USER@]HOST[:PORT]/DEST #PUSH 推，将本地文件推送到远程主机上 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 详细输出与测试选项-v # 显示rsync过程中详细信息。可使用&quot;-vvvv&quot;获取更详细信息。-P # 显示文件传输进度信息（等价于 --partial --progress）。-n --dry-run # 仅测试传输，不实际执行，常用于查看rsync行为。# 归档与递归传输选项-a --archive # 归档模式，递归传输并保持文件属性，等同于 -rtopgDl。-r --recursive # 递归进入目录传输文件。# 文件属性保持选项-t --times # 保持文件的修改时间（mtime）。-o --owner # 保持文件属主（owner）属性。-g --group # 保持文件属组（group）属性。-p --perms # 保持文件权限（perms，不包括特殊权限）。-D # 等价于 --device --specials，复制设备文件和特殊文件。-l --links # 复制软链接本身，而非其指向的目标文件。# 压缩与传输模式-z # 传输时压缩数据，提高网络效率。-W --whole-file # 禁用增量传输，始终全量传输。适合网络带宽高于磁盘I/O的场景。# 路径与结构控制-R --relative # 使用相对路径传输，保留命令行中的完整路径结构。-d --dirs # 仅拷贝目录本身，不递归内容（与-r相反）。# 文件选择与过滤--size-only # 仅根据文件大小判断是否需要传输（忽略mtime）。-u --update # 仅当源文件比目标文件新时才传输（基于mtime）。--max-size=SIZE # 限制传输的最大文件大小（如 --max-size=1.5m）。--min-size=SIZE # 限制传输的最小文件大小。--exclude=PATTERN # 排除匹配模式的文件或目录不传输。# 同步与删除策略--delete # 以源端为准，删除目标端多余文件（执行在接收端，排除规则后）。--existing # 只更新目标端已存在的文件，不创建新文件。--ignore-existing # 只传输目标端不存在的文件。--remove-source-files # 成功传输后删除源端文件（用于移动语义）。# 备份相关-b --backup # 对已存在的目标文件进行备份，默认后缀为 ~。--backup-dir=DIR # 指定备份文件存放目录。# 网络与连接配置-e REMOTE_SHELL # 指定远程shell程序（如ssh），用于远程传输。--port=PORT # 指定连接rsync daemon的端口（默认873）。--password-file=FILE # 指定rsync daemon认证的密码文件（非ssh密码）。--bwlimit=RATE # 限制传输带宽（单位MB/s，默认）。 2.1.3.2 实现 rsync daemon 服务开启服务方式 1234rsync --daemon服务方式（守护进程）：持续运行，后台执行，一般都有对应的配置文件命令方式：一次性，前台执行，ls -l -a 范例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#在备份服务器启动 rsync 进程，启动失败，因为 /etc/rsyncd.conf 配置文件不存在[root@backup-centos8 ~]#rsync --daemonFailed to parse config file: /etc/rsyncd.conf#创建空配置文件[root@backup-centos8 ~]#touch /etc/rsyncd.conf[root@backup-centos8 ~]#rsync --daemon#监听 873 端口[root@backup-centos8 ~]#ss -ntlp|grep rsyncLISTEN 0 5 0.0.0.0:873 0.0.0.0:* users:((&quot;rsync&quot;,pid=2921,fd=4)) LISTEN 0 5 [::]:873 [::]:* users:((&quot;rsync&quot;,pid=2921,fd=5)) #修改服务端配置，指定共享目录[root@backup-centos8 ~]#cat /etc/rsyncd.conf[backup] #对外给用户看到的共享名path = /data/backup/ #定义一个文件夹实现共享，实际的共享名 read only = no #指定可读写,默认只读#Ubuntu要重启，centos不用，直接生效[root@ubuntu ~]# systemctl restart rsync.service#查看rsync服务器的模块名称[root@data-centos8 ~]#rsync rsync://10.0.0.18backup#两种写法都可以， :: 表示走rsync协议[root@data-centos8 ~]#rsync 10.0.0.18::backup##访问rsync服务器的共享目录，看到有文件[root@data-centos8 ~]#rsync rsync://10.0.0.18/backupdrwxr-xr-x 19 2023/12/11 17:23:49 .-rw-r--r-- 0 2023/12/11 17:23:49 a.txt#拉[root@data-centos8 ~]#rsync 10.0.0.18::backup/* .[root@data-centos8 ~]#rsync rsync://10.0.0.18/backup/* .[root@data-centos8 ~]#lsanaconda-ks.cfg a.txt #推#传输文件到远程失败，此处的 root 是指rsync服务的用户，当前服务端并没有配置此信息，默认会被映射成 nobody#注意：Ubuntu的nobody的组为nogroup，centos8nobody的组为nobody[root@data-centos8 ~]#rsync /etc/hosts 10.0.0.18::backup/hosts.txtrsync: mkstemp &quot;/.hosts.txt.84dX4G&quot; (in backup) failed: Permission denied (13)rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1189) [sender=3.1.3]#指定目录给nobody权限，默认用户以nobody访问此目录[root@backup-centos8 ~]#setfacl -m u:nobody:rwx /data/backup/#现在使用的是匿名连接[root@data-centos8 ~]#rsync /etc/networks root@10.0.0.18::backup #默认所有用户都映射为nobody用户[root@data-centos8 ~]#rsync /etc/issue wang@10.0.0.18::backup #wang用户在服务端并不存在[root@data-centos8 ~]#rsync /etc/passwd 10.0.0.18::backup[root@data-centos8 ~]#rsync /etc/shells rsync://root@10.0.0.18/backup#服务端查看，属主属组都是 nobody[root@backup-centos8 backup]#ll /data/backup/total 16-rw-r--r-- 1 root root 0 Dec 11 17:23 a.txt-rw-r--r-- 1 nobody nobody 23 Dec 11 17:53 issue-rw-r--r-- 1 nobody nobody 58 Dec 11 17:52 networks-rw-r--r-- 1 nobody nobody 1522 Dec 11 17:53 passwd-rw-r--r-- 1 nobody nobody 44 Dec 11 17:53 shells 基于上述示例，添加指定用户（uid, gid）、用户认证后可访问（auth users, secrets file）功能 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#创建rsync服务器的配置文件，指定映射账号，指定日志文件，指定远程连接用户名和密码，禁用匿名连接[root@centos8 ~]#vi /etc/rsyncd.confuid = root #指定以哪个用户身份访问共享目录，默认为nobody,注意:共享目录需要给此用户权限,否则无法访问gid = root #指定以哪个组身份访问共享目录，默认为nobody,Ubuntu中为nogroup#port = 874 #可指定非标准端口,默认873/tcp#use chroot = nomax connections = 0 #最大并发连接数，0表示不限制ignore errors #有错误可以跳过exclude = lost+found/ #排除lost+found目录log file = /var/log/rsyncd.log #日志文件pid file = /var/run/rsyncd.pid #pid文件，存主进程编号lock file = /var/run/rsyncd.lock #锁文件，避免这个服务（多个进程）重复启动reverse lookup = no #反向解析 #hosts allow = 10.0.0.0/24[backup] #每个模块名对应一个不同的path目录，如果同名后面模块生效path = /data/backup/ comment = backup dir #描述read only = no #默认是yes,即只读auth users = rsyncuser #验证用户信息，必须以rsyncuser用户，并且以/etc/rsync.pas里存的密码成对验证才能访问，默认anonymous可以访问rsync服务器secrets file = /etc/rsync.pas#最后的配置文件内容[root@ubuntu2204 ~]#cat /etc/rsyncd.confuid = rootgid = rootmax connections = 0ignore errorsexclude = lost+found/log file = /var/log/rsyncd.logpid file = /var/run/rsyncd.pidlock file = /var/run/rsyncd.lockreverse lookup = no[backup]path = /data/backup/comment = backup dirread only = noauth users = rsyncusersecrets file = /etc/rsync.pas#服务器端准备目录[root@backup-centos8 ~]#mkdir -pv /data/backup#服务器端生成验证文件[root@backup-centos8 ~]#echo &quot;rsyncuser:123456&quot; &gt; /etc/rsync.pas[root@backup-centos8 ~]#chmod 600 /etc/rsync.pas#服务器端启动rsync服务[root@backup-centos8 ~]#rsync --daemon #可加入/etc/rc.d/rc.local实现开机启动[root@backup-centos8 ~]#systemctl start rsyncd #CentOS 7 以上版本[root@ubuntu ~]# systemctl restart rsync.service#客户端配置密码文件#也可将密码赋值给环境变量RSYNC_PASSWORD变量,但不安全#export RSYNC_PASSWORD=123456 [root@data-centos8 ~]#echo &quot;123456&quot; &gt; /etc/rsync.pas[root@data-centos8 ~]#chmod 600 /etc/rsync.pas #此为必要项,权限必须修改#查看远程rsync服务器的模块信息[root@data-server ~]#rsync rsync://rsync服务器IPbackup backup dir#交互式验证查看具体模块内的文件[root@data-server ~]#rsync rsync://rsyncuser@rsync服务器IP/backupPassword:#非交互式查看共享目录[root@data-server ~]#rsync --password-file=/etc/rsync.pas rsync://rsyncuser@rsync服务器IP/backup#客户端测试同步数据[root@data-centos8 ~]#rsync -avz --delete --password-file=/etc/rsync.pas /data/www/ rsyncuser@rsync服务器IP::backup[root@data-centos8 ~]#rsync -avz --delete --password-file=/etc/rsync.pas rsyncuser@rsync服务器IP::backup /data/www/ 2.1.4 inotify+rsync通过脚本实现实时数据同步 在运行这个脚本之前，需要先确保 rsync 服务器的/data/www/目录和目标服务器（rsync 服务器的backup模块对应目录）中的初始数据是完全一致的；因为这个脚本的作用是在初始数据同步的基础上，监控后续源目录的变化（如文件创建、删除、修改等），并自动将变化同步到目标主机，保持两者实时一致。 12345678910111213141516171819202122232425262728293031[root@data-centos8 ~]#vim inotify_rsync.sh#!/bin/bash# 源目录（注意末尾的斜杠，表示同步目录内内容）SRC=&#x27;/data/www/&#x27;# 目标地址（rsync服务器的备份模块）DEST=&#x27;rsyncuser@rsync服务器IP::backup&#x27;# Ubuntu 20.04不支持--password-file参数，通过环境变量传递密码export RSYNC_PASSWORD=123456# 安装依赖（可选，根据实际环境开启）# rpm -q inotify-tools &amp;&gt; /dev/null || yum -y install inotify-tools# rpm -q rsync &amp;&gt; /dev/null || yum -y install rsync# 监控源目录变化，触发同步inotifywait -mrq \\ --exclude=&quot;.*\\.swp&quot; \\ # 排除临时交换文件（如vim的.swp文件） --timefmt &#x27;%Y-%m-%d %H:%M:%S&#x27; \\ # 时间格式 --format &#x27;%T %w %f&#x27; \\ # 输出格式：时间 目录 文件 -e create,delete,moved_from,moved_to,close_write,attrib \\ # 监控的事件类型 $&#123;SRC&#125; | while read DATE TIME DIR FILE; do FILEPATH=$&#123;DIR&#125;$&#123;FILE&#125; # 同步源目录到目标，删除目标多余文件，并记录日志 rsync -az --delete $SRC $DEST &amp;&amp; \\ echo &quot;At $&#123;TIME&#125; on $&#123;DATE&#125;, file $FILEPATH was backuped up via rsync&quot; &gt;&gt; /var/log/changelist.log # 若系统支持--password-file，可替换为下面的命令 # rsync -az --delete --password-file=/etc/rsync.pas $SRC $DEST &amp;&amp; \\ # echo &quot;At $&#123;TIME&#125; on $&#123;DATE&#125;, file $FILEPATH was backuped up via rsync&quot; &gt;&gt; /var/log/changelist.logdone 12#查看文件传输日志[root@data-centos8 ~]#tail -f /var/log/changelist.log 后台执行 121 开启screen窗口2 nohup bash inotify_rsync.sh &amp;&gt;/dev/dull &amp; 2.2 sersync+rsync3.4.1 sersync 介绍inotify最大的不足在于事件重复 &#x2F; 冗余和无关事件干扰 产生重复事件：当监控的目录中有5个文件时，删除目录时会产生6个监控事件（1 个目录被删除事件和5 个目录内每个文件被删除事件），从而导致重复调用rsync命令。但实际上，删除目录是一个整体操作，只需要一次 rsync 就能同步这个变化，即删除目标端的对应目录就行，多余的 5 次 rsync 调用都是重复且无意义的，会浪费系统资源 无关事件干扰：用 vim 编辑文件时，会先创建一个临时文件，而inotify会监控到临时文件的事件，但这个事件相对于rsync来说是不应该被监控的，因为vim保存文件后会删除这个临时文件 所以sersync是基于inotify的基础上开发的，同样用于监控，克服了inotify的缺点，功能更加强大，特别适用于对实时性要求较高的环境，并且还可以进行二次开发。sersync项目地址；sersync下载地址 对linux系统文件系统产生的临时文件和重复的文件操作进行过滤，在结合rsync同步的时候节省了运行时耗和网络资源，因此更快 配置简单，其中提供了静态编译好的二进制文件和xml配置文件，直接使用即可 使用多线程进行同步，尤其在同步较大文件时，能够保证多个服务器实时保持同步状态 有出错处理机制，通过失败队列对出错的文件重新同步，如果仍旧失败，则按设定时长对同步失败的文件重新同步 不仅可以实现实时同步，另外还自带crontab功能，只需在xml配置文件中开启，即也可以按要求隔一段时间整体同步一次，而无需再额外配置crontab功能 同步原理 首先在Master服务器上面安装sersync服务及要确保有rsync命令，在Slave服务器上面配置rsync server 在Master服务器上开启sersync服务，sersync负责监控配置路径中文件系统事件的变化 Master服务器上面的sersync服务调用rsync命令把更新的文件推送到目标服务器Slave上面 3.4.2 部署sersync123456789101112131415#下载[root@ubuntu ~]# wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/sersync/sersync2.5.4_64bit_binary_stable_final.tar.gz#解压[root@ubuntu ~]# tar xf sersync2.5.4_64bit_binary_stable_final.tar.gz[root@ubuntu ~]# lsGNU-Linux-x86 sersync2.5.4_64bit_binary_stable_final.tar.gz snap[root@ubuntu ~]# ls GNU-Linux-x86/confxml.xml sersync2#移动[root@ubuntu ~]# mv GNU-Linux-x86/ /usr/local/sersync[root@ubuntu ~]# ls /usr/local/sersync/confxml.xml sersync2 sersync2 命令用法和参数 123456789101112131415[root@ubuntu sersync]# ./sersync2 -hset the system paramexecute：echo 50000000 &gt; /proc/sys/fs/inotify/max_user_watchesexecute：echo 327679 &gt; /proc/sys/fs/inotify/max_queued_eventsparse the command param_______________________________________________________参数-d：启用守护进程模式参数-r：在监控前，将监控目录与远程主机用rsync命令推送一遍参数-n：指定开启守护线程的数量，默认为10个参数-o：指定配置文件，默认使用confxml.xml文件参数-m：单独启用其他模块，不加-m参数，则默认执行同步程序 -m refreshCDN 开启刷新CDN模块 -m socket 开启socket模块 -m http 开启http模块________________________________________________________________ 3.4.3 基于rsync daemon实现数据同步123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147#在数据服务器上下载sersync，并拷贝至相应的目录，设置PATH变量[root@data-centos8 ~]#wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/sersync/sersync2.5.4_64bit_binary_stable_final.tar.gz[root@data-centos8 ~]#tar xf sersync2.5.4_64bit_binary_stable_final.tar.gz[root@data-centos8 ~]#cp -a GNU-Linux-x86 /usr/local/sersync[root@data-centos8 ~]#echo &#x27;PATH=/usr/local/sersync:$PATH&#x27; &gt;/etc/profile.d/sersync.sh[root@data-centos8 ~]#source /etc/profile.d/sersync.sh#sersync目录只有两个文件：一个是二进制程序文件，一个是xml格式的配置文件[root@data-centos8 ~]#ls /usr/local/sersync/confxml.xml sersync2#确认安装rsync客户端工具[root@data-centos8 ~]#rpm -q rsync &amp;&gt; /dev/null || dnf -y install rsync#备份sersync配置文件[root@data-centos8 ~]#cp /usr/local/sersync/confxml.xml&#123;,.bak&#125;#修改sersync配置文件[root@data-centos8 ~]#vim /usr/local/sersync/confxml.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&gt;&lt;head version=&quot;2.5&quot;&gt; &lt;host hostip=&quot;localhost&quot; port=&quot;8008&quot;&gt;&lt;/host&gt; &lt;debug start=&quot;false&quot;/&gt; #是否开启调试模式 &lt;fileSystem xfs=&quot;false&quot;/&gt; &lt;filter start=&quot;false&quot;&gt; #不开启文件过滤功能，当为true时,以下类型的文件将不同步 &lt;exclude expression=&quot;(.*)\\.svn&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;(.*)\\.gz&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;^info/*&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;^static/*&quot;&gt;&lt;/exclude&gt; &lt;/filter&gt; &lt;inotify&gt; #监控事件，默认监控delete/close_write/moved_from/moved_to/create folder &lt;delete start=&quot;true&quot;/&gt; &lt;createFolder start=&quot;true&quot;/&gt; &lt;createFile start=&quot;false&quot;/&gt; &lt;closeWrite start=&quot;true&quot;/&gt; &lt;moveFrom start=&quot;true&quot;/&gt; &lt;moveTo start=&quot;true&quot;/&gt; &lt;attrib start=&quot;true&quot;/&gt; #修改此行为true，文件属性变化后也会同步 &lt;modify start=&quot;false&quot;/&gt; &lt;/inotify&gt; &lt;sersync&gt; #rsync命令的配置段 &lt;localpath watch=&quot;/data/www&quot;&gt; #修改此行,需要同步的源目录或文件，建议同步目录 &lt;remote ip=&quot;备份服务器IP&quot; name=&quot;backup&quot;/&gt; #修改此行,指定远程主机地址和目录，即备份服务器地址和rsyncdaemon的模块名，如果下面开启了ssh start，此时name为远程shell方式运行时的目标目录 &lt;!--&lt;remote ip=&quot;192.168.8.39&quot; name=&quot;tongbu&quot;/&gt;--&gt; &lt;!--&lt;remote ip=&quot;192.168.8.40&quot; name=&quot;tongbu&quot;/&gt;--&gt; &lt;/localpath&gt; &lt;rsync&gt; &lt;commonParams params=&quot;-artuz&quot;/&gt; #指定rsync选项 &lt;auth start=&quot;true&quot; users=&quot;rsyncuser&quot; passwordfile=&quot;/etc/rsync.pas&quot;/&gt; #修改此行为true,指定备份服务器的rsync配置的用户和密码文件 &lt;userDefinedPort start=&quot;false&quot; port=&quot;874&quot;/&gt;&lt;!-- port=874 --&gt; #指定rsync的非标准端口号 &lt;timeout start=&quot;false&quot; time=&quot;100&quot;/&gt;&lt;!-- timeout=100 --&gt; &lt;ssh start=&quot;false&quot;/&gt; #默认使用rsync daemon运行rsync命令,true为使用远程shell模式 &lt;/rsync&gt; &lt;failLog path=&quot;/tmp/rsync_fail_log.sh&quot; timeToExecute=&quot;60&quot;/&gt;&lt;!--default every 60mins execute once--&gt; #错误重传及日志文件路径 &lt;crontab start=&quot;false&quot; schedule=&quot;600&quot;&gt;&lt;!--600mins--&gt; #不开启crontab功能 &lt;crontabfilter start=&quot;false&quot;&gt; #不开启crontab定时传输的筛选功能 &lt;exclude expression=&quot;*.php&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;info/*&quot;&gt;&lt;/exclude&gt; &lt;/crontabfilter&gt; &lt;/crontab&gt; &lt;plugin start=&quot;false&quot; name=&quot;command&quot;/&gt; &lt;/sersync&gt;#####################################以下行不需要修改#################################### &lt;plugin name=&quot;command&quot;&gt; &lt;param prefix=&quot;/bin/sh&quot; suffix=&quot;&quot; ignoreError=&quot;true&quot;/&gt; &lt;!--prefix /opt/tongbu/mmm.sh suffix--&gt; &lt;filter start=&quot;false&quot;&gt; &lt;include expression=&quot;(.*)\\.php&quot;/&gt; &lt;include expression=&quot;(.*)\\.sh&quot;/&gt; &lt;/filter&gt; &lt;/plugin&gt; &lt;plugin name=&quot;socket&quot;&gt; &lt;localpath watch=&quot;/opt/tongbu&quot;&gt; &lt;deshost ip=&quot;192.168.138.20&quot; port=&quot;8009&quot;/&gt; &lt;/localpath&gt; &lt;/plugin&gt; &lt;plugin name=&quot;refreshCDN&quot;&gt; &lt;localpath watch=&quot;/data0/htdocs/cms.xoyo.com/site/&quot;&gt; &lt;cdninfo domainname=&quot;ccms.chinacache.com&quot; port=&quot;80&quot; username=&quot;xxxx&quot;passwd=&quot;xxxx&quot;/&gt; &lt;sendurl base=&quot;http://pic.xoyo.com/cms&quot;/&gt; &lt;regexurl regex=&quot;false&quot; match=&quot;cms.xoyo.com/site([/a-zA-Z0-9]*).xoyo.com/images&quot;/&gt; &lt;/localpath&gt; &lt;/plugin&gt;&lt;/head&gt;#创建连接rsynd服务器的用户密码文件,并必须修改权限[root@data-centos8 ~]#echo 123456 &gt; /etc/rsync.pas[root@data-centos8 ~]#chmod 600 /etc/rsync.pas#以后台方式执行同步[root@data-centos8 ~]#sersync2 -dro /usr/local/sersync/confxml.xmlset the system paramexecute：echo 50000000 &gt; /proc/sys/fs/inotify/max_user_watchesexecute：echo 327679 &gt; /proc/sys/fs/inotify/max_queued_eventsparse the command paramoption: -d run as a daemonoption: -r rsync all the local files to the remote servers before the sersyncworkoption: -o config xml name： /usr/local/sersync/confxml.xmldaemon thread num: 10parse xml config filehost ip : localhost host port: 8008daemon start，sersync run behind the consoleuse rsync password-file :user is rsyncuserpasswordfile is /etc/rsync.pasconfig xml parse successplease set /etc/rsyncd.conf max connections=0 Manuallysersync working thread 12 = 1(primary thread) + 1(fail retry thread) +10(daemon sub threads)Max threads numbers is: 22 = 12(Thread pool nums) + 10(Sub threads)please according your cpu ，use -n param to adjust the cpu rate------------------------------------------rsync the directory recursivly to the remote servers onceworking please wait...#如果同步失败,可以手动执行下面命令,观察过程[root@data-centos8 ~]# cd /data/www &amp;&amp; rsync -artuz -R --delete ./ rsyncuser@backup-server::backup --password-file=/etc/rsync.pas &gt;/dev/null 2&gt;&amp;1run the sersync:watch path is: /data/www________________________________________________________________ #sersync支持多实例，也即监控多个目录时，只需分别配置不同配置文件，然后使用sersync2指定对应配置文件运行[root@data-centos8 ~]#sersync2 -rd -o /etc/sersync.d/nginx.xml 3.4.3 基于远程ssh实现数据同步1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#不需要配置rsync daemon,只需要配置基于key验证的ssh即可[root@data-centos8 ~]#ssh-keygen[root@data-centos8 ~]#ssh-copy-id backup-server#下载sersync，并拷贝至相应的目录，设置PATH变量同5.5.2#修改sersync配置文件[root@data-centos8 ~]#cat /usr/local/sersync/confxml.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;ISO-8859-1&quot;?&gt;&lt;head version=&quot;2.5&quot;&gt; &lt;host hostip=&quot;localhost&quot; port=&quot;8008&quot;&gt;&lt;/host&gt; &lt;debug start=&quot;false&quot;/&gt; &lt;fileSystem xfs=&quot;false&quot;/&gt; &lt;filter start=&quot;false&quot;&gt; &lt;exclude expression=&quot;(.*)\\.svn&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;(.*)\\.gz&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;^info/*&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;^static/*&quot;&gt;&lt;/exclude&gt; &lt;/filter&gt; &lt;inotify&gt; &lt;delete start=&quot;true&quot;/&gt; &lt;createFolder start=&quot;true&quot;/&gt; &lt;createFile start=&quot;false&quot;/&gt; &lt;closeWrite start=&quot;true&quot;/&gt; &lt;moveFrom start=&quot;true&quot;/&gt; &lt;moveTo start=&quot;true&quot;/&gt; &lt;attrib start=&quot;true&quot;/&gt; #修改此行为true &lt;modify start=&quot;false&quot;/&gt; &lt;/inotify&gt; &lt;sersync&gt; &lt;localpath watch=&quot;/data/www&quot;&gt; #修改此行,指定源数据目录 &lt;remote ip=&quot;备份服务器IP&quot; name=&quot;/data/backup&quot;/&gt; #修改此行指定备份服务器地址和备份目标目录 &lt;!--&lt;remote ip=&quot;192.168.8.39&quot; name=&quot;tongbu&quot;/&gt;--&gt; &lt;!--&lt;remote ip=&quot;192.168.8.40&quot; name=&quot;tongbu&quot;/&gt;--&gt; &lt;/localpath&gt; &lt;rsync&gt; &lt;commonParams params=&quot;-artuz&quot;/&gt; &lt;auth start=&quot;false&quot; users=&quot;root&quot; passwordfile=&quot;/etc/rsync.pas&quot;/&gt; #必须修改此行,不启用认证start=false &lt;userDefinedPort start=&quot;false&quot; port=&quot;874&quot;/&gt;&lt;!-- port=874 --&gt; &lt;timeout start=&quot;false&quot; time=&quot;100&quot;/&gt;&lt;!-- timeout=100 --&gt; &lt;ssh start=&quot;true&quot;/&gt; #修改此行为true,使用远程shell方式的rsync连接方式，无需在目标主机上配置启动rsync daemon服务 #####################################以下行不需要修改#################################### &lt;/rsync&gt; &lt;failLog path=&quot;/tmp/rsync_fail_log.sh&quot; timeToExecute=&quot;60&quot;/&gt;&lt;!--default every 60mins execute once--&gt; &lt;crontab start=&quot;false&quot; schedule=&quot;600&quot;&gt;&lt;!--600mins--&gt; &lt;crontabfilter start=&quot;false&quot;&gt; &lt;exclude expression=&quot;*.php&quot;&gt;&lt;/exclude&gt; &lt;exclude expression=&quot;info/*&quot;&gt;&lt;/exclude&gt; &lt;/crontabfilter&gt; &lt;/crontab&gt; &lt;plugin start=&quot;false&quot; name=&quot;command&quot;/&gt; &lt;/sersync&gt;#中间的行可以删除&lt;/head&gt;[root@data-centos8 ~]#sersync2 -dro /usr/local/sersync/confxml.xmlset the system paramexecute：echo 50000000 &gt; /proc/sys/fs/inotify/max_user_watchesexecute：echo 327679 &gt; /proc/sys/fs/inotify/max_queued_eventsparse the command paramoption: -d run as a daemonoption: -r rsync all the local files to the remote servers before the sersyncworkoption: -o config xml name： /apps/sersync/confxml.xmldaemon thread num: 10parse xml config filehost ip : localhost host port: 8008daemon start，sersync run behind the consoleconfig xml parse successplease set /etc/rsyncd.conf max connections=0 Manuallysersync working thread 12 = 1(primary thread) + 1(fail retry thread) +10(daemon sub threads)Max threads numbers is: 22 = 12(Thread pool nums) + 10(Sub threads)please according your cpu ，use -n param to adjust the cpu rate------------------------------------------rsync the directory recursivly to the remote servers onceworking please wait...execute command: cd /data/www &amp;&amp; rsync -auz -R --delete ./ -e ssh10.0.0.18:/data/backup &gt;/dev/null 2&gt;&amp;1run the sersync:watch path is: /data/www","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"日志服务","slug":"Linux/service-manage/log","date":"2025-08-20T08:52:09.000Z","updated":"2025-09-10T02:21:41.954Z","comments":true,"path":"Linux/service-manage/log/","permalink":"https://aquapluto.github.io/Linux/service-manage/log/","excerpt":"","text":"1 系统日志管理1.1 系统日志介绍在现实生活中，记录日志非常重要﹐比如:银行转账时会有转账记录﹔飞机飞行过程中的黑盒子（飞行数据记录器）记录着飞机的飞行过程. 那么将系统和应用发生的事件记录至日志中，也很意义,常可以助于排错和分析使用 日志记录的内容包括： 历史事件：时间，地点，人物，事件 日志级别：事件的关键性程度，Loglevel 1.1.1 sysklogd系统日志服务CentOS 5之前版本采用的日志管理系统服务 klogd: linux kernel 记录内核日志 syslogd: system application 记录应用日志 事件记录格式： 日期时间 主机 进程[pid]: 事件内容 C&#x2F;S架构：通过TCP或UDP协议的服务完成日志记录传送，将分布在不同主机的日志实现集中管理 1.1.2 rsyslog系统日志服务rsyslog是CentOS 6以后版本的系统管理服务，主要用于单机日志管理，它提供了高性能，出色的安全性和模块化设计。 尽管rsyslog最初是常规的syslogd，但已发展成为一种瑞士军刀式的记录工具，能够接受来自各种来源的输入，并将其转换，然后输出到不同的目的地。当应用有限的处理时，RSYSLOG每秒可以将超过一百万的消息传递到本地目的地。 即使在远程的目的地和更精细的处理中，性能通常也被认为是“惊人的”。官方网站 rsyslog 特性 多线程 UDP, TCP, SSL, TLS, RELP MySQL, PGSQL, Oracle实现日志存储 强大的过滤器，可实现过滤记录日志信息中任意部分 自定义输出格式 适用于企业级中继链 rsyslog会默认安装 1234567891011[root@rocky ~]# rpm -q rsyslogrsyslog-8.2102.0-7.el8.x86_64[root@ubuntu ~]# dpkg -l rsyslogDesired=Unknown/Install/Remove/Purge/Hold| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)||/ Name Version Architecture Description+++-==============-===================-============-=========================================ii rsyslog 8.2112.0-2ubuntu2.2 amd64 reliable system and kernel logging daemon 1.2 Rsyslog管理1.2.1 系统日志术语facility：设施，从功能或程序上对日志进行归类 在一台主机上会同时运行多个服务和软件，每个服务或软件都有可能会产生大量的日志，如果每个服务或软件产生的日志都独立存放管理，那文件数量就太多了，如果都放到一个文件中，似乎也不是很合适，所以 syslog 将日志进行了分类，相同类型的日志放一个文件，这样便于管理 123456789101112131415161718#syslog 内置分类LOG_AUTH #auth 安全和认证相关的日志LOG_AUTHPRIV #authpriv 安全和认证相关的日志，私有LOG_CRON #cron 系统定时任务 crontab 与 at 产生的相关日志LOG_DAEMON #daemon 各守护进程产生的日志LOG_FTP #ftp ftp守护进程产生的日志LOG_KERN #kern 内核产生的日志LOG_LOCAL0 -- LOG_LOCAL7 #local0-local7 自定义分类LOG_LPR #lpr 打印服务日志LOG_MAIL #mail 邮件服务日志LOG_NEWS #news 网络新闻服务产生的日志LOG_SYSLOG #syslog syslogd 服务自己的日志LOG_USER #user 用户等级LOG_UUCP #uucp uucp子系统的日志信息* #通配符，代表所有分类#自定义的分类local0-local7 #保留给本机用户使用的一些登录文件讯息，终端信息等 Priority 优先级别 rsyslog 在记录日志的时候，将各种产生日志的事件和行为进行了优先级的排序，使用者可以根据不同环境(测试&#x2F;生产)和需求，设置不同的级别来记录日志，这样可以保证，记录下来的内容都是是我们想要的 1234567891011#syslog 内置优先级分类，从高到低，如果在记录日志时，设置了优先级，则只会记录设定的优先级和高于设定优先级的日志LOG_EMERG #emerg/panic 紧急，致命错误LOG_ALERT #alert 告警，当前状态必须立即进行纠正LOG_CRIT #crit 关键状态的警告，例如 硬件故障LOG_ERR #err/error 其它错误LOG_WARNING #warning/warn 警告级别的信息LOG_NOTICE #notice 通知级别的信息，LOG_INFO #info 通告级别的信息LOG_DEBUG #debug 调试程序时的信息* #所有级别的日志none #不需要任何日志 查看帮助 123[root@centos8 ~]#yum -y install man-pages[root@centos8 ~]#man 3 syslog[root@centos8 ~]#man 3 logger 1.2.2 rsyslog相关文件 项目 文件 程序包 rsyslog 主程序 &#x2F;usr&#x2F;sbin&#x2F;rsyslogd 服务管理（Ubuntu） &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;rsyslog.service 服务管理（CentOS 6） &#x2F;etc&#x2F;rc.d&#x2F;init.d&#x2F;rsyslog {start|stop|restart|status} 服务管理（CentOS 7,8） &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;rsyslog.service 配置文件 &#x2F;etc&#x2F;rsyslog.conf &#x2F;etc&#x2F;rsyslog.d&#x2F;*.conf 库文件 &#x2F;lib64&#x2F;rsyslog&#x2F;*.so 1.2.3 rsyslog配置文件&#x2F;etc&#x2F;rsyslog.conf 配置文件格式：由三部分组成 MODULES：相关模块配置 GLOBAL DIRECTIVES：全局配置 RULES：日志记录相关的规则配置，ubuntu 系统中，默认 rule 规则是单独放在一个文件中的 Ubuntu的配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@ubuntu ~]# cat /etc/rsyslog.conf# Default logging rules can be found in /etc/rsyslog.d/50-default.conf##################### MODULES ##################### #rsyslog 在安装是有很多支持模块，但默认不是所有模块都开启，如果有需要，写在MODULES范围即可module(load=&quot;imuxsock&quot;) # provides support for local system logging#module(load=&quot;immark&quot;) # provides --MARK-- message capability# provides UDP syslog reception #UDP模块，默认没有启用#module(load=&quot;imudp&quot;)#input(type=&quot;imudp&quot; port=&quot;514&quot;)# provides TCP syslog reception #TCP 模块，默认没有启用#module(load=&quot;imtcp&quot;)#input(type=&quot;imtcp&quot; port=&quot;514&quot;)# provides kernel logging support and enable non-kernel klog messagesmodule(load=&quot;imklog&quot; permitnonkernelfacility=&quot;on&quot;) #内核日志需要的模块############################### GLOBAL DIRECTIVES ################################# Use traditional timestamp format.# To enable high precision timestamps, comment out the following line.#$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat #默认日志模板# Filter duplicated messages$RepeatedMsgReduction on #默认开启重复过滤## Set the default permissions for all log files.##创建日志文件的默认权限和属主属组$FileOwner syslog $FileGroup adm$FileCreateMode 0640$DirCreateMode 0755$Umask 0022$PrivDropToUser syslog$PrivDropToGroup syslog## Where to place spool and state files#$WorkDirectory /var/spool/rsyslog #工作目录，默认目录为空## Include all config files in /etc/rsyslog.d/#$IncludeConfig /etc/rsyslog.d/*.conf #独立配置文件引用目录[root@ubuntu2004 ~]#ll /var/log/syslog-rw-r----- 1 syslog adm 43342 Dec 8 14:22 /var/log/syslog[root@ubuntu2004 ~]#ls /etc/rsyslog.d/*.conf/etc/rsyslog.d/20-ufw.conf /etc/rsyslog.d/21-cloudinit.conf /etc/rsyslog.d/50-default.conf RULES配置格式： 每一行 rule 由两列组成，分别是选择器和处理动作，选择器将过滤后的日志交由处理动作处理；选择器可以同时有多个，用分号分隔，处理动作也可以同时有多个，用 &amp; 分隔，处理动作中可以指定模板，不同的模板会生成不同的日志内容，模板可以自定义。 选择器有以下几种定义方式： 用分类和优先级来过滤，同一条 rule 中，分类和优先级都可以有多个，用逗号分隔 基于日志内容中的指定字段来过滤 基于表达式构建脚本来过滤 处理动作有以下几种： 输出到日志文件或某个特定设备 保存到数据库 发送给指定用户，该用户必须己登录，可以同时指定多个用户，用逗号分隔 传送到远程主机 通过管道传送给其它命令 丢弃日志，不处理 1facility.priority; facility.priority… target facility格式： 12* #所有的facility facility1,facility2,facility3,... #指定的facility列表 priority格式： 12345* #所有级别none #没有级别，即不记录PRIORITY #指定级别（含）以上的所有级别=PRIORITY #仅记录指定级别的日志信息!priority #排除指定的 priority，这种写法不能单独使用 target格式： 12345678910111213141516171819202122232425输出到日志文件或设备# /path/file 将日志内容写到指定文件，通常在/var/log/# -/path/file 将日志内容写到指定文件，-表示异步写入，异步表示把数据收到后，会先写入内存（缓冲区）中，过一会再写入/path/file# /dev/null 将日志内容输出到指定设备保存到数据库，保存到数据库要开启相应模块module (load=&quot;ommysql&quot;)*.* action(type=&quot;ommysql&quot; server=&quot;10.0.0.210&quot; db=&quot;rsyslog&quot; uid=&quot;rsysloger&quot; pwd=&quot;123456&quot;)发送给指定用户# root 将日志内容发送给用户 root# root,tom 将日志内容发送给用户 root 和 tom# * 将日志内容发送给所有己登录用户发送到远程主机，即日志服务器 # @host 把日志送往至指定的远程UDP日志服务器，@@host 将日志发送到远程TCP日志服务器# @192.168.2.123 使用 UDP 协议发送到远程主机，默认端口514# @@log.magedu.com:256 使用 TCP 协议发送到远程主机 256 端口，默认端口514# @(z6)[fe80::20c:29ff:fe7e:ce82] 使用 UPD 协议发送到远程主机(IPV6地址)，启用zlib压缩，压缩级别为9通过管道传送给其它命令，管道必须有名管道，要事先创建，此功能在 rsyslog8 版本后才支持 # | COMMAND，转发给其它命令处理不处理# stop 不处理，丢弃 Ubuntu 系统中默认 rule 配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@ubuntu2004 ~]#cat /etc/rsyslog.d/50-default.conf # Default rules for rsyslog.## For more information see rsyslog.conf(5) and /etc/rsyslog.conf## First some standard log files. Log by facility.#auth,authpriv.* /var/log/auth.log #登录验证相关的日志记录在auth.log中,要经常看，黑客登陆失败或成功会有记录*.*;auth,authpriv.none -/var/log/syslog #除了登录校验之外的日志，都记录在syslog中，异步写，建议将优先级改成info#cron.* /var/log/cron.log #计划任务#daemon.* -/var/log/daemon.logkern.* -/var/log/kern.log #内核所有日志都记录在kern.log 中，异步写#lpr.* -/var/log/lpr.logmail.* -/var/log/mail.log #邮件相关日志记录在 mail.log中，异步写#user.* -/var/log/user.log #用户账号## Logging for the mail system. Split it up so that# it is easy to write scripts to parse these files.##mail.info -/var/log/mail.info#mail.warn -/var/log/mail.warnmail.err /var/log/mail.err #邮件服务 err 及以上的日志记录在 mail.err 中## Some &quot;catch-all&quot; log files.##*.=debug;\\# auth,authpriv.none;\\# news.none;mail.none -/var/log/debug#*.=info;*.=notice;*.=warn;\\# auth,authpriv.none;\\# cron,daemon.none;\\# mail,news.none -/var/log/messages## Emergencies are sent to everybody logged in.#*.emerg :omusrmsg:* #所有致命错误信息，调用omusrmsg 模块发给所有登录用户## I like to have messages displayed on the console, but only on a virtual# console I usually leave idle.##daemon,mail.*;\\# news.=crit;news.=err;news.=notice;\\# *.=debug;*.=info;\\# *.=notice;*.=warn /dev/tty8 1.2.4 在本机自定义服务日志范例：将ssh服务的日志记录至自定义的local的日志设备 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#sshd服务日志默认是归属于 AUTH 分类，默认级别是 INFO[root@ubuntu2004 ~]#vim /etc/ssh/sshd_config# Logging#SyslogFacility AUTH#LogLevel INFO#根据配置，此日志记录在 /var/log/auth.log 中[root@ubuntu ~]# cat /etc/rsyslog.d/50-default.conf | grep &quot;^auth&quot;auth,authpriv.* /var/log/auth.log#查看日志，能看到相关内容[root@ubuntu ~]# tail -n 3 /var/log/auth.logJun 28 20:00:35 ubuntu2204 sshd[920]: Received signal 15; terminating.Jun 28 20:00:35 ubuntu2204 sshd[3431]: Server listening on 0.0.0.0 port 22.Jun 28 20:00:35 ubuntu2204 sshd[3431]: Server listening on :: port 22.#修改 sshd 服务日志的配置项，分类改到 LOCAL6，级别不改[root@ubuntu2004 ~]#vim /etc/ssh/sshd_config# Logging#SyslogFacility AUTH#LogLevel INFOSyslogFacility LOCAL6#可以新增配置文件sshd.log，也可以在原来文件中将local6分类的日志写入[root@ubuntu2004 ~]#vim /etc/rsyslog.d/sshd.conf[root@ubuntu2004 ~]#vim /etc/rsyslog.d/50-default.conf local6.info /var/log/sshd.log[root@ubuntu2004 ~]#systemctl restart rsyslog[root@ubuntu2004 ~]#systemctl restart sshd#生成/var/log/sshd.log[root@ubuntu2004 ~]#ll /var/log/sshd.log -rw-r----- 1 syslog adm 149 Dec 8 15:53 /var/log/sshd.log[root@ubuntu2004 ~]#tail -f /var/log/sshd.logDec 8 15:53:48 ubuntu2004 sshd[19015]: Server listening on 0.0.0.0 port 22.Dec 8 15:53:48 ubuntu2004 sshd[19015]: Server listening on :: port 22.#测试[root@ubuntu2004 ~]#ssh 127.0.0.1[root@ubuntu2004 ~]#tail -f /var/log/sshd.log....Dec 8 15:55:54 ubuntu2004 sshd[19080]: Accepted password for root from 10.0.0.1 port 64271 ssh2Dec 8 15:56:09 ubuntu2004 sshd[19225]: Accepted password for root from 127.0.0.1 port 33988 ssh2Dec 8 15:56:18 ubuntu2004 sshd[19225]: Received disconnect from 127.0.0.1 port 33988:11: disconnected by userDec 8 15:56:18 ubuntu2004 sshd[19225]: Disconnected from user root 127.0.0.1 port 33988[root@ubuntu2004 ~]#logger -p local6.info &quot;hello sshd&quot;[root@ubuntu2004 ~]#tail -f /var/log/sshd.log....Dec 8 15:56:59 ubuntu2004 root: hello sshd 1.2.5 rsyslog日志内容和模板通常的日志文件的格式： 日志文件有很多，如： &#x2F;var&#x2F;log&#x2F;messages, cron, secure等，基本格式都是类似的。格式如下 1事件产生的日期时间（默认精确到秒） 主机 应用程序程序名[进程](pid)：事件内容 centos日志文件格式 123456789[root@centos8 ~]#tail /var/log/messagesNov 12 08:34:18 centos8 dnf[14114]: Metadata cache created.Nov 12 08:34:18 centos8 systemd[1]: Started dnf makecache.....[root@centos8 ~]#tail /var/log/secureNov 11 18:27:12 centos8 groupadd[11940]: group added to /etc/group: name=dhcpd,GID=177Nov 11 18:27:12 centos8 groupadd[11940]: group added to /etc/gshadow: name=dhcpd... Ubuntu 中日志内容 123456789[root@ubuntu ~]# tail /var/log/syslogMay 2 03:56:07 ubuntu systemd[1]: systemd-rfkill.service: Deactivatedsuccessfully.May 2 05:37:36 ubuntu systemd-resolved[87114]: Clock change detected. Flushingcaches.......[root@ubuntu ~]# tail /var/log/auth.logMay 2 03:54:07 ubuntu sshd[903]: Received signal 15; terminating.May 2 03:54:08 ubuntu sshd[87070]: Server listening on 0.0.0.0 port 22....... 日志内容由 template 决定，如果没有显式指定，默认使用 RSYSLOG_TraditionalFileFormat，其具体内容如下 12template(name=&quot;RSYSLOG_TraditionalFileFormat&quot; type=&quot;string&quot;string=&quot;%TIMESTAMP% %HOSTNAME% %syslogtag%%msg:::sp-if-no-1st-sp%%msg:::drop-last-lf%\\n&quot;) rsyslog 中有13个内置的模板，我们可以在配置文件中直接使用，其名称如下，具体定义的内容需要查询相关文档 https://www.rsyslog.com/doc/v8-stable/configuration/templates.html 12345678910111213RSYSLOG_TraditionalFileFormatRSYSLOG_FileFormatRSYSLOG_TraditionalForwardFormatRSYSLOG_SysklogdFileFormatRSYSLOG_ForwardFormatRSYSLOG_SyslogProtocol23FormatRSYSLOG_DebugFormatRSYSLOG_WallFmtRSYSLOG_StdUsrMsgFmtRSYSLOG_StdDBFmtRSYSLOG_StdPgSQLFmtRSYSLOG_spoofadrRSYSLOG_StdJSONFmt 除了使用内置模板外，我们还可以自定义模板。自定义模板可以直接写在配置文件中 生成的日志内容模板决定，而模板是由 rsyslog 中的相关属性组成，这些属性在生成日志内容时会被替换成具体内容。所谓属性是指rsyslog 中的一些特殊关键字，在模板语法中，使用 %属性名% 来表示一个字段 范例：启用高精度时间 12345678910111213141516171819202122232425#默认日志时间精确到秒，可以修改配置实现[root@ubuntu2204 ~]#head /var/log/syslogOct 29 02:27:39 ubuntu2204 systemd-modules-load[521]: Inserted module &#x27;msr&#x27;Oct 29 02:27:39 ubuntu2204 systemd-modules-load[521]: Inserted module&#x27;ipmi_devintf&#x27;Oct 29 02:27:39 ubuntu2204 lvm[506]: 1 logical volume(s) in volume group&quot;ubuntu-vg&quot; monitoredOct 29 02:27:39 ubuntu2204 systemd[1]: Mounted FUSE Control File System.Oct 29 02:27:39 ubuntu2204 systemd[1]: Mounted Kernel Configuration File System......[root@ubuntu2204 ~]#vi /etc/rsyslog.conf############################### GLOBAL DIRECTIVES ################################将下面行注释#$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat[root@ubuntu2204 ~]#systemctl restart rsyslog#验证结果[root@ubuntu2204 ~]#tail /var/log/syslog2022-12-09T19:21:21.717291+08:00 ubuntu2204 systemd[1]: Stopping System Logging Service...2022-12-09T19:21:21.721852+08:00 ubuntu2204 rsyslogd: [originsoftware=&quot;rsyslogd&quot; swVersion=&quot;8.2112.0&quot; x-pid=&quot;818&quot; x-info=&quot;https://www.rsyslog.com&quot;] exiting on signal 15.2022-12-09T19:21:21.729966+08:00 ubuntu2204 systemd[1]: rsyslog.service:Deactivated successfully.2022-12-09T19:21:21.730558+08:00 ubuntu2204 systemd[1]: Stopped System Logging Service.2022-12-09T19:21:21.732955+08:00 ubuntu2204 systemd[1]: Starting System Logging Service... 1.2.6 启用网络日志服务启用网络日志服务功能，可以使用 rsyslog 服务中的远程转发功能，通过 TCP 或 UDP 协议将多个远程主机的日志，发送到集中的日志服务器，进行集中存储，方便统一管理 配置 rsyslog 主机，开启 TCP, UDP 相关功能 123456789101112131415161718#启用相关模块, 去掉 UDP,TCP 模块的注释[root@rsyslog ~]#vim /etc/rsyslog.conf# provides UDP syslog receptionmodule(load=&quot;imudp&quot;)input(type=&quot;imudp&quot; port=&quot;514&quot;)# provides TCP syslog receptionmodule(load=&quot;imtcp&quot;)input(type=&quot;imtcp&quot; port=&quot;514&quot;)#重启服务[root@rsyslog ~]#systemctl restart rsyslog.service#查看端口[root@ubuntu ~]# ss -tunlp | grep 514udp UNCONN 0 0 0.0.0.0:514 0.0.0.0:* users:((&quot;rsyslogd&quot;,pid=3995,fd=5))udp UNCONN 0 0 [::]:514 [::]:* users:((&quot;rsyslogd&quot;,pid=3995,fd=6))tcp LISTEN 0 25 0.0.0.0:514 0.0.0.0:* users:((&quot;rsyslogd&quot;,pid=3995,fd=7))tcp LISTEN 0 25 [::]:514 [::]:* users:((&quot;rsyslogd&quot;,pid=3995,fd=8)) 配置 client-1 主机日志远程转发 123456789[root@ubuntu2004 ~]#hostnamectl set-hostname client-1#配置远程转发，转发到 10.0.0.180 的tcp和udp协议514端口,默认514可以省略[root@client-1 ~]#vim /etc/rsyslog.d/50-default.conf*.info @@10.0.0.180:514 #TCP*.info @10.0.0.180:514 #UDP#重启服务[root@client-1 ~]# systemctl restart rsyslog 根据 rsyslog 主机中的配置，转发过来的日志会被写到 &#x2F;var&#x2F;log&#x2F;messages 文件中 12345678910111213[root@rsyslog ~]#vim /etc/rsyslog.conf*.info;mail.none;authpriv.none;cron.none /var/log/messages#测试-在client-1主机上写入日志[root@client-1 ~]# logger &quot;this msg from client-1&quot;#client-1 主机上也有该日志[root@client-1 ~]#tail -f /var/log/syslogMar 20 07:08:40 client-1 root: this msg from client-1#在 rsyslog 上查看[root@rsyslog ~]#tail -f /var/log/messagesMar 20 07:10:01 client-1 root: this msg from client-1 配置 client-2 主机日志远程转发 123456789101112131415161718#配置远程转发，转发到10.0.0.18的tcp和udp协议514端口,默认514可以省略[root@client-2 ~]# vim /etc/rsyslog.conf*.info @@10.0.0.18:514 #TCP*.info @10.0.0.18:514 #UDP#重启服务[root@client-2 ~]# systemctl restart rsyslog.service#测试[root@client-2 ~]# logger &quot;this msg from client-2&quot;#本机查看[root@client-1 ~]#tail -f /var/log/syslogMar 20 07:08:40 client-1 root: this msg from client-2#log-server 查看[root@rsyslog ~]#tail -f /var/log/messagesMar 20 07:10:01 client-1 root: this msg from client-2 范例：CentOS 7 和 6 启用网络日志功能 123456789vim /etc/rsyslog.conf####MODULES##### Provides UDP syslog reception$ModLoad imudp$UDPServerRun 514# Provides TCP syslog reception$ModLoad imtcp$InputTCPServerRun 514 1.2.7 常见日志文件Rocky 中常见日志说明 12345678[root@rocky ~]# cat /etc/rsyslog.conf | grep -Ev &quot;^#|^$&quot;*.info;mail.none;authpriv.none;cron.none /var/log/messages #除了mail,authpriv,cron 之外都记录authpriv.* /var/log/secure #安全认证相关日志mail.* -/var/log/maillog #邮件服务相关日志cron.* /var/log/cron #定时任务相关日志*.emerg :omusrmsg:* #所有致命错误信息，调用omusrmsg发给所有登录用户uucp,news.crit /var/log/spooler #uucp,新闻相关日志local7.* /var/log/boot.log #操作系统启动流程日志 ubuntu 中常见日志说明 12345678910[root@ubuntu ~]# cat /etc/rsyslog.d/*conf | grep -Ev &quot;^#|^$&quot;:msg,contains,&quot;[UFW &quot; /var/log/ufw.log #ufw服务日志:syslogtag, isequal, &quot;[CLOUDINIT]&quot; /var/log/cloud-init.log #cloud-init服务日志&amp; stop #其它不处理auth,authpriv.* /var/log/auth.log #auth,authpriv的日志*.*;auth,authpriv.none -/var/log/syslog #除了auth,authpriv之外的日志kern.* -/var/log/kern.log #内核产生的日志mail.* -/var/log/mail.log #邮件服务日志mail.err /var/log/mail.err #邮件服务err(含)以上日志*.emerg :omusrmsg:* #所有致命错误信息，调用omusrmsg发给所有登录用户 除了在 rsyslog 中定义的日志之外，系统中默认还有 btmp, lastlog, wtmp 三个日志文件，这三个文件都是非文本格式，无法直接打开，这三个文件需要经常看，巡检黑客攻击 /var/log/secure，/var/log/auth.log：系统安全日志，文本格式，应周期性分析 /var/log/btmp：当前系统上，用户的失败尝试登录相关的日志信息，二进制格式，lastb命令进行查看 /var/log/wtmp：当前系统上，用户正常登录系统的相关日志信息，二进制格式，last命令可以查看 12#显示系统关机项和运行级别更改last -x, --system /var/log/lastlog：每一个用户最近一次的登录信息，二进制格式，lastlog命令可以查看 /var/log/dmesg：CentOS7 之前版本系统引导过程中的日志信息，文本格式，开机后的硬件变化将不再记录，也可以通过专用命dmesg查看，可持续记录硬件变化的情况 /var/log/boot.log 系统服务启动的相关信息，文本格式，Ubuntu无此文件 /var/log/messages(红帽系统)，/var/log/syslog (Ubuntu) ：系统中大部分的信息 /var/log/anaconda : anaconda的日志，Ubuntu无此文件 范例 1234[root@rocky8 ~]#file /var/log/&#123;btmp,lastlog,wtmp&#125;/var/log/btmp: data/var/log/lastlog: dBase III DBT, version number 0, next free block index 1653273886/var/log/wtmp: firmware 0 v0 (revision 0) V2, 0 bytes or less, UNKNOWN2 0x2e322e78, at 0x0 0 bytes , at 0x0 0 bytes 范例：找到失败登录的IP 123456[root@centos8 ~]#awk &#x27;/Failed password/&#123;print $(NF-3)&#125;&#x27; /var/log/secure192.168.39.7192.168.39.18192.168.39.18[root@ubuntu2004 ~]#awk &#x27;/authentication failure/&#123;print $(NF-3)&#125;&#x27; /var/log/auth.log 范例：找出失败登录次数最多的前10个IP 1234567891011121314151617181920212223[root@centos8 ~]#lastb -f btmp-test1 | awk &#x27;&#123;print $3&#125;&#x27;|sort | uniq -c|sort -nr|head8374 112.64.33.387041 221.125.235.46502 183.247.184.2205970 203.190.163.1255297 202.89.0.273062 119.163.122.322961 124.126.248.62921 92.222.1.402896 112.65.170.1861955 118.97.213.118 [root@centos8 ~]#lastb -f btmp-test2 | awk &#x27;&#123;ip[$3]++&#125;END&#123;for(i in ip)&#123;print ip[i],i&#125;&#125;&#x27;|sort -nr|head86294 58.218.92.3743148 58.218.92.2618036 112.85.42.20110501 111.26.195.10110501 111.231.235.4910501 111.204.186.20710501 111.11.29.19910499 118.26.23.2256288 42.7.26.1424236 58.218.92.30 1.3 日志相关工具1.3.1 dmesg命令dmesg 命令用来查看主机硬件相关日志 选项 说明 -c 显示内核消息后清空缓冲区（仅当前会话有效，内核仍会继续写入新消息）。 -w&#x2F;--follow 持续监听并显示新的内核消息（类似 tail -f）。 -T 显示消息的人类可读时间戳（默认显示自系统启动以来的秒数）。 -k 仅显示内核相关消息（过滤掉用户空间程序的消息）。 -l &lt;级别&gt; 仅显示指定日志级别的消息（如 err 错误、warn 警告、info 信息等）。 -H 以人类友好的格式输出（自动换行、可读性更强）。 1.3.2 logger命令logger 命令可以手动生成相关日志 12345678910logger [options] [&lt;message&gt;]#常用选项-p|--priority #指定优先级-f|--file #从文件中读取日志内容-t|--tag #指定日志tag-n|--server #指定远程主机IP或主机名-P|--port #指定远程主机端口-T|--tcp #指定使用TCP协议传输-d|--udp #指定使用UDP协议传输 范例 12345678910111213#无选项[root@ubuntu ~]# logger &quot;this is test msg&quot;root@ubuntu2204:~# tail -n 1 /var/log/syslogMay 3 07:07:08 ubuntu2204 root: this is test msg[root@ubuntu ~]# cat test.txtthis msg from test.txt#从文件中读取，指定tag，指定优先级[root@ubuntu ~]# logger -t test -f test.txt -p error[root@ubuntu ~]# tail -n 1 /var/log/syslogMay 4 14:09:53 ubuntu test: this msg from test.txt 1.3.3 journalctl在 systemd 为 1号进程的系统版本中，systemd 提供了一个集中的方式来处理所有来自进程，应用程序等的操作系统日志，所有这些日志事件都由 systemd 的 journald 守护进程来处理。journald 守护进程收集所有来自 Linux 操作系统的各种日志，并将其作为二进制数据存储在文件中。以二进制数据集中记录事件、系统问题的好处有很多。例如，由于系统日志是以二进制而不是文本形式存储的，你可以以文本、JSON 对象等多种方式进行转译，以满足各种需求。另外，由于日志是按顺序存储的，通过对日志的日期&#x2F;时间操作，超级容易追踪到单个事件 journald 守护进程的配置文件：/etc/systemd/journald.conf journalctl命令格式 1journalctl [OPTIONS...] [MATCHES...] 选项说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153### 日志显示格式与内容控制- --no-full, --full, -l 如果字段内容超长则以省略号(...)截断以适应列宽。默认显示完整的字段内容(超长的部分换行显示或者被分页工具截断)。老旧的 -l/--full 选项 仅用于撤销已有的 --no-full 选项，除此之外没有其他用处。 - -a, --all 完整显示所有字段内容， 即使其中包含不可打印字符或者字段内容超长。 - -o, --output= 控制日志的输出格式。 可以使用如下选项：short（默认，类似传统syslog格式）、short-iso（时间戳ISO 8601格式）、short-precise（时间戳精确到微秒）、short-monotonic（时间戳从内核启动计算）、short-unix（时间戳为UNIX时间秒数，精确到微秒）、verbose（结构化显示所有字段）、export（序列化为二进制字节流）、json（JSON格式，每条一行）、json-pretty（JSON格式，每个字段一行）、json-sse（JSON格式，适应Server-Sent Events）、cat（仅显示日志内容，无元数据）。 - --utc 以世界统一时间(UTC)表示时间 - --no-hostname 不显示来源于本机的日志消息的主机名字段。 此选项仅对 short 系列输出格式(见上文)有效。 - -x, --catalog 在日志的输出中增加一些解释性的短文本， 以帮助进一步说明日志的含义、问题的解决方案、支持论坛、 开发文档、以及其他任何内容。并非所有日志都有这些额外的帮助文本， 详见 Message Catalog Developer Documentation 文档。注意，如果要将日志输出用于bug报告， 请不要使用此选项。 - -q, --quiet 当以普通用户身份运行时， 不显示任何警告信息与提示信息。 例如：&quot;-- Logs begin at...&quot;, &quot;-- Reboot --&quot;### 日志范围与过滤- -f, --follow 只显示最新的日志项，并且不断显示新生成的日志项。 此选项隐含了 -n 选项。 - -e, --pager-end 在分页工具内立即跳转到日志的尾部。 此选项隐含了 -n1000 以确保分页工具不必缓存太多的日志行。 不过这个隐含的行数可以被明确设置的 -n 选项覆盖。 注意，此选项仅可用于 less(1) 分页器。 - -n, --lines= 限制显示最新的日志行数。 --pager-end 与 --follow 隐含了此选项。此选项的参数：若为正整数则表示最大行数； 若为 &quot;all&quot; 则表示不限制行数；若不设参数则表示默认值10行。 - --no-tail 显示所有日志行， 也就是用于撤销已有的 --lines= 选项(即使与 -f 连用)。 - -r, --reverse 反转日志行的输出顺序， 也就是最先显示最新的日志。 - -b [ID][±offset], --boot=[ID][±offset] 显示特定于某次启动的日志， 这相当于添加了一个 &quot;_BOOT_ID=&quot; 匹配条件。如果参数为空(也就是 ID 与 ±offset 都未指定)， 则表示仅显示本次启动的日志。如果省略了 ID ， 那么当 ±offset 是正数的时候， 将从日志头开始正向查找，否则(也就是为负数或零)将从日志尾开始反响查找。 举例来说， &quot;-b 1&quot;表示按时间顺序排列最早的那次启动， &quot;-b 2&quot;则表示在时间上第二早的那次启动； &quot;-b -0&quot;表示最后一次启动， &quot;-b -1&quot;表示在时间上第二近的那次启动， 以此类推。 如果 ±offset 也省略了， 那么相当于&quot;-b -0&quot;， 除非本次启动不是最后一次启动(例如用--directory 指定了另外一台主机上的日志目录)。如果指定了32字符的 ID ， 那么表示以此 ID 所代表的那次启动为基准计算偏移量(±offset)， 计算方法同上。 换句话说， 省略 ID 表示以本次启动为基准计算偏移量(±offset)。 - --list-boots 列出每次启动的 序号(也就是相对于本次启动的偏移量)、32字符的ID、第一条日志的时间戳、最后一条日志的时间戳。 - -k, --dmesg 仅显示内核日志。隐含了 -b 选项以及 &quot;_TRANSPORT=kernel&quot; 匹配项。 - -t, --identifier=SYSLOG_IDENTIFIER 仅显示 syslog 识别符为 SYSLOG_IDENTIFIER 的日志项。可以多次使用该选项以指定多个识别符。 - -u, --unit=UNIT|PATTERN 仅显示属于特定单元的日志。 也就是单元名称正好等于 UNIT 或者符合 PATTERN 模式的单元。 这相当于添加了一个 &quot;_SYSTEMD_UNIT=UNIT&quot; 匹配项(对于 UNIT 来说)，或一组匹配项(对于 PATTERN 来说)。可以多次使用此选项以添加多个并列的匹配条件(相当于用&quot;OR&quot;逻辑连接)。 - --user-unit= 仅显示属于特定用户会话单元的日志。 相当于同时添加了 &quot;_SYSTEMD_USER_UNIT=&quot; 与&quot;_UID=&quot; 两个匹配条件。可以多次使用此选项以添加多个并列的匹配条件(相当于用&quot;OR&quot;逻辑连接)。 - -p, --priority= 根据日志等级(包括等级范围)过滤输出结果。 日志等级数字与其名称之间的对应关系如下 (参见 syslog(3))： &quot;emerg&quot; (0), &quot;alert&quot; (1), &quot;crit&quot; (2), &quot;err&quot; (3),&quot;warning&quot; (4), &quot;notice&quot; (5), &quot;info&quot; (6), &quot;debug&quot; (7) 。若设为一个单独的数字或日志等级名称， 则表示仅显示小于或等于此等级的日志(也就是重要程度等于或高于此等级的日志)。 若使用 FROM..TO.. 设置一个范围，则表示仅显示指定的等级范围内(含两端)的日志。 此选项相当于添加了 &quot;PRIORITY=&quot; 匹配条件。 - -c, --cursor= 从指定的游标(cursor)开始显示日志。[提示]每条日志都有一个&quot;__CURSOR&quot;字段，类似于该条日志的指纹。 - --after-cursor= 从指定的游标(cursor)之后开始显示日志。 如果使用了 --show-cursor 选项，则也会显示游标本身。 - --show-cursor 在最后一条日志之后显示游标， 类似下面这样，以&quot;--&quot;开头：-- cursor: s=0639... 游标的具体格式是私有的(也就是没有公开的规范)， 并且会变化。 - -S, --since=, -U, --until= 显示晚于指定时间(--since=)的日志、显示早于指定时间(--until=)的日志。参数的格式类似 &quot;2012-10-30 18:17:16&quot; 这样。 如果省略了&quot;时:分:秒&quot;部分，则相当于设为 &quot;00:00:00&quot; 。 如果仅省略了&quot;秒&quot;的部分则相当于设为 &quot;:00&quot; 。如果省略了&quot;年-月-日&quot;部分， 则相当于设为当前日期。 除了&quot;年-月-日 时:分:秒&quot;格式，参数还可以进行如下设置：(1)设为 &quot;yesterday&quot;, &quot;today&quot;, &quot;tomorrow&quot;以表示那一天的零点(00:00:00)。 (2)设为 &quot;now&quot; 以表示当前时间。(3)可以在&quot;年-月-日 时:分:秒&quot;前加上 &quot;-&quot;(前移) 或 &quot;+&quot;(后移)前缀以表示相对于当前时间的偏移。 关于时间与日期的详细规范， 参见systemd.time(7) - -m, --merge 混合显示包括远程日志在内的所有可见日志。 - --system, --user 仅显示系统服务与内核的日志(--system)、 仅显示当前用户的日志(--user)。如果两个选项都未指定，则显示当前用户的所有可见日志。 - -M, --machine= 显示来自于正在运行的、特定名称的本地容器的日志。 参数必须是一个本地容器的名称。 - -D DIR, --directory=DIR 仅显示来自于特定目录中的日志， 而不是默认的运行时和系统日志目录中的日志。 - --file=GLOB GLOB 是一个可以包含&quot;?&quot;与&quot;*&quot;的文件路径匹配模式。 表示仅显示来自与指定的 GLOB模式匹配的文件中的日志， 而不是默认的运行时和系统日志目录中的日志。可以多次使用此选项以指定多个匹配模式(多个模式之间用&quot;OR&quot;逻辑连接)。 - --root=ROOT 在对日志进行操作时， 将 ROOT 视为系统的根目录。 例如 --update-catalog 将会创ROOT/var/lib/systemd/catalog/database### 日志字段与分类查询- -F, --field= 显示所有日志中某个字段的所有可能值。 [译者注]类似于SQL语句：&quot;SELECT DISTINCT 某字段 FROM 全部日志&quot; - -N, --fields 输出所有日志字段的名称 - --list-catalog [128-bit-ID...] 简要列出日志分类信息， 其中包括对分类信息的简要描述。如果明确指定了分类ID(128-bit-ID)， 那么仅显示指定的分类。 - --dump-catalog [128-bit-ID...] 详细列出日志分类信息 (格式与 .catalog 文件相同)。如果明确指定了分类ID(128-bit-ID)， 那么仅显示指定的分类。 - --update-catalog 更新日志分类索引二进制文件。每当安装、删除、更新了分类文件，都需要执行一次此动作。### 日志管理与维护- --new-id128 此选项并不用于显示日志内容， 而是用于重新生成一个标识日志分类的 128-bit ID 。此选项的目的在于 帮助开发者生成易于辨别的日志消息， 以方便调试。 - --header 此选项并不用于显示日志内容， 而是用于显示日志文件内部的头信息(类似于元数据)。 - --disk-usage 此选项并不用于显示日志内容，而是用于显示所有日志文件(归档文件与活动文件)的磁盘占用总量。 - --vacuum-size=, --vacuum-time=, --vacuum-files= 这些选项并不用于显示日志内容，而是用于清理日志归档文件(并不清理活动的日志文件)， 以释放磁盘空间。--vacuum-size= 可用于限制归档文件的最大磁盘使用量 (可以使用 &quot;K&quot;, &quot;M&quot;, &quot;G&quot;, &quot;T&quot;后缀)； --vacuum-time= 可用于清除指定时间之前的归档 (可以使用 &quot;s&quot;, &quot;m&quot;, &quot;h&quot;,&quot;days&quot;, &quot;weeks&quot;, &quot;months&quot;, &quot;years&quot; 后缀)； --vacuum-files=可用于限制日志归档文件的最大数量。 注意，--vacuum-size= 对 --disk-usage的输出仅有间接效果， 因为 --disk-usage 输出的是归档日志与活动日志的总量。同样，--vacuum-files= 也未必一定会减少日志文件的总数，因为它同样仅作用于归档文件而不会删除活动的日志文件。此三个选项可以同时使用，以同时从三个维度去限制归档文件。若将某选项设为零，则表示取消此选项的限制。 - --setup-keys 此选项并不用于显示日志内容， 而是用于生成一个新的FSS(Forward Secure Sealing)密钥对。 此密钥对包含一个&quot;sealing key&quot;与一个&quot;verification key&quot;。&quot;sealing key&quot;保存在本地日志目录中， 而&quot;verification key&quot;则必须保存在其他地方。详见 journald.conf(5) 中的 Seal= 选项。 - --force 与 --setup-keys 连用， 表示即使已经配置了FSS(Forward Secure Sealing)密钥对，也要强制重新生成。 - --interval= 与 --setup-keys 连用，指定&quot;sealing key&quot;的变化间隔。较短的时间间隔会导致占用更多的CPU资源， 但是能够减少未检测的日志变化时间。默认值是 15min - --verify 检查日志文件的内在一致性。 如果日志文件在生成时开启了FSS特性， 并且使用 --verify-key= 指定了FSS的&quot;verification key&quot;，那么，同时还将验证日志文件的真实性。 - --verify-key= 与 --verify 选项连用， 指定FSS的&quot;verification key&quot; - --sync 要求日志守护进程将所有未写入磁盘的日志数据刷写到磁盘上，并且一直阻塞到刷写操作实际完成之后才返回。 因此该命令可以保证当它返回的时候，所有在调用此命令的时间点之前的日志， 已经全部安全的刷写到了磁盘中。 - --flush 要求日志守护进程 将 /run/log/journal 中的日志数据 刷写到 /var/log/journal 中(如果持久存储设备当前可用的话)。 此操作会一直阻塞到操作完成之后才会返回，因此可以确保在该命令返回时， 数据转移确实已经完成。注意，此命令仅执行一个单独的、一次性的转移动作， 若没有数据需要转移，则此命令什么也不做， 并且也会返回一个表示操作已正确完成的返回值。 - --rotate 要求日志守护进程滚动日志文件。 此命令会一直阻塞到滚动完成之后才会返回。### 帮助与版本- -h, --help 显示简短的帮助信息并退出。- --version 显示简短的版本信息并退出。- --no-pager 不将程序的输出内容管道(pipe)给分页程序 范例：journalctl常用用法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081### 基础日志查看- 查看所有日志（默认仅本次启动） journalctl - 查看内核日志（不含应用日志） journalctl -k - 查看系统本次启动的日志 journalctl -b journalctl -b -0 - 查看上一次启动的日志 journalctl -b -1- 直接显示到日志尾部，并附带解释性文本 journalctl -xe### 按时间筛选日志- 查看指定时间的日志 journalctl --since=&quot;2017-10-30 18:10:30&quot; journalctl --since &quot;20 min ago&quot; journalctl --since yesterday journalctl --since &quot;2017-01-10&quot; --until &quot;2017-01-11 03:00&quot; journalctl --since 09:00 --until &quot;1 hour ago&quot; ### 按日志位置/数量筛选- 显示尾部最新10行日志 journalctl -n - 显示尾部指定行数的日志 journalctl -n 20 - 实时滚动显示最新日志 journalctl -f ### 按对象筛选日志- 查看指定服务的日志 journalctl /usr/lib/systemd/systemd - 查看指定进程的日志 journalctl _PID=1 - 查看某个脚本的日志 journalctl /usr/bin/bash - 查看指定用户的日志 journalctl _UID=33 --since today - 查看某个 Unit 的日志 journalctl -u nginx.service journalctl -u nginx.service --since today - 实时滚动显示某个 Unit 的最新日志 journalctl -u nginx.service -f - 合并显示多个 Unit 的日志 journalctl -u nginx.service -u php-fpm.service --since today ### 按日志级别筛选- 查看指定优先级（及以上）的日志（级别：0:emerg；1:alert；2:crit；3:err；4:warning；5:notice；6:info；7:debug） journalctl -p err -b ### 输出格式控制- 关闭分页，标准输出 journalctl --no-pager - 以 JSON 格式（单行）输出 journalctl -b -u nginx.service -o json - 以 JSON 格式（多行，可读性好）输出 journalctl -b -u nginx.service -o json-pretty ### 日志管理- 显示日志占据的硬盘空间 journalctl --disk-usage - 指定日志文件最大占用空间 journalctl --vacuum-size=1G - 指定日志文件保存时长 journalctl --vacuum-time=1years 2 利用MySQL存储日志信息 1210.0.0.180：rocky10.0.0.179：Ubuntu 2.1 目标在网络转发的基础上，，利用rsyslog日志服务，将收集的日志记录于MySQL中 2.2 环境准备123两台主机一台：rsyslog日志服务器，IP：10.0.0.180一台：mysql数据库服务器，IP：10.0.0.179 2.3 实现步骤2.3.1 在rsyslog服务器上安装连接mysql模块相关的程序包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@rsyslog ~]#yum -y install rsyslog-mysql[root@rsyslog ~]#yum install -y mysql#补充：Ubuntu[root@ubuntu2004 ~]#apt -y install rsyslog-mysql#模块配置文件，到时创建数据库和账号要按照这个来[root@rsyslog ~]#vim /etc/rsyslog.d/mysql.conf### Configuration file for rsyslog-mysql### Changes are preservedmodule (load=&quot;ommysql&quot;)*.* action(type=&quot;ommysql&quot; server=&quot;localhost&quot; db=&quot;Syslog&quot; uid=&quot;rsyslog&quot; pwd=&quot;&quot;)#查看包文件[root@rsyslog ~]#rpm -ql rsyslog-mysql /usr/lib/.build-id/usr/lib/.build-id/7e/usr/lib/.build-id/7e/30458f368f71ec11b45c0445303b98fbc47a9c/usr/lib64/rsyslog/ommysql.so #库文件/usr/share/doc/rsyslog/mysql-createDB.sql #mysql 数据表脚本文件#补充Ubuntu[root@ubuntu2004 ~]# dpkg -L rsyslog-mysql....../usr/lib/x86_64-linux-gnu/rsyslog/ommysql.so #库文件/usr/share/dbconfig-common/data/rsyslog-mysql/install/mysql #mysql 数据表脚本文件/usr/share/rsyslog-mysql/rsyslog-mysql.conf.template #rsyslog 配置文件模板#查看sql脚本文件内容[root@rsyslog ~]#cat /usr/share/doc/rsyslog/mysql-createDB.sqlCREATE DATABASE Syslog;USE Syslog;CREATE TABLE SystemEvents( ID int unsigned not null auto_increment primary key, CustomerID bigint, ReceivedAt datetime NULL, DeviceReportedTime datetime NULL, Facility smallint NULL, Priority smallint NULL, FromHost varchar(60) NULL, Message text, NTSeverity int NULL, Importance int NULL, EventSource varchar(60), EventUser varchar(60) NULL, EventCategory int NULL, EventID int NULL, EventBinaryData text NULL, MaxAvailable int NULL, CurrUsage int NULL, MinUsage int NULL, MaxUsage int NULL, InfoUnitID int NULL , SysLogTag varchar(60), EventLogType varchar(60), GenericFileName VarChar(60), SystemID int NULL);CREATE TABLE SystemEventsProperties( ID int unsigned not null auto_increment primary key, SystemEventID int NULL , ParamName varchar(255) NULL , ParamValue text NULL);#补充Ubuntu[root@centos8 ~]#cat /usr/share/dbconfig-common/data/rsyslog-mysql/install/mysqlCREATE DATABASE Syslog; #Ubuntu22.04和20.04没有这两行，需要手动创建数据库USE Syslog; 2.3.2 准备 MySQL12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#Ubuntu默认开启mysql服务[root@mysql ~]#apt update &amp;&amp; apt install -y mysql-server#但是只监听本机，无法远程连接[root@ubuntu2004 ~]#ss -ntlState Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 151 127.0.0.1:3306 0.0.0.0:* #修改文件，将两行注释[root@mysql ~]#vim /etc/mysql/mysql.conf.d/mysqld.cnf #bind-address = 127.0.0.1#mysqlx-bind-address = 127.0.0.1[root@mysql ~]#systemctl restart mysql[root@mysql ~]#ss -ntl State Recv-Q Send-Q Local Address:Port Peer Address:Port Process LISTEN 0 151 *:3306 *:* #因为Ubuntu的数据库脚本中没有创建数据库的命令，需要手动创建mysql&gt; create database Syslog;Query OK, 1 row affected (0.01 sec)mysql&gt; show databases;+--------------------+| Database |+--------------------+| Syslog || information_schema || mysql || performance_schema || sys |+--------------------+5 rows in set (0.00 sec)#根据模块文件的要求创建账号和授权mysql&gt; create user rsyslog@&#x27;10.0.0.%&#x27; identified by &#x27;123456&#x27;;Query OK, 0 rows affected (0.01 sec)mysql&gt; grant all on Syslog.* to rsyslog@&#x27;10.0.0.%&#x27;;Query OK, 0 rows affected (0.01 sec)#测试能否在rsyslog服务器上远程连接mysql服务器[root@rsyslog ~]#mysql -ursyslog -p123456 -h10.0.0.179mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || performance_schema |+--------------------+3 rows in set (0.00 sec)#方法1：在rsyslog服务器上导入数据库mysql&gt; source /usr/share/doc/rsyslog/mysql-createDB.sql;[root@rsyslog ~]#mysql -ursyslog -p123456 -h10.0.0.179 &lt; /usr/share/doc/rsyslog/mysql-createDB.sql[root@rsyslog ~]#mysql -ursyslog -p123456 -h10.0.0.179 -e &quot;source /usr/share/doc/rsyslog/mysql-createDB.sql&quot;#补充Ubuntu[root@ubuntu2204 ~]#mysql -ursyslog -p123456 -h10.0.0.179 Syslog &lt; /usr/share/dbconfig-common/data/rsyslog-ysql/install/mysql[root@ubuntu2204 ~]#mysql -ursyslog -p123456 -h10.0.0.179 Syslog -e &quot;source /usr/share/dbconfig-common/data/rsyslog-ysql/install/mysql&quot;#方法2：在mysql服务器上导入数据库#将sql脚本复制到数据库服库上[root@rsyslog ~]#scp /usr/share/doc/rsyslog/mysql-createDB.sql 10.0.0.179:mysql&gt; show tables;+------------------------+| Tables_in_Syslog |+------------------------+| SystemEvents || SystemEventsProperties |+------------------------+2 rows in set (0.01 sec)#两张表都为空mysql&gt; select count(*) from SystemEvents;+----------+| count(*) |+----------+| 0 |+----------+1 row in set (0.02 sec)mysql&gt; select count(*) from SystemEventsProperties;+----------+| count(*) |+----------+| 0 |+----------+1 row in set (0.01 sec) 2.3.3 配置日志服务器将日志发送至指定数据库1234567891011121314151617181920212223#配置rsyslog将日志保存到mysql中[root@rsyslog ~]#vim /etc/rsyslog.conf#####MODULES#####在 MODULES 语言下面，如果是 Ubuntu22.04,20.04和CentOS8 加下面行module(load=&quot;ommysql&quot;)#在 MODULES 语言下面，如果是 CentOS 7，6 加下面行$ModLoad ommysql#在RULES语句块加下面行的格式#facility.priority :ommysql:DBHOST,DBNAME,DBUSER, PASSWORD*.info :ommysql:10.0.0.179,Syslog,rsyslog,123456[root@rsyslog ~]#systemctl restart rsyslog.service#Ubuntu 自动生成以下配置文件，只需要按环境修改#由于原模块文件中没有设置密码，要加上去，把级别改了，把server改了，指向mysql服务器[root@ubuntu2004 ~]#vim /etc/rsyslog.d/mysql.confmodule (load=&quot;ommysql&quot;)*.info action(type=&quot;ommysql&quot; server=&quot;10.0.0.179&quot; db=&quot;Syslog&quot; uid=&quot;rsyslog&quot; pwd=&quot;123456&quot;)[root@ubuntu2004 ~]#systemctl restart rsyslog 2.3.4 测试123456789101112#测试将client1，client2的日志发送数据库[root@client-1 ~]#logger &quot;this msg from client-1&quot;[root@client-2 ~]#logger &quot;this msg from client-2&quot;mysql&gt; select FromHost,Message from SystemEvents order by id desc limit 3;+------------+---------------------------+| FromHost | Message |+------------+---------------------------+| client-2 | this msg from client-2 || client-1 | this msg from client-1 |+------------+---------------------------+3 rows in set (0.00 sec) 在此基础上，我们可以再实现一个 Web 服务，将 Mysql 中的日志数据在页面上显示出来 12#推荐的开源程序https://loganalyzer.adiscon.com/ 3 Logrotate日志转储3.1 Logrotate介绍日志是重要的系统文件，记录和保存了系统中所有的重要事件。但是日志文件也需要进行定期的维护，因为日志文件是不断增长的，如果完全不进行日志维护，而任由其随意递增，那么用不了多久，我们的硬盘就会被写满。如果服务器数量较多，日志文件大小增长较快，也会很容易触发告警。日志维护的最主要的工作就是把旧的日志文件删除，从而腾出空间保存新的日志文件。这项工作如果靠管理员手工来完成，那其实是非常烦琐的，而且也容易忘记。那么 Linux 系统是否可以自动完成日志的轮替工作呢？ 为了解决这种情况，我们可以使用日志转储服务。logrotate就是用来进行日志轮替（也叫日志转储）的，把旧的日志文件删除，并创建新的日志文件，节省磁盘空间。可以根据日志文件的大小，也可以根据其天数来转储，这个过程一般通过 cron 程序来执行 工作原理：系统计划任务每天执行一次脚本文件，在脚本中再执行 /usr/sbin/logrotate/etc/logrotate.conf ，即调用 logrotate 程序再配合定义好的转储规则对日志文件进行转储 12345678910[root@log-server ~]# ll /var/log/dmesg*-rw-r----- 1 root adm 132571 Jul 4 08:49 /var/log/dmesg-rw-r----- 1 root adm 132565 Jul 3 20:12 /var/log/dmesg.0-rw-r----- 1 root adm 26381 Jun 30 08:18 /var/log/dmesg.1.gz-rw-r----- 1 root adm 26443 Jun 28 22:01 /var/log/dmesg.2.gz[root@log-server ~]# ll /var/log/alternatives.log*-rw-r--r-- 1 root root 416 Jul 4 08:49 /var/log/alternatives.log-rw-r--r-- 1 root root 9415 Jun 30 08:31 /var/log/alternatives.log.1-rw-r--r-- 1 root root 3160 May 9 11:58 /var/log/alternatives.log.2.gz 3.2 日志文件的命名规则日志轮替最主要的作用就是把旧的日志文件移动并改名，同时建立新的空日志文件，当旧日志文件超出保存的范围时就删除。那么，旧的日志文件改名之后，如何命名呢？主要依靠 &#x2F;etc&#x2F;logrotate.conf 配置文件中的“dateext”参数。 如果配置文件中有“dateext”参数，那么日志会用日期来作为日志文件的后缀，如“secure-20130605”。这样日志文件名不会重叠，也就不需要对日志文件进行改名，只需要保存指定的日志个数，删除多余的日志文件即可。 如果配置文件中没有“dateext”参数，那么日志文件就需要进行改名了。当第一次进行日志轮替时，当前的“secure”日志会自动改名为“secure.1”，然后新建“secure”日志，用来保存新的日志；当第二次进行日志轮替时，“secure.1”会自动改名为“secure.2”，当前的“secure”日志会自动改名为“secure.1”，然后也会新建“secure”日志，用来保存新的日志，以此类推。 3.3 Logrotate配置软件包：logrotate 相关文件 计划任务：&#x2F;etc&#x2F;cron.daily&#x2F;logrotate，&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;logrotate.service，&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;logrotate.timer 程序文件：&#x2F;usr&#x2F;sbin&#x2F;logrotate 配置文件： &#x2F;etc&#x2F;logrotate.conf 日志文件：&#x2F;var&#x2F;lib&#x2F;logrotate&#x2F;logrotate.status 12345/etc/cron.daily/logrotate #定时任务脚本，放在 cron.daily 目录中，默认系统会每天执行一次/etc/logrotate.conf #主配置文件，定义日志转储策略/etc/logrotate.d/ #配置文件目录，定义日志转储策略/usr/sbin/logrotate #主程序/var/lib/logrotate/status #logrotate服务的日志文件 在配置文件中定义转储规则，配置文件中的主要配置项 1234567891011121314151617181920212223242526[root@log-server ~]# cat /etc/logrotate.conf# see &quot;man logrotate&quot; for details# global options do not affect preceding include directives# rotate log files weeklyweekly #默认每周一次转储# use the adm group by default, since this is the owning group# of /var/log/syslog.su root adm #默认使用adm组# keep 4 weeks worth of backlogsrotate 4 #默认保留最近4周的文件(4个文件)# create new (empty) log files after rotating old onescreate #转储完成后生成新的空文件存储新的日志# use date as a suffix of the rotated file#dateext #默认不使用日志后缀# uncomment this if you want your log files compressed#compress #默认不启用文件压缩# packages drop log rotation information into this directoryinclude /etc/logrotate.d #包含的子目录# system-specific logs may also be configured here. 配置文件主要参数如下： 配置参数 说明 compress 通过gzip压缩转储以后的日志 nocompress 不压缩 copytruncate 用于还在打开中的日志文件，把当前日志备份并截断 nocopytruncate 备份日志文件但是不截断 create mode owner group 转储文件，使用指定的权限，所有者，所属组创建新的日志文件 nocreate 不建立新的日志文件 delaycompress 和 compress 一起使用时，转储的日志文件到下一次转储时才压缩 nodelaycompress 覆盖 delaycompress 选项，转储同时压缩 errors address 专储时的错误信息发送到指定的Email 地址 ifempty 即使是空文件也转储，此为默认选项 notifempty 如果是空文件的话，不转储 mail address 把转储的日志文件发送到指定的E-mail 地址 nomail 转储时不发送日志文件 olddir directory 转储后的日志文件放入指定目录，必须和当前日志文件在同一个文件系统 noolddir 转储后的日志文件和当前日志文件放在同一个目录下 prerotate&#x2F;endscript 在转储以前需要执行的命令，这两个关键字必须单独成行 postrotate&#x2F;endscript 在转储以后需要执行的命令，这两个关键字必须单独成行 daily 指定转储周期为每天 weekly 指定转储周期为每周 monthly 指定转储周期为每月 rotate count 指定日志文件删除之前转储的次数，0 指没有备份，5 指保留5 个备份 tabooext [+] list 让logrotate不转储指定扩展名的文件，缺省的扩展名是：.rpm-orig，.rpmsave，v，和 ~ size size 当日志文件到达指定的大小时才转储，bytes(缺省)及KB或MB sharedscripts 默认，对每个转储日志运行prerotate和postrotate脚本，日志文件的绝对路径作为第一个参数传递给脚本。 这意味着单个脚本可以针对与多个文件匹配的日志文件条目多次运行（例如&#x2F;var&#x2F;log&#x2F;example&#x2F;*.log）。 如果指定此项sharedscripts，则无论有多少个日志与通配符模式匹配，脚本都只会运行一次 nosharedscripts 针对每一个转储的日志文件，都执行一次prerotate 和 postrotate脚本，此为默认值 missingok 如果日志不存在，不提示错误，继续处理下一个 nomissingok 如果日志不存在，提示错误，此为默认值 每个服务单独的配置文件，如果在单独配置文件中没有定义的配置项，则使用主配置文件中的配置项或默认配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849root@log-server ~]# ls /etc/logrotate.d/alternatives apport apt bootlog btmp dbconfig-common dpkg nginx php8.1-fpm rsyslog ubuntu-advantage-tools ufw unattended-upgrades wtmp[root@log-server ~]# cat /etc/logrotate.d/rsyslog/var/log/syslog/var/log/mail.info/var/log/mail.warn/var/log/mail.err/var/log/mail.log/var/log/daemon.log/var/log/kern.log/var/log/auth.log/var/log/user.log/var/log/lpr.log/var/log/cron.log/var/log/debug/var/log/messages&#123; #上述所有日志文件都适用于此转储规则 rotate 4 #保留最近4个文件，加上当前使用的，一个5个 weekly #每周转储 missingok #如果要转储的日志文件不存在，不提示错误，继续下一个 notifempty #如果是空文件，不转储 compress #启用gzip压缩转储后的日志文件 delaycompress #和 compress 一起使用时，转储的日志文件到下一次转储时才压缩 sharedscripts #运行脚本，分别是转储前和转储后脚本 postrotate #转储后脚本 /usr/lib/rsyslog/rsyslog-rotate endscript&#125;[root@log-server ~]# cat /etc/logrotate.d/nginx/var/log/nginx/*.log &#123; daily missingok rotate 14 compress delaycompress notifempty create 0640 www-data adm sharedscripts prerotate if [ -d /etc/logrotate.d/httpd-prerotate ]; then \\ run-parts /etc/logrotate.d/httpd-prerotate; \\ fi \\ endscript postrotate invoke-rc.d nginx rotate &gt;/dev/null 2&gt;&amp;1 #让nginx重新锚定新生成的日志文件 endscript&#125; 3.4 Logrotate自定义规则实现123456789101112logrotate [OPTION...] &lt;configfile&gt;#常用选项-?|--help #显示帮助信息-d|--debug #不执行任何操作，仅显示错误信息，类似于测试-f|--force #强制执行-m|--mail=command #指定执行邮件发送的命令，默认 /usr/bin/mail-s|--state=statefile #指定服务日志文件，默认 /var/lib/logrotate/status-v|--verbose #显示详细信息-l|--log=logfile #指定详细信息日志，加上此选项，会将详细信息写到指定文件--version #显示版本信息--skip-state-lock #不给 statefile 文件加锁 范例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#创建测试日志文件[root@log-server ~]# dd if=/dev/zero of=/var/log/test1.log bs=2M count=1[root@log-server ~]# dd if=/dev/zero of=/var/log/test2.log bs=2M count=1[root@log-server ~]# ll -h /var/log/test*-rw-r--r-- 1 root root 2.0M Jul 4 17:51 /var/log/test1.log-rw-r--r-- 1 root root 2.0M Jul 4 17:52 /var/log/test2.log#定义转储规则[root@log-server ~]# cat /etc/logrotate.d/test&#123;1,2&#125;/var/log/test1.log &#123; daily rotate 5 su root root compress delaycompress missingok size 1M notifempty create 0640 syslog adm postrotate echo `date +%F_%T` &gt;&gt; /tmp/test1.log endscript&#125;/var/log/test2.log &#123; daily rotate 5 su root root dateext compress delaycompress missingok size 1M notifempty create 644 root root postrotate echo `date +%F_%T` &gt;&gt; /tmp/test2.log endscript&#125;#手动执行转储[root@log-server ~]# logrotate /etc/logrotate.d/test1#查看日志，生成新的空文件，权限，属主属组都符合预设[root@log-server ~]# ls -lh /var/log/test1*-rw-r----- 1 syslog adm 0 Jul 5 22:58 /var/log/test1.log-rw-r--r-- 1 root root 2.0M Jul 5 21:05 /var/log/test1.log.1#再次转储，先保证日志达到转储条件[root@log-server ~]# dd if=/dev/zero of=/var/log/test1.log bs=3M count=1[root@log-server ~]# logrotate /etc/logrotate.d/test1[root@log-server ~]# ls -lh /var/log/test1*-rw-r----- 1 syslog adm 0 Jul 5 23:16 /var/log/test1.log-rw-r----- 1 syslog adm 3.0M Jul 5 23:03 /var/log/test1.log.1 #最新的转储-rw-r--r-- 1 root root 2.1K Jul 5 21:05 /var/log/test1.log.2.gz #前一个被转储的日志被压缩#查看日志文件，转储成功后命令被执行[root@log-server ~]# cat /tmp/test1.log2023-07-05_22:58:112023-07-05_23:16:12#调用主配置文件进行转储，主配置文件中包含了 test1 test2 配置，所以都会被转储[root@log-server ~]# logrotate /etc/logrotate.conf#test1 没有达到转储条件[root@log-server ~]# ls -lh /var/log/test1*-rw-r----- 1 syslog adm 0 Jul 5 23:16 /var/log/test1.log-rw-r----- 1 syslog adm 3.0M Jul 5 23:03 /var/log/test1.log.1-rw-r--r-- 1 root root 2.1K Jul 5 21:05 /var/log/test1.log.2.gz#test2 达到条件，且有日期后缀[root@log-server ~]# ls -lh /var/log/test2*-rw-r--r-- 1 root root 0 Jul 5 23:26 /var/log/test2.log-rw-r--r-- 1 root root 2.0M Jul 4 17:52 /var/log/test2.log-20230705 3.4 Logrotate配置范例Ubuntu22.04 包文件内容 12345678910111213141516171819202122232425262728293031323334353637[root@ubuntu2204 ~]#dpkg -L logrotate/lib/systemd/system/logrotate.service...[root@ubuntu2204 ~]#cat /lib/systemd/system/logrotate.service[Unit]Description=Rotate log filesDocumentation=man:logrotate(8) man:logrotate.conf(5)RequiresMountsFor=/var/logConditionACPower=true[Service]Type=oneshotExecStart=/usr/sbin/logrotate /etc/logrotate.conf# performance optionsNice=19IOSchedulingClass=best-effortIOSchedulingPriority=7# hardening options# details: https://www.freedesktop.org/software/systemd/man/systemd.exec.html# no ProtectHome for userdir logs# no PrivateNetwork for mail deliviery# no NoNewPrivileges for third party rotate scripts# no RestrictSUIDSGID for creating setgid directoriesLockPersonality=trueMemoryDenyWriteExecute=truePrivateDevices=truePrivateTmp=trueProtectClock=trueProtectControlGroups=trueProtectHostname=trueProtectKernelLogs=trueProtectKernelModules=trueProtectKernelTunables=trueProtectSystem=fullRestrictNamespaces=trueRestrictRealtime=true Rocky8包文件内容 123456789101112[root@rocky8 ~]#rpm -ql logrotate/etc/cron.daily/logrotate.....[root@rocky8 ~]#cat /etc/cron.daily/logrotate#!/bin/sh/usr/sbin/logrotate /etc/logrotate.confEXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate &quot;ALERT exited abnormally with [$EXITVALUE]&quot;fiexit $EXITVALUE 3.4.1 定制设置nginx的日志转储123456789101112131415161718192021222324252627282930313233343536373839404142434445cat /etc/logrotate.d/nginx/var/log/nginx/*.log &#123; daily rotate 100 missingok compress delaycompress notifempty create 644 ngnix nginx postrotate if [ -f /app/nginx/logs/nginx.pid ]; then kill -USR1 `cat /app/nginx/logs/nginx.pid` fi endscript&#125;#说明#终端1[root@rocky8 ~]#touch a.log[root@rocky8 ~]#tail -f a.log#终端2#查看进程和fd描述符[root@rocky8 ~]#lsof a.logCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEtail 6337 root 3r REG 253,0 0 33575489 a.log#/root/a.log的fd为3[root@rocky8 ~]#ll /proc/6337/fdtotal 0lrwx------ 1 root root 64 Dec 9 14:38 0 -&gt; /dev/pts/1lrwx------ 1 root root 64 Dec 9 14:38 1 -&gt; /dev/pts/1lrwx------ 1 root root 64 Dec 9 14:38 2 -&gt; /dev/pts/1lr-x------ 1 root root 64 Dec 9 14:38 3 -&gt; /root/a.log[root@rocky8 ~]#mv a.log b.log[root@rocky8 ~]#ll /proc/6337/fdtotal 0....lr-x------ 1 root root 64 Dec 9 14:38 3 -&gt; /root/b.log#终端1，虽然改名了，但是没有报错[root@rocky8 ~]#tail -f a.log所以这说明了tail -f 跟踪的不是文件名，而是文件描述符，所以USR1信号的作用就是重新加载日志文件，把新的日志文件写到新生成的文件中，在转储中，nginx旧的日志转储到了access.2023-12-09，随后生成新的文件access去存储新的日志文件，但是转储完后并没有去跟踪fd，还是原来的，即access.2023-12-09，那么新的日志就会写到这里面去，不会写到新的文件access，所以需要USR1信号，去指向新的fd，把新的日志文件写到新生成的文件中 3.4.2 nginx安装内置转储规则123456789101112131415161718192021222324252627282930313233[root@ubuntu2204 ~]#cat /etc/logrotate.d/nginx/var/log/nginx/*.log &#123; daily missingok rotate 14 compress delaycompress notifempty create 0640 www-data adm sharedscripts prerotate if [ -d /etc/logrotate.d/httpd-prerotate ]; then \\ run-parts /etc/logrotate.d/httpd-prerotate; \\ fi \\ endscript postrotate invoke-rc.d nginx rotate &gt;/dev/null 2&gt;&amp;1 endscript&#125;[root@rocky8 ~]#cat /etc/logrotate.d/nginx/var/log/nginx/*log &#123; create 0664 nginx root daily rotate 10 missingok notifempty compress sharedscripts postrotate /bin/kill -USR1 `cat /run/nginx.pid 2&gt;/dev/null` 2&gt;/dev/null || true endscript&#125; 3.4.3 Ubuntu22.04日志转储12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970[root@ubuntu2204 ~]#mkdir -p /var/log/test /data[root@ubuntu2204 ~]#dd if=/dev/zero of=/var/log/test/test1.log bs=1M count=2[root@ubuntu2204 ~]#ll /var/log/test/*-rw-r--r-- 1 root root 2097152 11月 18 12:21 /var/log/test/test1.log[root@ubuntu2204 ~]#cat /etc/logrotate.d/test1/var/log/test/test1.log &#123; #daily和size 1M要同时满足才会转储 daily rotate 5 compress delaycompress missingok size 1M notifempty create 0640 bin daemon sharedscripts postrotate echo `date +%F_%T` &gt;&gt; /data/test1.log endscript&#125;#手动转储，不等它满足条件才转储[root@ubuntu2204 ~]#logrotate /etc/logrotate.d/test1#查看结果[root@ubuntu2204 ~]#ll /var/log/test/总用量 2056drwxr-xr-x 2 600 root 4096 11月 18 12:22 ./drwxrwxr-x 12 root syslog 4096 11月 18 12:14 ../-rw-r----- 1 bin daemon 0 11月 18 12:22 test1.log #新生成文件准备转储新的日志-rw-r--r-- 1 root root 2097152 11月 18 12:21 test1.log.1 #旧的日志内容在这#添加日志[root@ubuntu2204 ~]#dd if=/dev/zero of=/var/log/test/test1.log bs=1M count=2#手动转储[root@ubuntu2204 ~]#logrotate /etc/logrotate.d/test1#观察结果，发现延迟压缩[root@ubuntu2204 ~]#ll /var/log/test/总用量 2060drwxr-xr-x 2 600 root 4096 11月 18 12:23 ./drwxrwxr-x 12 root syslog 4096 11月 18 12:14 ../-rw-r----- 1 bin daemon 0 11月 18 12:23 test1.log-rw-r----- 1 bin daemon 2097152 11月 18 12:23 test1.log.1 -rw-r--r-- 1 root root 2067 11月 18 12:21 test1.log.2.gz #之前旧的test1.log.1压缩[root@ubuntu2204 ~]#cat /data/test1.log2022-11-18_12:22:402022-11-18_12:23:07#修改全局配置[root@ubuntu2204 ~]#vim /etc/logrotate.conf#取消注释dateext#生成新日志[root@ubuntu2204 ~]#dd if=/dev/zero of=/var/log/test/test1.log bs=1k count=1025#使用全局配置[root@ubuntu2204 ~]#logrotate /etc/logrotate.conf#查看生成日志文件格式为时间后缀[root@ubuntu2204 ~]#ll /var/log/test/总计 2084drwxr-xr-x 2 root root 4096 5月 8 17:13 ./drwxrwxr-x 11 root syslog 4096 5月 8 17:00 ../-rw-r----- 1 bin daemon 1049600 5月 8 17:13 test1.log-rw-r----- 1 bin daemon 1049600 5月 8 17:10 test1.log.1-rw-r----- 1 bin daemon 1052 5月 8 17:12 test1.log-20230508.gz 3.4.4 对指定日志手动执行日志转储1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#生成测试日志[root@centos8 ~]#dd if=/dev/zero of=/var/log/test1.log bs=2M count=11+0 records in1+0 records out2097152 bytes (2.1 MB, 2.0 MiB) copied, 0.00291879 s, 719 MB/s[root@centos8 ~]#dd if=/dev/zero of=/var/log/test2.log bs=2M count=11+0 records in1+0 records out2097152 bytes (2.1 MB, 2.0 MiB) copied, 0.00200561 s, 1.0 GB/s#针对不同的日志创建转储配置文件#Ubuntu需加下面两行su bin syslogsharedscripts [root@centos8 ~]#cat /etc/logrotate.d/test1/var/log/test1.log &#123; daily rotate 5 compress delaycompress missingok size 1M notifempty create 640 bin daemon postrotate echo `date +%F_%T` &gt;&gt; /data/test1.log endscript&#125;[root@centos8 ~]#cat /etc/logrotate.d/test2/var/log/test2.log &#123; daily rotate 5 compress delaycompress missingok size 1M notifempty create 644 root root postrotate echo `date +%F_%T` &gt;&gt; /data/test2.log endscript &#125; #针对一个测试日志，手动执行日志转储[root@centos8 ~]#logrotate /etc/logrotate.d/test1 [root@centos8 ~]#ll /var/log/test*-rw-r----- 1 bin daemon 0 Dec 14 16:38 /var/log/test1.log-rw-r--r-- 1 root root 2097152 Dec 14 16:35 /var/log/test1.log.1-rw-r--r-- 1 root root 2097152 Dec 14 16:36 /var/log/test2.log[root@centos8 ~]#ls /datatest1.log[root@centos8 ~]#cat /data/test1.log2019-11-12_14:00:14#对所有日志进行手动转储[root@centos8 ~]#logrotate /etc/logrotate.conf[root@centos8 ~]#ll /var/log/test*-rw-r--r-- 1 bin daemon 0 Nov 12 14:00 /var/log/test1.log-rw-r--r-- 1 root root 2097152 Nov 12 13:59 /var/log/test1.log.1-rw-r--r-- 1 root root 0 Nov 12 14:01 /var/log/test2.log-rw-r--r-- 1 root root 2097152 Nov 12 13:59 /var/log/test2.log-20191112[root@centos8 ~]#ls /datatest1.log test2.log[root@centos8 ~]#cat /data/test1.log2019-11-12_14:01:51","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"计划任务","slug":"Linux/service-manage/plan-tasks","date":"2025-08-20T08:52:03.000Z","updated":"2025-09-09T06:02:49.343Z","comments":true,"path":"Linux/service-manage/plan-tasks/","permalink":"https://aquapluto.github.io/Linux/service-manage/plan-tasks/","excerpt":"","text":"1 一次性任务at 工具 由包 at 提供 依赖与atd服务,需要启动才能实现at任务 at队列存放在&#x2F;var&#x2F;spool&#x2F;at目录中,ubuntu存放在&#x2F;var&#x2F;spool&#x2F;cron&#x2F;atjobs目录下 执行任务时PATH变量的值和当前定义任务的用户身份一致 作业执行命令的结果中的标准输出和错误以执行任务的用户身份发邮件通知给 root 默认CentOS 8 最小化安装没有安装邮件服务,需要自行安装 at 命令： 123456789at [option] TIME-V 显示版本信息-t time 时间格式 [[CC]YY]MMDDhhmm[.ss]-l 列出指定队列中等待运行的作业；相当于atq-d N 删除指定的N号作业；相当于atrm-c N 查看具体作业N号任务-f file 指定的文件中读取任务-m 当任务被完成之后，将给用户发送邮件，即使没有标准输出 TIME：定义出什么时候进行 at 这项任务的时间 123456HH:MM [YYYY-mm-dd]noon：正午，中午12点midnight：午夜，晚上12点teatime：下午茶时间，下午4点tomorrow：明天now+#&#123;minutes,hours,days, OR weeks&#125; at 时间格式 12345678910111213#HH:MM 在今日的 HH:MM 进行，若该时刻已过，则明天此时执行任务02:00 #HH:MM YYYY-MM-DD 规定在某年某月的某一天的特殊时刻进行该项任务02:00 2016-09-20 #HH:MM[am|pm] [Month] [Date]06pm March 1717:20 tomorrow#HH:MM[am|pm] + number [minutes|hours|days|weeks]， 在某个时间点再加几个时间后才进行该项任务now + 5 min02pm + 3 days at 任务执行方式 交互式 输入重定向 at -f file &#x2F;etc&#x2F;at.{allow,deny} 控制用户是否能执行at任务 白名单：&#x2F;etc&#x2F;at.allow 默认不存在，只有该文件中的用户才能执行at命令 黑名单：&#x2F;etc&#x2F;at.deny 默认存在，拒绝该文件中用户执行at命令，而没有在at.deny 文件中的使用者则可执行 如果两个文件都不存在，只有 root 可以执行 at 命令 权限控制是allow的优先级更高，如果用户在allow 和 deny 中都存在，则是有权限执行的 范例 12345678910111213141516171819202122#在某个时间执行多个任务[root@centos ~]#systemctl start atd#添加今天16:03的定时任务[root@centos ~]#at 16:03at&gt; touch /root/at.txtat&gt; echo hello wprldat&gt; &lt;EOT&gt; #Ctrl+d结束job 7 at Sun Sep 17 16:03:00 2023[root@centos ~]#lsanaconda-ks.cfg apps at.log at.txt data dir motd_peiqi passwd pwd[root@centos ~]#cat at.txt You have new mail in /var/spool/mail/wu#五分钟之后执行[root@centos ~]#at now+5min#列出任务[root@centos ~]# at -l1 Sat May 20 00:05:00 2023 a root#查看任务具体内容[root@centos ~]# at -c 1 范例: ubuntu at任务存放路径 1234567891011121314151617#任务保存在这个目录中[root@ubuntu ~]# ls -l /var/spool/cron/atjobs/total 4-rwx------ 1 root daemon 2947 May 19 22:47 a0000201ac6709#查看文件，就是上面输入的内容[root@ubuntu ~]# cat /var/spool/at/a0000201ac6709#到时间查看执行结果[root@ubuntu ~]# ll /tmp/at-00-05#输出内容在邮件里面[root@ubuntu ~]# mail#原来生成的任务文件己经被删除了[root@ubuntu ~]# ls -l /var/spool/cron/atjobs/total 0 输入输出重定向 12345678910111213#创建[root@ubuntu ~]# echo reboot | at now+5minwarning: commands will be executed using /bin/shjob 11 at Sat May 20 00:13:00 2023#查看[root@ubuntu ~]# at -l11 Sat May 20 00:13:00 2023 a root#删除[root@ubuntu ~]# at -d 11[root@ubuntu ~]# at -l[root@ubuntu ~]# 定时任务一般都是要在将来的某个时间去执行，然后标准输出以及错误输出都不会写输出到终端，这是因为，在任务执行的时候，当前终端没有连接上来，或者不是创建任务的用户 2 周期性任务计划 croncron 守护进程每分钟都会自动检查 &#x2F;etc&#x2F;crontab 文件、etc&#x2F;cron.d&#x2F; 等目录中的改变。如果发现了改变，它们就会被载入内存。所以你如果修改了计划任务导致其crontab文件改变后，并不需要重启守护进程。 crontab的用户手册中推荐每一个命令使用绝对路径，例如调用rm命令时写作：&#x2F;bin&#x2F;rm，这是为了防止由于每一个用户的PATH环境变量不同而导致命令无法找到的错误。编写定时任务时，先在命令行上面执行一次，查看是否可以执行成功。 注意：计划任务不会在屏幕上打印，只会发邮件（需要开启邮件服务postfix），但是定时任务执行的结果我们通常是不需要的，不然系统会一直发送邮件信息日积月累会白白耗费磁盘空间。所以可以利用重定向，将定时任务执行的结果定向到空 &amp;&gt;/dev/null 周期性任务计划cron相关的程序包 程序包 描述 cronie 主程序包，提供crond守护进程及相关辅助工具 crontabs 包含CentOS提供系统维护任务 cronie-anacron cronie的补充程序，用于监控cronie任务执行状况，如:cronie中的任务在过去该运行的时间点未能正常运行，则anacron会随后启动一次此任务 cron 依赖于crond服务，确保crond守护处于运行状态。而cron任务分为两种 任务类型 配置文件 系统cron任务 系统维护作业，/etc/crontab 主配置文件， /etc/cron.d/ 子配置文件 用户cron任务 红帽系统保存在 /var/spool/cron/USERNAME，Ubuntu 系统存放在/var/spool/cron/crontabs/USERNAME，利用 crontab 命令管理 cron 程序计划任务日志 12红帽中的crontab任务日志放在var/log/cronubuntu中的crontab任务日志放在/var/log/syslog 2.1 系统cron计划任务crond任务相关文件 123456/etc/crontab #配置文件/etc/cron.d/ #配置文件/etc/cron.hourly/ #脚本/etc/cron.daily/ #脚本/etc/cron.weekly/ #脚本/etc/cron.monthly/ #脚本 /etc/crontab 格式说明 12345678910111213[root@centos8 ~]#cat /etc/crontabSHELL=/bin/bash #默认的SHELL类型PATH=/sbin:/bin:/usr/sbin:/usr/bin #默认的PATH变量值,可修改为其它路径MAILTO=root #默认标准输出和错误发邮件给root,可以指向其它用户# For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed 计划任务时间表示法 123456789101112131415161718192021(1)* 表示该位置上所有可以出现的值* * * * * #每分钟执行一次1 2 * * * #每天2时1分执行一次(2)N,N,... 表示该位置上多个值，离散取值1,3,5 2,4,6 * * * #表示第2，4，6 这三个小时中每个小时的第1,3,5 分的时候执行一次(3)N-N 表示范围取值1-5 2-6 * * * #表示第2到第6小时，每小时的第1到第5分每分钟执行一次(4)/N 表示频率，步长*/5 */6 * * * #表示每6小时，在该小时内，每5分钟执行一次(5)特定关健字@yearly #每年1月1日执行一次，相当于 0 0 1 1 *@annually #每年1月1日执行一次，相当于 0 0 1 1 *@monthly #每月1日执行一次，相当于 0 0 1 * *@weekly #每周日执行一次，相当于 0 0 * * 0@daily #每天0时执行一次，相当于 0 0 * * *@hourly #每小时0分执行一次，相当于 0 * * * *@reboot #重启后执行一次 2.2 用户计划任务crontab命令: 每个用户都有专用的cron任务文件：/var/spool/cron/USERNAME 如果想添加系统级的cron任务，写在此文件中/etc/crontab 控制用户执行计划任务：/etc/cron.&#123;allow,deny&#125;，ubuntu中无此文件 默认标准输出和错误会被发邮件给对应的用户，如用户名为wang创建的任务就发送至wang的邮箱 root能够修改其它用户的计划任务 用户的 cron 中默认 PATH=/usr/bin:/bin，所以使用命令的时候要查看是否在其路径下，不在的话要指明命令的路径，或者如果在任务文件的第一行加PATH=/path或者加入到计划任务执行的脚本中 第六个字段指定要运行的命令。 该行的整个命令部分，直至换行符或 “％” 字符，指定的shell执行，除非使用反斜杠（\\）进行转义，否则该命令中的 “％” 字符将变为换行符，并且第一个％之后的所有数据将作为标准输入发送到该命令。 运行结果的标准输出和错误以邮件通知给相关用户 cron任务中不建议使用%，它有特殊用途，它表示换行的特殊意义，且第一个%后的所有字符串会被将成当作命令的标准输入，如果在命令中要使用%，则需要用 \\ 转义。注意，将%放置于单引号中是不支持的 1234567crontab [-u user] [-l | -r | -e] [-i]-l 列出所有任务-e 编辑任务-r 移除所有任务-i 同-r一同使用，以交互式模式移除指定任务-u user 指定用户管理cron任务,仅root可运行 范例：修改默认的cron的文本编辑工具 1234567891011121314151617#Ubuntu默认的cron文本编辑器是nano可以修改为vimroot@ubuntu1804:~# crontab -eno crontab for root - using an empty oneSelect an editor. To change later, run &#x27;select-editor&#x27;. 1. /bin/nano &lt;---- easiest 2. /usr/bin/vim.basic 3. /usr/bin/vim.tiny 4. /bin/ed Choose 1-4 [1]:#如果没有在此处选择，可以写配置文件修改编辑工具[root@ubuntu ~]# echo &quot;export EDITOR=vim&quot; &gt;&gt; /etc/profile.d/env.sh[root@ubuntu ~]# . /etc/profile.d/env.sh[root@ubuntu ~]# cat /etc/profile.d/env.shexport EDITOR=vim 范例：PATH变量 在cron里面的PATH变量的路径很少，这就导致在计划任务里面，有其他路径的命令会报错，为了解决这一问题，有以下方法解决 12345678910111213#方法1,在计划任务配置中指定PATH[root@centos8 ~]#crontab -lPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin* * * * * useradd hehe;echo $PATH#方法2,在脚本中指定PATH变量[root@centos8 ~]#crontab -l* * * * * /data/test.sh[root@centos8 ~]#cat /data/test.sh#!/bin/bashPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/binuseradd heheecho $PATH 范例：磁盘检测 12345678910111213141516[root@centos8 ~]#cat /usr/bin/disk_check.sh#!/bin/bashPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/binWARNING=10# 第一种写法df | sed -En &#x27;/^\\/dev\\/sd/s@^([^ ]+).* ([0-9]+)%.*@\\1 \\2@p&#x27;| while read DEVICE USE;do [ $USE -gt $WARNING ] &amp;&amp; echo &quot;$DEVICE will be full,USE:$USE&quot; | mail -s diskfull rootdone# 第二种写法df | awk -F &#x27; +|%&#x27; &#x27;/^\\/dev\\/sd/&#123;print $1,$5&#125;&#x27;|while read DISK USE;doif [ $USE -gt $WARNING ];then echo &quot;$DISK will be full,use:$USE&quot; | mail -s diskwarning root@wangxiaochun.comfidone[root@centos8 ~]#crontab -l*/10 * * * * check_disk.sh 范例：查看用户文件和执行日志 123456789101112131415[root@centos ~]#crontab -e* * * * * echo $PATH &gt;&gt; /root/path.log[root@centos ~]#cat /var/spool/cron/root * * * * * echo $PATH &gt;&gt; /root/path.log #每分钟执行一次[root@centos ~]#tail -f /var/log/cron #查看是否有执行Sep 17 16:55:01 centos CROND[12590]: (root) CMD (echo $PATH &gt;&gt; /root/path.log)Sep 17 16:56:01 centos CROND[12609]: (root) CMD (echo $PATH &gt;&gt; /root/path.log)[root@centos ~]#lsanaconda-ks.cfg apps at2.txt at.log at.txt data dir motd_peiqi passwd path.log pwd[root@centos ~]#cat path.log /usr/bin:/bin 注意：运行结果的标准输出和错误以邮件通知给相关用户，如果不想有邮件，则可以在定时任务中加上重定向 12(1) COMMAND &gt; /dev/null(2) COMMAND &amp;&gt; /dev/null 范例： 在crontab中%的用法 1230 2 * * * /bin/cp -a /etc/ /data/etc`date +\\%F_\\%T`30 2 * * * /bin/cp -a /etc/ /data/etc`date +‘%F_%T’` #有问题 范例：交互式删除 123456789[root@ubuntu ~]# crontab -l* * * * * echo &quot;this is test cron from root&quot;* * * * * echo &quot;this is test cron222 from root&quot; &amp;&gt;/dev/null[root@ubuntu ~]# crontab -ircrontab: really delete root&#x27;s crontab? (y/n) y[root@ubuntu ~]# crontab -lno crontab for root 范例：重定向创建 1234567[root@ubuntu ~]# crontab -lno crontab for root[root@ubuntu ~]# echo @reboot echo &quot;this is test msg&quot; | crontab[root@ubuntu ~]# crontab -l@reboot echo this is test msg 2.3 定时任务示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#每分钟执行一次* * * * **/1 * * * *#每年2月14日凌晨2点执行00 02 14 2 *#每年6月的每个周五凌晨2点执行00 02 * 6 5#每年2月14日或2月的周日凌晨2点执行00 02 14 2 7#每年2月14日凌晨0点执行00 00 14 2 *#每年1月、5月、8月的每天凌晨2点执行00 02 * 1,5,8 *#每隔两天的8-11点，第3和15分钟执行3,15 8-11 */2 * * #23点到次日7点，每隔2小时执行一次0 23-7/2 * * *#周一到周五每天 21:15 执行15 21 * * 1-5#晚上9点10分运行echo命令,输出信息仍会发送到root 邮箱10 21 * * * wang /bin/echo &quot;Howdy!&quot;#半夜2点半执行脚本30 2 * * * /data/backup.sh#每3小时echo和wall命令0 */3 * * * wang /bin/echo “howdy”; wall “welcome to home!”#每周2和每周4的第1时，第3时，第5到8时，每5分钟执行一次*/5 1,3,5-8 * * 2,4#每月的1到10日，或每周1到周五的2时1分执行一次 1 2 1-10 * 1-5#每小时1-30分内，每5分钟执行一次1-30/5 * * * *#每周1,3,5执行* * * * 1,3,5#每个月的1号，10号，20号的半夜2点或者周六和周日的半夜2点每10分钟执行（因为1,10,20不一定是周六周日，会有冲突，所以是或者的关系）#要是想1,10,20号和周六周日是并且的关系，可以把0,6换成*，然后在脚本里面判断是否是周六周日*/10 2 1,10,20 * 0,6 2.4 白名单和黑名单当 /etc/cron.allow 和 /etc/cron.deny 两个文件同时存在时，系统首先检查 /etc/cron.allow 文件。 如果你的用户名在 /etc/cron.allow 文件中，你就可以使用cron配置定时任务。 如果你的用户名不在 /etc/cron.allow 文件中，系统将不会再检查 /etc/cron.deny 文件，因此你将无法使用cron。 总的来说，/etc/cron.allow 文件具有优先级，且只要你的用户名在该文件中，无论 /etc/cron.deny 的内容如何，你都将获得访问cron的权限。如果你没有在 /etc/cron.allow 文件中找到你的用户名，即使你的用户名也没有在 /etc/cron.deny 文件中，你也将不能够访问cron。 所以，在这两个文件同时存在的情况下，想要授权某个用户使用cron，你应在 /etc/cron.allow 文件中添加该用户的用户名。 相反，如果你想阻止某个用户访问cron, 你应确保该用户的用户名既不在 /etc/cron.allow 文件中，也在 /etc/cron.deny 文件中。 2.5 crontab不执行的问题123456789101112131415161718192021222324252627282930第一，脚本代码有问题，解决：先手动调试跑通 第二，执行环境问题：手动执行成功而crontab不能执行的时候，很可能就是执行环境的问题，例如相关路径的设置问题，可以在代码最前面执行 source /home/user/.bash_profile 第三，系统时间不正确。这种问题最好理解，也是比较常见和隐蔽的问题，解决方案：date -s ******** 第四，就是我们的脚本是否有可执行权限。必须保证执行脚本的用户有执行改文件的权限。 第五，crontab 守护进程死掉了。这种情况是极少发生的，但也不排除，当我们实在是找不到其他原因的时候可以用。解决方案：重启该进程。 第六，crontab不执行的问题困扰了好长时间，脚本写的都正确，但是就是不执行，最终解决方法如下：crontab -u root /var/spool/cron/root这样root用户的crontab就生效了[root@localhost ~]# systemctl restart crond重启下服务就好了 第七，crond没有启动 第八，脚本编码问题，脚本在window下编写，传到linux下后报“锘?!/bin/bash”，用vi编辑器新建新shell脚本，输入内容后保存。 第九：特殊符号无法识别，需要添加转义#例子* * * * * tar czf /tmp/`date &#x27;+%Y&#x27;` /etc 该计划任务中命令的执行流程是crond-&gt;tar命令，而crond在执行tar命令时，无法识别通配符%的意思（shell能识别），所以该命令无法正常执行 解决方案一：添加转义符号* * * * * tar czf /tmp/`date &#x27;+\\%Y&#x27;` /etc 解决方案二：直接将命令扔到脚本里通常都会把要执行的操作放到文件中，然后/bin/bash a.sh去执行，* * * * * /bin/bash a.sh ，这样的执行流程就变成了crond-&gt;bash shell-&gt;a.sh,这样a.sh内即便是写%号，也能被识别出来 2.6 注意问题使用脚本执行定时任务（只有一条简单命令的可以直接使用命令执行） 运行脚本一定要用绝对路径执行，并且最好统一脚本位置 定时任务中date命令的百分号需转义才能使用 命令或脚本结果(正确及错误)定向到空(&amp;&gt;&#x2F;dev&#x2F;null)或追加到文件中 &amp;&gt;&gt;&#x2F;tmp&#x2F;run.log 避免不必要的程序及命令输出,如打包命令，tar -v的显示过程的选项 打包压缩使用相对路径（切到目标目录的上一级打包目标，否则可能会带着一层目录） tar czf a.tar.gz /test1 cd /test1，tar czf ../b.tar.gz ./* 定时任务脚本中的程序文件，尽量用绝对路径（iptables命令在计划任务中必须写绝对路） 系统与命令位置有关的环境变量问题,建议脚本中重新定义环境变量PATH","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"域名DNS解析服务","slug":"Linux/service-manage/DNS","date":"2025-08-20T08:51:47.000Z","updated":"2025-09-08T14:33:07.808Z","comments":true,"path":"Linux/service-manage/DNS/","permalink":"https://aquapluto.github.io/Linux/service-manage/DNS/","excerpt":"","text":"一、DNS服务相关技术linux上的DNS文件&#x2F;etc&#x2F;hostshosts文件：直接定义域名与IP地址的对应关系，当主机访问某个域名时，会先从hosts文件中寻找与该域名对应的IP地址，如果找到则直接请求该IP地址，如果找不到才会将该域名提交DNS服务请求解析该域名对应的IP地址 远程登录linux主机过慢问题：有时客户端想远程登录一台linux主机，但每次登录输入密码后都会等很长一段时间才会进入，这是因为linux主机在返回信息时需要解析ip，如果在linux主机的hosts文件事先加入客户端的ip地址，这时再从客户端远程登录linux就会变很快。当然！这里所说的远程登录不仅仅是ssh登录，还可以是mysql远程登录，或是文件共享的查询等。 Windows系统中的hosts文件 1234%windir%\\System32\\drivers\\etc\\hosts# %windir% 是windows 系统中的环境变量写法，表示 Windows 安装目录，上述路径一般是C:\\Windows\\System32\\drivers\\etc Linux系统中的hosts文件 12345/etc/hosts #只适合机器少的场景，机器多就不适合用#格式122.10.117.2 www.magedu.org. [www]93.46.8.89 www.google.com. [google] 范例：实现域名解析 12345678[root@Rocky8 ~]#vim /etc/chrony.conf server ntp.wujl.org iburst[root@Rocky8 ~]#vim /etc/hosts10.0.0.183 ntp.wujl.org[root@Rocky8 ~]#ping ntp.wujl.orgPING ntp.wujl.org (10.0.0.183) 56(84) bytes of data. &#x2F;etc&#x2F;resolv.confresolv.conf文件：DNS客户机配置文件，用于设置DNS服务器的IP地址及DNS域名，还包含了主机的域名搜索顺序。该文件是由域名解析器（resolver，一个根据主机名解析IP地址的库）使用的配置文件。 作用 可以提供DNS服务器域名和IP地址，帮助解析 search选项可以补全短域名 假如resolv.conf没有任何配置并且网络没有配置DNS，你可能是这样的状态： 1234[root@nick ~]# ping www.baidu.comping: unknown host www.baidu.com连不通外网！！！ 我们可以往里面加一个域名服务器 1nameserver 114.114.114.114 DNS解析的步骤： 查找&#x2F;etc&#x2F;hosts 根据nameserver查找域名 如果在nameserver查找不到域名就进行search补全，重新走1~2步 1Client --&gt;hosts文件 --&gt; Client DNS Service Local Cache（缓存） --&gt; DNS Server (recursion递归) --&gt; DNS Server Cache --&gt; DNS iteration(迭代) --&gt; 根 --&gt; 顶级域名DNS --&gt;二级域名DNS… &#x2F;etc&#x2F;nsswitch.conf配置解析顺序，是先解析hosts文件，还是先解析DNS服务器 nsswitch.conf与系统获取解析的顺序有关。 123[root@f5ha.com ~]# vi /etc/nsswitch.conf #找到hosts关键字#hosts: db files nisplus nis dnshosts: files dns #此为默认配置 从配置文件就可以看出系统是先files（&#x2F;etc&#x2F;hosts）解析，再从dns（&#x2F;etc&#x2F;resolv.conf）解析。 &#x2F;etc&#x2F;hosts和DNS的优先级 123456789vim /etc/hosts10.0.0.8 www.baidu.comvim /etc/sysconfig/nerwork-scripts/ifcfg-eth0....DOMAIN=magedu.com....由于hosts比DNS优先级高，所以在/etc/hosts将www.baidu.com指向的IP地址为10.0.0.8后，去ping www.baidu.com得出的IP地址是10.0.0.8，就不是原先百度的地址 Ubuntu中的 systemd-resolved 服务在 ubuntu 系统中，虽然在网卡中配置了 DNS 服务器的IP地址，但在使用相关命令进行 DNS 解析时，默认的 DNS 服务器使用的是 127.0.0.53，而并不是我们在网卡上配置的DNS 服务器地址。 systemd-resolved 服务为本地应用程序提供了网络名字解析服务, 系统通过它对外进行 dns 请求 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@ubuntu ~]# cat /etc/resolv.conf......nameserver 127.0.0.53 #默认DNS 配置在此处options edns0 trust-adsearch magedu.com magedu.org#直接修改上述文件[root@ubuntu ~]# vim /etc/resolv.conf......#nameserver 127.0.0.53nameserver 223.6.6.6#测试[root@ubuntu ~]# nslookup www.magedu.comServer: 223.6.6.6Address: 223.6.6.6#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192#但是只要再次重启网络相关，该内容会被还原[root@ubuntu ~]# netplan apply[root@ubuntu ~]# cat /etc/resolv.conf | grep nameservernameserver 127.0.0.53#修改软链接文件指向,保证永久生效[root@ubuntu ~]# ll /etc/resolv.conflrwxrwxrwx 1 root root 39 Apr 21 2022 /etc/resolv.conf -&gt;../run/systemd/resolve/stub-resolv.conf[root@ubuntu ~]# rm -f /etc/resolv.conf[root@ubuntu ~]# ln -sv /run/systemd/resolve/resolv.conf /etc/resolv.conf&#x27;/etc/resolv.conf&#x27; -&gt; &#x27;/run/systemd/resolve/resolv.conf&#x27;[root@ubuntu ~]# ll /etc/resolv.conflrwxrwxrwx 1 root root 32 Jun 1 11:26 /etc/resolv.conf -&gt;/run/systemd/resolve/resolv.conf[root@ubuntu ~]# nslookup www.magedu.comServer: 223.6.6.6Address: 223.6.6.6#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192 设置全局DNS 1234567891011121314151617181920[root@ubuntu ~]# vim /etc/systemd/resolved.conf......DNS=223.5.5.5 223.6.6.6#重启服务[root@ubuntu ~]# systemctl restart systemd-resolved.service#查看[root@ubuntu ~]# cat /etc/resolv.conf......nameserver 223.5.5.5nameserver 223.6.6.6#测试[root@ubuntu ~]# nslookup www.magedu.comServer: 223.5.5.5Address: 223.5.5.5#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192 DNS服务器的分类 根DNS服务器：有13个，根域名服务器并不直接对域名进行解析，而是返回域名所属顶级域名的顶级域名服务器的IP地址 顶级域名DNS服务器：负责管理二级域名，即知道哪个域名服务器管理着二级域名 主&#x2F;权限&#x2F;授权DNS服务器：负责管理某个区的域名。他里面存的解析记录并不是从别人那里缓存到本地的，而是实打实配置存到自己机器上的，从这里拿到的记录具有权威性 从DNS服务器：又称之为辅助dns，这是一个备份服务器，从这里拿到的记录也具有权威性 缓存DNS服务器：不负责本地解析，采用递归方式转发客户机查询请求，并返回结果给客户机的DNS服务器，同时缓存查询回来的结果 转发器：这台DNS发现非本机负责的请求后，不再向根发起请求，而是直接转发给指定的一台或多台服务器，自身并不保存查询结果 DNS解析答案 肯定答案：存在对应的查询结果 否定答案：请求的条目不存在等原因导致无法返回结果 权威答案：直接由存有此查询结果的DNS服务器（权威服务器）返回的答案 非权威答案：由其它非权威服务器返回的查询答案 DNS解析的优先级 用浏览器访问：Chrome DNS 缓存 &gt; HOSTS文件 &gt; 系统DNS缓存 &gt; DNS服务器 不用浏览器访问：HOSTS文件 &gt; 系统DNS缓存 &gt; DNS服务器 DNS资源记录具体解析规则 /etc/bind/db.* 该文件定义域名的具体解析规则，该文件有多条资源记录组成，每一行都是一条资源记录，在RFC文档中，DNS解析记录被称为Resource Recode（资源记录），缩写为 RR 资源记录定义1NAME TTL CLASS TYPE VALUE NAME：资源记录名称，根据TYPE不一样，写法会有不同 TTL：缓存有效期，默认单位是秒 其他单位M(分)，H(时)， D(天)，W(周) 开头写，全局继承：$TTL 1D CLASS：资源记录类别，最常用IN（第一条写IN后，后面的可以省略，默认继承上一条） TYPE：解析记录类型 VALUE：值是由TYPE（类型）决定的 注意： TTL可从全局继承 使用 “@” 符号可用于引用当前区域的域名 同一个名字可以通过多条记录定义多个不同的值；此时DNS服务器会以轮询方式响应 同一个值也可能有多个不同的定义名字；通过多个不同的名字指向同一个值进行定义；此仅表示通过多个不同的名字可以找到同一个主机 解析记录类型SOA记录起始授权记录；一个区域解析库有且仅能有一个SOA记录，必须位于解析库的第一条记录，用于于设置当前DNS服务器的某些规则，规定主服务器，即在众多NS记录里哪一台才是主要的服务器 SOA 记录表示此DNS是该域名的权威解析服务器，当在查询的过程中，各级缓存都没有要查询的内容时，最后会通过递归查询的方式到达此DNS服务器，并请求此域名的SOA记录 1234567891011121314151617magedu.org. 86400 IN SOA ns.magedu.org. nsadmin.magedu.org. ( 2015042201 ; 2H ; 10M ; 1W ; 1D ; )#也可以这么写magedu.org. 86400 IN SOA ns.magedu.org. nsadmin.magedu.org. (2015042201 2H 10M 1W 1D)ns.magedu.org. #DNS服务器名称nsadmin.magedu.org. #服务器管理员邮箱2015042201 #版本号2H #从服务器更新间隔10M #失败重试间隔1W #从服务器数据失效时长1D #无效记录缓存时长 VALUE 字段中从左到右具体内容如下 字段 说明 DNS服务器名称 描述性字段，表示当前DNS服务器名称，注意最后要加 “.” 服务器管理员邮箱 邮箱中的@要写成 . 当前数据库的版本号 主从服务器要同步数据，此字段就是数据更新的标识，判断数据库有无发生变化，记住改数据库文件后也要改这个版本号，不然主服务器不会推新的数据给从服务器 从服务器拉取数据的时间间隔 从服务器在主服务器拉数据的周期 从服务器同步失败后重试时间间隔 上次同步失败后，间隔多久重试，假如从服务器在主服务器拉数据时恰好网断了，可以重试 从服务器同步失败超过多长时间从服务器失败 同步失败时长超过此值，则认为从服务器数据无效，假如网断了，从服务器迟迟和主服务器连接不上，这时主服务器已经数据更新，从服务器还是老旧的数据，避免这种情况，设置过期时间，超了这个时间，从服务器还是不能与主服务器同步，就不能对外提供服务 不存在的记录缓存时长 当查询一个不存在的解析记录时，该记录在指定时间内直接返回不存在，假如有用户查询不存在的东西，那么将这个不存在的东西缓存下来，让用户在一段时间内，如果他查的是不存在的记录，直接结果就是不存在，就不需要去查询，消耗服务器的资源 注意： 当前区域的主DNS服务器的FQDN，也可以使用当前区域的名字，只是注释功能，可以不需要配置对应的NS记录和A记录 当前区域管理员的邮箱地址；但地址中不能使用@符号，一般用 . 替换，例如：admin.magedu.org 主从服务区域传输相关定义以及否定的答案的统一的TTL NS记录NS记录和SOA记录是任何一个DNS区域都不可或缺的两条记录，NS记录也叫名称服务器记录，用于说明这个区域有哪些DNS服务器负责解析，即说明了在这个区域里，有多少个服务器来承担解析的任务 一般来说，为了服务的安全可靠，一个域名，至少应该有两条NS记录，保证服务的冗余，防止出现单点失败，即规定从服务器 注意： 相邻的两个资源记录的name相同时，后续的可省略 对NS记录而言，任何一个NS记录后面的服务器名字，都应该在后续有一个A记录 一个区域可以有多个NS记录 多个从服务器就多条NS记录 从服务器的名字可以随便，但是最后都要解析成A记录 123456# 比如你想查找www.linux-magedu.com，然后linux-magedu.com这个域的解析工作是由dns1.linux-magedu.com和dns2.linux-magedu.com负责的，这时你就知道去dns1.linux-magedu.com服务器或者dns2.linux-magedu.com服务器上去继续查询www.linux-magedu.com，然后这个域名的服务器会向你返回www.linux-magedu.comlinux-magedu.com. 86400 IN NS dns1.linux-magedu.com.linux-magedu.com. 86400 IN NS dns2.linux-magedu.com.dns1.linux-magedu.com. 86400 IN A 10.0.0.206dns2.linux-magedu.com. 86400 IN A 10.0.0.208 A记录把域名解析为IP，例:www.egonlin.com IN A 1.1.1.1 避免用户写错名称时给错误答案，可通过泛域名解析进行解析至某特定地址 1234567891011121314www.magedu.org. IN A 1.1.1.1www.magedu.org. IN A 2.2.2.2 # 如果有多条A记录，且有A记录的IP与DNS机器IP相同，则优先返回mx1.magedu.org. IN A 3.3.3.3mx2.magedu.org. IN A 4.4.4.4$GENERATE 1-254 HOST$ IN A 1.2.3.$*.magedu.org. IN A 5.5.5.5 # 泛解析，匹配所有以magedu.org结束的域名或主机名magedu.org. IN A 6.6.6.6linux-magedu.com. 86400 IN A 10.0.0.167@ 86400 IN A 10.0.0.167 # @代表域名，此条记录含义同上 范例：阿里云 AAAA记录将域名转为IPv6地址 CNAME记录把一个域名解析为另外一个域名 1www.magedu.org. IN CNAME websrv.magedu.org. PTR记录反向DNS解析，即将IPv4或IPv6地址映射回域名 注意：网络地址及后缀可省略；主机地址依然需要反着写 1234567#A记录www.magedu.org. IN A 1.2.3.4#与其对应的PTR记录4.3.2.1.in-addr.arpa. IN PTR www.magedu.org.#如1.2.3为网络地址，可简写成：4 IN PTR www.magedu.org. MX记录用于邮件交换，指定邮件服务器，将一个域的电子邮件定向到托管该域用户帐号的服务器(SMTP服务器) 比如 A 用户向 B 用户发送一封邮件，那么他需要向DNS查询 B 的MX记录，DNS在定位到了 B 的MX记录后反馈给A 用户，然后 A 用户把邮件投递到B用户的MX记录服务器里 一个域可以定义多条MX记录，但每条MX记录的优先级不同，如果邮件通过最高优先级记录无法递送，则采用第二优先级，以此类推。 注意： 一个区域内，MX记录可有多个；但每个记录的value之前应该有一个数字(0-99)，表示此服务器的优先级；数字越小优先级越高 对MX记录而言，任何一个MX记录后面的服务器名字，都应该在后续有一个A记录 1234magedu.org. IN MX 10 mx1.magedu.org. IN MX 20 mx2.magedu.org.mx1 A 10.0.0.100mx2 A 10.0.0.200 TXT记录对域名进行标识和说明的一种方式，一般做验证记录时会使用此项 1_dnsauth.linux-magedu.com. 86400 IN TXT 2024dtetmvzwclwf6wsl0y6jcpvwga2wkibgyb1a103yd7re2 互联网域名域名注册 代理商：万网, 新网, godaddy 注册完成以后，想自己用专用服务来解析 管理后台：把NS记录指向的服务器名称，和A记录指向的服务器地址 范例：阿里云DNS管理后台界面 HttpDNSHttpDNS是使用HTTP协议向DNS服务器的80端口进行请求，代替传统的DNS协议向DNS服务器的53端口进行请求。也就是使用Http协议去进行DNS解析请求，DNS服务器返回的解析结果（域名对应的服务器IP），直接向该IP发起对应的API服务请求，代替使用域名。 HttpDNS的原理非常简单，主要有两步： 客户端直接访问HttpDNS接口，获取业务在域名配置管理系统上配置的访问延迟最优的IP。（基于容灾考虑，还是保留次选使用运营商LocalDNS解析域名的方式） 客户端向获取到的IP后就向直接往此IP发送业务协议请求。以Http请求为例，通过在header中指定host字段，向HttpDNS返回的IP发送标准的Http请求即可。 bind 是一款实现DNS服务的开放源码软件，能够提供双向解析，转发，子域授权，view 等功能，使用广泛，目前Internet上半数以上的DNS服务器都是由Bind来实现的 二、DNS软件bind能实现DNS功能的软件有很多，像 bind，powerdns，dnsmasq，unbound，coredns等 bind 是一款实现DNS服务的开放源码软件，能够提供双向解析，转发，子域授权，view 等功能，使用广泛，目前Internet上半数以上的DNS服务器都是由Bind来实现的 CentosBIND相关程序包 bind：服务器 bind-utils: 客户端 bind-libs：相关库,依赖关系自动安装 bind-chroot: 安全包，将dns相关文件放至 &#x2F;var&#x2F;named&#x2F;chroot&#x2F; 安装bind软件 1[root@centos8 ~]#dnf -y install bind bind-utils 启动服务 named会开启TCP和UDP的53端口， 953端口是给管理工具使用的 1[root@centos ~]# systemctl enable --now named 将其它机器的DNS指向本机 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113[root@rocky8 ~]#hostname -I10.0.0.179[root@centos8 ~]#vim /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.176PREFIX=24GATEWAY=10.0.0.2DNS1=10.0.0.179ONBOOT=yes[root@centos8 ~]#nmcli con reload [root@centos8 ~]#nmcli con up eth0 [root@centos8 ~]#cat /etc/resolv.conf # Generated by NetworkManagernameserver 10.0.0.179#失败了，显示不可达[root@centos8 ~]#dig www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.11.26-RedHat-9.11.26-6.el8 &lt;&lt;&gt;&gt; www.baidu.com;; global options: +cmd;; connection timed out; no servers could be reached#原因是rocky上udp/53指向的是127.0.0.1，而centos的DNS指向的是10.0.0.179[root@rocky8 ~]#ss -ntluNetid State Recv-Q Send-Q Local Address:Port Peer Address:Port Process udp UNCONN 0 0 127.0.0.1:53 0.0.0.0:* #解决方法就是将rocky的udp/53指向所有[root@rocky8 ~]#vim /etc/named.confoptions &#123; listen-on port 53 &#123; 127.0.0.1;10.0.0.179; &#125;; #第一种，注意要加“;”，但不推荐，万一本机的IP地址变了就不适用了 options &#123; listen-on port 53 &#123; localhost; &#125;; #第二种，localhost代表了本机所有的IPoptions &#123;// listen-on port 53 &#123; 127.0.0.1; &#125;; #第三种，加注释，由于是C语言风格，所以加//#重启[root@rocky8 ~]#rndc reloadserver reload successful[root@rocky8 ~]#ss -ntluNetid State Recv-Q Send-Q Local Address:Port Peer Address:Port Process udp UNCONN 0 0 10.0.0.179:53 0.0.0.0:* #失败，显示拒绝[root@centos8 ~]#dig www.baidu.com.....;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: REFUSED, id: 4293 #显示拒绝......;; SERVER: 10.0.0.179#53(10.0.0.179) #但是能连上10.0.0.179.....#原因如下[root@rocky8 ~]#vim /etc/named.confoptions &#123; ..... allow-query &#123; localhost; &#125;; #只允许本机IP查询#修改options &#123; ..... allow-query &#123; localhost;10.0.0.0/24; &#125;; #第一种，允许本机和10网段查询，不推荐 options &#123; ..... allow-query &#123; localhost;any; &#125;; #第二种，允许本机和其他所有机器查询 options &#123; .....// allow-query &#123; localhost; &#125;; #第三种，默认允许所有 [root@rocky8 ~]#rndc reloadserver reload successful#成功[root@centos8 ~]#dig www.baidu.com......;; ANSWER SECTION:www.baidu.com. 1200 IN CNAME www.a.shifen.com.www.a.shifen.com. 120 IN A 183.2.172.42www.a.shifen.com. 120 IN A 183.2.172.185......#如果将53/udp禁掉[root@rocky8 ~]#iptables -A INPUT -p udp --dport 53 -j REJECT#这个时候就失败了[root@centos8 ~]#dig www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.11.26-RedHat-9.11.26-6.el8 &lt;&lt;&gt;&gt; www.baidu.com;; global options: +cmd;; connection timed out; no servers could be reached#如果将53/tcp禁掉[root@rocky8 ~]#iptables -R INPUT 1 -p tcp --dport 53 -j REJECT#不影响，这说明了udp53端口做解析用的[root@centos8 ~]#dig www.baidu.com......;; ANSWER SECTION:www.baidu.com. 1200 IN CNAME www.a.shifen.com.www.a.shifen.com. 120 IN A 183.2.172.42www.a.shifen.com. 120 IN A 183.2.172.185......#如果将953端口禁掉[root@rocky8 ~]#iptables -R INPUT 1 -p tcp --dport 953 -j REJECT#管理工具会使用失败[root@rocky8 ~]#rndc reloadrndc: connect failed: 127.0.0.1#953: connection refused 自己就是DNS服务器，将DNS指向自己 1234567891011121314151617181920212223242526272829#现在指向的是10.0.0.2和100.76.76.76[root@rocky8 ~]#cat /etc/resolv.conf# Generated by NetworkManagernameserver 10.0.0.2nameserver 100.76.76.76#修改[root@rocky8 ~]#vim /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0NAME=eth0BOOTPROTO=noneIPADDR=10.0.0.179PREFIX=24GATEWAY=10.0.0.2DNS1=127.0.0.1ONBOOT=yes[root@rocky8 ~]#nmcli con reload[root@rocky8 ~]#nmcli con up eth0[root@rocky8 ~]#cat /etc/resolv.conf# Generated by NetworkManagernameserver 127.0.0.1#能ping成功，理论上只要安装了DNS服务，就能有解析服务[root@rocky8 ~]#ping www.baidu.comPING www.a.shifen.com (183.2.172.185) 56(84) bytes of data.64 bytes from 183.2.172.185 (183.2.172.185): icmp_seq=1 ttl=128 time=8.33 ms64 bytes from 183.2.172.185 (183.2.172.185): icmp_seq=2 ttl=128 time=7.95 ms bind己经内置了13个根域名服务器地址 12[root@centos ~]# cat /var/named/named.ca[root@ubuntu ~]# cat /usr/share/dns/root.hints 范例: DNS客户端相关库 12345678910111213[root@centos8 ~]#ping www.baidu.comPING www.a.shifen.com (110.242.68.4) 56(84) bytes of data.64 bytes from 110.242.68.4 (110.242.68.4): icmp_seq=1 ttl=128 time=10.9 ms64 bytes from 110.242.68.4 (110.242.68.4): icmp_seq=2 ttl=128 time=10.5 ms^C--- www.a.shifen.com ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 3msrtt min/avg/max/mdev = 10.539/10.698/10.857/0.159 ms[root@centos8 ~]#ldd `which ping` | grep libresolv.so libresolv.so.2 =&gt; /lib64/libresolv.so.2 (0x00007f230739b000) [root@centos8 ~]#ldd `which curl` |grep libresolv.so libresolv.so.2 =&gt; /lib64/libresolv.so.2 (0x00007fe95048a000) BIND包相关文件 BIND主程序：/usr/sbin/named 服务脚本和Unit名称：/etc/rc.d/init.d/named，/usr/lib/systemd/system/named.service 主配置文件：/etc/named.conf, /etc/named.rfc1912.zones, /etc/rndc.key 管理工具：/usr/sbin/rndc，remote name domain controller，默认与bind安装在同一主机，且只能通过127.0.0.1连接named进程，提供辅助性的管理功能；953&#x2F;tcp 解析库文件：/var/named/ZONE_NAME.ZONE 注意 一台物理服务器可同时为多个区域提供解析 必须要有根区域文件；named.ca 应该有两个（如果包括ipv6的，应该更多）实现localhost和本地回环地址的解析库 主配置文件 &#x2F;etc&#x2F;named.conf1234567891011121314[root@rocky8 ~]#vim /etc/named.conf options &#123; .... directory &quot;/var/named&quot;; #域名数据库文件存放路径 dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; secroots-file &quot;/var/named/data/named.secroots&quot;; recursing-file &quot;/var/named/data/named.recursing&quot;; ....&#125;;........include &quot;/etc/named.rfc1912.zones&quot;;include &quot;/etc/named.root.key&quot;; 配置 配置字段 说明 全局配置 options{}; 全局配置选项 日志子系统配置 logging{}; 运行日志 区域定义 zone “ZONE_NAME” IN {}; 定义了要解析的域名与具体解析规则之间的对应关系，本机能够为哪些zone进行解析，就要定义哪些zone 注意： 任何服务程序如果期望其能够通过网络被其它主机访问，至少应该监听在一个能与外部主机通信的 IP地址上缓存名称服务器的配置：监听外部地址即可 dnssec: 建议关闭dnssec，设为no 常用全局配置选项 12345678910111213141516options &#123; #此配置表示DNS服务只监听了本机127.0.0.1的53端口，如果对外提供DNS服务，可以将此行注释或值改成any listen-on port 53 &#123; 127.0.0.1; &#125;; #监听IPV6的53端口，配置方法同上 listen-on-v6 port 53 &#123; ::1; &#125;; #监听本机所有IPV6地址，不想监听IPV6地址，可以将 any 改成 none listen-on-v6 &#123; any; &#125;; #此配置表示仅本机可以使用DNS服务的解析查询功能，如果对外提供DNS服务，可以将此行注释或值改成any allow-query &#123; localhost; &#125;; #是否启用加密验证，在使用转发的时候，将此项改为 no dnssec-validation auto; #转发服务器 forwarders &#123; 10.0.0.207; &#125;; #转发策略,具体见后续章节 forward first;&#125;; 中间配置文件 &#x2F;etc&#x2F;named.rfc1912.zones 定义了要解析的域名与具体解析规则之间的对应关系，本机能够为哪些zone进行解析，就要定义哪些zone 将自己要配置的域名和域名数据库之间的关联关系都放在这个文件当中，最好不要放在 &#x2F;etc&#x2F;named.conf，这样子不好管理 12345678[root@rocky8 ~]#vim /etc/named.rfc1912.zones ......zone &quot;localhost&quot; IN &#123; #IN 可以省略不写 type master; #类型 master,slave 用于表示DNS主服务器或者从服务器,forward表示转发 file &quot;named.localhost&quot;; #具体解析规则文件路径（也就是域名数据库文件） allow-update &#123; none; &#125;;&#125;;...... 具体解析规则 &#x2F;var&#x2F;etc&#x2F;*该文件定义域名的具体解析规则，该文件有多条资源记录组成，每一行都是一条资源记录，在RFC文档中，DNS解析记录被称为Resource Recode（资源记录），缩写为 RR UbuntuBIND相关程序包 bind9：服务器 bind9-utils: 客户端 bind9-libs：相关库,依赖关系自动安装 bind9-chroot: 安全包，将dns相关文件放至 &#x2F;var&#x2F;named&#x2F;chroot&#x2F; 安装bind软件 1[root@ubuntu2004 ~]#apt -y install bind9 bind9-utils 启动服务 named会开启TCP和UDP的53端口， 953端口是给管理工具使用的 1[root@ubuntu ~]# systemctl enable --now named.service BIND包相关文件1234567891011121314[root@ubuntu bind]# dpkg -L bind9....../etc/bind/etc/bind/bind.keys/etc/bind/db.0 #db.* 名具体解析规则文件/etc/bind/db.127/etc/bind/db.255/etc/bind/db.empty/etc/bind/db.local/etc/bind/named.conf #主配置文件/etc/bind/named.conf.default-zones #中间配置文件，该文件中定义了域名和具体解析规则文件的对应关系/etc/bind/named.conf.local #中间配置文件，引用/etc/bind/zones.rfc1918，被注释/etc/bind/named.conf.options #bind配置项/etc/bind/zones.rfc1918 #中间配置文件，该文件中定义了域名和具体解析规则文件的对应关系 主配置文件 &#x2F;etc&#x2F;bind&#x2F;named.conf12345[root@ubuntu bind]# cat /etc/bind/named.conf......include &quot;/etc/bind/named.conf.options&quot;;include &quot;/etc/bind/named.conf.local&quot;;include &quot;/etc/bind/named.conf.default-zones&quot;; 选项配置文件 &#x2F;etc&#x2F;bind&#x2F;named.conf.options该文件主要包括以下几部份内容，默认只有全局配置部份 配置 配置字段 备注 全局配置 options{}; 全局配置选项 日志子系统配置 logging{}; 运行日志 网络自定义集合 acl 将某个网段或某个具体IP地址定义在一个集合里面 视图 view 配合acl将不同的请求来源用不同的解析规则返回，实现智能DNS 常用全局配置选项 12345678910111213141516options &#123; #此配置表示DNS服务只监听了本机127.0.0.1的53端口，如果对外提供DNS服务，可以将此行注释或值改成any listen-on port 53 &#123; 127.0.0.1; &#125;; #监听IPV6的53端口，配置方法同上 listen-on-v6 port 53 &#123; ::1; &#125;; #监听本机所有IPV6地址，不想监听IPV6地址，可以将 any 改成 none listen-on-v6 &#123; any; &#125;; #此配置表示仅本机可以使用DNS服务的解析查询功能，如果对外提供DNS服务，可以将此行注释或值改成any allow-query &#123; localhost; &#125;; #是否启用加密验证，在使用转发的时候，将此项改为 no dnssec-validation auto; #转发服务器 forwarders &#123; 10.0.0.207; &#125;; #转发策略,具体见后续章节 forward first;&#125;; 中间配置文件 &#x2F;etc&#x2F;bind&#x2F;named.conf.default-zones该文件中定义了要解析的域名与具体解析规则之间的对应关系 1234zone &quot;ZONE_NAME&quot; IN &#123; #IN 可以省略不写 type &#123;master|slave|hint|forward&#125;; #类型 master,slave 用于DNS主从,forward表示转发 file &quot;file_path&quot;; #具体解析规则文件路径&#125;; 具体解析规则 &#x2F;etc&#x2F;bind&#x2F;db.*该文件定义域名的具体解析规则，该文件有多条资源记录组成，每一行都是一条资源记录，在RFC文档中，DNS解析记录被称为Resource Recode（资源记录），缩写为 RR allow 访问控制指令在named配置中有四个allow开头的字段，主要用来实现访问控制 配置 说明 allow-query{}; 允许查询本DNS的主机，白名单，注释就代表所有主机都可使用本机当DNS allow-transfer{}; 允许区域传送的主机，白名单，注释代表所有，一般用在主从DNS配置时指定从节点 allow-recursion{}; 允许递归的主机,建议全局使用 allow-update{}; 允许可以远程更新解析规则的主机 三、DNS测试和管理工具dig 命令dig只用于测试dns系统，不会查询本地 hosts文件中定义的域名和IP对应关系 1[root@ubuntu ~]# apt install bind9 123456789101112131415161718192021222324252627282930313233343536373839dig [@global-server] [domain] [q-type] [q-class] &#123;q-opt&#125; &#123;global-d-opt&#125; host [@local-server] &#123;local-d-opt&#125; [ host [@local-server] &#123;local-d-opt&#125; [...]] dig [-t type] name [@SERVER] [query options]#参数说明@global-server #指定DNS服务器domain #要查询的域名q-type #要查询的记录类型(a,any,mx,ns,soa,hinfo,axfr,txt,...)，默认aq-class #要查询的解析类型(in|hs|ch)，默认 ind-opt #查询选项 +[no]trace #是否追踪查询过程 +[no]cmd #是否在查询结果中显示头信息 +[no]recurse #是否进行递归解析查询 +[no]all #是否显示所有信息，如果否，要指明具体显示内容 +[no]answer #是否显示answer部份 +[no]question #是否显示question部份 +[no]authority #是否显示authority部份 +[no]comment #是否显示comment部份 +[no]stat #是否显示status部份 +[no]short #是否只显示关键信息q-opt #选项 -h #显示帮助 -v #显示版本号 -4 #仅查询IPV4的DNS服务器 -6 #仅查询IPV6的DNS服务器 -b address[#port] #使用指定客户端IP去查询DNS -f filename #从文件中获取要查询的域名 -p port #指定DNS服务查询端口 -t type #指定要查询的资源记录类型A|NS|AAA|PTR|... -u #以微秒显示打印时间 -x dot-notation #反向解析 #常用组合dig domaindig @dns-erver domain | dig domain @dns-serverdig -t q-type domain | dig domain q-typegit -x IP | dig -t ptr reverseIP.in-addr.arpa #reverseIP 表示将要查询的IP倒序输出 范例：查询DNS解析，使用默认DNS服务器 1234567891011121314151617181920212223242526272829303132333435363738394041[root@ubuntu ~]# dig www.jose-404.com; &lt;&lt;&gt;&gt; DiG 9.18.12-0ubuntu0.22.04.1-Ubuntu &lt;&lt;&gt;&gt; www.jose-404.com #dig命令版本和参数，查询参数为www.jose-404.com;; global options: +cmd #默认选项，此项表示显示头部软件版本和参数信息#查询结果;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2948 #QUERY 表示是执行查询操作，NOERROR 表示解析成功，id: 12947 此次查询的ID，在dns协议中，通过ID编号匹配查询请求和返回结果;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1#flags: qr rd ra 标志位#qr(query，查询标志，代表查询操作)#rd(recursion desired, 表示客户端希望进行递归查询)#ra(recursive available, 表示DNS服务器支持递归查询)#aa(Authoritative Answer, 权威回复，如果查询结果由管理域名的域名服务器而不是缓存服务器提供的，则称为权威回复)#QUERY: 1 查询数，表示1个查询，对应下面 QUESTION SECTION中的记录数#ANSWER: 13 查询结果，表示有13个查询结果，对应下面 ANSWER SECTION 中的记录数#AUTHORITY: 0 权威域名服务器记录数量，此处表示有0个权威域名服务器#ADDITIONAL: 1 额外记录数量，此处表示有1个额外记录，此处缺失该部份内容#选项;; OPT PSEUDOSECTION:#EDNS: Extended DNS 扩展用户数据报文协议#version:0 协议版本为 0#flag:; 标记位为空#udp:65494 数据包大小; EDNS: version: 0, flags:; udp: 65494#查询域名，此处表示查www.jose-404.com;; QUESTION SECTION:;www.jose-404.com. IN A#具体查询结果;; ANSWER SECTION:www.jose-404.com. 1 IN A 47.94.245.255#第一列是要要询的域名#第二列是TTL(time to live),表示该记录的缓存时间，单位是秒#第三列是要查询的信息类型，IN代表类别为IP协议，即Internet#第四列是要查询的记录类型，NS表示name server，即域名服务器#第五列表示查询得到的值;; Query time: 15 msec #本次查询消耗时长;; SERVER: 127.0.0.53#53(127.0.0.53) (UDP) #DNS服务器为10.0.0.2 端口是53;; WHEN: Tue May 30 16:46:53 CST 2023 #查询时间;; MSG SIZE rcvd: 61 #返回内容长度为61字节 范例：指定DNS服务器，指定本机请求DNS服务的IP 1[root@ubuntu ~]# dig www.jose-404.com @114.114.114.114 -b 10.0.0.206 范例：反向解析 123#两种方法[root@rocky86 ~]# dig -x 47.94.245.255 +nocmd[root@ubuntu ~]# dig -t ptr 255.245.94.47.in-addr.arpa +nocmd 范例：短格式 12[root@ubuntu ~]# dig www.jose-404.com +short47.94.245.255 范例：从文件中获取要查询的域名 123456789101112[root@ubuntu ~]# cat domain.txtwww.baidu.comwww.jd.com[root@ubuntu ~]# dig -f domain.txt +shortwww.a.shifen.com.124.237.176.4124.237.176.3www.jd.com.gslb.qianxun.com.www.jd.com.s.galileo.jcloud-cdn.com.wwwv6.jcloudimg.com.111.225.218.3 范例：只查询别名解析 12345[root@ubuntu ~]# dig -t cname www.jd.com......[root@ubuntu ~]# dig www.jd.com in cname..... 范例：模拟区域传送 12345dig -t axfr ZONE_NAME @SERVERdig -t axfr magedu.org @10.10.10.11dig –t axfr 100.1.10.in-addr.arpa @172.16.1.1dig -t NS . @114.114.114.114dig -t NS . @a.root-servers.net host命令host 命令可以根据域名查询得到对应的服务器IP地址，不会查询本地 hosts文件中定义的域名和IP对应关系 1234567891011121314151617host [option] hostname [server]a #显示所有信息-c #指定查询类型 HS|CH|IN-C #查询SOA-d #同 -v-p #指定端口-r #不递归查询-t #指定查询类型 CNAME|NS|SOA|TXT|DNSKEY|AXFR|...-T #使用TCP进行DNS查询-U #使用UDP进行DNS查询-v #显示执行过程-V #显示命令版本-w #如果没有查询结果，则阻塞，一直等待-W N #等待N秒后超时-4 #仅查询IPV4的DNS server-6 #仅查询IPV4的DNS server 范例 1234567[root@ubuntu ~]# host www.magedu.comwww.magedu.com has address 10.0.0.206[root@ubuntu ~]# host www.baidu.comwww.baidu.com is an alias for www.a.shifen.com.www.a.shifen.com has address 124.237.176.3www.a.shifen.com has address 124.237.176.4 范例：指定DNS服务器 123456[root@ubuntu ~]# host www.magedu.com 114.114.114.114Using domain server:Name: 114.114.114.114Address: 114.114.114.114#53Aliases:www.magedu.com has address 140.143.156.192 范例 12345host -t NS magedu.org 172.16.0.1host -t soa magedu.orghost -t mx magedu.orghost -t axfr magedu.orghost 1.2.3.4 nslookup命令nslookup：主要用来查询DNS记录，查看域名解析是否正常，也可用来诊断网络问题 支持交互式和非交互式两种执行方式，在Windows系统中和Linux系统中都可以使用 不会查询本地 hosts文件中定义的域名和IP对应关系，也不能查询dns的递归或者迭代 123nslookup [-option] [name | -] [server]-type #指定查询类型 A|AAAA|CNAME|... 范例：交互式模式 1234nslookup&gt;server IP: 指明使用哪个DNS server进行查询set q=RR_TYPE: 指明查询的资源记录类型NAME: 要查询的名称 范例：非交互式查询 1234567[root@ubuntu ~]# nslookup www.magedu.comServer: 127.0.0.53Address: 127.0.0.53#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192 范例：交换式查询 12345678[root@ubuntu ~]# nslookup&gt; www.magedu.comServer: 127.0.0.53Address: 127.0.0.53#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192 范例：指定DNS服务器 1234567891011121314151617181920212223#非交互式[root@ubuntu ~]# nslookup www.magedu.com 114.114.114.114Server: 114.114.114.114Address: 114.114.114.114#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192#交互式[root@ubuntu ~]# nslookup&gt; server 223.5.5.5Default server: 223.5.5.5Address: 223.5.5.5#53&gt; www.magedu.com;; communications error to 223.5.5.5#53: timed outServer: 223.5.5.5Address: 223.5.5.5#53Non-authoritative answer:Name: www.magedu.comAddress: 140.143.156.192 范例：指定查询类型 1[root@ubuntu ~]# nslookup -type=cname www.baidu.com 范例：查看默认信息 12[root@ubuntu ~]# nslookup&gt; set all 范例：Windows系统中使用 1234567891011C:\\Users\\44301&gt;nslookup默认服务器: xd-cache-1.bjtelecom.netAddress: 219.141.136.10&gt; www.magedu.com服务器: xd-cache-1.bjtelecom.netAddress: 219.141.136.10非权威应答:名称: www.magedu.comAddress: 140.143.156.192 rndc 命令rndc 是 bind 程序的客户端工具，默认使用 TCP的 953 端口连接 bind 服务器，进行管理DNS 12345678910111213rndc COMMANDCOMMAND: status: 查看状态 reload: 重载主配置文件和区域解析库文件 reload zonename: 重载区域解析库文件 retransfer zonename: 手动启动区域传送，而不管序列号是否增加 notify zonename: 重新对区域传送发通知 reconfig: 重载主配置文件 querylog: 开启或关闭查询日志文件/var/log/message trace: 递增debug一个级别 trace LEVEL: 指定使用的级别 notrace：将调试级别设置为 0 flush：清空DNS服务器的所有缓存记录 重启主配置文件和区域解析库文件 12#相当于systemctl restart named，使用前提是named服务已经开启[root@centos ~]#rndc reload whois 命令whois 命令可以查询域名注册信息 范例: whois 查询域名信息 12[root@centos7 ~]#yum -y install whois[root@centos7 ~]#whois magedu.com 可以从网站查询信息,查询链接 1https://www.toolnb.com/domaininfo/wangxiaochun.com.html 四、在公有云上配置DNS解析要在互联网上运行一个可以被访问的项目，我们至少需要一个域名，一个服务器，一个固定IP地址。 目前国内主流的公有云平台包括阿里云，腾迅云，华为云，亚马逊云(AWS) 等。 这些平台都提供云主机，域名注册与解析等服务，过程也大相径庭。 我们以阿里云平台为例，描述一下在公有云平台上配置域名解析的过程 打开阿里云官网 https://account.aliyun.com/ 当我们己经购买了域名和主机后，就可以开始配置域名解析工作了 在左侧控制台中，选择 云解析DNS，进入 域名解析 页面 在列表中选择要进行解析的域名，点击操作列中的 解析设置 点击 添加记录 参数说明 参数 说明 值 记录类型 解析类型，默认选A，将域名解析到服务器IP A 解析请求来源 指定只响应符合来源的解析请求 默认值 记录值 填写主机域名对应的服务器的IP地址 47.92.245.255 TTL 解析结果在本地DNS缓存中的生命周期时长 10分钟 测试解析状态 123456[root@ubuntu ~]# ping www.jose-404.com -c1PING www.jose-404.com (47.94.245.255) 56(84) bytes of data.64 bytes from 47.94.245.255 (47.94.245.255): icmp_seq=1 ttl=128 time=10.2 ms--- www.jose-404.com ping statistics ---1 packets transmitted, 1 received, 0% packet loss, time 0msrtt min/avg/max/mdev = 10.248/10.248/10.248/0.000 ms","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"shell脚本介绍","slug":"Linux/shell/introduce","date":"2025-08-20T08:50:45.000Z","updated":"2025-09-09T09:58:15.688Z","comments":true,"path":"Linux/shell/introduce/","permalink":"https://aquapluto.github.io/Linux/shell/introduce/","excerpt":"","text":"1 脚本基本结构格式要求：首行shebang机制 12345#!/bin/bash#!/usr/bin/python#!/usr/bin/perl#!/usr/bin/ruby#!/usr/bin/lua 2 脚本创建过程第一步：使用文本编辑器来创建文本文件 第一行必须包括shell声明序列：#! 添加注释,注释以#开头 第二步：加执行权限 给予执行权限，在命令行上指定脚本的绝对或相对路径 第三步：运行脚本 直接运行解释器，将脚本作为解释器程序的参数运行 3 脚本注释规范1、第一行一般为调用使用的语言 2、程序名，避免更改文件名为无法找到正确的文件 3、版本号 4、更改后的时间 5、作者相关信息 6、该程序的作用，及注意事项 7、最后是各版本的更新简要说明 范例：初始化操作 123456789101112131415161718192021222324[root@centos7 ~]#vim .vimrc #个人[root@centos7 ~]#vim /etc/.vimrc #全局set ts=4set expandtabset ignorecaseset shiftwidth=4autocmd BufNewFile *.sh exec &quot;:call SetTitle()&quot;func SetTitle() if expand(&quot;%:e&quot;) == &#x27;sh&#x27; call setline(1,&quot;#!/bin/bash&quot;) call setline(2,&quot;#&quot;) call setline(3,&quot;#*******************************************#&quot;) call setline(4,&quot;#Author: wangxiaochun&quot;) call setline(5,&quot;#QQ 999999999999&quot;) call setline(6,&quot;#Date: &quot;.strftime(&quot;%Y-%m-%d&quot;)) call setline(7,&quot;#FileName: &quot;.expand(&quot;%&quot;)) call setline(8,&quot;#URL: http://www.baidu.com&quot;) call setline(9,&quot;Description: The test script&quot;) call setline(10,&quot;Copyright (C): &quot;.strftime(&quot;%Y&quot;).&quot; All rights reserved&quot;) call setline(11,&quot;#******************************************#&quot;) call setline(12,&quot;&quot;) endifendfuncautocmd BufNewFile * normal G 4 执行脚本的方式直接执行 1bash hello.sh 重定向执行 12cat /data/hello.sh | bashbash &lt; /data/hello.sh 添加权限后以路径执行 1234chmod +x /data/hello.sh绝对路径：/data/scripts hello.sh相对路径：./hello.sh 软链接执行 123echo $PATH #查看PATH下的目录路径ln -s /data/scripts/hello.sh /user/local/bin/ #创建软链接hello.sh #直接执行 共享网站上执行远程主机的脚本 1curl (-s) http://www.wangxiaochun.com/testdir/hello.sh | bash 在远程主机运行本地shell脚本 1ssh 10.0.0.18 /bin/bash &lt; test.sh 5 脚本错误语法错误，会导致后续的命令不继续执行，可以用 bash -n 检查错误，提示的出错行数不一定是准确的 123456789#!bin/bashecho startingifecho continue#cmd1是错误命令，不可以执行后续命令[root@centos scripts]#bash text.sh startingtext.sh: line 7: syntax error: unexpected end of file 命令错误，默认后续的命令还会继续执行，用bash -n 无法检查出来 ，可以使用 bash -x 进行观察 12345678910#!bin/bashecho startingcmd1echo continue#虽然cmd1是错误命令，但还是可以执行后续命令[root@centos scripts]#bash text.sh startingtext.sh: line 5: cmd1: command not foundcontinue 逻辑错误：例如单词写错了，shell不会认为错误，你输什么执行什么，只能使用 bash -x 进行观察 有一些错误是因为不可见字符导致的，可以set line 查看并删掉 Windows和Linux换行符的错误：Windows 行尾符（CR+LF），这在 Unix&#x2F;Linux 环境中会导致语法错误 12345dos2unix database-bakup.shsed &#x27;s/\\r$//&#x27; database-bakup.sh &gt; database-bakup.sh.fixedmv database-bakup.sh.fixed database-bakup.shtr -d &#x27;\\r&#x27; &lt; database-bakup.sh &gt; database-bakup.sh.fixedmv database-bakup.sh.fixed database-bakup.sh 6 脚本安全set 命令实现脚本安全（在工作中为了安全脚本加上set -e -u） 1234567-u 在扩展一个没有设置的变量时，显示错误信息， 等同set -o nounset-e 如果一个命令返回一个非0退出状态值(失败)就退出， 等同set -o errexit-o option 显示，打开或者关闭选项 显示选项：set -o 打开选项：set -o 选项 关闭选项：set +o 选项-x 当执行命令时，打印命令及其参数,类似 bash -x $- 变量 变量名 说明 h（hashall） Shell会将命令所在的路径hash下来，避免每次都要查询。通过 set +h 将 h 选项关闭 i（interactive-comments） 包含这个选项说明当前的 shell 是一个交互式的 shell m（monitor） 打开监控模式，就可以通过 Job control 来控制进程的停止、继续，后台或者前台执行等 B（braceexpand） 大括号扩展 H（history） H选项打开，可以展开历史列表中的命令，可以通过 ! 来完成，例如 “!!” 返回上最近的一个历史命令，“!n” 返回第 n 个历史命令 范例 12345678910111213141516171819#关闭hash缓存功能[root@centos8 ~]#echo $-himBHs[root@centos8 ~]#set +h[root@centos8 ~]#echo $-imBHs[root@centos8 ~]#hash-bash: hash: hashing disabled#关闭大括号展开[root@centos8 ~]#echo &#123;1..10&#125;1 2 3 4 5 6 7 8 9 10[root@centos8 ~]#echo $-imBHs[root@centos8 ~]#set +B[root@centos8 ~]#echo $-imHs[root@centos8 ~]#echo &#123;1..10&#125;&#123;1..10&#125; 范例：限制使用没声明的变量 123456789[root@ubuntu2204 ~]# cat set2.sh#!/bin/bashset -uDIR=/testrm -rf $&#123;DIr&#125;/* #这个变量其实不存在，如果没有 set -u 选项，则会删除根目录rm -rf /*[root@ubuntu2204 ~]# bash set2.shset2.sh: line 17: DIr: unbound variable 范例：遇到错误终止 123456789101112[root@ubuntu2204 ~]# vim set3.sh#!/bin/bashset -eecho 123cd /test2/ #这个目录不存在，如果没有 set -e，则也会删根rm -rf *echo 456#遇到错误行就终止[root@ubuntu2204 ~]# bash set3.sh123set3.sh: line 17: cd: /test2/: No such file or directory 7 bash shell的配置文件7.1 生效范围分类全局配置：针对所有用户皆有效 123/etc/profile/etc/profile.d/*.sh/etc/bashrc 个人配置：只针对特定用户有效 12~/.bash_profile~/.bashrc 7.2 功能分类profile类为交互式登录的shell提供配置 用于定义环境变量 运行命令或脚本 bashrc类：为非交互式和交互式登录的shell提供配置 定义命令别名和函数 定义本地变量 7.3 配置文件生效修改profile和bashrc文件后需生效两种方法: 重新启动shell进程 source| . 配置文件 注意：source 会在当前shell中执行脚本,所有一般只用于执行置文件，或在脚本中调用另一个脚本的场景 1. ~/.bashrc 7.4 bash退出任务保存在 ~/.bash_logout 文件中（用户），在退出登录shell时运行 功能： 创建自动备份 清除临时文件","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"}]},{"title":"时间同步服务","slug":"Linux/service-manage/time-synchronization","date":"2025-08-20T08:10:20.000Z","updated":"2025-09-09T06:21:25.737Z","comments":true,"path":"Linux/service-manage/time-synchronization/","permalink":"https://aquapluto.github.io/Linux/service-manage/time-synchronization/","excerpt":"","text":"1 前言加密和安全当前都离不开时间的同步，否则各种网络服务可能不能正常运行 多主机协作工作时，各个主机的时间同步很重要，时间不一致会造成很多重要应用的故障，如：加密协议，日志，集群等， 利用NTP（Network Time Protocol） 协议使网络中的各个计算机时间达到同步。目前NTP协议属于运维基础架构中必备的基本服务之一 时间同步软件实现： ntp：使用渐进性同步机制，如果本地时间与标准时间相差较大，则需要一定的时间才能同步完成 chrony 2 一次性同步时间范例： ntpdate 一次性同步时间（centos7） 12345678910[root@centos7 ~]#date -s &#x27;-1 year&#x27; #修改时间Sat May 25 16:26:13 CST 2019[root@centos7 ~]#dateSat May 25 16:26:14 CST 2019[root@centos7 ~]#ntpdate ntp.aliyun.com #网上搜时间同步服务器网站25 May 16:26:30 ntpdate[24545]: step time server 203.107.6.88 offset31622399.992886 sec[root@centos7 ~]#dateMon May 25 16:26:34 CST 2020 范例： rdate 一次性同步时间 123456[root@centos7 ~]#yum -y install rdate[root@centos7 ~]#date -s &#x27;1 year&#x27;Tue Jan 18 21:33:13 CST 2022[root@centos7 ~]#rdate -s -u time.nist.gov[root@centos7 ~]#dateMon Jan 18 21:33:22 CST 2021 3 chrony实现NTP协议的软件，可使系统时钟与NTP服务器，参考时钟（例如GPS接收器）以及使用手表和键盘的手动输入进行同步，还可以作为NTPv4（RFC 5905）服务器和对等体运行，为网络中的计算机提供时间服务。 chrony传统的ntpd（也是实现NTP协议的软件）的优势 更快的同步只需要数分钟而非数小时时间，从而最大程度减少了时间和频率误差，对于并非全天24 小时运行的虚拟计算机而言非常有用 能够更好地响应时钟频率的快速变化，对于具备不稳定时钟的虚拟机或导致时钟频率发生变化的节能技术而言非常有用 在初始同步后，它不会停止时钟，以防对需要系统时间保持单调的应用程序造成影响 在应对临时非对称延迟时（例如，在大规模下载造成链接饱和时）提供了更好的稳定性 无需对服务器进行定期轮询，因此具备间歇性网络连接的系统仍然可以快速同步时钟 官方网站：https://chrony.tuxfamily.org/ 官方文档：https://chrony.tuxfamily.org/documentation.html 3.1 chrony文件组成包：chrony 主要程序：chronyd 和 chronyc 前者是后台运行的守护进程，用于调整内核中运行的系统时钟和时钟服务器同步。它确定计算机增减时间的比率，并对此进行补偿 后者是命令行用户工具，用于监控性能并进行多样化的配置。它可以在chronyd实例控制的计算机上工作，也可在一台不同的远程计算机上工作 配置文件：/etc/chrony.conf 服务unit 文件：/usr/lib/systemd/system/chronyd.service 监听端口： 12服务端: 123/udp #其它机器通过此端口连接本机，将本机当作ntp服务器客户端: 323/udp #本机通过此端口同步时间 3.2 配置文件chrony.conf官方文档，常用字段说明 字段 说明 server 可用于时钟服务器，iburst选项表示当服务器可达时，发送一个八个数据包而不是通常的一个数据包，包间隔通常为2秒,作用是加速与时间服务器的同步过程 pool 该指令的语法与server 指令的语法相似，不同之处在于它用于指定NTP服务器池而不是单个NTP服务器。池名称应解析为随时间可能会变化的多个地址 driftfile 根据实际时间计算出计算机增减时间的比率，将它记录到一个文件中，会在重启后为系统时钟作出补偿 rtcsync 启用内核模式，系统时间每11分钟会拷贝到实时时钟（RTC） allow &#x2F; deny 指定一台主机、子网，或者网络以允许&#x2F;拒绝访问本服务器 cmdallow &#x2F; cmddeny 指定哪台主机可以&#x2F;不可以通过chronyd使用控制命令 bindcmdaddress 允许chronyd监听哪个接口来接收由chronyc执行的命令 makestep 通常chronyd将根据需求通过减慢或加速时钟，使得系统逐步纠正所有时间偏差。在某些特定情况下，系统时钟可能会漂移过快，导致该调整过程消耗很长的时间来纠正系统时钟。该指令强制chronyd在调整期大于某个阀值时调整系统时钟 local stratum 10 即使server指令中时间服务器不可用，也允许将本地时间作为标准时间授时给其它客户端 3.3 NTP客户端工具chronycchronyc 可以运行在交互式和非交互式两种方式，支持以下子命令 12345678910help #命令可以查看更多chronyc的交互命令accheck #检查是否对特定主机可访问当前服务器activity #显示有多少NTP源在线/离线sources [-v] #显示当前时间源的同步信息sourcestats [-v] #显示当前时间源的同步统计信息add server #手动添加一台新的NTP服务器clients #报告已访问本服务器的客户端列表delete #手动移除NTP服务器或对等服务器settime #手动设置守护进程时间tracking #显示系统时间信息 范例 12345678910111213141516[root@ubuntu ~]# chronycchronyc&gt; trackingReference ID : CA701FC5 (dns2.synet.edu.cn) #当前同步的NTP服务器ID和IP地址Stratum : 2 #层次，跳数Ref time (UTC) : Tue May 30 01:00:39 2023 #源最后一次获取到的UTC时间System time : 0.000000001 seconds fast of NTP time #当前系统时间与NTP服务时间的偏移量Last offset : +0.000733123 seconds #最后偏移上次时钟更新时本地偏移量RMS offset : 0.000733123 seconds #偏移量平均值Frequency : 12.340 ppm fast #系统时钟偏差值的速率，单位为百万分之一Residual freq : +588.341 ppm #当前源的剩余频率Skew : 9.803 ppm #估计误差范围，单位为百万分之一Root delay : 0.022467736 seconds #到根设备的网络延迟总和Root dispersion : 0.003619912 seconds #到根设备的网络延迟平均值Update interval : 0.0 seconds #最近两次时钟更新之间的间隔Leap status : Normal #跳跃状态 范例：列出配置中所有ntp服务源的状态 123456789101112131415161718192021chronyc&gt; sourcestatsName/IP Address NP NR Span Frequency Freq Skew Offset Std Dev==============================================================================prod-ntp-4.ntp4.ps5.cano&gt; 8 4 520 +4.121 108.727 -32ms 8633usalphyn.canonical.com 10 7 527 +1.811 20.994 +1955us 2343usprod-ntp-5.ntp1.ps5.cano&gt; 10 5 463 -5.653 24.302 -36ms 2702uspugot.canonical.com 11 6 527 +2.124 7.392 -30ms 1047usntp1.flashdance.cx 10 6 526 +0.550 31.722 +1606us 3371usntp.wdc2.us.leaseweb.net 11 5 529 -3.635 17.252 -3281us 2297usdns2.synet.edu.cn 12 7 525 -0.105 12.072 -2299ns 1467ustick.ntp.infomaniak.ch 12 9 527 +8.028 28.147 +3958us 3535us#字段说明Name/IP Address #NTP服务器IP地址或主机名，或者参考时钟的refid值NP #当前服务器可用的采样点，用这些点执行线性回归方法来估算偏移值NR #最后一次回归计算后具有相同符号的偏差值的运行次数Span #最旧样本和最新样本之间的间隔，默认单位秒Frequency #NTP服务器的估算偏差值的速率，单位为百万分之一Freq Skew #Freq的估计误差范围，单位为百万分之一Offset #NTP源服务器的偏移量Std Dev #估算的样本标准偏差 范例：检查同步情况 123456789101112131415[root@centos7 ~]#chronyc sources -vMS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^* dns2.synet.edu.cn 1 6 377 58 +1648us[+2216us] +/- 13ms^- tick.ntp.infomaniak.ch 1 6 377 56 +111us[ +111us] +/- 82ms#字段说明M #NTP源 ^表示服务器, = 表示二级时钟, # 表示本地时钟S #NTP源状态，*此源己同步, +可接收的源, -合并算法排除的可接受源, ?没连上的源, x认为该源有错, ~不确定的源Name/IP address #NTP服务器主机名或IP地址或refid值Stratum #层次，跳数，1 表示本地时钟，2 表示通过第一层级的服务器实现同步，以此类推Poll #NTP源的轮询频率，以秒为单位，值为基数2的对数，6表示64秒进行一次同步，chronyd会自动调整此值Reach #8进制数，表示源的可达性，每次对钟收发8个数据包，337表示最后一次同步8个数据包都收到，377二进制就是8个1LastRx #多久前从源收到最后一次数据，默认单位是秒Last sample #上次同步时NTP服务器与本地时间的偏移值 调整后偏移量[实际偏移量]实际测量中的误差范围,+表示正偏移，本地快 修改配置文件 1234567[root@ubuntu ~]# vim /etc/chrony/chrony.conf#注释pool行#添加下列行server ntp.aliyun.com iburst#重启服务[root@ubuntu ~]# systemctl restart chronyd.service 修改时间，并查看同步过程 1234567891011121314151617181920212223242526272829#先重启一下服务，才可以很快的观察到过程，不然要等很久，因为其实质是渐进性同步[root@ubuntu ~]# systemctl restart chrony.service[root@ubuntu ~]# date +%F-%T2023-05-30-10:43:23[root@ubuntu ~]# date -s &quot;-1 day&quot;Mon May 29 10:43:29 AM CST 2023[root@ubuntu ~]# date +%F-%T2023-05-29-10:43:30#查看同步过程[root@ubuntu ~]# chronyc sourcesMS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^? 203.107.6.88 0 6 17 - +0ns[ +0ns] +/- 0ns#己经测得差了1天MS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^? 203.107.6.88 2 6 37 25 -86400s[-86400s] +/- 24ms#此次同步完成MS Name/IP address Stratum Poll Reach LastRx Last sample ===============================================================================^* 203.107.6.88 2 6 377 43 +139us[+1222us] +/- 24ms[root@ubuntu ~]# date &quot;+%F %T&quot;2023-05-30 10:50:02 chrony 是渐进式同步，如果差距过大，想立即同步完成，则可以重启服务 1234567[root@ubuntu ~]# date +%F-%T2023-05-29 10:50:02[root@ubuntu ~]# systemctl restart chronyd.service[root@ubuntu ~]# date +%F-%T2023-05-30 10:50:02 3.4 公共NTP服务器 类别 服务器地址 &#x2F; 域名 说明 阿里云公共 NTP ntp.aliyun.com 适用于 Unix&#x2F;Linux 系统 ntp1-7.aliyun.com 适用于 Unix&#x2F;Linux 系统（多节点） time.pool.aliyun.com 适用于 Windows 系统 腾讯公共 NTP time1-5.cloud.tencent.com 腾讯提供的公共 NTP 服务器 大学 NTP 服务 s1a.time.edu.cn 北京邮电大学提供 s1b.time.edu.cn 清华大学提供 s1c.time.edu.cn 北京大学提供 国家授时中心 210.72.145.44 中国国家授时中心服务器 美国标准技术院 time.nist.gov 美国国家标准与技术研究院（NIST）提供 4 时间管理工具12345678910111213141516timedatectl [OPTIONS...] COMMAND ...#常用选项-h|--help #显示帮助信息--version #显示版本信息-a|--all #显示所有属性--value #查询时仅显示值，不显示字段标题#常用子命令status #显示当前时间设置，默认项show #以友好格式显示，具体同容同 statusset-time TIME #修改时间set-timezone ZONE #修改时区list-timezones #列出当前可用时区set-local-rtc BOOL #RPC时间是否关联本地时区set-ntp BOOL #是否开启ntp 服务 范例 123456789101112131415161718#查看日期时间、时区及NTP状态：timedatectl#查看时区列表：timedatectl list-timezones#修改时区：timedatectl set-timezone Asia/Shanghai#修改时区root@ubuntu2004:~# rm -f /etc/localtimeroot@ubuntu2004:~# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime#修改日期时间：timedatectl set-time &quot;2017-01-23 10:30:00&quot;#开启NTP：timedatectl set-ntp true/false 范例 123456789101112[root@ubuntu ~]# timedatectl Local time: Tue 2023-05-30 10:54:31 CST #本机时间, CST表示北京时间 Universal time: Tue 2023-05-30 02:54:31 UTC #世界标准时间，UTC表示世界标准时间 RTC time: Tue 2023-05-30 02:54:32 #RTC时间，硬件时间 Time zone: Asia/Shanghai (CST, +0800) #本机时区System clock synchronized: yes #系统时间是否己同步完成 NTP service: active #NTP时间同步服务是否启用 RTC in local TZ: no #RTC时间是否关联本机时区 #RTC Real-Time Clock 硬件时间，来自于时钟芯片#UTC Coordinated Universal Time 世界协调时间，又称世界标准时间#GMT Greenwich Mean Time 格林尼治(天文台)标准时间 范例：开启RTC时间与本地时区绑定 1234567891011#开启RTC时间与本地时区一致[root@ubuntu ~]# timedatectl set-local-rtc 1[root@ubuntu ~]# timedatectl Local time: Tue 2023-05-30 11:06:33 CST Universal time: Tue 2023-05-30 03:06:33 UTC RTC time: Tue 2023-05-30 11:06:32 #与本地时间一致 Time zone: Asia/Shanghai (CST, +0800)System clock synchronized: yes NTP service: active RTC in local TZ: yes 范例：查看 12345678[root@ubuntu ~]# timedatectl show -aTimezone=Asia/ShanghaiLocalRTC=noCanNTP=yesNTP=yesNTPSynchronized=noTimeUSec=Wed 2042-10-15 00:00:18 CSTRTCTimeUSec=Wed 2042-10-15 00:00:18 CST 范例：开启ntp时间同步服务 12345678910111213141516[root@ubuntu ~]# systemctl is-active chronyd.serviceinactive[root@ubuntu ~]# timedatectl set-ntp 1[root@ubuntu ~]# systemctl is-active chronyd.serviceactive#NTP服务开启，时间被同步回来[root@ubuntu ~]# timedatectl Local time: Tue 2023-05-30 11:25:52 CST Universal time: Tue 2023-05-30 03:25:52 UTC RTC time: Tue 2023-05-30 03:25:52 Time zone: Asia/Shanghai (CST, +0800) System clock synchronized: yes NTP service: active RTC in local TZ: no 5 实现私有的NTP服务器 在同一个网络内，如果有多个需要进行时间同步的服务器，则我们可以在内网自建NTP Server，这样可以节约访问外网的网络资源；另一方面，如果外网不可用，则至少可以保证，内网的NTP服务还是可用的。 原理：有两台服务器作为客户端，去同步国内的NTP服务器，同时作为时间同步服务器，为企业其他服务器提供时间同步服务 范例：假设10.0.0.183作为企业内部的时间同步服务器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[root@centos7 ~]#yum -y install chrony[root@centos7 ~]#vim /etc/chrony.confserver 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst#查询到该ip地址是国外的，不靠谱，要用中国的[root@centos7 ~]#ping 0.centos.pool.ntp.orgPING 0.centos.pool.ntp.org (193.182.111.14) 56(84) bytes of data. #修改为国内的时间同步服务器[root@centos7 ~]#vim /etc/chrony.conf server ntp.aliyun.com iburstserver time1-5.cloud.tencent.com iburstserver s1b.time.edu.cn iburst[root@centos7 ~]#systemctl restart chronyd[root@centos7 ~]#ps aux | grep chronychrony 40167 0.0 0.0 117808 1644 ? S 22:05 0:00 /usr/sbin/chronydroot 40181 0.0 0.0 112812 980 pts/0 S+ 22:06 0:00 grep --color=auto chrony#多了323端口，就是作为客户端，与阿里云那些服务器同步，但是不能作为服务器给别的客户端连[root@centos7 ~]#ss -ntlu Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port udp UNCONN 0 0 127.0.0.1:323 *:* #只能自己跟别的服务器连 udp UNCONN 0 0 [::1]:323 [::]:* tcp LISTEN 0 128 *:22 *:* tcp LISTEN 0 100 127.0.0.1:25 *:* tcp LISTEN 0 128 [::]:22 [::]:* tcp LISTEN 0 100 [::1]:25 [::]:* [root@centos7 ~]#chronyc sources -v^* 203.107.6.88 2 6 377 48 -950us[-1642us] +/- 32ms #代表已同步^? 202.112.1.34 0 8 0 - +0ns[ +0ns] +/- 0ns #不可到达#设置作为服务器，允许哪些客户端可以和它同步[root@centos7 ~]#vim /etc/chrony.conf # Allow NTP client access from local network.allow 0.0.0.0/0 #允许任何网段进行同步[root@centos7 ~]#systemctl restart chronyd#服务启动后会打开端口123/udp[root@centos7 ~]#ss -ntlu Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port udp UNCONN 0 0 *:123 *:* #可以让别人连接自己[root@centos7 ~]#date -s &#x27;2 year&#x27; #假装时间错误Sun Oct 12 22:32:19 CST 2025[root@centos7 ~]#dateSun Oct 12 22:32:53 CST 2025#客户端配置[root@Rocky8 ~]#vim /etc/chrony.conf #现在将10.0.0.183作为该服务器的时间同步服务器server 10.0.0.183 iburst[root@Rocky8 ~]#systemctl restart chronyd #跟随10.0.0.183的时间[root@Rocky8 ~]#date 2025年 10月 12日 星期日 22:35:44 CST[root@centos7 ~]#systemctl restart chronyd [root@centos7 ~]#chronyc sources -v#恢复原来时间[root@centos7 ~]#date Thu Oct 12 22:36:35 CST 2023[root@Rocky8 ~]#systemctl restart chronyd [root@Rocky8 ~]#chronyc sources -v[root@Rocky8 ~]#date2023年 10月 12日 星期四 22:38:42 CST#假如10.0.0.183和互联网断开了，就会影响内部服务器的同步时间，所以需要修改文件[root@centos7 ~]#vim /etc/chrony.conf #删除此行注释,当互联网无法连接,仍然可以为客户端提供时间同步服务local stratum 10","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"}]},{"title":"正则表达式","slug":"Linux/basics/regular-expression","date":"2025-08-19T06:03:03.000Z","updated":"2025-09-06T11:16:46.592Z","comments":true,"path":"Linux/basics/regular-expression/","permalink":"https://aquapluto.github.io/Linux/basics/regular-expression/","excerpt":"","text":"1 基本正则表达式正则表达式匹配的是文本中的字符串，与文件通配符不一样 正则表达式要用双引号或者单引号引起来 1.1 字符匹配123456789101112131415161718192021. #匹配任意单个字符(除了\\n)，可以是一个汉字或其它国家的文字，如果要表示圆点字符本身，需要用反斜刚“\\”转义，或者在中括号的内部，也表示圆点.这个字符本身[] #匹配指定范围内的任意单个字符，示例：[wang] [0-9] [a-z] [a-zA-Z]；一些字符在[]中表示本身，就不用\\转义[^] #匹配指定范围外的任意单个字符,示例：[^wang][:alnum:] #字母和数字[:alpha:] #代表任何英文大小写字符，亦即 A-Z, a-z[:lower:] #小写字母,示例:[[:lower:]],相当于[a-z][:upper:] #大写字母[:blank:] #空白字符（空格和制表符）[:space:] #包括空格、制表符(水平和垂直)、换行符、回车符等各种类型的空白,比[:blank:]包含的范围广[:cntrl:] #不可打印的控制字符（退格、删除、警铃...）[:digit:] #十进制数字[:xdigit:]#十六进制数字[:graph:] #可打印的非空白字符[:print:] #可打印字符[:punct:] #标点符号-----------------\\s #匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [\\f\\r\\t\\v]。注意 Unicode正则表达式会匹配全角空格符\\S #匹配任何非空白字符。等价于 [^\\f\\r\\t\\v]\\w #匹配一个字母,数字,下划线,汉字,其它国家文字的字符，等价于[_[:alnum:]字]\\W #匹配一个非字母,数字,下划线,汉字,其它国家文字的字符，等价于[^_[:alnum:]字]\\d #匹配数字 范例 123456789101112131415161718192021[root@centos8 ~]#ls /etc/ | grep &#x27;rc[.0-6]&#x27;[root@centos8 ~]#ls /etc/ | grep &#x27;rc[.0-6].&#x27;rc0.drc1.drc2.drc3.drc4.drc5.drc6.drc.drc.local[root@centos8 ~]#ls /etc/ | grep &#x27;rc[0-6]&#x27;[root@centos8 ~]#ls /etc/ | grep &#x27;rc[.0-6]\\.&#x27;rc0.drc1.drc2.drc3.drc4.drc5.drc6.d 范例 12^ab.[0-9]$ -- 表示ab开头，中间任意1个字符后面是0-9任意一个数字^[a-z].&#123;3&#125;$ -- 表示a-z中任意一个小写字母，后面跟三个任意字符 范例：[] 方括号表示某些字符允许在一个字符串中某一个特定位置出现 12345^[ab]$ 表示一个字符串中有一个a 或 b &lt;===&gt; ^a|b$^[a-d]$ 表示一个字符串包含小写 abcd 中的一个 &lt;==&gt;^[abcd]$&lt;==&gt;^a|b|c|d$^[0-9]a$ 表示0-9中任意一个数字后面跟一个小写字母a^[a-z]&#123;2&#125;$ 表示a-z中任意的字符串总共出现2个^[A-Z]+$ 表示A-Z中任意一个大写字母至少出现1次 范例：方括号[]中使用 ^ ，表示不希望出现字符，简称过滤字符，而且 ^ 必须用在[]中的第一位 1^a[^0-9]%$ # 表示a和 % 中间不能出现数字 1.2 匹配次数用在要指定次数的字符后面，用于指定前面的字符要出现的次数 贪婪匹配 当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。 特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配-舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 懒惰匹配 当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。 特性：从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。 12345678* #匹配前面的字符任意次，包括0次，贪婪模式：尽可能长的匹配.* #任意长度的任意字符，具有贪婪的性质，匹配到不能匹配为止，根据后面的正则表达式，会进行回溯\\? #匹配其前面的字符出现0次或1次,即:可有可无\\+ #匹配其前面的字符出现最少1次,即:肯定有且 &gt;=1 次\\&#123;n\\&#125; #匹配前面的字符n次\\&#123;m,n\\&#125; #匹配前面的字符至少m次，至多n次\\&#123;,n\\&#125; #匹配前面的字符至多n次,&lt;=n\\&#123;n,\\&#125; #匹配前面的字符至少n次 懒惰量词是在贪婪量词后面加个&quot;?&quot; 123456.*? #表示匹配任意字符到下一个符合条件的字符*? #匹配任意次，但尽可能少重复+? #匹配至少1次，，但尽可能少重复?? #重复0次或1次，但尽可能少重复&#123;n,&#125;? #匹配至少n次，但尽可能少重复&#123;n,m&#125;? #匹配至少n次，至多m次，但尽可能少重复 范例 12345678910111213[15:34:46 root@10 data[]#cat f1.txt goooooooooglegooglegoglegooooglegooogle[15:34:52 root@10 data[]#grep &#x27;gooo*gle&#x27; f1.txt #前两个o一定有，第三个o包括0次，任意次goooooooooglegooglegooooglegooogle 范例 123456789101112131415[15:41:26 root@10 data[]#cat f1.txtgoooooooooglegooglegoglegooooglegoooglegdnsadgle[15:41:33 root@10 data[]#grep &#x27;g.*gle&#x27; f1.txt goooooooooglegooglegoglegooooglegoooglegdnsadgle 范例 12345[15:41:44 root@10 data[]#echo /etc/sysconfig/ | grep &#x27;/etc/sysconfig/\\?&#x27;/etc/sysconfig/[15:51:35 root@10 data[]#echo /etc/sysconfig | grep &#x27;/etc/sysconfig/\\?&#x27;/etc/sysconfig 范例 123456789^abc*$ #表示 c 可能出现0次或者至少1次, ab必须出现1次^bcd+$ #表示 d 至少出现1次，bc必须出现1次^cba?$ #表示 a 可能出现0次或者1次，cb必须出现1次^(abc)+$ #表示 abc 这个整体至少出现1次^[abc]+$ #表示 abc 中任意一个字符，至少出现1次^ab+c$ #表示 a 必须出现1次 ,b至少出现1次 ，c必须出现1次^ab&#123;0,2&#125;$ #a出现1次，b至少0次，最多2次^ab&#123;3&#125;$ #a出现1次，b出现3次 ^ab&#123;3&#125;$ &lt;====&gt; ^abbb$^ab&#123;2,&#125;$ #a出现1次，b出现至少2次 范例：匹配正负数 12345678[root@centos8 ~]#echo -1 -2 123 -123 234 |grep &#x27;\\-\\?[0-9]\\+&#x27;-1 -2 123 -123 234[root@centos8 ~]#echo -1 -2 123 -123 234 |grep -E &#x27;\\-?[0-9]+&#x27;-1 -2 123 -123 234[root@centos8 ~]#echo -1 -2 123 -123 234 |grep -E -- &#x27;-?[0-9]+&#x27;-1 -2 123 -123 234[root@centos8 ~]#echo -1 -2 123 -123 234 |grep -E &#x27;(-)?[0-9]+&#x27;-1 -2 123 -123 234 范例: 取IP地址 12345[root@centos8 ~]#ifconfig eth0|grep netmask |grep -o &#x27;[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;&#x27;|head -n110.0.0.8[root@centos8 ~]#ifconfig eth0|grep -o &#x27;[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;\\.[0-9]\\&#123;1,3\\&#125;&#x27;|head -n110.0.0.8 1.3 位置锚定位置锚定可以用于定位出现的位置 12345678910^ #行首锚定, 用于模式的最左侧$ #行尾锚定，用于模式的最右侧^PATTERN$ #用于模式匹配整行，^tom$：表示以tom开头，并以tom结束的字符串^$ #空行^[[:space:]]*$ #空白行\\&lt; 或 \\b #词首锚定，用于单词模式的左侧\\&gt; 或 \\b #词尾锚定，用于单词模式的右侧\\&lt;PATTERN\\&gt; #匹配整个单词#注意: 单词是由字母,数字,下划线组成 范例：取#开头的行 1[16:10:09 root@10 ~[]#echo /etc/fstab | grep &#x27;^#&#x27; /etc/fstab 范例：取bash结尾的行 1[16:13:27 root@10 ~[]#grep &#x27;bash$&#x27; /etc/passwd 范例：排除掉空行的行 1[16:15:03 root@10 ~[]#grep -v &#x27;^$&#x27; /etc/fstab 范例：排除掉空行和#开头的行 1234[root@centos8 ~]#grep -v &#x27;^$&#x27; /etc/profile | grep -v &#x27;^#&#x27;[root@centos8 ~]#grep &#x27;^[^#]&#x27; /etc/profile[root@centos8 ~]#grep -v &#x27;^$\\|#&#x27; /etc/profile[root@ubuntu2204 ~]# grep -v &#x27;^\\(#\\|$\\)&#x27; /etc/apache2/apache2.conf 范例 1234567[16:15:32 root@10 ~[]#echo wujunlin | grep &#x27;\\bwu&#x27;wujunlin[16:17:26 root@10 ~[]#echo junlin-wu | grep &#x27;\\bwu&#x27;junlin-wu[16:17:37 root@10 ~[]#echo junlinwu | grep &#x27;wu\\b&#x27;junlinwu 1.4 分组其他1.4.1 分组分组：() 将多个字符捆绑在一起，当作一个整体处理，如：(root)+ 后向引用：分组括号中的模式匹配到的内容会被正则表达式引擎记录于内部的变量中，这些变量的命名方式为: \\1, \\2, \\3, … \\1 表示从左侧起第一个左括号以及与之匹配右括号之间的模式所匹配到的字符 注意: \\0 表示正则表达式匹配的所有字符 注意：后向引用引用前面的分组括号中的模式所匹配字符，而非模式本身 123\\(string1\\(string2\\)\\)\\1 ：string1\\(string2\\)\\2 ：string2 范例 12345678910111213141516171819#abc作为一个整体出现3次[root@ubuntu2204 ~]# echo abcabcabc | grep &quot;\\(abc\\)\\&#123;3\\&#125;&quot;abcabcabc#后向引用[root@ubuntu2204 ~]# echo abcabcabc | grep &quot;\\(abc\\)\\1&quot;abcabcabc[root@ubuntu2204 ~]# echo abcabcabc | grep &quot;\\(abc\\)\\1\\1&quot;abcabcabc#\\1表示引用第一个分组的内容，即 abc[root@ubuntu2204 ~]# echo &quot;abcdefabc&quot; | grep &quot;\\(abc\\)def\\1&quot;abcdefabc#\\1表示引用第一个分组的内容，即abc,\\&#123;3\\&#125;表示引用内容出现3次[root@ubuntu2204 ~]# echo abc-def-abcabcabc | grep &quot;^\\(abc\\)-\\(def\\)-\\1\\&#123;3\\&#125;&quot;abc-def-abcabcabc[root@ubuntu2204 ~]# echo abc-def-abcabcabc-def-abc-defdef | grep &quot;^\\(abc\\)-\\(def\\)-\\1\\&#123;3\\&#125;-\\2-\\1-\\2\\&#123;2\\&#125;&quot;abc-def-abcabcabc-def-abc-defdef 1.4.2 或者或者：\\ | 123a\\|b #a或b C\\|cat #C或cat \\(C\\|c\\)at #Cat或cat 范例：排除空行和#开头的行 1234[root@centos6 ~]#grep -v &#x27;^#&#x27; /etc/httpd/conf/httpd.conf |grep -v ^$[root@centos6 ~]#grep -v &#x27;^#\\|^$&#x27; /etc/httpd/conf/httpd.conf[root@centos6 ~]#grep -v &#x27;^\\(#\\|$\\)&#x27; /etc/httpd/conf/httpd.conf[root@centos6 ~]#grep &quot;^[^#]&quot; /etc/httpd/conf/httpd.conf 1.5 捕获捕获是指正则表达式中使用圆括号()来标记需要保存匹配结果的部分。当正则表达式中包含圆括号时，这些括号内的匹配结果会被保存，可以使用\\1, \\2, … 等反向引用在正则表达式中再次使用，或者在匹配后通过编程语言的API访问。 代码 说明 举例 (pattern) 使用小括号指定一个子表达式，也叫分组捕获后会自动分配组号从1开始可以改变优先级 \\数字 匹配对应的分组 (very) \\1 匹配very very，捕获的组group是very (?:pattern) 如果仅仅为了改变优先级，就不需要捕获分组 (?:w|f)oodindustr(?:y|ies) 等价industry|industries (?&lt;name&gt;exp)(?&#39;name&#39;exp) 命名分组捕获，但是可以通过name访问分组Python语法必须是 (?P&lt;name&gt;exp) 1.6 零宽断言零宽断言是用来确定某个位置前后条件是否满足的，但不消耗任何字符 零宽断言常用于避免不必要的匹配或用于更精确的定位匹配的位置 1.6.1 正向零宽断言正向零宽断言（Positive Lookahead）：(?=...)，它断言当前位置之后的文本满足括号中的表达式，但不包括这部分文本 测试字符串为wood took foot food 代码 说明 举例 (?&#x3D;exp) 零宽度正预测先行断言断言exp一定在匹配的右边出现也就是说断言后面一定跟个exp f(?=oo) f 后面一定有oo出现 (?&lt;&#x3D;exp) 零宽度正回顾后发断言断言exp一定出现在匹配的左边出现也就是说前面一定有个exp前缀 (?&lt;=f)ood、(?&lt;=t)ook 分别匹配ood、ookook前一定有t出现 1.6.2 负向零宽断言(?!...)，与正向零宽断言相反，它断言当前位置之后的文本不满足括号中的表达式 代码 说明 举例 (?!exp) 零宽度负预测先行断言断言exp一定不会出现在右侧也就是说断言后面一定不是exp \\d&#123;3&#125;(?!\\d)匹配3位数字，断言3位数字，后面一定不能是数字foo(?!d) foo后面一定不是d (?&lt;!exp) 零宽度负回顾后发断言断言exp一定不能出现在左侧也就是说断言前面一定不能是exp (?&lt;!f)ood ood的左边一定不是f (?#comment) 注释 f(?&#x3D;oo)(?#这个后断言不捕获) 注意：断言会不会捕获呢？也就是断言占不占分组号呢？ 断言不占分组号。断言如同条件，只是要求匹配必须满足断言的条件。 分组和捕获是同一个意思。 使用正则表达式时，能用简单表达式，就不要复杂的表达式。 1.7 引擎选项 代码 说明 Python IgnoreCase 匹配时忽略大小写 re.Ire.IGNORECASE Singleline 单行模式 . 可以匹配所有字符，包括\\n这改变了默认情况下 . 无法匹配换行符的行为 re.Sre.DOTALL Multiline 多行模式 ^ 行首、$ 行尾 re.Mre.MULTILINE IgnorePatternWhitespace 忽略表达式中的空白字符，如果要使用空白字符用转义，#可以用来做注释 re.Xre.VERBOSE 单行模式： . 可以匹配所有字符，包括换行符 ^ 表示整个字符串的开头，$整个字符串的结尾 多行模式： . 可以匹配除了换行符之外的字符，多行不影响 . 点号，跟默认模式一样 ^ 表示行首，$行尾，只不过这里的行是每一个行，即可以在每一行的开头和结尾处进行匹配，而不只是在整个输入字符串的开头和结尾 默认模式：可以看做待匹配的文本是一行，就是长长的字符串，不能看做多行，. 点号不能匹配换行符，^ 和 $ 表示行首和行尾，而行首行尾就是整个字符串的开头和结尾 单行模式：基本和默认模式一样，只是 . 点号终于可以匹配任意一个字符包括换行符，这时所有文本就是一个长长的只有一行的字符串。^ 就是这一行字符串的行首，$ 就是这一行的行尾。 多行模式：重新定义了行的概念，但不影响 . 点号的行为，^ 和 $ 还是行首行尾的意思，只不过因为多行模式可以识别换行符了。”开始”指的是 \\n 后紧接着下一个字符；”结束”指的是 \\n 前的字符，注意最后一行结尾可以没有 \\n 简单讲，单行模式只影响 . 点号行为，多行模式重新定义行影响了 ^ 和 $ 注意：字符串中看不见的换行符，\\r\\n 会影响 e$ 的测试，e$ 只能匹配 e\\n 123456very very happymy primary key# 上面2行happy之后，有可能是\\r\\n结尾。# y$ 单行匹配key的y，多行匹配happy和key的y。# .$指的是此行的结尾，而默认模式和单行模式都是一行，指的是这个大字符串的最后一个字符，就是key的y。 2 扩展正则表达式在基本正则表达式中，?、+、*、()、| 等元字符需要加 \\ 转义才能表示其特殊含义；而在扩展正则表达式中，这些元字符可以直接使用，无需加 \\ 转义 但需要注意的是，扩展正则表达式中仍有部分元字符需要转义，例如 . 表示任意字符，若要匹配字面量 . 仍需转义为 \\.；^ 和 $ 分别表示行首和行尾，若要匹配字面量 ^ 或 $ 需转义为 \\^ 或 \\$ 等 3 正则表达式的误区与常见错误1. 过于复杂的正则 有时，我们会尝试创建一个完美、一劳永逸的正则表达式，但这往往会导致正则变得难以阅读和维护。建议： 分解复杂的正则，使其更具可读性，并加入必要的注释。 2. 对**.**的误解 . 在正则表达式中匹配任何字符，但很多人忘记了它不匹配换行符（除非使用了re.DOTALL标志）。 3. 忘记转义特殊字符 有些字符，如 ., *, + 在正则中有特殊含义。如果你想匹配这些字符本身，记得使用 \\ 进行转义。 4. 不考虑边界情况 例如，\\d&#123;1,2&#125; 可以匹配1到99之间的数字，但它也会匹配100中的10。使用\\b来匹配单词边界，避免这类问题。 5. 使用**.\\***而不加思索 .* 会尝试匹配尽可能多的字符，这可能不是你想要的。考虑使用非贪婪匹配.*?或更具体的匹配模式。 6. 忽视大小写 除非使用了re.IGNORECASE标志，否则正则表达式匹配是区分大小写的。 7. 仅测试正常情况 当编写正则表达式时，确保你不仅仅测试预期的匹配，还要测试不应该匹配的内容。 8. 不利用测试工具 在线工具，如 regex101，可以为你提供匹配的实时反馈和解释，帮助你更好地理解和调试正则表达式。 9. 使用正则表达式解析复杂结构 虽然技术上可以使用正则表达式来解析HTML或XML，但这并不是一个好主意。使用专门为此设计的解析器更为可靠和高效。 4 常用的正则表达式校验数字的表达式 12345678910111213141、数字：^[0-9]*$2、n位的数字：^\\d&#123;n&#125;$3、至少n位的数字：^\\d&#123;n,&#125;$4、m-n位的数字：^\\d&#123;m,n&#125;$5、零和非零开头的数字：^(0|[1-9][0-9]*)$6、非零开头的最多带两位小数的数字：^([1-9][0-9]*)+(\\.[0-9]&#123;1,2&#125;)?$7、带1-2位小数的正数或负数：^(\\-)?\\d+(\\.\\d&#123;1,2&#125;)$8、正数、负数、和小数：^(\\-|\\+)?\\d+(\\.\\d+)?$9、有两位小数的正实数：^[0-9]+(\\.[0-9]&#123;2&#125;)?$10、有1~3位小数的正实数：^[0-9]+(\\.[0-9]&#123;1,3&#125;)?$11、非零的正整数：^[1-9]\\d*$ 或 ^([1-9][0-9]*)&#123;1,3&#125;$ 或 ^\\+?[1-9][0-9]*$12、非零的负整数：^\\-[1-9][]0-9&quot;*$ 或 ^-[1-9]\\d*$13、非负整数：^\\d+$ 或 ^[1-9]\\d*|0$14、非正整数：^-[1-9]\\d*|0$ 或 ^((-\\d+)|(0+))$ 校验字符的表达式 1234567891011121、汉字：^[\\u4e00-\\u9fa5]&#123;0,&#125;$2、英文和数字：^[A-Za-z0-9]+$ 或 ^[A-Za-z0-9]&#123;4,40&#125;$3、长度为3-20的所有字符：^.&#123;3,20&#125;$4、由26个英文字母组成的字符串：^[A-Za-z]+$5、由26个大写英文字母组成的字符串：^[A-Z]+$6、由26个小写英文字母组成的字符串：^[a-z]+$7、由数字和26个英文字母组成的字符串：^[A-Za-z0-9]+$8、由数字、26个英文字母或者下划线组成的字符串：^\\w+$ 或 ^\\w&#123;3,20&#125;$9、中文、英文、数字包括下划线：^[\\u4E00-\\u9FA5A-Za-z0-9_]+$10、中文、英文、数字但不包括下划线等符号：^[\\u4E00-\\u9FA5A-Za-z0-9]+$ 或 ^[\\u4E00-\\u9FA5A-Za-z0-9]&#123;2,20&#125;$11、可以输入含有^%&amp;&#x27;,;=?$\\&quot;等字符：[^%&amp;&#x27;,;=?$\\x22]+12、禁止输入含有~的字符：[^~\\x22]+ 特殊需求表达式 1234567891011121314151617181920212223242526272829303132333435363738391、Email地址：^\\w+([-+.]\\w+)*@\\w+([-.]\\w+)*\\.\\w+([-.]\\w+)*$2、域名：[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;(\\.[a-zA-Z0-9][-a-zA-Z0-9]&#123;0,62&#125;)+\\.?3、InternetURL：[a-zA-z]+://[^\\s]* 或 ^http://([\\w-]+\\.)+[\\w-]+(/[\\w-./?%&amp;=]*)?$4、手机号码：^(13[0-9]|14[5|7]|15[0|1|2|3|4|5|6|7|8|9]|18[0|1|2|3|5|6|7|8|9])\\d&#123;8&#125;$5、电话号码(&quot;XXX-XXXXXXX&quot;、&quot;XXXX-XXXXXXXX&quot;、&quot;XXX-XXXXXXX&quot;、&quot;XXX-XXXXXXXX&quot;、&quot;XXXXXXX&quot;和&quot;XXXXXXXX)：^(\\(\\d&#123;3,4&#125;-)|\\d&#123;3.4&#125;-)?\\d&#123;7,8&#125;$6、国内电话号码(0511-4405222、021-87888822)：\\d&#123;3&#125;-\\d&#123;8&#125;|\\d&#123;4&#125;-\\d&#123;7&#125;7、电话号码正则表达式（支持手机号码，3-4位区号，7-8位直播号码，1－4位分机号）: ((\\d&#123;11&#125;)|^((\\d&#123;7,8&#125;)|(\\d&#123;4&#125;|\\d&#123;3&#125;)-(\\d&#123;7,8&#125;)|(\\d&#123;4&#125;|\\d&#123;3&#125;)-(\\d&#123;7,8&#125;)-(\\d&#123;4&#125;|\\d&#123;3&#125;|\\d&#123;2&#125;|\\d&#123;1&#125;)|(\\d&#123;7,8&#125;)-(\\d&#123;4&#125;|\\d&#123;3&#125;|\\d&#123;2&#125;|\\d&#123;1&#125;))$)8、身份证号(15位、18位数字)，最后一位是校验位，可能为数字或字符X：(^\\d&#123;15&#125;$)|(^\\d&#123;18&#125;$)|(^\\d&#123;17&#125;(\\d|X|x)$)9、帐号是否合法(字母开头，允许5-16字节，允许字母数字下划线)：^[a-zA-Z][a-zA-Z0-9_]&#123;4,15&#125;$10、密码(以字母开头，长度在6~18之间，只能包含字母、数字和下划线)：^[a-zA-Z]\\w&#123;5,17&#125;$11、强密码(必须包含大小写字母和数字的组合，不能使用特殊字符，长度在 8-10 之间)：^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z])[a-zA-Z0-9]&#123;8,10&#125;$12、强密码(必须包含大小写字母和数字的组合，可以使用特殊字符，长度在8-10之间)：^(?=.*\\d)(?=.*[a-z])(?=.*[A-Z]).&#123;8,10&#125;$13、日期格式：^\\d&#123;4&#125;-\\d&#123;1,2&#125;-\\d&#123;1,2&#125;14、一年的12个月(01～09和1～12)：^(0?[1-9]|1[0-2])$15、一个月的31天(01～09和1～31)：^((0?[1-9])|((1|2)[0-9])|30|31)$16、xml文件：^([a-zA-Z]+-?)+[a-zA-Z0-9]+\\\\.[x|X][m|M][l|L]$17、腾讯QQ号：[1-9][0-9]&#123;4,&#125; (腾讯QQ号从10000开始)18、中国邮政编码：[1-9]\\d&#123;5&#125;(?!\\d) (中国邮政编码为6位数字)19、IPv4地址：((2(5[0-5]|[0-4]\\d))|[0-1]?\\d&#123;1,2&#125;)(\\.((2(5[0-5]|[0-4]\\d))|[0-1]?\\d&#123;1,2&#125;))&#123;3&#125;20、空白行的正则表达式：\\n\\s*\\r (可以用来删除空白行)","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"用户和权限管理","slug":"Linux/basics/user-permission","date":"2025-08-19T06:02:56.000Z","updated":"2025-09-06T11:20:42.676Z","comments":true,"path":"Linux/basics/user-permission/","permalink":"https://aquapluto.github.io/Linux/basics/user-permission/","excerpt":"","text":"1 Linux权限模型资源分派： Authentication：认证，验证用户身份 Authorization：授权，不同的用户设置不同权限 Accouting|Audition：审计 当用户登录成功时，系统会自动分配令牌 token，包括：用户标识和组成员等信息 3A认证：又称AAA认证，是一套针对网络设备的网络访问控制策略安全模型 1.1 用户Linux系统是多用户系统，可以同时存在多个用户，每个用户之间都是互相隔离的 Linux中每个用户是通过 User Id （UID）来唯一标识的 管理员：root, 0 普通用户：1-60000 自动分配 系统用户：1-499 （CentOS 6以前）, 1-999 （CentOS 7以后）对守护进程获取资源进行权限分配 登录用户：500+ （CentOS6以前）, 1000+（CentOS7以后）给用户进行交互式登录使用 1.2 用户组Linux中可以将一个或多个用户加入用户组中，组就是包含0个或多个用户的集合，用户组是通过Group ID（GID） 来唯一标识的 管理员组：root, 0 普通组： 系统组：1-499（CentOS 6以前）, 1-999（CentOS7以后）, 对守护进程获取资源进行权限分配 普通组：500+（CentOS 6以前）, 1000+（CentOS7以后）, 给用户使用 1.3 用户和组的关系 一个用户至少有一个组，也可以有多个组； 一个组至少有0个用户，也可以有多个用户； 用户的主要组(primary group)：又称私有组，一个用户必须属于且只有一个主组，创建用户时，默认会创建与其同名的组作为主组； 用户的附加组(supplementary group)：又称辅助组，一个用户可以属于0个或多个附加组； 使用组，可以对用户进行批量管理，比如对一个组授权，则该组下所有的用户能能继承这个组的权限； 1.4 安全上下文Linux安全上下文Context 运行中的程序，即进程 (process)，以进程发起者的身份运行，进程所能够访问资源的权限取决于进程的运行者的身份 程序，进程，用户之间的关系是怎样的 只有可以被执行的文件，才能叫作程序； 对于同一个程序，也不是所有用户都可以运行的，这要取决于当前用户对该程序有没有可执行权限； 用户张三，运行了某个程序，那么，张三就发起了一个进程，该进程的发起者，就是张三，该进程是以张三的身份在运行； 比如：分别以 root 和 wang 的身份运行 /bin/cat /etc/shadow ，得到的结果是不同的，资源能否能被访问，是由运行者的身份决定，非程序本身 进程的访问资源 一个进程能不能访问某些资源，是由进程发起者决定的（跟进程本身的程序文件无关），比如某进程要读写某个文件，则要看该进程发起者有没有权限读取该文件； 123456789[wang@centos8 ~]$cat /etc/shadowcat: /etc/shadow: Permission denied[root@centos8 ~]#cat /etc/shadowroot:$6$zsrWEC56PrKifAEz$hylCuGySe.H6l6O2MRvbtqy/VZgnZbau.y57dE85.YHq03MTJVV4UvQVIDcYA1IJzbgpWE0vTU.BtPHLbNBNn0:18246:0:99999:7:::bin:*:18027:0:99999:7:::daemon:*:18027:0:99999:7:::adm:*:18027:0:99999:7:::lp:*:18027:0:99999:7::: 进程来用文件的时候，如何匹配权限的呢？ 进程启动（必须赋予某种身份） 默认为当前登录用户 在配置文件里修改指定进程启动的身份 拿着进程的用户身份，去匹配目标文件的权限 进程的用户身份会依次配文件的ugo的rwx 2 用户管理2.1 用户创建1234567891011121314151617181920useradd [options] LOGIN-u UID #指明UID-g GID #指明用户所属主组，可为组名，也可以GID-s SHELL #指明用户的默认shell程序，可用列表在/etc/shells文件中-d HOME_DIR #以指定的路径(不存在)为家目录-m #创建家目录，用于系统用户-M #不创建家目录，用于非系统用户-o #配合-u 选项，不检查UID的唯一性，即允许使用重复的 UID 创建用户-G GROUP1[,GROUP2,...] #为用户指明附加组，组须事先存在-N #不创建私用（同名）组做主组，使用users组做主组-r #创建系统用户，不会创建邮箱-p #指定加密的密码-c &quot;COMMENT“ #用户的注释信息-D #显示或更改默认的 useradd 配置，默认配置文件是/etc/default/useradd-e #指定账户的过期日期 YYYY-MM-DD 格式-f #密码过期之后，账户被彻底禁用之前的天数，0 表示密码过期立即禁用，-1表示不使用此功能-k #指定家目录模板，创建家目录，会生成一些默认文件，如果指定，就从该目录复制文件，默认是/etc/skel/，要配合-m-K #不使用 /etc/login.defs 中的默认值，自己指定，比如-K UID_MIN=100-l|--no-log-init #不将用户添加到最近登录和登录失败记录，前面讲到的3a认证审计，就在此处lastlog|lastb|cat /var/log/secure 当创建一个用户时，如果没有指定用户的主组，将会创建一个同名的组作为用户的主组 useradd创建用户时，对于未指定的选项（-u、-g等等），会以/etc/login.defs、/etc/default/useradd两个配置文件中的配置作为参照物 配置文件/etc/login.defs详解 123456789101112131415161718[root@egon ~]# grep -Ev &quot;^#|^$&quot; /etc/login.defsMAIL_DIR /var/spool/mailPASS_MAX_DAYS 99999 #密码最大有效期PASS_MIN_DAYS 0 #两次修改密码的最小间隔时间PASS_MIN_LEN 5 #密码最小长度，对于root无效PASS_WARN_AGE 7 #密码过期前多少天开始提示UID_MIN 1000 #用户ID的最小值UID_MAX 60000 #用户ID的最大值SYS_UID_MIN 201 #系统用户ID的最小值SYS_UID_MAX 999 #系统用户ID的最大值GID_MIN 1000 #组ID的最小值GID_MAX 60000 #组ID的最大值SYS_GID_MIN 201 #系统用户组ID的最小值SYS_GID_MAX 999 #系统用户组ID的最大值CREATE_HOME yes #使用useradd的时候是可以创建用户家目录UMASK 077 #创建家目录时umask的默认控制权限USERGROUPS_ENAB yes #删除用户的时候是否同时删除用户组ENCRYPT_METHOD SHA512 #密码加密规则 配置文件/etc/default/useradd详解 12345678[root@egon ~]# cat /etc/default/useraddGROUP=100 #依赖于/etc/login.defs的USERGRUUPS_ENAB参数，如果为no，则在此处控制HOME=/home #把用户的家目录建在/home中。INACTIVE=-1 #是否启用账号过期停权,-1表示不启用。EXPIRE= #账号终止日期,不设置表示不启用。SHELL=/bin/bash #新用户默认所有的shell类型。SKEL=/etc/skel #配置新用户家目录的默认文件存放路径。CREATE_MAIL_SPOOL=yes #创建mail文件。 范例 12useradd -u 123 -g mysql -s /sbin/nologin -d /data/mysql -M mysqlID-u，组名-g，默认程序-s，家目录-d【不想创建-M，想创建-m】，用户名 显示或更改默认设置 1234useradd -Duseradd –D -s SHELLuseradd –D –b BASE_DIRuseradd –D –g GROUP 新建用户的相关文件：当使用useradd创建用户时，创建的用户家目录下会存在.bash_* 环境变量相关的文件，这些环境变量文件默认从/etc/skel目录中拷贝。这个默认拷贝环境变量位置是由/etc/default/useradd配置文件中定义的。 123/etc/default/useradd #cat /etc/default/useradd 可以查看新建账号的属性/etc/skel/* #新建的文件的家目录的默认文件是在/etc/skel里的，/etc/skel包含一个新用户在创建后所拥有的默认文件和目录，在创建新用户时，系统会将/etc/skel目录下的文件和目录复制到新用户的家目录中/etc/login.defs 批量创建用户 1newusers passwd 格式文件 批量修改用户口令 1echo username:passwd | chpasswd 范例：误删除了用户git的家目录，请重建并恢复该用户家目录及相应的权限属性 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@centos8 data]# useradd git[root@centos8 data]# ll -d /home/gitdrwx------. 3 git git 78 Nov 20 21:51 /home/git[root@centos8 data]# ll -a /home/gittotal 12drwx------. 3 git git 78 Nov 20 21:51 .drwxr-xr-x. 10 root root 113 Nov 20 21:51 ..-rw-r--r--. 1 git git 18 Nov 9 2019 .bash_logout-rw-r--r--. 1 git git 141 Nov 9 2019 .bash_profile-rw-r--r--. 1 git git 312 Nov 9 2019 .bashrcdrwxr-xr-x. 4 git git 39 Nov 20 13:27 .mozilla[root@centos8 data]# rm -rf /home/git[root@centos8 data]# ll /hometotal 0drwx------. 3 docker docker 99 Nov 20 21:14 dockerdrwx------. 3 hf hf 120 Nov 20 16:49 hfdrwx------. 3 mongodb mongodb 112 Nov 20 21:31 mongodbdrwx------. 3 neteagle neteagle 99 Nov 20 15:59 neteagledrwx------. 3 redis redis 99 Nov 20 21:33 redisdrwx------. 3 tomcat tomcat 78 Nov 20 21:41 tomcatdrwx------. 3 zabbix zabbix 99 Nov 20 21:40 zabbix[root@centos8 data]# ll -a /etc/skel/total 24drwxr-xr-x. 3 root root 78 Nov 20 13:28 .drwxr-xr-x. 135 root root 8192 Nov 20 21:51 ..-rw-r--r--. 1 root root 18 Nov 9 2019 .bash_logout-rw-r--r--. 1 root root 141 Nov 9 2019 .bash_profile-rw-r--r--. 1 root root 312 Nov 9 2019 .bashrcdrwxr-xr-x. 4 root root 39 Nov 20 13:27 .mozilla[root@centos8 data]# cp -a /etc/skel/ /home/git[root@centos8 data]# ll -a /home/gittotal 12drwxr-xr-x. 3 root root 78 Nov 20 13:28 .drwxr-xr-x. 10 root root 113 Nov 20 21:59 ..-rw-r--r--. 1 root root 18 Nov 9 2019 .bash_logout-rw-r--r--. 1 root root 141 Nov 9 2019 .bash_profile-rw-r--r--. 1 root root 312 Nov 9 2019 .bashrcdrwxr-xr-x. 4 root root 39 Nov 20 13:27 .mozilla[root@centos8 data]# ll -d /home/gitdrwxr-xr-x. 3 root root 78 Nov 20 13:28 /home/git[root@centos8 data]# chown git.git /home/git[root@centos8 data]# ll -d /home/gitdrwxr-xr-x. 3 git git 78 Nov 20 13:28 /home/git[root@centos8 data]# chmod 700 /home/git[root@centos8 data]# ll -d /home/gitdrwx------. 3 git git 78 Nov 20 13:28 /home/git#简化[root@centos7 ~]# cp -r /etc/skel/. /home/git[root@centos7 ~]# chmod 700 /home/git[root@centos7 ~]# chown -R git.git /home/git 2.2 用户属性修改12345678910111213usermod [OPTION] login-u UID: 新UID-g GID: 新主组-G GROUP1[,GROUP2,...[,GROUPN]]]：新附加组，原来的附加组将会被覆盖；若保留原有，则要同时使用-a选项-s SHELL：新的默认SHELL-c &#x27;COMMENT&#x27;：新的注释信息-d HOME: 新家目录不会自动创建；若要创建新家目录并移动原家数据，同时使用-m选项-l login_name: 新的名字-L: lock指定用户,在/etc/shadow 密码栏的增加 !-U: unlock指定用户,将 /etc/shadow 密码栏的 ! 拿掉-e YYYY-MM-DD: 指明用户账号过期日期-f INACTIVE: 设定非活动期限，即宽限期 2.3 删除用户1234userdel [OPTION]... Login-f, --force 强制-r, --remove 删除用户家目录和邮箱 范例: 强制删除用户和数据 12345678910111213141516171819[root@centos8 ~]#useradd test[root@centos8 ~]#id testuid=1001(test) gid=1001(test) groups=1001(test)#在另一终端用test登录[root@centos8 ~]#su - test[test@centos8 ~]$#删除正在登录的用户失败[root@centos8 ~]#userdel -r testuserdel: user test is currently used by process 29909[root@centos8 ~]#id testuid=1001(test) gid=1001(test) groups=1001(test)#强制删除用户[root@centos8 ~]#userdel -rf testuserdel: user test is currently used by process 29909[root@centos8 ~]#id testid: ‘test’: no such user 2.4 查看用户相关的ID信息123456id [OPTION]... [USER]-u: 显示UID-g: 显示GID-G: 显示用户所属的组的ID-n: 显示名称，需配合ugG使用 2.5 设置密码2.5.1 openssl passwd在Linux系统中我们要向手动生成一个密码可以采用opensll passwd来生成一个密码作为用户账号的密码。Linux系统中的密码存放在&#x2F;etc&#x2F;shadow文件中，并且是以加密的方式存放的，根据加密方式的不同，所产生的加密后的密码的位数也不同 openssl passwd的作用是用来计算密码hash的，目的是为了防止密码以明文的形式出现 123456789101112语法格式： openssl passwd [option] passwdopenssl passwd常用的选项如下：-1：表示采用的是MD5加密算法。-salt：指定salt值，不使用随机产生的salt。在使用加密算法进行加密时，即使密码一样，salt不一样，所计算出来的hash值也不一样，除非密码一样，salt值也一样，计算出来的hash值才一样。salt为8字节的字符串。 示例：[tom@localhost ~]$ openssl passwd -1 -salt &#x27;i have a dream&#x27; ##注意&#x27;i have a dream&#x27; 不是密码而是密码的盐,注意密码的盐里不要有中文Password: ##这里输入的是密码$1$12345678$1qWiC4czIc07B4J8bPjfC0 ##这是生成的密文密码##将生成的密码串，手动添加到/etc/shadow中就可用作用户的登陆密码了。 2.5.2 passwd用于更新用户的身份验证令牌（口令&#x2F;密码） 123456789101112passwd [OPTIONS] UserName-d：删除指定用户密码-l：锁定指定用户-u：解锁指定用户-e：强制用户下次登录修改密码-f：强制操作-n mindays：指定最短使用期限-x maxdays：最大使用期限-w warndays：提前多少天开始警告-i inactivedays：非活动期限--stdin：从标准输入接收用户密码,Ubuntu无此选项 范例：非交互式修改用户密码 12345#此方式更通用，适用于各种Linux版本，如:ubuntu[root@centos8 ~]#echo -e &#x27;123456\\n123456&#x27; | passwd mage #因为交互式中需要输入两次密码#适用于Rocky和Centos，不适用于Ubuntu[root@centos8 ~]#echo &#x27;123456&#x27; | passwd --stdin mage 范例：设置用户下次必须更改密码 12345678910111213141516171819202122232425262728293031[root@centos8 ~]#useradd wang[root@centos8 ~]#echo 123456 | passwd --stdin wangChanging password for user wang.passwd: all authentication tokens updated successfully.[root@centos8 ~]#getent shadow wangwang:$6$4f78ko7hJ4fcMvIH$lpbOkFfziDBLT.8XBCi8c/N7wysDAejN5H9Fgxkt99HRDLTEosO43CKYi2XSSVHxAK568Olj3C5bwfNExlves/:18348:0:99999:7:::[root@centos8 ~]#passwd -e wang[root@centos8 ~]#su - mageLast login: Fri Mar 27 09:55:27 CST 2020 on pts/0[mage@centos8 ~]$su - wangPassword:You are required to change your password immediately (administrator enforced)Current password:New password:Retype new password:Last login: Fri Mar 27 10:01:20 CST 2020 on pts/0Last failed login: Fri Mar 27 10:02:37 CST 2020 on pts/0There was 1 failed login attempt since the last successful login.[wang@centos8 ~]$exitlogout[mage@centos8 ~]$exitlogout[root@centos8 ~]#getent shadow wangwang:$6$TX0iLjF52ByHh1zH$g.WI4LNfauuwgnxpRhd7ePqFKHZ85YU3r6Lh2S0PWRXWGjGlDVtomLWqpdiWrT.vwqD/Wzok.kzQhUHc8UCs91:18348:0:99999:7::: 2.5.3 chpassed用于批量更新密码。注意：命令内没有用户名和密码，回车后以”用户名:密码”的格式输入（密码一般为明文），chpasswd根据选项加密 123456chapasswd [选项] -c #使用指定的方法加密。加密方法有DES，MD5，NONE，SHA256，SHA512-e #提供的密码已经加密-h #帮助-m #md5算法 范例：非交互式修改密码 123456789echo &quot;user003:123456&quot; | chpasswd[root@rocky8 ~]#cat &gt; passwd.txtnginx:123456k8s:123456^C#注意要写绝对路径[root@rocky8 ~]#chpasswd &lt; /root/passwd.txt 默认普通用户是没有chpasswd的权限，但是可以通过修改命令文件权限来修改 1chmod 4755 /usr/sbin/chpasswd 2.6 修改用户密码策略123456789chage [OPTION]... LOGIN-d LAST_DAY #更改密码的时间-m --mindays MIN_DAYS #两次修改密码之间相距的最小天数-M --maxdays MAX_DAYS #密码保持有效的最大天数-W --warndays WARN_DAYS #密码过期前，提前收到警告信息的天数-I --inactive INACTIVE #密码过期INACTIVE天数后，设定密码为失效状态-E --expiredate EXPIRE_DATE #用户的有效期，0表示马上过期，-1表示永不过期-l #显示密码策略 密码过期：设置的密码经过一段的时间后，系统会认为该密码不安全，于是将密码设置为过期状态，用户登录的时候，系统会提示用户进行密码修改 密码失效：经过一段时间，如果用户没有进行密码修改，则系统会将该密码设置为失效状态（此时用户不可通过该密码进行登录 范例：下一次登录强制重设密码 1[root@centos8 ~]#chage -d 0 wang 范例 1234567891011121314151617#test用户将在2019年4月29日失效（不可登陆）[root@centos8 ~]#chage -E 2019-04-29 test#设置test用户最后一次修改密码的日期为2019年6月30日[root@centos8 ~]#chage -d 2019-06-30 test#从最近修改密码的日期开始的5天内，用户test不能再次修改密码[root@centos8 ~]#chage -m 5 test#test用户自修改密码的日期开始，修改后的密码将在8天后过期[root@centos8 ~]#chage -M 8 test#用户test从密码过期开始算起，3天不修改密码则密码失效[root@centos8 ~]#chage -I 3 test#用户test的密码快要过期的的8天内，系统会持续对用户进行警告[root@centos8 ~]#chage -W 8 test 范例 12345678[root@centos7 ~]#chage -l wuLast password change : never #最近一次密码修改时间Password expires : never #密码过期时间Password inactive : never #密码失效时间Account expires : never #帐户过期时间Minimum number of days between password change : 0 #两次改变密码之间相距的最小天数 Maximum number of days between password change : 99999 #两次改变密码之间相距的最大天数Number of days of warning before password expires : 7 #在密码过期之前警告的天数 范例 1234567891011[root@centos7 ~]#cat /etc/shadowtest:!!:18077:5:8:8:3:18012:第一个字段为：用户名;第二个字段为：加密的密码第三个字段为：密码最后一次修改的时间 （chage -d）第四个字段为：密码最小修改间隔时间 （chage -m）第五个字段为：密码的有效期 （chage -M）第六个字段为：密码需要变更前的警告天数 （ chage -W）第七个字段为：密码过期后的宽限天数 （chage -I）第八个字段为：账号失效时间 （chage -E）第九个字段为：保留 2.7 用户文件2.7.1 &#x2F;etc&#x2F;passwd 1root:x:0:0:root:/root:/bin/bash 第一字段：用户名（也被称为登录名)； 第二字段：口令；在例子中我们看到的是一个x，其实密码已被映射到&#x2F;etc&#x2F;shadow 文件中； 第三字段：UID ；请参看本文的UID的解说； 第四字段：GID；请参看本文的GID的解说； 第五字段：描述信息，可选 第六字段：用户的家目录所在位置； 第七字段：用户所用SHELL的类型 2.7.2 &#x2F;etc&#x2F;shadow 12small_egon:$1$VE.Mq2Xf$2c9Qi7EQ9JP8GKF8gH7PB1:13072:0:99999:7:::big_egon:$1$IPDvUhXP$8R6J/VtPXvLyXxhLWPrnt/:13072:0:99999:7::13108: 第一字段：用户名（也被称为登录名)，在&#x2F;etc&#x2F;shadow中，用户名和&#x2F;etc&#x2F;passwd 是相同的，这样就把passwd 和shadow中用的用户记录联系在一起；这个字段是非空的； 第二字段：密码（已被加密)，如果是有些用户在这段是x，表示这个用户不能登录到系统；这个字段是非空的； 第三字段：上次修改口令的时间；这个时间是从1970年01月01日算起到最近一次修改口令的时间间隔（天数)，您可以通过passwd 来修改用户的密码，然后查看&#x2F;etc&#x2F;shadow中此字段的变化； 第四字段：两次修改口令间隔最少的天数；如果设置为0,则禁用此功能；也就是说用户必须经过多少天才能修改其口令；此项功能用处不是太大；默认值是通过&#x2F;etc&#x2F;login.defs文件定义中获取，PASS_MIN_DAYS 中有定义； 第五字段：两次修改口令间隔最多的天数；这个能增强管理员管理用户口令的时效性，应该说在增强了系统的安全性；如果是系统默认值，是在添加用户时由&#x2F;etc&#x2F;login.defs文件定义中获取，在PASS_MAX_DAYS 中定义； 第六字段：提前多少天警告用户口令将过期；当用户登录系统后，系统登录程序提醒用户口令将要作废；如果是系统默认值，是在添加用户时由&#x2F;etc&#x2F;login.defs文件定义中获取，在PASS_WARN_AGE 中定义； 第七字段：在口令过期之后多少天禁用此用户；此字段表示用户口令作废多少天后，系统会禁用此用户，也就是说系统会不能再让此用户登录，也不会提示用户过期，是完全禁用； 第八字段：用户过期日期；此字段指定了用户作废的天数（从1970年的1月1日开始的天数)，如果这个字段的值为空，帐号永久可用； www.hackdig.com 第九字段：保留字段，目前为空，以备将来Linux发展之用； 2.7.3 其他文件/etc/skel/：用户老家的模板 /home/xxx：用户家目录 /var/spool/mail/xxx：用户邮箱文件 3 组管理3.1 创建组1234groupadd [OPTION]... group_name-g GID 指明GID号；[GID_MIN, GID_MAX]-r 创建系统组名，CentOS 6之前: ID&lt;500，CentOS 7以后: ID&lt;1000 范例 1groupadd -g 48 -r apache 3.2 修改组1234groupmod [OPTION]... group-n group_name: 新名字-g GID: 新的GID 范例 1groupmod -n new_group_name old_group_name #重命名一个用户组 3.3 删除组如果一个组是一个用户的主组，那么该组不能被删除，删掉用户会默认一起删掉他的主组 123groupdel [options] GROUP-f, --force 强制删除，即使是用户的主组也强制删除组,但会导致无主组的用户不可用无法登录 3.4 添加、更改和查看组成员对于用户来说，组是分类的 一类是基本组或称主组，用户只能有一个基本组，创建时可通过-g指定，如未指定则创建一个默认的组(与用户同名) 附加组，基本组不能满足授权要求，创建附加组，将用户加入该组，用户可以属于多个附加组，加入一个组后就拥有了该组的权限 groupmems 12345678#更改组成员groupmems [options] [action]-g, --group groupname #更改为指定组 (只有root)-a, --add username #指定用户加入组-d, --delete username #从组中删除用户-p, --purge #从组中清除所有成员-l, --list #显示组成员列表 groups 12#查看用户所属组列表groups [OPTION].[USERNAME]... 范例 12345678910111213141516[root@centos8 ~]#groupmems -l -g admins[root@centos8 ~]#groupmems -a mage -g admins[root@centos8 ~]#id mageuid=1001(mage) gid=1001(mage) groups=1001(mage),1002(admins)[root@centos8 ~]#groupmems -l -g adminsmage[root@centos8 ~]#groupmems -a wang -g admins[root@centos8 ~]#groupmems -l -g adminsmage wang[root@centos8 ~]#groupmems -d wang -g admins[root@centos8 ~]#groups wangwang : wang[root@centos8 ~]#groupmems -l -g adminsmage[root@centos8 ~]#groupmems -p -g admins[root@centos8 ~]#groupmems -l -g admins 范例 123456789#创建组 groupadd g1 groupadd g2 groupadd g3#加入组 usermod -G g1,g2,g3 wu groupmems -a wu -g g1(g2,g3)#查看组成员 groupmems -l -g g1(g2,g3) **gpasswd：**将用户添加到组或从组中删除，只针对已存在的用户 12345gpasswd [OPTION] GROUP-a user 将user添加至指定组中-d user 从指定附加组中移除用户user-A user1,user2,... 设置有管理权限的用户列表 范例 1234567891011121314151617181920212223242526272829#增加组成员[root@centos8 ~]#groupadd admins[root@centos8 ~]#id wanguid=1000(wang) gid=1000(wang) groups=1000(wang)[root@centos8 ~]#gpasswd -a wang adminsAdding user wang to group admins[root@centos8 ~]#id wanguid=1000(wang) gid=1000(wang) groups=1000(wang),1002(admins)[root@centos8 ~]#groups wangwang : wang admins[root@centos8 ~]#getent group adminsadmins:x:1002:wang#删除组成员[root@centos8 ~]#gpasswd -d wang adminsRemoving user wang from group admins[root@centos8 ~]#groups wangwang : wang[root@centos8 ~]#id wanguid=1000(wang) gid=1000(wang) groups=1000(wang)[root@centos8 ~]#getent group adminsadmins:x:1002: 我们可以为组设置密码，然后让一些非组成员的用户通过命令”newgrp”临时切换到组内并输入密码的方式获取用户组的权限和特性 123456789101112131415161718192021222324[root@localhost ~]# groupadd group1[root@localhost ~]# gpasswd group1正在修改 group1 组的密码新密码：请重新输入新密码：[root@localhost ~]# touch /tmp/a.txt[root@localhost ~]# ll /tmp/a.txt -rw-r--r-- 1 root root 0 8月 10 21:01 /tmp/a.txt[root@localhost ~]# chown .group1 /tmp/a.txt [root@localhost ~]# !llll /tmp/a.txt -rw-r--r-- 1 root group1 0 8月 10 21:01 /tmp/a.txt[root@localhost ~]# chmod g+w /tmp/a.txt [root@localhost ~]# gpasswd group1[root@localhost ~]# su - egon上一次登录：一 8月 10 21:01:46 CST 2020pts/0 上[egon@localhost ~]$ echo 123 &gt;&gt; /tmp/a.txt # 此时没有权限-bash: /tmp/a.txt: 权限不够[egon@localhost ~]$ newgrp group1 # 临时切换到组group1下，拥有其权限密码：[egon@localhost ~]$ echo 123 &gt;&gt; /tmp/a.txt [egon@localhost ~]$ cat /tmp/a.txt 123 3.5 组文件3.5.1 &#x2F;etc&#x2F;group 3.5.2 &#x2F;etc&#x2F;gshadow 4 权限管理4.1 修改文件所有者和所属组4.1.1 修改文件所有者12345678chown [OPTION]... [OWNER][:[GROUP]] FILE...chown [OPTION]... --reference=RFILE FILE...OWNER #只修改所有者OWNER:GROUP #同时修改所有者和属组:GROUP #只修改属组，冒号也可用 . 替换--reference=RFILE #参考指定的的属性，来修改 -R #递归，此选项慎用，非常危险！ 范例 123456789101112131415161718192021[root@centos8 data]#chown wang f1.txt[root@centos8 data]#lltotal 4-rw-r--r-- 1 wang root 709 Dec 18 10:13 f1.txt[root@centos8 data]#chown :admins f1.txt[root@centos8 data]#ll f1.txt-rw-r--r-- 1 wang admins 709 Dec 18 10:13 f1.txt[root@centos8 data]#chown root.bin f1.txt[root@centos8 data]#lltotal 4-rw-r--r-- 1 root bin 709 Dec 18 10:13 f1.txt[root@centos8 data]#lltotal 8-rw-r--r-- 1 wang admins 709 Dec 18 10:13 f1.txt-rw-r--r-- 1 root root 23 Dec 18 10:15 f2.txt[root@centos8 data]#chown --reference=f1.txt f2.txt[root@centos8 data]#lltotal 8-rw-r--r-- 1 wang admins 709 Dec 18 10:13 f1.txt-rw-r--r-- 1 wang admins 23 Dec 18 10:15 f2.txt 范例：把data下的所有文件的所有者和所属组都改了（很危险） 1[root@centos8 ~]#chown -R wang.admins /data/ 4.1.2 修改文件所属组1234chgrp [OPTION]... GROUP FILE...chgrp [OPTION]... --reference=RFILE FILE...-R #递归，此选项慎用，非常危险！ 范例 12345[root@centos8 data]#ll f1.txt-rw-r--r-- 1 wang root 709 Dec 18 10:13 f1.txt[root@centos8 data]#chgrp admins f1.txt[root@centos8 data]#ll f1.txt-rw-r--r-- 1 wang admins 709 Dec 18 10:13 f1.txt 4.2 文件权限4.2.1 文件权限说明文件的权限主要针对三类对象进行定义 123owner 属主, ugroup 属组, gother 其他, o 注意 用户的最终权限，是从左向右进行顺序匹配，即，所有者，所属组，其他人，一旦匹配权限立即生效，不再向右查看其权限 r 和 w 权限对root用户无效，对没有读写权限的文件，root用户也可读可写 只要所有者,所属组或other三者之一有x权限,root就可以执行 每个文件针对每类访问者都定义了三种权限 123r Readable 4w Writable 2x eXcutable 1 对文件的权限 123456r 可使用文件查看类工具，比如：cat，可以获取其内容w 可修改其内容,文件的是否被删除和文件的权限无关x 可以把此文件提请内核启动为一个进程，即可以执行（运行）此文件（此文件的内容必须是可执行）二进制的命令：/usr/bin/ls #只需要拥有对它的x权限就可以执行脚本文件(内容是一对普通文本) #需要对它拥有r+x权限 对目录的权限 123456789r 可以使用ls查看此目录中文件名列表,但无法看到文件的属性meta信息,包括inode号,不能查看文件的内容w 可在此目录中创建文件，也可删除此目录中的文件，而和此被删除的文件的权限无关x 可以cd进入此目录，可以使用ls -l file或stat file 查看此目录中指定文件的元数据，当预先知道文件名称时,也可以查看文件的内容,属于目录的可访问的最小权限X 分配给目录或有部分x权限的文件的x权限，对无任意x权限的文件则不会分配x权限#目录权限常见组合- 不能访问目录r-x 只读目录rwx 可读也可写目录 &#x3D;&#x3D;！！！注意 ！！！&#x3D;&#x3D; vim修改的原理是将源文件删掉，然后再将内容的内容覆盖写入了新文件，新文件名重命名为原文件名 当前用户对沿途所有文件夹都有x权限 并且当前用户对目标文件夹有w权限 但是当前用户对目标文件没有w权限 此时当前用可以vim编辑文件内容，并且可以wq!强制保存（输入y确认）退出完成文件修改，其实是将源文件删掉了，可以通过查看前后操作的文件inode号来确定 &#x3D;&#x3D;！！！注意 ！！！&#x3D;&#x3D; 软链接是没有权限的，改它的权限实际是改的是它所指向的文件，也就是原始文件 去看一个文件是否有权限，不仅要知道这个文件的权限，还要考虑到是否允许进入到它所在的这个目录的权限 如果没有执行权限，root也无法执行 即使对一个文件有所有权限，但是在它所在的目录下没有w权限，也无法修改其内容；相反，如果对一个文件什么权限也没有，但是在其目录下有所有权限，也可以修改其内容 文件的执行是危险的，但是目录的执行权限是常规的 4.2.2 修改文件权限1234chmod [OPTION]... MODE[,MODE]... FILE...chmod [OPTION]... OCTAL-MODE FILE...#参考RFILE文件的权限，将FILE的修改为同RFILE，就是复制该文件的权限信息给指定文件chmod [OPTION]... --reference=RFILE FILE... 模式法 12345678chmod who opt per filewho:u(属主)，g(属组)，o(其他)，a(所有)opt(操作):+(增加)，-(删除)，=(赋予)per(权限):r(读)，w(修改)，x(执行)chmod o+w a.txtug=rx #属主属组权限改为读和执行o= #other用户无任何权限 a=rwx #所有用户都有读写执行权限 数字法 12345678910111213#有权限1无权限0--- 000 0--x 001 1-w- 010 2-wx 011 3r-- 100 4r-x 101 5rw- 110 6rwx 111 7rwxrw---- a.txt （二：111 110 000 十：7 6 0）chmod 760 a.txtchmod ugo+rwx directory1 4.2.3 新建文件和目录的默认权限umask值可以控制创建文件、文件夹时的默认权限 新建文件的默认权限: 666-umask值，如果所得结果某位存在执行（奇数）权限，则将其权限+1,偶数不变 基于安全考虑，默认新建的文件没有执行权限，这就是为什么奇数加1的原因 新建目录的默认权限: 777-umask值 root用户的umask默认值为022 文件的权限：644 文件夹的权限：755 其他用户的umask默认值为002 文件的权限：664 文件夹的权限：775 umask设置越小，权限越大，慎用！ 查看umask 12345umask#模式方式显示umask –S#输出可被调用umask –p 范例 123456[root@centos8 ~]#umask0022 #0表示八进制。022影响新建文件的权限[root@centos8 ~]#umask -Su=rwx,g=rx,o=rx[root@centos8 ~]#umask -pumask 0022 修改umask 12umask 002umask u=rw,g=r,o= 持久保存umask 全局设置： &#x2F;etc&#x2F;bashrc 用户设置：~&#x2F;.bashrc 范例：只实现临时文件权限为000 123456#第一种touch a.txt ;chmod 000 a.txt#第二种umask 777 ;touch a.txt ;umask 022#第三种(umask 777;touch a.txt) 4.2.4 特殊权限4.2.4.1 SUIDSUID 作用于二进制可执行文件上，用户将继承此程序所有者的权限 给命令文件加的权限 chmod u+s /usr/bin/passwd 加了suid权限的命令在启动时的用户身份会用自己的命令文件的属主身份 数字代号：4 前提：进程有属主和属组；文件有属主和属组 任何一个可执行程序文件能不能启动为进程,取决发起者对程序文件是否拥有执行权限 启动为进程之后，其进程的属主为发起者,进程的属组为发起者所属的组 进程访问文件时的权限，取决于进程的发起者 二进制的可执行文件上SUID权限功能： 任何一个可执行程序文件能不能启动为进程：取决发起者对程序文件是否拥有执行权限 启动为进程之后，其进程的属主为原程序文件的属主 SUID只对二进制可执行程序有效 SUID设置在目录上无意义 SUID权限设定 123chmod u+s FILE...chmod 4xxx FILEchmod u-s FILE... 范例 12[root@centos8 ~]#ls -l /usr/bin/passwd-rwsr-xr-x. 1 root root 34928 May 11 2019 /usr/bin/passwd 4.2.4.2 SGID作于于目录上, 此目录中新建的文件的所属组将自动从此目录继承 可以给文件夹加 chmod g+s /aaa 后续在&#x2F;aaa 文件夹下创建的文件、文件夹的属组都会继承自&#x2F;aaa文件夹的属组 数字代号：2 二进制的可执行文件上SGID权限功能： 任何一个可执行程序文件能不能启动为进程：取决发起者对程序文件是否拥有执行权限 启动为进程之后，其进程的属组为原程序文件的属组 SGID权限设定 123chmod g+s FILE...chmod 2xxx FILEchmod g-s FILE... 目录上的SGID权限功能：默认情况下，用户创建文件时，其属组为此用户所属的主组，一旦某目录被设定了SGID，则对此目录有写权限的用户在此目录中创建的文件所属的组为此目录的属组，通常用于创建一个协作目录 4.2.4.3 StickySTICKY 作用于目录上，此目录中的文件只能由所有者或者root来删除，sticky 设置在文件上无意义 chmod o+t /share 在&#x2F;share文件下的文件只能被root用户及属主自己操作 数字代号：1 Sticky权限设定 123chmod o+t DIR...chmod 1xxx DIRchmod o-t DIR... 4.3 特殊属性设置文件的特殊属性，可以访问 root 用户误操作删除或修改文件，限制root的权限 不能删除，改名，更改：chattr +i file 只能追加内容，不能删除，改名：chattr +a file 显示特定属性：lsattr 范例 1234567891011121314[root@centos7 ~]#chattr +i a.txt [root@centos7 ~]#lsattr a.txt----i----------- a.txt[root@centos7 ~]#rm -rf a.txtrm: cannot remove ‘a.txt’: Operation not permitted[root@centos7 ~]#echo aaa &gt;&gt; a.txt-bash: a.txt: Permission denied[root@centos7 ~]#mv a.txt p.txtmv: cannot move ‘a.txt’ to ‘p.txt’: Operation not permitted[root@centos7 ~]#cat a.txtBEGIN&#123;print strftime(&quot;%Y-%m-%dT%H:%M&quot;,systime()-3600)&#125;[root@centos7 ~]#chattr -i a.txt [root@centos7 ~]#lsattr a.txt---------------- a.txt 范例 1234567chattr +a file1 #只允许以追加方式读写文件chattr +c file1 #允许这个文件能被内核自动压缩/解压chattr +d file1 #在进行文件系统备份时，dump程序将忽略这个文件chattr +i file1 #设置成不可变的文件，不能被删除、修改、重命名或者链接chattr +s file1 #允许一个文件被安全地删除chattr +S file1 #一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘chattr +u file1 #若文件被删除，系统会允许你在以后恢复这个被删除的文件 4.4 访问控制列表 ACL4.4.1 ACL权限功能ACL：实现灵活的权限管理 除了文件的所有者，所属组和其它人，可以对更多的用户设置权限，比如对单个用户设置权限 CentOS7 默认创建的xfs和ext4文件系统具有ACL功能 CentOS7 之前版本，默认手工创建的ext4文件系统无ACL功能,需手动增加 12tune2fs –o acl /dev/sdb1mount –o acl /dev/sdb1 /mnt/test ACL生效顺序：所有者，自定义用户，所属组|自定义组，其他人 4.4.2 ACL相关命令setfacl 可设置ACL权限 12345678910111213141516setfacl [-bkndRLPvh] [&#123;-m|-x&#125; acl_spec] [&#123;-M|-X&#125; acl_file] file ...-m #修改acl权限-M #从文件读取规则-x #删除文件acl 权限-Xe #从文件读取规则-b #删除文件所有acl权限-k #删除默认acl规则-n #不重新计算mask值-d #在目录上设置默认acl-R #递归执行-L #将acl 应用在软链接指向的目标文件上，与-R一起使用-P #将acl 不应用在软链接指向的目标文件上，与-R一起使用--set #用新规则替换旧规则，会删除原有ACL项，用新的替代，一定要包含UGO的设置，不能象 -m一样只有 ACL--set-file #从文件读取新规则--mask #重新计算mask值 范例 1234567891011121314151617181920#对单个用户设置权限setfacl -m u:username:rwx|0(-) file #u表示用户，rwx表示赋予什么权限，不赋予权限表示0(-)##清除所有ACL权限setfacl -b file1#复制file1的acl权限给file2getfacl file1 | setfacl --set-file=- file2 #设置 tom 无任何权限[root@ubuntu2204 tmp]# setfacl -m u:tom:- f1[root@ubuntu2204 tmp]# getfacl f1# file: f1# owner: root# group: rootuser::rw-user:tom:---group::r--mask::r--other::r-- 范例：给组加ACL 123456789101112[root@ubuntu2204 tmp]# setfacl -m g:tom:rwx f1[root@ubuntu2204 tmp]# getfacl f1# file: f1# owner: root# group: rootuser::rw-user:mage:---user:jerry:rw-group::r--group:tom:rwxmask::rwxother::r-- 范例：–set替换 123456789101112131415161718192021222324252627[root@ubuntu2204 tmp]# ll f2-rw-rwxr--+ 1 root root 3 Jun 26 20:17 f2[root@ubuntu2204 tmp]# getfacl f2# file: f2# owner: root# group: rootuser::rw-user:mage:---user:jerry:rw-group::r--group:tom:rwxmask::rwxother::r--[root@ubuntu2204 tmp]# setfacl --set u::rw,u:jerry:-,g::-,o::- f2[root@ubuntu2204 tmp]# ll f2-rw-------+ 1 root root 3 Jun 26 20:17 f2[root@ubuntu2204 tmp]# getfacl f2# file: f2# owner: root# group: rootuser::rw-user:jerry:---group::---mask::---other::--- 范例：设置wu用户不能访问data目录下的a2.txt文件 123456789101112131415161718[root@centos7 data]#ll a2.txt -rw-r--r--. 1 root root 12 Aug 16 16:59 a2.txt[root@centos7 data]#su wu[wu@centos7 data]$cat a2.txt aefdgh[wu@centos7 data]$exitexit[root@centos7 data]#setfacl -m u:wu:0 a2.txt [root@centos7 data]#ll a2.txt -rw-r--r--+ 1 root root 12 Aug 16 16:59 a2.txt[root@centos7 data]#su wu[wu@centos7 data]$cat a2.txt cat: a2.txt: Permission denied getfacl 可查看设置的ACL权限 123456789[root@centos7 data]#getfacl a2.txt# file: a2.txt# owner: root# group: rootuser::rw-user:wu:---group::r--mask::r--other::r-- mask 权限 mask只影响除所有者和other的之外的人和组的最大权限 mask需要与用户的权限进行逻辑与运算后，才能变成有限的权限(Effective Permission) 用户或组的设置必须存在于mask权限设定范围内才会生效 范例 1setfacl -m mask::rx file 对于脚本程序来讲，必须先要有读权限，才能执行 4.5 切换用户或以其他用户身份执行命令如果在当前登录终端中，要执行某条命令，但当前登录用户又没有可执行权限或没有某些资源权限；则在此种情况下，我们可以： 让有权限的用户登录终端，再执行相应的操作； 在当前终端终，临时切换，以有权限的用户的身份去执行命令； 4.5.1 su切换用户su: 即 switch user，命令可以切换用户身份，并且以指定用户的身份执行命令 1234567su [options...] [-] [user [args...]]-c&lt;指令&gt;或--command=&lt;指令&gt; #不切换用户，而是临时使用该用户权限和环境执行命令-f或--fast #适用于csh与tsch，使shell不用去读取启动文件。-l或--login #完全切换，改变身份时，也同时变更工作目录，以及HOME,SHELL,USER,LOGNAME。此外，也会变更PATH变量 -m,-p或--preserve-environment #变更身份时，不要变更环境变量。-s&lt;shell&gt;或--shell=&lt;shell&gt; #指定要执行的shell（bash csh tcsh 等），预设值为 /etc/passwd 内的该使用者（USER） shell 按照进入shell环境方式的不同，分为两种 登录shell：登录账号密码，su - egon 会读取目标用户的配置文件，切换至自已的家目录 非登录shell：在shell环境中直接输入命令进入的shell环境，su egon 不会读取目标用户的配置文件，不改变当前工作目录 切换新用户后，使用 exit 退回旧的用户身份，而不要再用 su 切换至旧用户，否则会生成很多的bash子进程，环境可能会混乱 二者的区别的就是加载的配置文件不同 登录shell 12345/etc/profile/etc/profile.d/脚本/etc/bashrc # rocky9才加载的~/.bash_profile~/.bashrc 非登录shell 123~/.bashrc/etc/bashrc/etc/profile.d/脚本 注意: 如果全局配置和个人配置产生冲突，以个人配置为准。 直接以另一个用户执行命令，而不切换用户 1su [-] UserName -c &#x27;COMMAND&#x27; 范例 12345678[root@centos8 ~]#su - wang -c &#x27;touch wang.txt&#x27;[root@centos8 ~]#ll ~wang/total 0-rw-rw-r-- 1 wang wang 0 Mar 27 09:31 wang1.txt-rw-rw-r-- 1 wang wang 0 Mar 27 09:32 wang2.txt[root@centos8 ~]#su -s /bin/bash -c &#x27;whoami&#x27; binbin 范例 1234[root@centos8 ~]#su -s /bin/bash binbash-4.4$ whoamibinbash-4.4$ 完全切换和不完全切换的区别 1234567891011121314151617181920jose@ubuntu2204:~$ pwd/home/josejose@ubuntu2204:~$ echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin#不完全切换jose@ubuntu2204:~$ su rootPassword:[root@ubuntu2204 jose]# pwd/home/jose[root@ubuntu2204 jose]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin#完全切换jose@ubuntu2204:~$ su - rootPassword:[root@ubuntu2204 ~]# pwd/root[root@ubuntu2204 ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/root/bin 4.5.2 sudo让普通用户只用自己的账号密码进行认证，操作时可以临时获取管理的某种权限 修改sudo配置文件的两种方式 visudo 专门编辑文件&#x2F;etc&#x2F;sudoers visudo -c # 检查文件语法 vi /etc/sudoers 详细看 《加密与安全》 4.5.3 超级用户说明1、在生产环境中，一般会禁止root帐号通过SSH远程连接服务器（保护好皇帝），当然了，也会更改默认的SSH端口（保护好皇宫），以加强系统安全。 2、企业工作中：没有特殊需求，应该尽量不要登录root用户进行操作，应该在普通用户下操作任务，然后用sudo管理普通用户的权限，可以细到每个命令权限分配。 3、在linux系统中，uid为0的用户就是超级用户。但是通常不这么做，如果确实有必要在某一操作上用到管理的权限的话，那就用sudo单独授权，也不要直接用uid为0的用户。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"文件结构与IO重定向","slug":"Linux/basics/file-structure-and-IO-redirects","date":"2025-08-19T05:52:56.000Z","updated":"2025-09-06T11:18:24.293Z","comments":true,"path":"Linux/basics/file-structure-and-IO-redirects/","permalink":"https://aquapluto.github.io/Linux/basics/file-structure-and-IO-redirects/","excerpt":"","text":"1 文件系统目录结构 文件和目录被组织成一个单根倒置树结构 文件系统从根目录下开始，用“&#x2F;”表示 根文件系统(rootfs)：root filesystem 标准Linux文件系统（如：ext4），文件名称大小写敏感，例如：MAIL, Mail, mail, mAiL以 . 开头的文件为隐藏文件 路径分隔的 &#x2F; 文件名最长255个字节 包括路径在内文件名称最长4095个字节 蓝色–&gt;目录 绿色–&gt;可执行文件 红色–&gt;压缩文件 浅蓝色–&gt;链接文件 灰色–&gt;其他文件 除了斜杠和NUL,所有字符都有效.但使用特殊字符的目录名和文件不推荐使用，有些字符需要用引号来引用 每个文件都有两类相关数据：元数据：metadata，即属性， 数据：data，即文件内容 Linux的文件系统分层结构：FHS Filesystem Hierarchy Standard 参考文档： http://www.pathname.com/fhs/ 1.1 各个目录功能12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#命令相关目录● /usr/bin：普通用户使用命令● /usr/sbin：root用户使用命令#启动目录● /boot：存放的启动相关文件，例如kernel，grub（引导装载程序）#系统文件目录● /usr/lib或/usr/lib64：库文件Glibc#用户家目录● /home：普通用户● /root：root用户#配置文件目录● /etc/sysconfig/network-script/：网络配置文件目录，rocky9有变动为/etc/NetworkManger/system-connections● /etc/hostname：系统主机名配置文件● /etc/resolv.conf：dns客户端配置文件● /etc/hosts：本地域名解析配置文件● /etc/fstab：系统挂载目录，开机自启动挂载列表● /etc/passwd：系统用户文件#设备目录文件● /dev/cdrom和/dev/sr0：系统光盘镜像设备● /dev/null：黑洞设备，只进不出，类似于垃圾回收站● /dev/random：生成随机数的设备● /dev/zero：能源源不断的产生数据● /dev/pts/0：虚拟的Bash Shell终端，提供给远程客户用，0代表第一个终端● /dev/stderr：错误输出● /dev/stdin：标准输入● /dev/stdout：标准输出#可变目录，存放一些变化文件，比如数据库，日志，邮件等● mysql：/var/lib/mysql● vsftpd：/var/ftp● mail：/var/spool/mail● cron：/var/spool/cron● log：/var/log/messages系统日志；/var/log/secure系统登陆日志● tmp：/var/tmp程序产生的临时文件#虚拟文件系统，反应出来的是内核，进程信息或实时状态，类似于汽车的仪表盘● /proc/meminfo：内存信息● /proc/cpuinfo：cpu信息#存储设备挂载目录● /media：移动设备默认挂载点● /mnt：手工挂载设备挂载点● /opt：早期第三方厂商的软件存放目录● /tmp：临时存放文件目录#centos7特有目录● /lost+found：使用ext2/ext3才有，存储发生意外后丢失的文件，只有root用户可以打开● /lost+found/run：存放程序运行后产生的pid文件● /lost+found/srv：物理设备产生的文件● /lost+found/sys：硬件设备的驱动程序信息 1.2 Linux下的文件类型1234567- 普通文件d 目录文件directoryl 符号链接文件linkb 块设备blockc 字符设备characterp 管道文件pipes 套接字文件socket linux 系统中的文件类型颜色标识 linux 系统中，每种颜色，都有对应的含义，可以根据文件在终端中显示的颜色，来判断是什么类型的文件 颜色与文件类型对应关系，由配置文件定义，可更改（此处的文件类型，可以理解为文件格式） 123456789101112vim /etc/DIR_COLORS普通文件 #白色 目录文件 #蓝色 符号链接文件 #浅蓝色 块设备 #黄色 字符设备 #黄色管道文件 #青黄套接字文件 #粉红图片文件 #粉红压缩文件或文件包 #红色其他文件 #灰色 管道文件 所谓管道，是指用于连接一个读进程和一个写进程，以实现它们之间通信的共享文件，又称 pipe 文件。 套接字文件 Socket本身有“插座”的意思，在Unix&#x2F;Linux环境下，用于表示进程间网络通信的特殊文件类型。本质为内核借助缓冲区形成的伪文件。 2 文件系统和inode表 在Linux系统中，一切皆文件，每个文件，又分为文件元数据和具体内容两部份，一个文件元数据和其具体内容数据，在磁盘分区上，是分开存放的。 这种存储文件元数据的区域就叫 inode，中文译作 “索引节点”，每个文件都有一个inode和n(n&gt;&#x3D;1)个block 数据块，inode 存储文件元数据，数据块存储文件具体内容数据 磁盘在格式化时，系统会自动将磁盘分为两个区域，一个是 inode 区（inode table），用来存放文件的 inode，另一个是数据区，分成很多个block(块)，用来存放文件的具体内容数据 2.1 文件系统文件系统：操作系统内核中用来控制硬盘的一种程序 每个硬盘分区都要有一个文件系统 硬盘分区—-》打隔断，分割出一个个小空间 硬盘的最小存取单位-》扇区（512字节，相当于0.5KB） 文件系统—-》对一个个小空间做装修，负责把空间的数据组织好 操作系统的最小存取单位-》block块（4KB，8个扇区） 操作系统读取硬盘的时候，不会一个扇区一个扇区地读取，这样效率太低，于是操作系统中的文件系统负责将磁盘的多扇区组织成一个个的block块，这样操作系统就可以一次性读取一个”块”（block），即一次性连续读取多个扇区，所以文件系统组织好了之后带来的方便之处 使用者—–》block块（文件系统）—–》n个扇区（硬盘的读写单位） 一个文件系统包含的三大类块 inode block块：存放文件的元数据 ls -l 的结果如权限、属主、数组 对一个文件来说，inode block就1个 data block块：存放文件的内容数据 cat看到的结果，真正的内容 对一个文件来说，如果过大，data block可能有多个 superblock超级块：记录此filesystem的整体信息，包括inode&#x2F;block的总量、使用量、剩余量，以及文件系统的格式与相关信息等； superblock一个文件系统整体就一个 硬盘满了，分两种情况 inode号耗尽 磁盘空间耗尽 如上图，每一个inode表记录对应的保存了以下信息： inode number 节点号 文件类型 权限 UID GID 链接数（指向这个文件名路径名称个数） 该文件的大小和不同的时间戳 指向磁盘上文件的数据块指针 有关文件的其他数据 总结 磁盘在格式化时，系统会自动将磁盘分为两个区域，一个是 inode 区（inode table），用来存放文件的 inode，另一个是数据区，分成很多个block(块)，用来存放文件的具体内容数据 一个磁盘分区上有多少个inode和多少个block，由系统自行决定，跟文件系统，磁盘分区大小，数据块大小有关 一个磁盘分区，能存放多少个文件，由文件大小，磁盘分区大小，inode数量决定 2.2 查找文件内容的底层流程打开一个文件以后，系统就以inode号码来识别这个文件，不再考虑文件名。因此，通常来说，系统无法从inode号码得知文件名 文件夹也是文件： 元数据：权限、属主、属组——》inode block块 内容数据：存的该文件夹包含的——–》data block块 子文件名—–》inode块的编号 子文件夹名—–》inode块的编号 普通文件： 元数据：权限、属主、属组——–》inode block块 内容数据：你写的文件中的数据——–》data block块 如图，比如查看&#x2F;etc&#x2F;passwd：/ ---&gt; etc ---&gt; passwd 2.3 创建、修改、删除文件对于inode号的影响创建和复制文件，会占用inode号 移动文件，目标和源在相同的文件系统，不影响inode号码；在不同的文件系统，相当于cp和rm 重命名文件，只是改变文件名，不影响inode号 删除文件，释放的inode号可以被重用，数据实际上不会马上被删除，当另一个文件使用数据块时将被覆盖 当每次修改完服务器配置文件后，为什么都需要重新加载一下配置文件呢？因为vim每次修改完后，Inode号都会变，系统还是读取的原来inode号的配置文件，每次修改完服务器的配置文件，都要重启服务，重新读一下配置文件 123456#分区越大，可用的节点编号就越多，当前分区有多少个节点编号，就是说能在当前分区上创建多少个文件#如果当前分区上的节点编号用光，则无法再创建新文件，系统会提示 “No space left on device”#如果当前分区上的空间用光，同样无法创建新文件，系统同样提示 “No space left on device”磁盘分区还有空间，但提示没有足够空间创建文件，这就是因为 inode 编号耗尽的原因inode 编号资源还有，但磁盘数据空间被耗尽，同样无法创建文件 3 硬链接和软链接3.1 硬链接硬链接：指向文件系统中某个文件的实际物理位置的直接链接。在Linux中，文件实际上是存储在磁盘上的数据块（或inode）的引用。硬链接就是这些数据块的另一个名称或引用。创建硬链接相当于给文件增加了一个新的名字 目标文件与源文件指向同一个inode号 改动一个文件元数据或内容，另外一个也跟着变 删除源文件，仅仅只是解除了源文件名与inode号的关联关系，所以不会影响目标文件 硬链接无法跨分区 不能对目录做硬链接 使用场景 备份**：硬链接常用于备份，因为它们不占用额外**的磁盘空间（不需要额外存储数据和inode结构，但是需要存储目录项）。 文件重命名或移动**：**在重命名或移动文件时，硬链接可以保持文件的一致性。 多人共享**：**当多人需要对同一个文件进行操作的时候，如果每次都是直接操作原始文件，一旦有一个人执行了误删除，则该文件将立即永久消失。但如果每个人都在私人目录中创建一个该文件的硬链接，即使有一个人误删了他自己的文件，也不会导致原始文件被删除，大幅降低文件意外丢失的概率 1234ln filename [linkname ]# 给a.txt起了aa.txt的别名ln a.txt aa.txt 3.2 软链接软连接：类似于 Windows 中的快捷方式，包含的是另一个文件路径名的文本指针。当访问软链接时，系统会读取软链接文件中存储的路径信息，然后根据这个路径找到并访问目标文件。如果目标文件被移动或删除，软链接将失效，因为它存储的路径不再指向一个有效的文件 目标文件指向的是源文件的文件名，包含的不是文件的实际数据，具有不同的inode号 改动一个文件内容，另外一个也跟着变，但改元数据的话，彼此之间不会互相影响 删除源文件，目标文件不可用 软连接可以跨分区，跨文件系统，因为是指向文件名 可以对目录做软链接 注意问题 软链接路径：使用相对路径时，源文件路径（第一个参数）是相对于软链接文件所在目录的，软链接路径（第二个参数）是相对于当前工作目录的，即它们都相对于同一个当前目录，不可以是不同的目录 权限问题：软链接的权限总是lrwxrwxrwx，但实际访问权限取决于源文件。 备份和恢复**：**在备份和恢复时，软链接可能需要特别处理，以保持其指向正确的位置。 123456ln -s 源文件路径 软链接路径[root@ubuntu2204 ~]#ln -s /etc/issue /boot/issue.link /boot/issue.link这个符号链接指向/etc/issue文件访问/boot/issue.link相当于访问/etc/issue 范例：删除软链接文件 1234rm -rf /boot/issue.link # 只删除软链接本身（链接文件），不会删除源目录内容rm -rf /boot/issue.link/ # 删除源目录内容（链接指向的目录），但不会删除链接文件。此方法非常危险# 注意: 删除此软链接务必不要加-r选项 4 IO重定向和管道4.1 文件描述符文件描述符的概念：进程但凡打开一个文件，操作性系统都会为打开的文件分配一个编号（&gt;&#x3D;0的证书），这个编号就称之为文件描述符（也称之为文件句柄） Linux给程序提供三种 I&#x2F;O 设备 标准输入（STDIN） 0文件描述符：代表标准输入 标准输出（STDOUT） 1文件描述符：代表标准正确输出 标准错误（STDERR） 2文件描述符：代表标准错误输出 在Linux系统中，一切皆文件，所以，这三个设备也是以文件的形式存在于系统中；程序从标准输入文件中获取数据，再将运行结果和错误信息输出到标准输出设备和标准错误输出设备 一个进程运行起来之后，我们要用它的时候无非就是与之交互，交互&#x3D;输入+输出 整体的关系：进程(打开文件)———–》文件描述符——-》操作系统内核——–》硬盘 输入输出背后对应的机制是什么？在linux系统中很简单，一切皆文件，所以对于进程来讲 输入操作—-》某个打开的文件—》某个文件描述符 输出操作—-》某个打开的文件—》某个文件描述符 $$：当前运行的 shell 进程的进程 ID（PID） 123456[root@centos7 ~]#ll /proc/$$/fdtotal 0lrwx------ 1 root root 64 Oct 23 16:21 0 -&gt; /dev/pts/2lrwx------ 1 root root 64 Oct 23 16:21 1 -&gt; /dev/pts/2lrwx------ 1 root root 64 Oct 23 16:21 2 -&gt; /dev/pts/2lrwx------ 1 root root 64 Oct 23 16:23 255 -&gt; /dev/pts/2 范例：假设我们打开了另外一个窗口，用 tail -f 命令打开了一个文件（还处于打开状态），如果我们想看到这个文件描述符，就去之前的窗口输入ll /proc/pidof tail/fd，数字是随机分的，但是012是固定的 123456789101112[root@centos7 ~]#tail -f anaconda-ks.cfg [root@centos7 ~]#ll /proc/`pidof tail`/fd #文件处于打开状态total 0lrwx------ 1 root root 64 Oct 23 16:28 0 -&gt; /dev/pts/3lrwx------ 1 root root 64 Oct 23 16:28 1 -&gt; /dev/pts/3lrwx------ 1 root root 64 Oct 23 16:28 2 -&gt; /dev/pts/3lr-x------ 1 root root 64 Oct 23 16:28 3 -&gt; /root/anaconda-ks.cfglr-x------ 1 root root 64 Oct 23 16:28 4 -&gt; anon_inode:inotify[root@centos7 ~]#ll /proc/`pidof tail`/fd #文件关闭ls: cannot access /proc//fd: No such file or directory 4.2 I&#x2F;O重定向 redirect4.2.1 标准输出和错误重定向12命令 操作符号 文件名-：表示标准输出或者标准输入 支持的操作符号 123451&gt; 或 &gt; #把STDOUT重定向到文件2&gt; #把STDERR重定向到文件&amp;&gt; #把标准输出和错误都重定向&gt;&amp; #和上面功能一样，建议使用上面方式注意：以上如果文件已存在，文件内容会被覆盖 追加 12&gt;&gt; #追加标准输出重定向至文件2&gt;&gt; #追加标准错误重定向至文件 标准输出和错误输出各自定向至不同位置 1COMMAND &gt; /path/to/file.out 2&gt; /path/to/error.out 合并标准输出和错误输出为同一个数据流进行重定向 12345&amp;&gt; 覆盖重定向&amp;&gt;&gt; 追加重定向COMMAND &gt; /path/to/file.out 2&gt;&amp;1 （顺序很重要）COMMAND &gt;&gt; /path/to/file.out 2&gt;&amp;1 范例：标准输出至其它终端 1234567[root@centos7 ~]#tty/dev/pts/2[root@centos7 ~]#ls &gt; /dev/pts/3[root@centos7 ~]#tty/dev/pts/3[root@centos7 ~]#anaconda-ks.cfg 范例：命令输出重定向 12345678910111213[root@centos7 ~]#hostname &gt; /root/data/data.txt[root@centos7 ~]#cat /root/data/data.txtcentos7[root@centos7 ~]#uname -r &gt; /root/data/data.txt #会覆盖之前的内容[root@centos7 ~]#cat /root/data/data.txt 3.10.0-1160.el7.x86_64[root@centos7 ~]#hostname &gt; /root/data/data.txt[root@centos7 ~]#uname -r &gt;&gt; /root/data/data.txt #追加不会覆盖[root@centos7 ~]#cat /root/data/data.txtcentos73.10.0-1160.el7.x86_64 范例：合并多个命令的结果至一个文件中 123456789[root@centos7 ~]#&#123; hostname;uname -r;&#125; &gt; /root/data/data.log[root@centos7 ~]#cat /root/data/data.logcentos73.10.0-1160.el7.x86_64[root@centos7 ~]#( hostname -I;uname -r ) &gt; /root/data/data.log[root@centos7 ~]#cat /root/data/data.log10.0.0.183 3.10.0-1160.el7.x86_64 范例：标准输出和标准错误分别放到一个文件里 1234567[root@centos7 ~]#ls data/ /xxx &gt; stdout.log 2&gt; stderr.log[root@centos7 ~]#cat stdout.log data/:scripts[root@centos7 ~]#cat stderr.log ls: cannot access /xxx: No such file or directory 范例：标准输出和标准错误都放到一个文件里 123456789101112131415161718192021222324#第一种[root@centos7 ~]#ls anaconda-ks.cfg xxx &amp;&gt;g.txt[root@centos7 ~]#cat g.txt ls: cannot access xxx: No such file or directoryanaconda-ks.cfg#第二种[root@centos7 ~]#ls anaconda-ks.cfg xxx &gt;f.txt 2&gt;&amp;1[root@centos7 ~]#cat f.txt ls: cannot access xxx: No such file or directoryanaconda-ks.cfg#第三种[root@centos7 ~]#ls anaconda-ks.cfg xxx 2&gt;t.txt 1&gt;&amp;2[root@centos7 ~]#cat t.txt ls: cannot access xxx: No such file or directoryanaconda-ks.cfg#错误写法#标准错误输出重定向至标准输出之前，要先定义好标准输出的文件[root@ubuntu2204 ~]# ls fstab null 2&gt;&amp;1 &gt; out.log#标准输出重定向至标准错误输出之前，要先定义好标准错误输出的文件[root@ubuntu2204 ~]# ls fstab null 1&gt;&amp;2 2&gt;out.log 范例: 实现标准输出和错误的互换 1234567#子进程中借用中间文件描述符3，将标准输出和标准错误输出作了对换[root@centos8 ~]#( cat /etc/centos-release /etc/xxx 3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3 ) &gt; f1.txt 2&gt; f2.txt[root@centos8 ~]#cat f1.txtcat: /etc/xxx: No such file or directory[root@centos8 ~]#cat f2.txtCentOS Linux release 8.2.2004 (Core) 4.2.2 标准输入重定向从文件中导入STDIN，代替当前终端的输入设备，使用 &lt; 来重定向标准输入 某些命令能够接受从文件中导入的STDIN 标准输入重定向是使用文件来代替键盘的输入，从文件中读取数据，代替当前终端的输入设备输入的数据 怎么判断命令能使用标准输入重定向？不跟任何选项参数，直接回车，看是否等待标准输入，如果是，则该命令可以使用标准输入重定向。 实现标准输入重定向的符号 COMMAND &lt; FILE bc 命令：可以避免在键盘屏幕上输入内容做计算 123456789[root@centos7 ~]#echo 2*3 &gt; data.txt # data.txt存放echo 2*3的标准输出[root@centos7 ~]#bc &lt; data.txt # data.txt作为bc的标准输入6[root@centos7 ~]#seq -s+ 10 &gt; seq.log # seq.log存放seq -s+ 10的标准输出[root@centos7 ~]#bc &lt; seq.log # seq.log作为bc的标准输入55注意：data.txt和seq.log只是临时文件，要是不想是临时文件，需要和管道符应用 cat命令 一行一行重定向 cat &gt; file 123456789[root@centos7 ~]#cat &gt; a.logabc^C[root@centos7 ~]#cat a.log abc 多行重定向 使用 “&lt;&lt;终止词” 命令从键盘把多行重导向给STDIN，直到终止词位置之前的所有文本都发送给STDIN 终止词可以是任何一个或多个符号，比如：!，@，$，EOF（End Of File），magedu等，其中EOF比较常用 cat &gt; file &lt;&lt;终止词 123456789[root@centos7 ~]#cat &gt; b.log &lt;&lt;EOF&gt; a&gt; b&gt; c&gt; EOF[root@centos7 ~]#cat b.log abc 4.2.3 高级重定向写法格式1 123456cmd &lt;&lt;&lt;&quot;string&quot; #cmd要支持标准输入，字符串把传给cmd，作为这个命令的标准输入[root@centos7 ~]#bc &lt;&lt;&lt; &quot;2+2&quot;4[root@rocky8 ~]#tr &#x27;a-z&#x27; &#x27;A-Z&#x27; &lt;&lt;&lt;&quot;I am wang&quot;I AM WANG 格式2 123456789cmd1 &lt; &lt;(cmd2) #把cmd2的输出写到一个临时文件里，然后把这个文件的内容传给cmd1，作为cmd1的标准输入&lt;(cmd2) 表示把cmd2的输出写入一个临时文件，注意：&lt;和（之间无空格cmd1 &lt; 标准输入重定向把两个合起来，就是把cmd2的输出stdout传递给cmd1作为输入stdin, 中间通过临时文件做传递[root@rocky8 ~]#tr &#x27;a-z&#x27; &#x27;A-Z&#x27; &lt; &lt;(echo I am wang)I AM WANG[root@rocky8 ~]#ll &lt;(echo I am wang)lr-x------ 1 root root 64 Nov 26 17:20 /dev/fd/63 -&gt; &#x27;pipe:[30384]&#x27; 4.3 管道符管道（使用符号“|”表示）用来连接多个命令，将前一个命令的输出作为后一个命令的输入 使用管道，要求前一个命令必须支持标准输出，后一个命令必须支持标准输入 格式：命令1 | 命令2 | 命令3 | … 功能说明： 将命令1的STDOUT发送给命令2的STDIN，命令2的STDOUT发送到命令3的STDIN 所有命令会在当前shell进程的子shell进程中执行 组合多种工具的功能 STDERR默认不能通过管道转发，可利用2&gt;&amp;1 或 |&amp; 实现，格式如下 12命令1 2&gt;&amp;1 | 命令2命令1 |&amp; 命令2 范例 1[root@centos8 ~]#df | tr -s &#x27; &#x27; 范例 1234567891011121314151617181920[root@centos8 ~]#ls /data /xxx | tr &#x27;a-z&#x27; &#x27;A-Z&#x27;ls: cannot access &#x27;/xxx&#x27;: No such file or directory/DATA:ALL.LOGF1.TXTPASSWD.LOG[root@centos8 ~]#ls /data /xxx 2&gt;&amp;1 | tr &#x27;a-z&#x27; &#x27;A-Z&#x27;LS: CANNOT ACCESS &#x27;/XXX&#x27;: NO SUCH FILE OR DIRECTORY/DATA:ALL.LOGF1.TXTPASSWD.LOG[root@centos8 ~]#ls /data /xxx |&amp; tr &#x27;a-z&#x27; &#x27;A-Z&#x27;LS: CANNOT ACCESS &#x27;/XXX&#x27;: NO SUCH FILE OR DIRECTORY/DATA:ALL.LOGF1.TXTPASSWD.LOG 范例 123456#转换为大写字母ls | tr ‘a-z’ ‘A-Z’#less实现分页地查看输入ls -l /etc | less#算术运算echo &quot;2^3&quot; |bc 范例：发送邮件 12345678910[root@rocky86 ~]# vim /etc/mail.rcset from=1701785325@qq.comset smtp=smtp.qq.comset smtp-auth-user=1701785325@qq.comset smtp-auth-password=meenopnxjawzbfccset smtp-auth=loginset ssl-verify=ignore#mail通过电子邮件发送输入echo &quot;test email&quot; | mail -s &quot;test&quot; wang@example.com 范例：用户密码修改 123456789101112131415161718192021[root@rocky86 ~]# passwd --stdin joseChanging password for user jose.magedupasswd: all authentication tokens updated successfully.[root@centos8 ~]# cat pass.txtcentos[root@rocky86 ~]# passwd --stdin jose &lt; pass.txtChanging password for user jose.passwd: all authentication tokens updated successfully.[root@rocky86 ~]# cat pass.txt | passwd --stdin joseChanging password for user jose.passwd: all authentication tokens updated successfully.[root@rocky86 ~]# echo magedu | passwd --stdin joseChanging password for user jose.passwd: all authentication tokens updated successfully.[root@rocky86 ~]# echo magedu | passwd --stdin jose &amp;&gt; /dev/null","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"introduce","slug":"Linux/kernel-manage","date":"2025-08-18T08:20:18.000Z","updated":"2025-09-09T15:26:02.287Z","comments":true,"path":"Linux/kernel-manage/","permalink":"https://aquapluto.github.io/Linux/kernel-manage/","excerpt":"","text":"1 linux系统的启动流程&#x3D;&#x3D;1、通电自检阶段&#x3D;&#x3D; 通电后，首先执行BIOS（传统）或 UEFI 固件（现代主板，替代 BIOS 的更先进标准），运行POST（加电自检），检查 CPU、内存、硬盘等核心硬件是否正常。 BIOS 的配置数据存放在CMOS芯片中（由主板电池供电保持）； UEFI 则通常将配置存放在主板的非易失性存储器中，功能更强大（支持更大硬盘、图形界面、安全启动等）。 &#x3D;&#x3D;2、引导设备选择与引导程序加载&#x3D;&#x3D; 传统 BIOS：读取启动盘的MBR（主引导记录，位于磁盘第一个扇区，512 字节），其中前 446 字节是引导程序（bootloader），后 64 字节是分区表，最后 2 字节是结束标志（0x55AA）。MBR 将引导程序加载到内存执行。 UEFI：不依赖 MBR，而是从启动盘的ESP 分区（EFI 系统分区） 中读取 EFI 引导程序（如grubx64.efi），直接加载到内存执行，避免了 MBR 的容量限制（如仅支持 2TB 以下磁盘）。 &#x3D;&#x3D;3、Bootloader 阶段&#x3D;&#x3D; 引导程序（如 GRUB2）的作用是定位并加载内核和初始化内存盘（initramfs&#x2F;initrd）： 传统 BIOS 中，MBR 引导程序可能需要加载第二阶段引导程序（如 GRUB 的 stage2），最终找到内核文件（如/boot/vmlinuz-xxx）和初始化内存盘（如/boot/initramfs-xxx.img）。 加载后，引导程序将内核和 initramfs 载入内存，并将控制权交给内核。 &#x3D;&#x3D;4、内核初始化阶段&#x3D;&#x3D; 内核启动后，首先执行自身初始化（如检测硬件、初始化进程管理、内存管理等）。 挂载initramfs（临时根文件系统）：initramfs 包含启动阶段必需的驱动程序（如硬盘控制器、文件系统驱动），确保内核能识别并挂载真正的根文件系统（如/dev/sda1）。 切换根文件系统：内核通过 initramfs 中的工具找到并挂载磁盘上的根文件系统（switch_root），然后卸载 initramfs。 &#x3D;&#x3D;5、用户空间初始化（systemd）&#x3D;&#x3D; 内核启动完成后，运行用户空间的第一个进程systemd（PID&#x3D;1），后续流程由 systemd 主导： 执行default.target（默认目标，如multi-user.target或graphical.target，相当于传统运行级），并按依赖关系启动相关服务。 先启动sysinit.target（初始化系统，如挂载/proc、/sys，加载模块等）和basic.target（基础服务准备）。 启动multi-user.target下的服务（如网络、ssh 等服务器服务），若默认目标是图形界面，还会启动graphical.target依赖的桌面服务。 执行/etc/rc.d/rc.local（仅兼容传统脚本，需chmod +x才会执行，现代推荐用 systemd 服务替代）。 启动getty.target（终端登录服务），提供控制台或图形登录界面。 通过systemd-analyze 工具可以了解启动的详细过程 1234[root@centos8 ~]#systemd-analyze blame# 生成网页systemd-analyze plot &gt; boot.html 操作系统启动起来负责管理一系列进程，这些进程可以分为两大类 内核先启动一个老祖宗程序，pid为0，0号进程负责运行两个顶级程序，产生两个顶级的进程 运行init程序，pid号为1：是所有用户态进程的祖宗 它是系统的第一个进程，负责产生其他所有用户进程 init 以守护进程方式存在，是所有其他进程的祖先 具有启动守护进程，收养孤儿，定期发起wait或waitpid的功能去回收成为僵尸的儿子，将操作系统信号转发给子进程的功能 运行kthreadd程序，pid2：是所有内核态进程的祖宗 在内核完成了系统的各种初始化之后，这个程序需要执行的第一个用户态程就是 init 进程，PID号为1，该进程是系统中所有其他进程的祖宗，在centos6中该祖宗进程称之为init，在centos7之后该祖宗进程名为systemd。即操作系统启动时是先执行内核态代码，然后在内核里调用1号进程的代码，从内核态切换到用户态 2 程序启动加载器bootloaderlinux中的bootloader功能丰富，提供菜单，允许用户选择要启动系统或不同的内核版本；把用户选定的内核装载到内存中的特定空间中，解压、展开，并把系统控制权移交给内核 LILO：LInux LOader，早期的bootloader，功能单一 GRUB: GRand Unified Bootloader，CentOS 7 以后使用GRUB 2.02 2.1 GRUB2启动阶段1、Primary Boot Loader（第一阶段引导程序） 1st stage（第一阶段） 存储在硬盘的0磁道0扇区（MBR 的前 446 字节），这是计算机启动时 BIOS&#x2F;UEFI 首先读取的区域。功能极其简单：仅负责定位并加载下一阶段的引导程序（1.5 stage），因空间限制（仅 446 字节），无法直接识别文件系统或执行复杂操作。 1.5 stage（过渡阶段） 存储在 MBR 后续的扇区（1 扇区到 2047 扇区，约 1MB 空间），主要作用是加载文件系统驱动。由于 1st stage 无法识别分区的文件系统（如 ext4、xfs 等），1.5 stage 通过提供驱动，让引导程序能够识别第二阶段（2nd stage）所在分区的文件系统，确保后续程序可被正确读取。 当 1.5 stage 成功识别文件系统后，会加载 2nd stage 程序，再由它读取配置文件，显示启动菜单并引导用户选择要启动的操作系统。 2、Secondary Boot Loader（第二阶段引导程序）：2nd stage（第二阶段） 存储在磁盘的/boot/grub/目录（通常位于单独的/boot分区），是功能完整的引导程序核心。其配置文件 /boot/grub/grub.conf（或menu.lst）负责定义操作系统的启动信息，包括： 操作系统内核的位置（如/vmlinuz-xxx） 初始化内存盘（initrd）的路径 启动菜单选项、超时时间等 2.2 GRUB2命令grub2常用子命令 12345678910help #获取帮助列表help KEYWORD #获取指定内容帮助boot #使用选中的内核启动set root=(hd#,#) #适用于MBR分区，指定操作系统所在的设备和分区，即指定根分区 hd#: 磁盘编号，用数字表示；从0开始编号 #: 分区编号，用数字表示; 从0开始编号 示例：(hd0,0) 第一块硬盘，第一个分区set root=(hd#,gpt#) #适用于GPT分区linux /PATH/TO/KERNEL_FILE #指定kernel文件位置，后面可以加参数initrd16 /PATH/TO/INITRAMFS_FILE #设定为选定的内核提供额外文件的ramdisk 查看当前启动内核的启动时的参数 12345root@ubuntu2204:~# cat /proc/cmdlineBOOT_IMAGE=/vmlinuz-5.15.0-142-generic root=UUID=7c41b884-0744-438c-a734-dac2c74495f8 ro[root@rocky9 ~]#cat /proc/cmdlineBOOT_IMAGE=(hd0,msdos1)/vmlinuz-5.14.0-570.17.1.el9_6.x86_64 root=/dev/mapper/rl-root ro crashkernel=1G-4G:192M,4G-64G:256M,64G-:512M resume=/dev/mapper/rl-swap rd.lvm.lv=rl/root rd.lvm.lv=rl/swap 2.3 GRUB2管理文件GRUB2的主配置文件：/boot/grub2/grub.cfg GRUB2的配置逻辑是/etc/default/grub（全局参数） + /etc/grub.d/（脚本文件） → 经 grub-mkconfig 生成 → grub.cfg（最终生效配置）。 用户无需直接编辑 grub.cfg，而是通过修改 /etc/default/grub 或 /etc/grub.d/ 下的脚本，再用 grub-mkconfig 更新配置 范例：红帽系统 12345678910111213[root@rocky8 ~]#cat /etc/default/grubGRUB_TIMEOUT=5GRUB_DISTRIBUTOR=&quot;$(sed &#x27;s, release .*$,,g&#x27; /etc/system-release)&quot;GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT=&quot;console&quot;GRUB_CMDLINE_LINUX=&quot;crashkernel=auto resume=/dev/mapper/rl-swap rd.lvm.lv=rl/root rd.lvm.lv=rl/swap rhgb quiet net.ifnames=0&quot;GRUB_DISABLE_RECOVERY=&quot;true&quot;GRUB_ENABLE_BLSCFG=true[root@rocky8 ~]#ls /etc/grub.d/00_header 01_users 10_linux 12_menu_auto_hide 20_ppc_terminfo 30_uefi-firmware 41_custom00_tuned 08_fallback_counting 10_reset_boot_success 20_linux_xen 30_os-prober 40_custom README 范例：Ubuntu 1234567891011121314151617181920212223242526272829303132333435363738root@ubuntu2204:~# cat /etc/default/grub# If you change this file, run &#x27;update-grub&#x27; afterwards to update# /boot/grub/grub.cfg.# For full documentation of the options in this file, see:# info -f grub -n &#x27;Simple configuration&#x27;GRUB_DEFAULT=0GRUB_TIMEOUT_STYLE=hiddenGRUB_TIMEOUT=0GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`GRUB_CMDLINE_LINUX_DEFAULT=&quot;&quot;GRUB_CMDLINE_LINUX=&quot;&quot;# Uncomment to enable BadRAM filtering, modify to suit your needs# This works with Linux (no patch required) and with any kernel that obtains# the memory map information from GRUB (GNU Mach, kernel of FreeBSD ...)#GRUB_BADRAM=&quot;0x01234567,0xfefefefe,0x89abcdef,0xefefefef&quot;# Uncomment to disable graphical terminal (grub-pc only)#GRUB_TERMINAL=console# The resolution used on graphical terminal# note that you can use only modes which your graphic card supports via VBE# you can see them in real GRUB with the command `vbeinfo&#x27;#GRUB_GFXMODE=640x480# Uncomment if you don&#x27;t want GRUB to pass &quot;root=UUID=xxx&quot; parameter to Linux#GRUB_DISABLE_LINUX_UUID=true# Uncomment to disable generation of recovery mode menu entries#GRUB_DISABLE_RECOVERY=&quot;true&quot;# Uncomment to get a beep at grub start#GRUB_INIT_TUNE=&quot;480 440 1&quot;root@ubuntu2204:~# ls /etc/grub.d/00_header 10_linux 20_linux_xen 30_uefi-firmware 40_custom README 05_debian_theme 10_linux_zfs 30_os-prober 35_fwupd 41_custom 2.4 GRUB2加密Centos 123456789101112centos7.2 + 中引入了新的实用程序“ grub2-setpassword ”，为grub加密，防止破解系统root密码(1) 执行 grub2-setpassword 命令# grub2-setpasswordEnter password:Confirm password: (2）如果现在重新启动系统并尝试修改引导条目，系统将要求提供凭据，但是可以在没有凭据的情况下修改引导条目。为了阻止未经授权的修改和未经授权的启动，我们需要对 /boot/grub2/grub.cfg 文件进行更改打开文件并使用密码搜索需要保护的启动条目，它以menuentry开头。找到条目后，从中删除 --unrestricted 参数(3) reboot 重启验证，只有当输入正确的用户名和密码时，才能进入 grub 菜单或者修改引导条目。 12345678910111213141516171819#添加grub2密码,会生在一个配置文件[root@centos8 ~]#grub2-setpasswordEnter password:Confirm password:[root@centos8 ~]#ls -l /boot/grub2/user.cfg-rw------- 1 root root 298 Jan 19 18:20 /boot/grub2/user.cfg[root@centos8 ~]#cat /boot/grub2/user.cfgGRUB2_PASSWORD=grub.pbkdf2.sha512.10000.60AAA29A65F4DC77E8861EF25BDE2034C9B30CE1E07EE688D7F30460E7E87E7356B0893A6DFFB250B27D2EB9D3ED3E9207199C494D7882E2E8C772C82E2DDB7A.5E42FD69FA04293DECD68F077E83875A8E4572A7FBB89BA9F161B15EAFE54FBA963FE5D52E16764944823396231803E5118DA1D9CAF3EB73C175A7D7A3682A90#清空grub密码[root@centos8 ~]#cat /dev/null &gt; /boot/grub2/user.cfg#或者直接删除文件[root@centos8 ~]#rm -f /boot/grub2/user.cfg Ubuntu 123456789101112131415161718192021[root@ubuntu ~]# grub-mkpasswd-pbkdf2Enter password:Reenter password:PBKDF2 hash of your password isgrub.pbkdf2.sha512.10000.DE4B1993A5CB6504E9DA91746673413CD4C3B3FD390FDD4001EAD672DE281448CF2AF64000B95C378D7948E2243771A45E10E1B99FF879C4F8F08EDDA25C5536.331999A3DFD5ECF205440CBBC48B2B1A2F4D08BB8731FA00E03ADBA990C228AD4D3D5D515EAED2350B20DBF74810700A2E0003FAEF3E5F25D24E877385A3714F#修改此文件，将上述生成的内容追加到该文件最后[root@ubuntu ~]# vim /etc/grub.d/40_customset superusers=&quot;tom&quot;password_pbkdf2 tomgrub.pbkdf2.sha512.10000.DE4B1993A5CB6504E9DA91746673413CD4C3B3FD390FDD4001EAD672DE281448CF2AF64000B95C378D7948E2243771A45E10E1B99FF879C4F8F08EDDA25C5536.331999A3DFD5ECF205440CBBC48B2B1A2F4D08BB8731FA00E03ADBA990C228AD4D3D5D515EAED2350B20DBF74810700A2E0003FAEF3E5F25D24E877385A3714F#更新[root@ubuntu ~]# update-grub#重启，下次再选择grub,要输入用户名和密码 2.5 GRUB2故障修复Linux下的grub2引导修复 主要配置文件：&#x2F;boot&#x2F;grub2&#x2F;grub.cfg 修复配置文件：grub2-mkconfig &gt; &#x2F;boot&#x2F;grub2&#x2F;grub.cfg 案例1：CentOS 7,8 破坏MBR后进行恢复 123dd if=/dev/zero of=/dev/sda bs=1 count=446光盘进入救援模式grub2-install --root-directory=/mnt/sysimage /dev/sda 案例2：CentOS 7,8 删除 &#x2F;boot&#x2F;grub2&#x2F; 所有内容进行恢复，即第1阶段或第1.5阶段二进制数据损失，重装 grub2 即可 1234#光盘进入救援模式chroot /mnt/sysimagegrub2-install /dev/sdagrub2-mkconfig -o /boot/grub2/grub.cfg 案例3：CentOS 7,8 删除 &#x2F;boot&#x2F; 下所有文件，即第二阶段，则要重装内核，重装grub2，并重新生成配置文件 123456789101112131415161718192021222324252627#删除/boot/下所有内容[root@rocky86 ~]# rm -rf /boot/*[root@rocky86 ~]# ls /boot/#重启，从光盘启动，进救援模式#切根chroot /mnt/sysimage #centos7chroot /mnt/sysroot #centos8#安装grub2，特别说明：Centos8 必须先修复grub，再安装kernel,否则安装kernel-core时会提示grub出错grub2-install /dev/sda #BIOS环境grub2-install #UEFI环境#挂载光盘mount /dev/sr0 /mnt#安装内核rpm –ivh /mnt/Packages/kernel-3.10.0-1062.el7.x86_64.rpm --force #centos7rpm -ivh /mnt/BaseOS/Packages/k/kernel-core-4.18.0-372.9.1.el8.x86_64.rpm --force #centos8#生成配置文件grub2-mkconfig -o /boot/grub2/grub.cfg#重启exitreboot 2.6 重置root密码2.6.1 CentOS 7后12345678910111213141516171819202122#1、进入bios、从光盘启动#2、点击Troubleshooting#3、进入到Troubleshooting界面选择：Rescue a CentOS Linux system#4、三:进入到Rescue选项 按 ENTER键 选1 ，其他选项意思如下1)continue:救援模式程序会自动查找系统中已有的文件系统，并可读写挂载到/mnt/sysimage目录下。2)Read-Only:会以只读的方式挂载已有的文件系统。3)Skip to shell: 手动挂载#5、sh切换bash模式chroot /mnt/sysimage/#6、执行命令passwd root#7、如果SELinux是启用的,才需要执行下面操作,如查没有启动,不需要执行touch /.autorelabel#8、退出，重启exitreboot 2.6.2 Ubuntu1开机shift键，看到下面界面 1按下面的提示，按e键，进入下面界面 1修改上面的linux开头的行，为下面形式： rw init=/bin/bash 1按下面提示，按ctrl-x或者F10键，进入下面界面 1执行命令passwd 直接修改密码后重启即可 2.7 调整默认启动内核1234567891011121314[root@centos8 ~]#cat /boot/grub2/grubenv# GRUB Environment Blocksaved_entry=5b85fc7444b240a992c42ce2a9f65db5-5.6.12-wanglinux-6.6.6kernelopts=root=UUID=f7f53add-b184-4ddc-8d2c-5263b84d1e15 ro crashkernel=autoresume=UUID=eebe3bc7-6d52-4ad9-86aa-916f1a123fd4 rhgb quiet net.ifnames=0boot_success=0[root@centos8 ~]#ls /boot/loader/entries/5b85fc7444b240a992c42ce2a9f65db5-0-rescue.conf5b85fc7444b240a992c42ce2a9f65db5-4.18.0-147.el8.x86_64.conf5b85fc7444b240a992c42ce2a9f65db5-5.6.12-wanglinux-6.6.6.conf#以下命令是修改 /boot/grub2/grubenv 实现[root@centos8 ~]#grub2-set-default 1 3 根文件系统根文件系统（rootfs）是 Linux 内核启动时挂载的第一个文件系统，是系统运行的基础。它并非仅基于内存（注：早期启动阶段可能使用临时内存文件系统如 initramfs 辅助，但最终根文件系统通常位于磁盘等存储设备），其核心作用包括：提供内核启动必需的文件（如内核映像依赖的关键配置）、包含初始化系统的脚本（如 systemd 进程、rcS、inittab 等）、提供根目录 “&#x2F;” 及 &#x2F;bin、&#x2F;sbin 等目录下的基础命令（如 ls、cd）和 &#x2F;lib 目录下的库文件，以及存储挂载其他文件系统所需的配置（如 &#x2F;etc&#x2F;fstab）。 作为所有文件系统的 “根”，它是其他文件系统挂载的基础 —— 只有成功挂载根文件系统，系统才能继续启动并加载后续文件系统；若挂载失败，系统会启动出错。Linux 启动流程中，早期可能通过内存文件系统临时过渡，待磁盘驱动和文件系统加载完成后，会将根目录从临时内存文件系统切换到实际的根文件系统。总之，Linux 系统仅靠内核无法独立工作，必须配合根文件系统中的配置、命令和库文件，才能完成初始化并正常运行。 4 加载kernelLinux 内核启动时需完成自身初始化，过程包括：探测硬件设备、加载硬件驱动程序（依赖 initramfs&#x2F;ramdisk 临时提供驱动，避免依赖根文件系统）、以只读方式挂载根文件系统，最终运行用户空间第一个程序 /sbin/init（现在被systemd替代）。Linux 内核执行文件一般会放在 &#x2F;boot 目录下，文件名类似 vmlinuz*，如下 12345678root@ubuntu2204:~# ll /boot/ |grep vmlrwxrwxrwx 1 root root 26 Jun 20 16:03 vmlinuz -&gt; vmlinuz-5.15.0-142-generic-rw------- 1 root root 11704776 May 19 10:36 vmlinuz-5.15.0-142-genericlrwxrwxrwx 1 root root 26 Jun 20 16:03 vmlinuz.old -&gt; vmlinuz-5.15.0-142-generic[root@rocky9 ~]#ll /boot/ |grep vm-rwxr-xr-x. 1 root root 14916424 Jun 22 14:02 vmlinuz-0-rescue-b7a30c2fbcc04db982247dad4c877914-rwxr-xr-x. 1 root root 14916424 May 24 07:01 vmlinuz-5.14.0-570.17.1.el9_6.x86_64 关于根文件系统与驱动的依赖问题，实际逻辑是：根文件系统的驱动（如 ext4、xfs 等）若仅存在于根文件系统中，会导致 “加载根文件系统需驱动，而驱动又在根文件系统内” 的循环依赖。解决方案并非将 /boot 作为独立文件系统（/boot 本质是根文件系统的一部分，用于存放内核、GRUB 等引导文件），而是通过 initramfs（初始化内存盘）—— 这是一个临时的内存文件系统，包含启动阶段必需的文件系统驱动和工具，内核启动时先加载 initramfs，借助其中的驱动完成根文件系统的挂载，从而打破循环依赖。 Linux 内核具有模块化特性，支持 .ko 格式的内核模块（如文件系统、硬件驱动等）动态装载和卸载，模块可配置为 “按需加载（m）” 或 “强制集成在内核中（y）”。其核心功能包括进程管理、内存管理、网络管理、驱动程序、文件系统及安全功能等，采用 “宏内核（monolithic kernel）” 设计 —— 虽将所有系统服务集成于内核，但通过模块化实现了动态扩展，吸收了微内核的灵活性；而微内核（如 Windows、HarmonyOS 采用）则将多数功能放在用户态，虽简化内核但性能较差。 12345678910[root@ubuntu2004 ~]#grep -i ext4 /boot/config-5.4.0-166-generic CONFIG_EXT4_FS=yCONFIG_EXT4_USE_FOR_EXT2=yCONFIG_EXT4_FS_POSIX_ACL=yCONFIG_EXT4_FS_SECURITY=y[root@ubuntu2004 ~]#grep -i ntfs /boot/config-5.4.0-167-generic CONFIG_NTFS_FS=m# CONFIG_NTFS_DEBUG is not set# CONFIG_NTFS_RW is not set 所以完整的 Linux 系统由两部分组成：内核（kernel）负责核心功能实现；根文件系统（rootfs）包含二进制执行程序、glibc 库（提供函数集合及调用接口）等，为用户空间程序提供运行环境。两者协同工作，缺一不可。 4.1 lsmod命令查看当前已加载的内核模块 123456789101112[root@rocky9 ~]#lsmodModule Size Used bytls 159744 0nft_fib_inet 12288 0nft_fib_ipv4 12288 1 nft_fib_inetnft_fib_ipv6 12288 1 nft_fib_inetnft_fib 12288 3 nft_fib_ipv6,nft_fib_ipv4,nft_fib_inetnft_reject_inet 12288 0nf_reject_ipv4 16384 1 nft_reject_inetnf_reject_ipv6 24576 1 nft_reject_inetnft_reject 12288 1 nft_reject_inet.... Module：模块名称； Size：模块占用内存大小（字节）； Used by：引用该模块的模块数（数字）及具体模块名称（若有）。 4.2 modinfo命令查询内核模块的详细信息 1234567891011121314151617181920212223242526[root@ubuntu2004 ~]#modinfo ext4name: ext4filename: (builtin) #集成在内核，因为值为y，强制集成在内核中softdep: pre: crc32c license: GPLdescription: Fourth Extended Filesystemauthor: Remy Card, Stephen Tweedie, Andrew Morton, Andreas Dilger, Theodore Ts&#x27;o and othersalias: fs-ext4alias: ext3alias: fs-ext3alias: ext2alias: fs-ext2[root@ubuntu2004 ~]#modinfo ntfsfilename: /lib/modules/5.4.0-166-generic/kernel/fs/ntfs/ntfs.ko #单独的文件，因为值为m，按需加载license: GPLversion: 2.1.32description: NTFS 1.2/3.x driver - Copyright (c) 2001-2014 Anton Altaparmakov and Tuxera Inc.author: Anton Altaparmakov &lt;anton@tuxera.com&gt;alias: fs-ntfssrcversion: 6C7B16106E7029F585BF2BAdepends: retpoline: Yintree: Yname: ntfsvermagic: 5.4.0-166-generic SMP mod_unload modversions 4.3 modprobe命令手动加载还没加载的按需加载的模块 1modprobe nfs 4.4 rmmod命令卸载已加载的内核模块 1rmmod nfs 5 systemd初始化内核启动（己被载入内存，开始运行，并己经初始化了所有设备驱动和数据结构等）之后，就通过启动一个用户级的程序来完成引导进程。而systemd进程（早期是init）就是一个由内核启动的第一个用户级进程，进程树的树根，即1号进程 5.1 systemd介绍Systemd：从 CentOS 7 版本之后开始用 systemd 实现init进程，系统启动和服务器守护进程管理器，负责在系统启动或运行时，激活系统资源，服务器进程和其它进程 systemd 核心概念：unit，表示不同类型的systemd对象，通过配置文件进行标识和配置，文件中主要包含了系统服务、监听socket、保存的系统快照以及其它与init相关的信息，使用 systemctl -t help 命令可以查看unit类型 service unit:：用于定义系统服务 Socket unit:：定义进程间通信用的socket文件，也可在系统启动时，延迟启动服务，实现按需启动 Target unit:：不同服务的集合，用于模拟实现运行级别 Device unit:：用于定义内核识别的设备 Mount unit:：定义文件系统挂载点 Automount unit：文件系统的自动挂载点 Snapshot unit:：管理系统快照 Swap unit：用于标识swap设备 Timer unit：用于安排激活另一个单元的计时器 Path unit：用于定义文件系统中的一个文件或目录使用,常用于当文件系统变化时，延迟激活服务，如：spool 目录 Slice unit：通过 Linux 控制组节点 (cgroups) 限制资源 Scope unit：systemd 总线接口的信息，常用于管理外部系统进程 unit 的配置文件 1234/usr/lib/systemd/system #每个服务最主要的启动脚本设置，类似于之前的/etc/init.d//lib/systemd/system #ubutun的对应目录,兼容于CentOS7,8和Ubuntu（自己写的服务脚本都放在这）/run/systemd/system #系统执行过程中所产生的服务脚本，比上面目录优先运行/etc/systemd/system #管理员建立的执行脚本，类似于/etc/rcN.d/Sxx的功能，比上面目录优先运行 5.2 systemd的分离模式在传统的服务管理中（init），服务程序（比如 sshd、nginx）通常需要自己负责创建 socket（例如监听 22 端口、80 端口），并在启动时就占用这些端口。即使此时没有客户端连接，服务程序也必须持续运行以持有 socket，否则端口会被释放，导致新连接无法建立。 在 systemd 中，这种逻辑被拆分为两部分： socket 单元（.socket 文件）：由 systemd 自身负责创建和监听 socket（如网络端口、管道、文件描述符等），并在系统启动时就提前占用这些资源。例如，sshd.socket 会先于 sshd.service 启动，负责监听 22 端口，而此时 sshd 服务本身可能并未运行 服务单元（.service 文件）：实际提供服务的程序（如 sshd）仅在有请求到达时（比如客户端尝试连接 22 端口），才会被 systemd 激活（启动）。当请求处理完成后，若长时间无新请求，服务程序甚至可以自动退出，释放资源，而 socket 仍由 systemd 保持监听。 范例: systemd实现部分服务的 socket 和 service 分离 1234567891011121314151617[root@rocky8 ~]#yum -y install httpd[root@rocky8 ~]#systemctl start httpd.socket[root@rocky8 ~]#systemctl is-active httpd.socketactive[root@rocky8 ~]#systemctl is-active httpd.serviceinactive[root@rocky8 ~]#ss -ntlp |grep :80LISTEN 0 128 *:80 *:* users:((&quot;systemd&quot;,pid=1,fd=32))#访问httpd的socket,自动激活service[root@rocky8 ~]#curl 127.0.0.1[root@rocky8 ~]#systemctl is-active httpd.serviceactive[root@rocky8 ~]#ss -ntlp |grep :80LISTEN 0 128 *:80 *:* users:((&quot;httpd&quot;,pid=16775,fd=3),(&quot;httpd&quot;,pid=16774,fd=3),(&quot;httpd&quot;,pid=16773,fd=3),(&quot;httpd&quot;,pid=16771,fd=3),(&quot;systemd&quot;,pid=1,fd=32)) 5.3 启动级别Linux分为7个启动级别： 运行级别0：系统停机状态，系统默认运行级别不能设为0，否则不能正常启动 运行级别1：单用户工作状态，root权限，用于系统维护，禁止远程登陆 运行级别2：多用户状态(没有NFS) 运行级别3：完全的多用户状态(有NFS)，登陆后进入控制台命令行模式 运行级别4：系统未使用，保留 运行级别5：X11控制台，登陆后进入图形GUI模式 运行级别6：系统正常关闭并重启，默认运行级别不能设为6，否则不能正常启动 这些启动级别何时加载的呢？ 12345678910111213141516# 1.加电自检(BIOS)# 2.MBR引导 (512k) dd &lt;/dev/zero &gt;/dev/sda bs=1k count=400 # 3.GRUB2菜单(选择系统)# 4.运行systemd- 检查/etc/systemd/system/default.target -&gt; /usr/lib/systemd/system/multi-user.target- 找到/etc/systemd/system/multi-user.target.wants/所有服务，并启动 # 5.建立终端# 6.用户登录## 运行级别: - 0:关机 poweroff.target- 1:单用户模式 rescue.target- 2:多用户模式(没有文件系统，没有网络) multi-user.target- 3:多用户模式(命令行) multi-user.target- 4:没有被使用 multi-user.target- 5:图形化界面 graphical.target- 6:重启 reboot.target 设置与获取启动级别 12345#获取默认启动的targetsystemctl get-default#设置默认启动的targetsystemctl set-default graphical.target 5.3.1 红帽系统Centos7后，完全采用 systemd 作为初始化系统，在 systemd 中，运行级别被 target 替代，不再保留传统的 /etc/rc0.d 到 /etc/rc6.d 等按运行级别划分的目录，虽然还存在/etc/rc.d 单一目录，只不过是为了兼容传统场景而保留的 1234567891011[root@rocky9 ~]#ll /lib/systemd/system/*.targetlrwxrwxrwx. 1 root root 13 Apr 24 14:27 /lib/systemd/system/ctrl-alt-del.target -&gt; reboot.targetlrwxrwxrwx. 1 root root 16 Apr 24 14:27 /lib/systemd/system/default.target -&gt; graphical.targetlrwxrwxrwx. 1 root root 15 Apr 24 14:27 /lib/systemd/system/runlevel0.target -&gt; poweroff.targetlrwxrwxrwx. 1 root root 13 Apr 24 14:27 /lib/systemd/system/runlevel1.target -&gt; rescue.targetlrwxrwxrwx. 1 root root 17 Apr 24 14:27 /lib/systemd/system/runlevel2.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 17 Apr 24 14:27 /lib/systemd/system/runlevel3.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 17 Apr 24 14:27 /lib/systemd/system/runlevel4.target -&gt; multi-user.targetlrwxrwxrwx. 1 root root 16 Apr 24 14:27 /lib/systemd/system/runlevel5.target -&gt; graphical.targetlrwxrwxrwx. 1 root root 13 Apr 24 14:27 /lib/systemd/system/runlevel6.target -&gt; reboot.target.... 和运行级别的对应关系 12345670 ==&gt; runlevel0.target, poweroff.target1 ==&gt; runlevel1.target, rescue.target2 ==&gt; runlevel2.target, multi-user.target3 ==&gt; runlevel3.target, multi-user.target4 ==&gt; runlevel4.target, multi-user.target5 ==&gt; runlevel5.target, graphical.target6 ==&gt; runlevel6.target, reboot.target 范例 123456789101112131415161718# 系统目标（target）与运行级别管理systemctl list-dependencies graphical.target # 查看指定 target 的依赖单元systemctl isolate graphical.target # 切换到指定 target（需 AllowIsolate=yes） # 注意：修改 .target 文件后需执行 systemctl daemon-reload# 系统关机与重启systemctl halt # 停止系统（不关闭电源）systemctl poweroff # 关闭系统并切断电源systemctl reboot # 重启系统# 系统挂起与休眠systemctl suspend # 挂起（Suspend to RAM）systemctl hibernate # 休眠（Suspend to Disk）systemctl hybrid-sleep # 混合休眠（同时挂起到内存和磁盘）# 模式切换systemctl rescue # 切换到救援模式（Rescue Mode），使用最小化服务集systemctl emergency # 切换到紧急模式（Emergency Mode），仅启动最基本功能，无服务启动 范例：禁用ctrl+alt+delete 重启快捷键 12345678[root@centos8 ~]#ls -l /lib/systemd/system/ctrl-alt-del.target lrwxrwxrwx. 1 root root 13 May 23 2019 /lib/systemd/system/ctrl-alt-del.target-&gt; reboot.target[root@centos8 ~]#systemctl mask ctrl-alt-del.targetCreated symlink /etc/systemd/system/ctrl-alt-del.target → /dev/null.[root@centos8 ~]#init q[root@centos8 ~]#systemctl daemon-reload 5.3.2 UbuntuUbuntu虽然默认使用 systemd 作为初始化系统，但为了兼容传统的机制，仍然保留了 /etc/rc0.d 到 /etc/rc6.d 以及 /etc/rcS.d 这些目录 123456789root@ubuntu2204:~# ll /etc/rc*.d -ddrwxr-xr-x 2 root root 267 Apr 21 2022 /etc/rc0.d/drwxr-xr-x 2 root root 169 Apr 21 2022 /etc/rc1.d/drwxr-xr-x 2 root root 292 Jun 20 16:03 /etc/rc2.d/drwxr-xr-x 2 root root 292 Jun 20 16:03 /etc/rc3.d/drwxr-xr-x 2 root root 292 Jun 20 16:03 /etc/rc4.d/drwxr-xr-x 2 root root 292 Jun 20 16:03 /etc/rc5.d/drwxr-xr-x 2 root root 267 Apr 21 2022 /etc/rc6.d/drwxr-xr-x 2 root root 263 Apr 21 2022 /etc/rcS.d/ 说明： K: K##：表示关闭服务，##运行次序；数字越小，越先运行，越先被停止，这类服务通常是要依赖其它服务 S: S##：表示开启服务，##运行次序；数字越小，越先运行，越先被启动，这类服务通常是要被其它服务依赖 5.4 systemctl命令管理系统服务service unit 1systemctl COMMAND name1.service [name2.service]... 服务状态 123456789101112loaded Unit #配置文件已处理active(running) #一次或多次持续处理的运行active(exited) #成功完成一次性的配置active(waiting) #运行中，等待一个事件inactive #不运行enabled #开机启动disabled #开机不启动static #开机不启动，但可被另一个启用的服务激活indirect #重定向到别处#使用该查看更多显示状态systemctl list-unit-files --type service --all systemctl 命令示例 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 单元状态查看systemctl # 显示所有活动单元状态（默认）systemctl list-units # 同上，显示所有活动单元# 服务状态查看systemctl --type=service # 显示所有活动的服务单元systemctl list-units --type=service --all # 显示所有服务单元（包括非活动）systemctl status sshd.service # 查看指定服务详细状态systemctl --l status sshd.service # 同上，显示完整输出（-l 表示 --full）# 服务活动状态判断systemctl is-active sshd.service # 检查服务当前是否处于激活（运行）状态# 服务启停控制systemctl start sshd.service # 启动服务systemctl stop sshd.service # 停止服务systemctl restart sshd.service # 重启服务systemctl reload sshd.service # 重新加载配置（不中断服务）# 开机自启管理systemctl enable sshd.service # 设置服务开机自启systemctl disable sshd.service # 禁用开机自启systemctl is-enabled sshd.service # 查看服务是否设置为开机自启systemctl list-unit-files --type=service # 列出所有服务的启用/禁用状态# 立即生效的启用/禁用systemctl enable --now network # 开机自启并立即启动服务systemctl disable --now network # 禁止开机自启并立即停止服务# 服务屏蔽与解除systemctl mask network.service # 屏蔽服务：禁止手动和自动启动（创建指向 /dev/null 的符号链接）systemctl unmask network.service # 解除屏蔽：恢复服务正常启动能力# 依赖关系查看systemctl list-dependencies sshd.service # 列出服务的依赖单元# 配置文件查看systemctl cat sshd.service # 查看服务单元文件的实际内容# 失败服务查询systemctl --failed --type=service # 列出当前处于失败状态的服务# 杀掉服务进程systemctl kill unitname # 向服务发送信号以终止其进程 5.5 systemctl管理脚本systemctl脚本存放在：/usr/lib/systemd/，有系统（system）和用户（user）之分 /usr/lib/systemd/system ：系统服务，开机不需要登陆就能运行的程序（相当于开机自启） /usr/lib/systemd/user ：用户服务，需要登录后才能运行的程序 usr&#x2F;lib&#x2F;systemd&#x2F;目录下又存在两种类型的文件： *.service ：服务unit文件 *.target ：开机级别unit service unit文件格式 以 “#” 开头的行后面的内容会被认为是注释 相关布尔值，1、yes、on、true 都是开启，0、no、off、false 都是关闭 时间单位默认是秒，所以毫秒（ms）分钟（m）等须显式说明 service unit file文件通常由三部分组成 [Unit]：定义与Unit类型无关的通用选项，主要提供unit的描述信息、unit行为及依赖关系等 [Service]：与特定类型相关的专用选项 [Install]：用于定义服务安装（启用）后与其他目标单元（target unit）的关联关系（比如该服务属于哪个运行级别，被哪些目标依赖等），可设置为多用户的 multi-user.target 5.5.1 Unit段的常用选项 配置项 描述信息 Description 用于描述该服务的功能或用途（说明这个服务是做什么的）。 After&#x2F;Before 定义单元（unit）的启动次序，After表示当前单元需要在指定的单元启动后再启动，Before与之相反（After和Before字段只涉及启动顺序，不涉及依赖关系）。 Requires 指定当前单元所依赖的其他单元，属于强依赖关系。若被依赖的单元无法激活，当前单元也无法激活。 Wants 指定当前单元所依赖的其他单元，属于弱依赖关系。即使被依赖的单元无法激活，当前单元仍可尝试激活。 Conflicts 定义单元之间的冲突关系。若指定的单元被激活，当前单元会被停止；反之亦然。 5.5.2 Service段的常用选项 选项 描述 Type 定义进程启动类型 EnvironmentFile 环境变量配置文件路径。说明：添加连词号 - 表示抑制错误，即发生错误时，不影响其他命令的执行，避免配置文件不存在的异常。例如：EnvironmentFile=-/etc/sysconfig/elasticsearch PIDFile 进程 ID 文件的路径 ExecStart 启动服务的命令 &#x2F; 脚本的绝对路径（核心） ExecStartPre ExecStart 运行前执行的命令（可多条） ExecStartPost ExecStart 运行后执行的命令（可多条） ExecReload 重新加载配置的命令 &#x2F; 脚本 ExecStop 停止服务的命令 &#x2F; 脚本 KillSignal 杀死进程的信号（默认 SIGTERM） KillMode 杀死进程的方式（control-group&#x2F;process&#x2F;mixed&#x2F;none） TimeoutStopSec 超时后未停止则强制终止（配合 SIGKILL 等） Restart 定义服务进程退出后，systemd的重启方式，默认是不重启 RestartSec 重启前的暂停时间（默认 100ms，单位可指定，如 20s） PrivateTmp 布尔值，是否生成私有 &#x2F;tmp 目录（true 则生成独立目录） User 服务运行的用户 Group 服务运行的用户组 5.5.2.1 Type类型 类型 描述 simple 默认值，以 ExecStart 字段启动的进程作为主进程 forking ExecStart 字段以 fork () 方式启动，父进程退出后子进程成为主进程（后台运行），常用此类型 oneshot 类似 simple，但仅执行一次，systemd 会等待其执行完毕再启动其他服务 dbus 类似 simple，但需等待获取 D-Bus 信号后才启动 notify 类似 simple，但进程结束后会发出通知信号，systemd 收到后再启动其他服务 idle 类似 simple，但需等其他任务执行完毕后才启动该服务 5.5.2.2 Killmode的类型 类型 描述 control-group（默认） 当前控制组中所有的子进程都会被杀死 process 仅杀死主进程，不处理子进程 mixed 主进程收到 SIGTERM（终止信号），子进程收到 SIGKILL（无条件终止信号） none 不主动杀死任何进程，仅执行服务自身定义的 stop 命令 5.5.2.3 Restart类型 类型 描述 no（默认） 服务退出后不进行任何重启操作 on-success 仅当服务正常退出（退出状态码为 0）时，才会重启 on-failure 服务非正常退出时重启（包括信号终止、超时等非 0 状态码的情况）。守护进程推荐使用 on-abnormal 仅当服务因信号终止或超时退出时，才会重启 on-abort 仅当服务收到未捕捉的信号导致终止时，才会重启 on-watchdog 仅当服务因超时（watchdog 机制触发）退出时，才会重启 always 无论服务以何种原因退出（正常或异常），都会重启 5.5.2.4 各种Exec*字段Exec*字段后面的命令，仅接受 ‘指令 参数 参数..’ 格式，不能接受 &lt;&gt; |&amp; 等特殊字符，很多bash语法也不支持，如果想要支持bash语法，需要设置 Tyep=oneshot 注意：[Service]部分的启动、重启、停止命令全部要求使用绝对路径，使用相对路径则会报错 5.5.3 Install段的常用选项 配置项 描述 Alias 服务的别名，可通过 systemctl [命令] 别名.service 操作该服务（如 Alias=my-service.service）。 RequiredBy 定义当前服务被哪些单元（units）强依赖。若这些单元被激活，当前服务必须同时激活；若当前服务失效，依赖它的单元也会受影响。通常用于指定服务所属的运行级别目标（如 multi-user.target）。 WantedBy 定义当前服务被哪些单元（units）弱依赖。若这些单元被激活，会尝试激活当前服务，但当前服务失效不影响依赖它的单元。常用作指定服务的默认启动目标（如 graphical.target）。 Also 安装当前服务时，同时安装（启用）指定的其他相关服务（如依赖的辅助服务）。 5.5.4 自定义service的unit文件说明：对于新创建的unit文件，或者修改了的unit文件，要使用 systemctl daemon-reload 通知systemd重载此配置文件，而后可以选择重启服务，如果之前没有启动过，可以直接启动 123456789101112131415161718192021222324[root@centos8 ~]#vim /lib/systemd/system/hello.service[Unit]Description=Hello World[Service]TimeoutStartSec=0ExecStart=/bin/sh -c &quot;while true; do echo Hello World; sleep 1; done&quot;ExecStop=/bin/kill sh[Install]WantedBy=multi-user.target[root@centos8 ~]#systemctl daemon-reload[root@centos8 ~]#systemctl enable --now hello[root@centos8 ~]#systemctl status hello[root@centos8 ~]#tail /var/log/messages -fHello WorldHello WorldHello World...#上面serivce文件也可支持ubuntu[root@ubuntu1804 ~]#tail /var/log/syslog 5.6 开机启动文件开机启动文件是指系统启动过程中会自动执行的文件（如脚本、配置文件等），例如：/etc/rc.local（传统启动脚本）、systemd 的服务单元文件（.service）、/etc/fstab（挂载配置文件）等，都属于开机启动文件。这些文件本身是 “被系统读取并执行” 的对象，用于定义启动时要做的操作。 上述之前说明了，Centos7后 rc.local 被 target 替代，但仍然还保留着此文件；而Ubuntu还是跟传统的一样使用 rc.local 。所以我们有两种方式实现我们想开机即启动的配置 创建 systemd 服务（推荐） 在 /etc/rc.local 中添加，并赋予执行权限 范例：使用 /etc/rc.local 123456789101112131415161718192021222324#创建测试脚本[root@ubuntu2204 ~]# vim rc-local-test.sh#!/bin/bashsleep 3000#加执行权限[root@ubuntu2204 ~]# chmod a+x rc-local-test.sh#加入开机启动[root@ubuntu2204 ~]# vim /etc/rc.local#!/bin/shtouch /var/lock/subsys/local/root/rc-local-test.sh#重启，测试看是否启动[root@ubuntu2204 ~]# uptime19:18:33 up 1 min, 1 user, load average: 0.15, 0.04, 0.01[root@ubuntu2204 ~]# ps aux | grep rc-localroot 1843 0.0 0.0 108112 1244 ? S 19:17 0:00 /bin/bash/root/rc-local-test.shroot 1890 0.0 0.0 103152 820 pts/1 S+ 19:21 0:00 grep rc-local[root@ubuntu2204 ~]# pstree -p | grep rc-local |-rc(982)---S99local(1841)---rc-local-test.s(1843)---sleep(1844) 6 内核参数和&#x2F;proc目录管理内核把自己内部状态信息及统计信息，以及可配置参数通过proc伪文件系统加以输出，而sysctl 命令允许改变正在运行中的Linux系统的接口，用来配置针对整个系统的内核参数，这些参数以文件的形式显示在 /proc/sys/ 目录中，配置项就是目录名加文件名，值就是该文件中的内容。但不是所有内核参数都是可以被修改的，有些内核参数是只读的，只有可写的内核参数才可以修改 系统在启动时，会按下列顺序加载配置文件，读取参数值 123456/run/sysctl.d/*.conf/etc/sysctl.d/*.conf/usr/local/lib/sysctl.d/*.conf/usr/lib/sysctl.d/*.conf/lib/sysctl.d/*.conf/etc/sysctl.conf 所有内核参数都以文件形式存在于 /proc/sys/ 目录下，这是内核在运行时向用户空间暴露的配置接口。例如，某个参数的实际路径可能是 /proc/sys/xxx/yyy/zzz。当使用 sysctl -w 临时修改参数，或在 /etc/sysctl.conf（及 /etc/sysctl.d/ 下的配置文件）中永久配置参数时，不需要写完整的 /proc/sys/ 路径，而是将路径中的 / 替换为 .，并省略开头的 /proc/sys/。例如，对应 /proc/sys/xxx/yyy/zzz 的参数，在 sysctl 中应表示为 xxx.yyy.zzz。 临时修改：sysctl -w xxx.yyy.zzz=值 永久配置：在配置文件中写入 xxx.yyy.zzz = 值 sysctl命令的修改是立即生效且临时的（重启后失效），或者通过 echo 命令以重定向方式 123#修改主机名的内核参数sysctl -w kernel.hostname=testecho “websrv” &gt; /proc/sys/kernel/hostname 通过修改 sysctl.conf 配置文件，达到永久生效。 1234[root@centos8 ~]#vim /etc/sysctl.confnet.ipv4.ip_forward=1[root@centos8 ~]#sysctl -p #生效 查看指定项 1234567891011121314[root@ubuntu ~]# sysctl net.ipv4.ip_forwardnet.ipv4.ip_forward = 0#仅显示名称[root@ubuntu ~]# sysctl -N net.ipv4.ip_forwardnet.ipv4.ip_forward#仅显示值[root@ubuntu ~]# sysctl -n net.ipv4.ip_forward0#直接查看文件内容[root@ubuntu ~]# cat /proc/sys/net/ipv4/ip_forward0 查看所有生效参数 1[root@ubuntu ~]# sysctl -a","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"introduce","slug":"introduce","permalink":"https://aquapluto.github.io/tags/introduce/"}]},{"title":"linux介绍","slug":"Linux/basics/introduce","date":"2025-08-18T08:20:18.000Z","updated":"2025-09-10T14:43:31.711Z","comments":true,"path":"Linux/basics/introduce/","permalink":"https://aquapluto.github.io/Linux/basics/introduce/","excerpt":"","text":"1 系统构成与shell核心1.1 linux系统构成从功能维度去划分： 系统调用接口：负责跟上层的应用程序打交道 内核：负责跟下层的硬件打交道 应用程序本身是无法操作硬件的，但凡想操作硬件都要给系统发请求 从文件维度进行划分 操作系统源自iso镜像文件，镜像文件本质就是一个压缩包，压缩里放着一系列的系统的文件 这些文件分为两大类：bootfs+rootfs bootfs(系统启动前)：包含启动文件（bootloader程序，不是以文件的形式存在，是直接写入硬盘的第一个扇区/mbr）、内核文件(/boot/vm...) rootfs(系统启动后)：本质就是一堆文件夹&#x2F;文件 1.2 shell解释器介绍在默认启动级别为3的情况下，linux系统启动之后默认会启动一个命令解释器铺满全屏幕给你去（称之为字符终端），只能在里面敲命令 linux系统中的命令解释器称之为shell，翻译为壳，表达了对系统接口封装的思想 具体来说shell解释器分为很多种类，默认用的bash这种 1.3 shell命令的种类与优先级带着路径用命令 绝对路径：从根开始的路径 相对路径: 不是从根开始的路径，相对于你当前所在的文件夹作为起始点往后查找 不带着路径前缀去使用命令 别名：用来alias命令制作的命令（alias xxx=&quot;ls /etc/sysconfig/network-scripts;echo 123&quot;） Compound Commands复合命令：for((i=0;i&lt;3;i++));do echo 66666; done function定义命令的函数：function f()&#123; echo 123; &#125; built_in内置命令（内置在shell解释器中，解释器内部集成）：type ls hash缓存机制：优化机制，把敲得命令缓存内存中，下次直接会省去查找与加载的开销，直接使用即可 环境变量PATH（与命令查找有关系、负责兜底）：查找命令的机制，在系统任意位置都能访问到，是全局有效的变量，PATH变量的值存的是冒号分隔开的的一堆存放命令的文件夹 想要不加任何前缀去调用mmm命令的方法有两种 把该脚本移动到PATH的某个文件夹下面 把该脚本所在的文件夹添加到PATH里 优先级 1234567891011bash shell查找命令顺序：==&gt;以路径（绝对路径，相对路径）开始命令，例如：/bin/ls 或 cd /bin; ./ls ==&gt; alias ==&gt; Compound Commands复合命令 ==&gt; function ==&gt; build_in，如cd，kill，pwd、alias、echo等，可以用&quot;type -a 命令&quot;查看 ==&gt; hash：命令的本质是文件，将命令缓存到内存中，下次直接从内存取 ==&gt; $PATH，环境变量，查看环境变量echo $PATH，例如/bin/ls ==&gt; error: command not found # ps：查看命令的位置：which 命令 1.3.1 命令执行方式多个命令一起执行 1cmd1;cmd2;cmd3;echo -e &quot;\\a&quot; 单个命令换行敲 123[root@ubuntu2004 ~]#host\\&gt; nameubuntu2004 1.3.2 合并多个命令1(CMD1;CMD2......) 或者 &#123; CMD1;CMD2;....; &#125; ( list ) 会开启子shell，并且list中变量赋值及内部命令是临时性的，执行后将不再影响后续的环境 &#123; list; &#125; 不会开启子shell, 在当前shell中运行,会影响当前shell环境 连接多个命令组成一条大命令 分号：左边命令运行完毕，无论成功与否，都会执行右边的命令 &amp;&amp; ：左边命令运行成功之后，才能执行右边的命令 1.4 linux系统安装CentOS 7安装教程 CentOS 8安装流程 Centos Stream安装教程 Rocky Linux 9安装流程 Ubuntu 22.04安装流程 Ubuntu 24.04安装流程 如果安装后有以下问题，可以查看文章 解决Ubuntu安装后不能远程连接的问题 解决centos8安装后不能使用yum的问题 补充：VMware中的网络模式 连接模式 特点 NAT 虚拟机通过Vmnet8这个HUB互相连接，再通过物理机上的Vmnet8网卡连接物理机，能访问外网，物理机充当路由器 桥接 虚拟机和物理机连接同一网络，两者之间是并列关系，通过Vmnet0 这个HUB连接，跟所处环境有关，可通过Windows的以太网网卡（物理网卡）的ip地址确定 仅主机 虚拟机通过Vmnet1这个HUB互相连接，再通过物理机上的Vmnet1网卡连接物理机，不能访问外网 桥接模式：虚拟机和真机的地位一样，都是直接连接到路由器转发数据包。不过从物理层面来说，虚拟机的数据包还是会通过物理机的网卡转发到路由器，再由路由器转发出去 在物理机的网络连接中可以看到两张虚拟网卡VMnet1和VMnet8, 这两种网卡分别作用于仅主机模式与NAT模式。如果将这两块不小心卸载, 可以在vmware的 “编辑” 下的 “虚拟网络编辑器” 中点击 “还原默认设置”。 1.5 centos的平替方案centos停更的风险 安全：安全漏洞攻击，补丁与更新 兼容性：新软件可能不再支持旧版本的centos Ubuntu&#x2F;Debian 一直稳定开源，长期坚持稳定性优先策略，默认禁用root直接登录，LTS版本5年维护周期(更新较慢但是稳定) Rocky&#x2F;Alma 完全兼容RHEL生态，完全免费开源 两者区别 包管理器：前者使用dpkg和apt，后者使用rpm和dnf 安全：前者默认不启用SELinux安全模块，后者默认启用 2 linux基础命令2.1 系统信息2.1.1 查看 cpu12345678910111213141516171819202122232425262728#查看CPU信息[root@ubuntu2004 ~]#lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianAddress sizes: 45 bits physical, 48 bits virtualCPU(s): 4On-line CPU(s) list: 0-3Thread(s) per core: 1 #每个内核有几个线程Core(s) per socket: 2 #每个槽位有2个内核Socket(s): 2 #服务器面板上有2个cpu 槽位NUMA node(s): 1 #nodes的数量Vendor ID: GenuineIntelCPU family: 6Model: 154Model name: 12th Gen Intel(R) Core(TM) i7-12650HStepping: 3CPU MHz: 2688.003BogoMIPS: 5376.00Hypervisor vendor: VMwareVirtualization type: fullL1d cache: 192 KiBL1i cache: 128 KiBL2 cache: 5 MiBL3 cache: 48 MiBNUMA node0 CPU(s): 0-3 #对应的core[root@centos8 ~]# cat /proc/cpuinfo 2.1.2 查看系统架构12[root@ubuntu2204 ~]# archx86_64 2.1.3 查看内核版本12[root@ubuntu1804 ~]#uname -r4.15.0-29-generic 2.2 关机和重启12345678910111213# 关机haltpoweroffinit 0shutdown -h now# 重启reboot-f: 强制，不调用shutdown-p: 切断电源init 6shutdown -r nowctrl+alt+delete 三个键 2.3 文件传输123yum(apt) -y install lrzszsz ss.log # 将Linux创建的文件传输到Windowsrz # 将在Windows的文件传输的Linux 2.4 用户登录信息查看命令1234567891011121314151617181920212223242526272829303132333435# 显示当前用户的用户名whoami# 显示当前用户的用户名 终端 登录时间 来源IPwho am iwho [选项]... [ 文件 | 参数1 参数2 ] # 显示当前已登录的用户信息。who # 列出在当前主机上所有登录用户who -u | --users # 列出当前主机上所有用户的空闲时间 . 表示最近一分钟还是活跃状态 old 表示用户己经空闲超过24小时who -s | --short # 列出在当前主机上所有登录用户，等同于whowho -q | --count # 登录用户统计who -b | --boot # 上次系统启动时间who -a | --all # 多选项组合who -m # who am iw [options] # 显示当前所有登录用户的具体信息[root@ubuntu2004 ~]#w 12:28:22 up 16:28, 3 users, load average: 0.00, 0.02, 0.05USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot tty1 - 19Nov23 89days 0.01s 0.00s -bashroot pts/0 10.0.0.1 Wed07 2days 0.05s 0.05s -bashroot pts/1 10.0.0.1 11:59 0.00s 0.02s 0.00s w参数说明：USER:登录名 TTY:终端 FROM:来源IP LOGIN@:登录时间 IDLE:空闲时间 JCPU:当前终端中所有进程使用cpu的时间,不包括后台作业占用的时间 PCPU:当前进程使用的cpu的时间WHAT:当前进程load average 表示平均负载，最近一分钟，最近五分钟，最近15分钟# 查看特定用户w root 2.5 主机名123456# 查看当前主机名 hostname# 修改主机名，只能数字，横线，字母组成，下划线不行，点后面不能是数字，可以是字母hostnamectl set-hostname xxx（永久生效） # 支持CentOS7和Ubuntu18.04以上版本hostname xxx（临时生效） 范例：错误的主机名可能会导致某些服务无法启动 1234[root@centos8 ~]#hostnamectl set-hostname centos8.3[root@centos8 ~]#systemctl restart postfixJob for postfix.service failed because the control process exited with error code.See &quot;systemctl status postfix.service&quot; and &quot;journalctl -xe&quot; for details. 2.6 登陆提示2.6.1 登陆前提示在命令行模式下本地终端(tty1~tty6)登录界面，会有几行提示文字， 这些文字都保存在&#x2F;etc&#x2F;issue文件中，可以自行修改。 1234567891011vi /etc/issue，输入你想要提示的内容issue 支持转义字符，全部可用的转义字符可以通过 man agetty 查看，这里列出常用的\\d #显示当前系统日期\\S #显示操作系统名称\\m #显示硬件体系结构，如i386、i686等\\n #显示主机名\\o #显示域名\\r #显示内核版本\\t #显示当前系统时间\\u #显示当前登录用户的序列号 如果是远程终端ssh 登录，则其登录前信息，可以放在&#x2F;etc&#x2F;issue.net 中，但是该文件中的内容不支持转义 如果要使用远程终端ssh 登录前的提示信息，还需要修改sshd的相关配置文件 1234567vim /etc/ssh/sshd_config#Banner none 将此处的banner 指向对应的文件即可Banner /etc/issue.net#重启sshd 服务service sshd restart 2.6.2 登陆后提示当用户从终端登录时，此文件的内容将会显示在终端上，如果shell工具支持中文，也可显示。 内容由使用者定制，经常用于通告信息，欢迎提示等。 但是，此文件只适用于命令行界面，如果是图形界面登录，将不显示此文件内容。 123456789vim /etc/motd,输入你想要提示的内容有文件可以拖进去，然后cp xxx(文件名) /etc/motd#ubuntu2204中没有该文件，可自行创建[root@ubuntu2204 ~]# ls /etc/motdls: cannot access &#x27;/etc/motd&#x27;: No such file or directory#登录后的提示来自于此目录下不同文件，如果不需要默认提示，可以将该目录清空[root@ubuntu2204 ~]# ls /etc/update-motd.d/ 2.7 命令提示符登录Linux后，默认的系统命令提示符毫无没有个性，无法明显辨别生产和测试环境，而导致误操作。可以通过修改PS1变量实现个性的提示符格式，避免这种低级错误 1234567891011#Ubunturoot@ubuntu2023：~##Rocky/Centos[root@10 wu ~]#root：用户名ubuntu2023|10 wu：主机名~：表示在哪个文件夹（目录）$：表示普通用户身份#：表示管理员身份 显示提示符格式 12345678910111213echo $PS1[\\u@\\h \\W]\\$\\e 控制符 \\u 用户名 \\h 主机名监测 \\H 主机名 \\w 当前所在目录 \\W 当前目录基名 \\t 24小时时间格式 \\T 12小时时间格式 ! 命令历史数# 开机后命令历史数 修改提示符 12345678910PS1=&quot;\\[\\e[1;5;41;33m\\][\\u@\\h \\W]\\\\$\\[\\e[0m\\]&quot;PS1=&quot;\\[\\e[1;32m\\][\\t \\[\\e[1;33m\\]\\u\\[\\e[35m\\]@\\h\\[\\e[1;31m\\] \\W[\\e[1;32m\\]]\\[\\e[0m\\]\\\\$&quot;PS1=&quot;\\[\\e]0;\\u@\\h: \\w\\a\\]$&#123;debian_chroot:+($debian_chroot)&#125;\\u@\\h:\\w\\$&quot;1：加强亮度5：闪烁41-47：命令符的底色31-37：命令符字体的颜色0m：表示颜色的结束 永久生效的办法 12345678910#Rocky/Centosvim /etc/profile(bashrc),进入之后将命令复制粘贴到文件的最后面，并保存#Ubuntu第一种：vim.bashrc，进入之后将命令复制粘贴到文件的最后面，并保存第二种：vim.bashrcforce_color_prompt=yes 2.8 命令别名123456789101112#修改并保存vim .bashrcalias NAME=&#x27;VALUE&#x27; #定义别名NAME，其相当于执行命令VALUE#显示当前shell进程所有可用的命令别名alias#删除别名unalias nameunalias -a #取消所有别名当别名和内外部命令重名时，优先运行别名命令 注意：在命令行中定义的别名，仅对当前shell进程有效 如果想永久有效，要定义在配置文件中 仅对当前用户：~&#x2F;.bashrc 对所有用户有效：&#x2F;etc&#x2F;bashrc 编辑配置给出的新配置不会立即生效，bash进程重新读取配置文件 12source /path/to/config_file. /path/to/config_file 如果别名同原命令同名，如果要执行原命令，可使用 12345\\ALIASNAME“ALIASNAME”‘ALIASNAME’command ALIASNAME/path/commmand #只适用于外部命令 2.9 获取帮助2.9.1 whatis &amp; whereiswhatis 使用数据库来显示命令的简短描述，以及对应的man手册的章节 whereis 可以列出命令或系统文件路径 1whatis|whereis cmd 2.9.2 查看命令类型区别指定的命令是内部或外部命令 1type cmd 范例: 查看是否存在对应内部和外部命令 123[root@centos8 ~]#type -a echoecho is a shell builtinecho is /usr/bin/echo 内部命令：由shell自带的，而且通过某命令形式提供, ,用户登录后自动加载并常驻内存中 1234567#查询内部命令的用法help cmd#管理内部命令enable cmd 启用内部命令enable –n cmd 禁用内部命令enable –n 查看所有禁用的内部命令 外部命令：在文件系统路径下有对应的可执行程序文件,当执行命令时才从磁盘加载至内存中,执行完毕后从内存中删除 12#查询外部命令的用法cmd --help 或 cmd -h 查看外部命令路径： 12which -a |--skip-aliaswhereis 2.9.3 man帮助123456789101112#提供命令帮助的文件man cmd #查看章节man1 man1p man2...（1,1p，2表示各个命令的章节的意思）man 1 passwd #查看passwd第一章的帮助man 5 passwd #查看passwd第五章的帮助#搜素关键字当文件太多时，想要找到自己想要的帮助时，可以/关键字/second按键n可以往下划，N往上划 2.9.4 info 命令info 是自由软件基金会的GNU项目，是GNU的超文本帮助系统，整个结构类似于一个网站，有导航，支持链接跳转不带参数，默认进入的是首页 1234567891011121314#info [OPTION]... [MENU-ITEM...]info #进入整个info文档info ls #在info 中查看ls的信息#常用快捷键向上方向键 #上移一行向下方向键 #下移一行PgUp #向上一屏PgDn #向下一屏Tab #在链接间滚动Enter #进入链接查看具体内容s|/ #搜索n/p/u/l #进入下/前/上一层/最后一个链接q #退出 2.10 Hash缓存表系统初始hash表为空，当外部命令执行时，默认会从PATH路径下寻找该命令，找到后会将这条命令的路径记录到hash表中，当再次使用该命令时，shell解释器首先会查看hash表，存在将执行之，如果不存在，将会去PATH路径下寻找，利用hash缓存表可大大提高命令的调用速率 123456hash #显示hash缓存hash -l #显示hash缓存，可作为输入使用hash -p path name #将命令全路径path起别名为namehash -t name #打印缓存中name的路径hash -d name #清除name缓存hash -r #清除缓存 2.11 会话管理命令行的典型使用方式是，打开一个终端窗口（terminal window，以下简称”窗口”），在里面输入命令。用户与计算机的这种临时的交互，称为一次”会话”（session） 会话的一个重要特点是，窗口与其中启动的进程是连在一起的。打开窗口，会话开始；关闭窗口，会话结束，会话内部的进程也会随之终止，不管有没有运行完 一个典型的例子就是，SSH 登录远程计算机，打开一个远程窗口执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了。 为了解决这个问题，会话与窗口可以”解绑”：窗口关闭时，会话并不终止，继续运行，等到以后需要的时候，再让会话”绑定” 其他窗口终端复用器软件就是会话与窗口的”解绑”工具，将它们彻底分离。 它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。 它可以让新窗口”接入”已经存在的会话。 它允许每个会话有多个连接窗口，因此可以多人实时共享会话。 它还支持窗口任意的垂直和水平拆分。 2.11.1 screen工具有一些长时间的操作，或者做备份的时候，可以通过screen新建一个会话窗口，在新的会话窗口里面执行操作，这样可以避免因为xshell突然崩溃，网络崩溃或者不小心关掉了窗口而停止程序运行 利用screen 可以实现会话管理,如：新建会话,共享会话等 注意：CentOS7 来自于base源，CentOS8 来自于epel源 1234567891011121314151617181920212223#CentOS7 安装screen[root@centos7 ~]#yum -y install screen#CentOS8 安装screen[root@centos8 ~]#dnf -y install epel-release[root@centos8 ~]#dnf -y install screen#创建新screen会话screen –S [SESSION]#加入screen会话screen –x [SESSION]#退出并关闭screen会话exit#剥离当前screen会话Ctrl+a,d#显示所有已经打开的screen会话screen -ls#恢复某screen会话screen -r [SESSION] 2.11.2 tmuxTmux 是一个终端复用器（terminal multiplexer），类似 screen，但是更易用，也更强大，它将会话与窗口”解绑”，将它们彻底分离，功能如下 它允许在单个窗口中，同时访问多个会话。这对于同时运行多个命令行程序很有用。 它可以让新窗口”接入”已经存在的会话。 它允许每个会话有多个连接窗口，因此可以多人实时共享会话。 它还支持窗口任意的垂直和水平拆分 123456#安装yum install tmux#启动与退出tmuxexit tmux 窗口有大量的快捷键。所有快捷键都要通过前缀键唤起。默认的前缀键是 Ctrl+b ，即先按下Ctrl+b ，快捷键才会生效。帮助命令的快捷键是 Ctrl+b ? 然后，按下 q 键，就可以退出帮助 新建会话第一个启动的 Tmux 窗口，编号是0，第二个窗口的编号是1，以此类推。这些窗口对应的会话，就是 0号会话、1 号会话。使用编号区分会话，不太直观，更好的方法是为会话起名。下面命令新建一个指定名称的会话。 给会话窗口命名 1tmux new -s &lt;session-name&gt; 查看当前所有的 Tmux 会话 121 tmux ls2 tmux list-session 分离会话 121 tmux ls2 tmux list-session 接入对话 12tmux attach -t &lt;session-name&gt;tmux attach -t 0 杀死会话 1tmux kill-session -t &lt;session-name&gt; 切换会话 1tmux switch -t &lt;session-name&gt; 上下分窗格 121 tmux split-window2 ctrl+b,&quot; 左右分窗格 121 tmux split-window -h2 ctrl+b,% 窗格快捷键 123456789101112131415Ctrl+b %：划分左右两个窗格Ctrl+b &quot;：划分上下两个窗格Ctrl+b &lt;arrow key&gt;：光标切换到其他窗格。&lt;arrow key&gt;是指向要切换到的窗格的方向键，比如切换到下方窗格，就按方向键↓Ctrl+b ;：光标切换到上一个窗格Ctrl+b o：光标切换到下一个窗格。Ctrl+b &#123;：当前窗格左移Ctrl+b &#125;：当前窗格右移Ctrl+b Ctrl+o：当前窗格上移Ctrl+b Alt+o：当前窗格下移Ctrl+b x：关闭当前窗格Ctrl+b !：将当前窗格拆分为一个独立窗口Ctrl+b z：当前窗格全屏显示，再使用一次会变回原来大小Ctrl+b Ctrl+&lt;arrow key&gt;：按箭头方向调整窗格大小Ctrl+b q：显示窗格编号 除了将一个窗口划分成多个窗格，Tmux 也允许新建多个窗口新建窗口 创建新窗口 1tmux new-window 新建一个指定名称的窗口 1tmux new-window -n &lt;window-name&gt; 切换到指定编号的窗口 1tmux select-window -t &lt;window-number&gt; 切换到指定名称的窗口 1tmux select-window -t &lt;window-name&gt; 窗口快捷键 123456Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。Ctrl+b n：切换到下一个窗口。Ctrl+b &lt;number&gt;：切换到指定编号的窗口，其中的&lt;number&gt;是状态栏上的窗口编号Ctrl+b w：从列表中选择窗口Ctrl+b ,：窗口重命名 列出所有快捷键，及其对应的 Tmux 命令 1tmux list-keys 列出所有 Tmux 命令及其参数 1tmux list-commands 2.12 输出信息echo语法 12345echo [-neE][字符串]-E （默认）不支持 \\ 解释功能-n 不自动换行-e 启用 \\ 字符的解释功能 显示变量 12echo &quot;$VAR_NAME” #用变量值替换，弱引用echo &#x27;$VAR_NAME’ #变量不会替换，强引用 启用命令选项-e，若字符串中出现以下字符，则特别加以处理，而不会将它当成一般文字输出 12345678910\\a 发出警告声\\b 退格键\\c 最后不加上换行符号\\e escape，相当于\\033\\n 换行且光标移至行首\\r 回车，即光标移至行首，但不换行\\t 插入tab\\\\ 插入\\字符\\0nnn 插入nnn（八进制）所代表的ASCII字符\\xHH插入HH（十六进制）所代表的ASCII数字（man 7 ascii） 在终端中，ANSI定义了用于屏幕显示的Escape屏幕控制码，可以显示具有颜色的字符，其格式如下 123456789101112131415161718192021222324&quot;\\033[字符背景颜色;字体颜色m字符串\\033[0m&quot;\\033[30m -- \\033[37m 设置前景色\\033[40m -- \\033[47m 设置背景色#字符背景颜色范围: 40--47 40:黑 41:红 42:绿 43:黄 44:蓝 45:紫 46:深绿 47:白色 #字体颜色: 30--3730: 黑31: 红32: 绿33: 黄34: 蓝35: 紫36: 深绿37: 白色 加颜色只是以下控制码中的一种，下面是常见的一些ANSI控制码 123456789101112131415161718\\033[0m 关闭所有属性 \\033[1m 设置高亮度 \\033[4m 下划线 \\033[5m 闪烁 \\033[7m 反显 \\033[8m 消隐 \\033[nA 光标上移n行 \\033[nB 光标下移n行 \\033[nC 光标右移n列 \\033[nD 光标左移n列 \\033[x;yH 设置光标位置x行y列 \\033[2J 清屏 \\033[K 清除从光标到行尾的内容 \\033[s 保存光标位置 \\033[u 恢复光标位置 \\033[?25l 隐藏光标 \\033[?25h 显示光标\\033[2J\\033[0;0H 清屏且将光标置顶 2.13 语言环境默认系统为英文环境，可以修改为中文环境，从而查看帮助或提示可以变为中文 范例：临时修改LANG变量实现中文语言提示 12345678910[root@centos7 ~]#echo $LANGen_US.UTF-8[root@centos7 ~]#magedu-bash: magedu: command not found[root@centos7 ~]#LANG=zh_CN.UTF-8[root@centos7 ~]#echo $LANGzh_CN.UTF-8[root@centos7 ~]#magedu-bash: magedu: 未找到命令 范例: Rocky 8 修改语言环境为中文 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@rocky8 ~]#localectl status System Locale: LANG=en_US.UTF-8 VC Keymap: us X11 Layout: us [root@rocky8 ~]#echo $LANGen_US.UTF-8[root@rocky8 ~]#localectl list-localesC.utf8en_AGen_AUen_AU.utf8en_BWen_BW.utf8en_CAen_CA.utf8....[root@rocky8 ~]#yum list lang*[root@rocky8 ~]#yum -y install langpacks-zh_CN.noarch[root@rocky8 ~]#localectl list-localesC.utf8en_AGen_AU.......zh_CNzh_CN.gb18030zh_CN.gbkzh_CN.utf8zh_HKzh_HK.utf8....#通用方法[root@rocky8 ~]#localectl set-locale LANG=zh_CN.utf8#或者下面方式,CentOS8支持,但ubuntu和Centos7不支持,不建议使用[root@rocky8 ~]#localectl set-locale zh_CN.utf8[root@rocky8 ~]#localectl status System Locale: LANG=zh_CN.utf8 VC Keymap: us X11 Layout: us [root@rocky8 ~]#echo $LANGzh_CN.utf8#重新登录后可以看到中文环境[root@rocky8 ~]#exit 范例: Ubuntu 修改语言环境为中文 123456789101112131415[root@ubuntu2204 ~]#localectl status System Locale: LANG=en_US.UTF-8 VC Keymap: n/a X11 Layout: us X11 Model: pc105 [root@ubuntu2204 ~]#apt install language-pack-zh-hans -y[root@ubuntu2204 ~]#localectl list-localesC.UTF-8en_US.UTF-8zh_CN.UTF-8zh_SG.UTF-8[root@ubuntu2204 ~]#localectl set-locale LANG=zh_CN.utf8[root@ubuntu2204 ~]#exit 2.14 命令行历史2.14.1 查看命令行历史当执行命令后，系统默认会在内存记录执行过的命令 当用户正常退出时，会将内存的命令历史存放对应历史文件中，默认是 ~/.bash_history 登录shell时，会读取命令历史文件中记录下的命令加载到内存中 登录进shell后新执行的命令只会记录在内存的缓存区中；这些命令会用户正常退出时“追加”至命令历史文件中 利用命令历史。可以用它来重复执行命令，提高输入效率 1234567891011historyCtrl R可以在命令行历史中查找所需命令-c: 清空命令历史-d offset: 删除历史中指定的第offset个命令n: 显示最近的n条历史-a: 追加本次会话新执行的命令历史列表至历史文件-r: 读历史文件附加到历史列表-w: 保存历史列表到指定的历史文件-n: 读历史文件中未读过的行到历史列表-p: 展开历史参数成多行，但不存在历史列表中-s: 展开历史参数成一行，附加在历史列表后 命令历史相关环境变量 1234567891011HISTSIZE #命令历史记录的条数HISTFILE #指定历史文件，默认为~/.bash_historyHISTFILESIZE #命令历史文件记录历史的条数HISTTIMEFORMAT=&quot;%F %T `whoami` &quot; #显示时间和用户HISTIGNORE=&quot;str1:str2*:…&quot; #忽略str1命令，str2开头的历史HISTCONTROL=ignoredups|ignorespace|ignoreboth|erasedups #控制命令历史的记录方式ignoredups #是默认值，可忽略重复的命令，连续且相同为“重复”ignorespace #忽略所有以空白开头的命令ignoreboth #相当于ignoredups, ignorespace的组合erasedups #删除重复命令 持久保存变量，以上变量可以 export 变量名=&quot;值&quot; 形式存放在 &#x2F;etc&#x2F;profile 或 ~&#x2F;.bash_profile 范例： 1234567891011[root@centos8 ~]#cat .bash_profileexport PATHexport HISTCONTROL=ignorebothexport HISTTIMEFORMAT=&quot;%F %T &quot;[root@centos8 ~]#history 1 2019-12-13 08:39:05 ls /data 2 2019-12-13 08:39:05 date 3 2019-12-13 08:39:05 vie0 4 2019-12-13 08:39:05 nano .bash_profile 5 2019-12-13 08:39:05 exit 2.14.2 调用命令行历史123456789101112131415161718192021222324252627282930313233343536373839404142434445#重复前一个命令方法重复前一个命令使用上方向键，并回车执行按 !! 并回车执行输入!-1 并回车执行按 Ctrl+p 并回车执行!:0 #执行前一条命令（去除参数）!n #执行history命令输出对应序号n的命令!-n #执行history历史中倒数第n个命令!string #重复前一个以“string”开头的命令!?string #重复前一个包含string的命令!string:p #仅打印命令历史，而不执行!$:p #打印输出 !$ （上一条命令的最后一个参数）的内容!*:p #打印输出 !*（上一条命令的所有参数）的内容^string #删除上一条命令中的第一个string^string1^string2 #将上一条命令中的第一个string1替换为string2!:gs/string1/string2 #将上一条命令中所有的string1都替换为 string2使用up（向上）和down（向下）键来上下浏览从前输入的命令ctrl-r来在命令历史中搜索命令（reverse-i-search）`’：Ctrl+g：从历史搜索模式退出#要重新调用前一个命令中最后一个参数!$ #表示前一个命令中最后一个参数Esc, . #点击Esc键后松开，然后点击 . 键Alt+ . #按住Alt键的同时点击 . 键command !^ #利用上一个命令的第一个参数做command的参数command !$ #利用上一个命令的最后一个参数做command的参数command !* #利用上一个命令的全部参数做command的参数command !:n #利用上一个命令的第n个参数做command的参数command !n:^ #调用第n条命令的第一个参数command !n:$ #调用第n条命令的最后一个参数command !n:m #调用第n条命令的第m个参数command !n:* #调用第n条命令的所有参数command !string:^ #从命令历史中搜索以 string 开头的命令，并获取它的第一个参数command !string:$ #从命令历史中搜索以 string 开头的命令,并获取它的最后一个参数command !string:n #从命令历史中搜索以 string 开头的命令，并获取它的第n个参数command !string:* #从命令历史中搜索以 string 开头的命令，并获取它的所有参数 3 符号用法与快捷键3.1 反向单引号 &#96;&#96;&#96;&#96;里面一定是可执行的命令，变量和命令都可识别 一个命令CMD1想调用另一个命令CMD2的执行结果，就需要将CMD2放在反向引号中 CMD2 $()与&#96;&#96;等价 1234567[root@centos7 ~]#touch `date +%F`.log[root@centos7 ~]#ls2023-10-22.log anaconda-ks.cfg touch是创建文件的命令，用``将date +%F括起来后，先执行date +%F这个命令，后执行touch命令#思维例如今天是2023-7-31，执行touch `date +%F`.log后，便可创建2023-7-31.log这个文件，明天是2023-8-1，再执行touch `date +%F`.log，便可创建2023-8-1.log这个文件。就不可能说每天查一下今天日期多少，然后再执行touch 2023-7-31.log才可以创建2023-7-31.log这个文件，这样子效率太慢。 3.2 单引号’’变量和命令都不能识别 12[root@centos7 ~]#echo &#x27;echo $PATH&#x27;echo $PATH 3.3 双引号””只能识别变量，不能识别命令 1234[root@centos7 ~]#echo &quot;echo $PATH&quot;echo /apps/tree/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@centos7 ~]#echo $PATH/apps/tree/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin 3.4 花括号{}实现打印重复字符串的简化形式 {元素1..元素2..元素3} 12345678910111213141516171819202122[root@centos7 ~]#echo &#123;1..10&#125;1 2 3 4 5 6 7 8 9 10[root@centos7 ~]#echo &#123;1..10..2&#125;1 3 5 7 9[root@centos7 ~]#echo &#123;a..z..3&#125;a d g j m p s v y[root@centos7 ~]#echo &#123;20..10..2&#125;20 18 16 14 12 10[root@centos7 ~]#echo &#123;a b c&#125;&#123;a b c&#125;[root@centos7 ~]#echo &#123;a,b,c&#125;a b c[root@centos7 ~]#echo &#123;a,b,c&#125;.&#123;txt,log&#125;a.txt a.log b.txt b.log c.txt c.log[root@centos7 ~]#echo file&#123;1..5&#125;.&#123;txt,log&#125;file1.txt file1.log file2.txt file2.log file3.txt file3.log file4.txt file4.log file5.txt file5.log[root@centos7 ~]#echo /data/mysql/&#123;txt,log,pid&#125;/data/mysql/txt /data/mysql/log /data/mysql/pid[root@centos7 ~]#echo file&#123; ,bak&#125;file&#123; ,bak&#125;[root@centos7 ~]#echo file&#123;,bak&#125;file filebak 3.5 bash快捷键12345678910111213141516171819202122232425262728293031Ctrl + l #清屏，相当于clear命令Ctrl + o #执行当前命令，并重新显示本命令Ctrl + s #阻止屏幕输出，锁定Ctrl + q #允许屏幕输出，解锁Ctrl + c #终止命令Ctrl + z #挂起命令Ctrl + a #光标移到命令行首，相当于HomeCtrl + e #光标移到命令行尾，相当于EndCtrl + f #光标向右移动一个字符Ctrl + b #光标向左移动一个字符Ctrl + xx #光标在命令行首和光标之间移动ctrl+ &gt;(方向键) #光标向右移动一个单词尾，相当于 Alt + fctrl+ &lt;(方向键) #光标向左移动一个单词首，相当于 Alt + bCtrl + u #从光标处删除至命令行首Ctrl + k #从光标处删除至命令行尾Alt + r #删除当前整行Ctrl + w #从光标处向左删除至单词首Alt + d #从光标处向右删除至单词尾Alt + Backspace #删除左边单词Ctrl + d #删除光标处的一个字符Ctrl + h #删除光标前的一个字符Ctrl + y #将删除的字符粘贴至光标后Alt + c #从光标处开始向右更改为首字母大写的单词Alt + u #从光标处开始，将右边一个单词更改为大写Alt + l #从光标处开始，将右边一个单词更改为小写Ctrl + t #交换光标处和之前的字符位置Alt + t #交换光标处和之前的单词位置Alt + # #提示输入指定字符后，重复显示该字符#次 注意：Alt 组合快捷键经常和其它软件冲突 范例：xshell中启动 alt 键 4 字符集和编码及语言环境许多场合下，字符集与编码这两个概念常被混为一谈，但两者是有差别的。字符集与字符集编码是两个不同层面的概念 charset是character set的简写，即字符集，即二进制和字符的对应关系，不关注最终的存储形式 encoding是charset encoding的简写，即字符集编码，简称编码，实现如何将字符转化为实际的二进制进行存储或相反，编码决定了空间的使用的大小 4.1 ASCII码计算机内部，所有信息最终都是一个二进制值。上个世纪60年代，美国制定了一套字符编码，对英语字符与二进制位之间的关系，做了统一规定，即ASCII（American Standard Code for InformationInterchange） 码ASCII 码一共规定了128个字符的编码，占用了一个字节的后面7位，最前面的一位统一规定为 0 Oct 8进制 Dec 10进制 ascii 编码 Hex 16进制 Bin 2进制 Char 字符 范例：查看 ascii 表 12[root@centos8 ~]#dnf -y install man-pages[root@centos8 ~]#man ascii 4.2 Unicode由于计算机是美国人发明的，因此，最早只有128个字符被编码到计算机里，即ASCII编码，但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。 全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc-kr里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码 为了表示世界上所有语言中的所有字符。每一个符号都给予一个独一无二的编码数字，Unicode 是一个很大的集合，现在的规模可以容纳100多万个符号。Unicode 仅仅只是一个字符集，规定了每个字符对应的二进制代码，至于这个二进制代码如何存储则没有规定 Unicode编码方案： UTF-8： 变长，1到4个字节 UTF-16：变长，2或4个字节 UTF-32：固定长度，4个字节 UTF-8 是目前互联网上使用最广泛的一种 Unicode 编码方式，可变长存储。使用 1- 4 个字节表示一个字符，根据字符的不同变换长度。编码规则如下：对于单个字节的字符，第一位设为 0，后面的 7 位对应这个字符的 Unicode 码。因此，对于英文中的 0 127 号字符，与 ASCII 码完全相同。这意味着 ASCII 码的文档可用 UTF-8 编码打开对于需要使用 N 个字节来表示的字符（N &gt; 1），第一个字节的前 N 位都设为 1，第 N + 1 位设为0，剩余的 N - 1 个字节的前两位都设为 10，剩下的二进制位则使用这个字符的 Unicode 码来填充 编码转换和查询参考链接： 1234567https://home.unicode.org/https://unicode.yunser.com/unicodehttp://www.chi2ko.com/tool/CJK.htmhttps://www.bejson.com/convert/unicode_chinese/https://javawind.net/tools/native2ascii.jsp?action=transformhttp://tool.oschina.net/encodehttp://web.chacuo.net/charsetescape 5 环境变量5.1 配置文件说明profile类文件：设定环境变量, 登陆前运行的脚本和命令。 bashrc类文件：设定本地变量, 定义命令别名。 5.2 环境变量配置方法5.2.1 export PATH使用export命令直接修改PATH的值，配置MySQL进入环境变量的方法: 1234export PATH=/home/uusama/mysql/bin:PATH#或者把PATH放在前面 export PATH=PATH:/home/uusama/mysql/bin 注意事项： 生效时间：立即生效 生效期限：当前终端有效，窗口关闭后无效 生效范围：仅对当前用户有效 配置的环境变量中不要忘了加上原来的配置，即$PATH部分，避免覆盖原来配置 5.2.2 vim ~&#x2F;.bashrc通过修改用户目录下的~&#x2F;.bashrc文件进行配置： 1234vim ~/.bashrc#在最后一行加上export PATH=$PATH:/home/uusama/mysql/bin 注意事项： 生效时间：使用相同的用户打开新的终端时生效，或者手动source ~&#x2F;.bashrc生效 生效期限：永久有效 生效范围：仅对当前用户有效 如果有后续的环境变量加载文件覆盖了PATH定义，则可能不生效 5.2.3 vim ~&#x2F;.bash_profile和修改~&#x2F;.bashrc文件类似，也是要在文件最后加上新的路径即可： 1234vim ~/.bash_profile#在最后一行加上export PATH=$PATH:/home/uusama/mysql/bin 注意事项： 生效时间：使用相同的用户打开新的终端时生效，或者手动source ~&#x2F;.bash_profile生效 生效期限：永久有效 生效范围：仅对当前用户有效 如果没有&#x2F;.bash_profile文件，则可以编辑&#x2F;.profile文件或者新建一个 5.2.4 vim &#x2F;etc&#x2F;bashrc该方法是修改系统配置，需要管理员权限（如root）或者对该文件的写入权限： 1234567#如果/etc/bashrc文件不可编辑，需要修改为可编辑chmod -v u+w /etc/bashrcvim /etc/bashrc#在最后一行加上export PATH=$PATH:/home/uusama/mysql/bin 注意事项： 生效时间：新开终端生效，或者手动source &#x2F;etc&#x2F;bashrc生效 生效期限：永久有效 生效范围：对所有用户有效 5.2.5 vim &#x2F;etc&#x2F;profile该方法修改系统配置，需要管理员权限或者对该文件的写入权限，和vim &#x2F;etc&#x2F;bashrc类似： 1234567#如果/etc/profile文件不可编辑，需要修改为可编辑chmod -v u+w /etc/profilevim /etc/profile#在最后一行加上export PATH=$PATH:/home/uusama/mysql/bin 注意事项： 生效时间：新开终端生效，或者手动source &#x2F;etc&#x2F;profile生效 生效期限：永久有效 生效范围：对所有用户有效 5.2.6 vim &#x2F;etc&#x2F;environment该方法是修改系统环境配置文件，需要管理员权限或者对该文件的写入权限： 1234567#如果/etc/bashrc文件不可编辑，需要修改为可编辑chmod -v u+w /etc/environmentvim /etc/profile#在最后一行加上export PATH=$PATH:/home/uusama/mysql/bin 注意事项： 生效时间：新开终端生效，或者手动source &#x2F;etc&#x2F;environment生效 生效期限：永久有效 生效范围：对所有用户有效 5.3 读取环境变量读取环境变量的方法： export命令显示当前系统定义的所有环境变量 echo $PATH命令输出当前的PATH环境变量的值 其中PATH变量定义了运行命令的查找路径，以冒号:分割不同的路径，使用export定义的时候可加双引号也可不加。 5.4 环境变量加载顺序环境变量可以简单的分成用户自定义的环境变量以及系统级别的环境变量。 用户级别环境变量定义文件：&#x2F;.bashrc、&#x2F;.profile（部分系统为：~&#x2F;.bash_profile） 系统级别环境变量定义文件：&#x2F;etc&#x2F;bashrc、&#x2F;etc&#x2F;profile(部分系统为：&#x2F;etc&#x2F;bash_profile）、&#x2F;etc&#x2F;environment Linux加载环境变量的顺序如下：系统环境变量 -&gt; 用户自定义环境变量 /etc/environment -&gt; /etc/profile -&gt; ~/.profile 另外在用户级别环境变量中，系统会首先读取&#x2F;.bash_profile（或者&#x2F;.profile）文件，如果没有该文件则读取&#x2F;.bash_login，根据这些文件中内容再去读取&#x2F;.bashrc。","categories":[{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"}],"tags":[{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"}]},{"title":"JumpServer","slug":"Middleware/JumpServer","date":"2023-08-21T02:59:41.000Z","updated":"2025-09-13T13:56:25.422Z","comments":true,"path":"Middleware/JumpServer/","permalink":"https://aquapluto.github.io/Middleware/JumpServer/","excerpt":"","text":"1 堡垒机和JumpServer1.1 跳板机和堡垒机跳板机 跳板机是一种用于单点登陆的主机应用系统。跳板机通常是由一台服务器能过特定的软件实现，维护人员在维护过程中，首先要统一登录到这台服务器上，然后从这台服务器再登录到目标设备进行维护。但跳板机没有实现对运维人员操作行为的控制和审计，此外，跳板机存在严重的安全风险，一旦跳板机系统被攻入，则将后端资源风险完全暴露无遗。对于一些服务（如:telnet）可以通过跳板机来完成一定的控制访问，但是对于更多的服务（SSH、RDP等）来讲，就显得力不从心了。 堡垒机 由于跳板机的不足，更多的组织需要更先进、更好的安全技术,来实现运维操作管理和安全。堡垒机开始以独立的产品形态被广泛部署，有效降低了运维操作风险，使得运维操作管理变得更简单、更安全。堡垒机能满足角色管理与授权审批、信息资源访问控制、操作记录和审计、系统变更和维护控制要求，并生成一些统计报表配合管理规范，从而不断提升IT内控的合规性。 1.2 JumpServer简介JumpServer 是全球首款完全开源的堡垒机，使用 GNU GPL v2.0 开源协议, 是符合 4A 的专业运维审计系统。为互联网企业提供了认证，授权，审计，自动化运维等功能。使用 Python &#x2F; Django 进行开发, 遵循 Web 2.0 规范, 配备了业界领先的 Web Terminal 解决方案，交互界面美观、用户体验好。采纳分布式架构, 支持多机房跨区域部署, 中心节点提供 API, 各机房部署登录节点, 可横向扩展、无并发访问限制。现已支持管理 SSH、 Telnet、 RDP、 VNC 协议资产。 官方地址： http://www.jumpserver.org/ github项目: https://github.com/jumpserver 1.3 JumpServer生产应用场景 1.4 JumpServer的优势和功能1.4.1 特色优势 开源: 零门槛，线上快速获取和安装 分布式: 轻松支持大规模并发访问； 无插件: 仅需浏览器，极致的 Web Terminal 使用体验 多云支持: 一套系统，同时管理不同云上面的资产 云端存储: 审计录像云端存储，永不丢失 多租户: 一套系统，多个子公司和部门同时使用 1.4.2 功能列表堡垒机四个核心能力: 运维安全审计的4A规范 身份认证 Authentication 登录认证 资源统一登录与认证 LDAP&#x2F;AD 认证 RADIUS 认证 OpenID 认证（实现单点登录） CAS 认证 （实现单点登录） MFA认证 MFA 二次认证（Google Authenticator） RADIUS 二次认证 登录复核（X-PACK） 用户登录行为受管理员的监管与控制 账号管理 Account 集中账号 管理用户管理 系统用户管理 统一密码 资产密码托管 自动生成密码 自动推送密码 密码过期设置 批量改密（X-PACK） 定期批量改密 多种密码策略 多云纳管（X-PACK） 对私有云、公有云资产自动统一纳管 收集用户（X-PACK） 自定义任务定期收集主机用户 密码匣子（X-PACK） 统一对资产主机的用户密码进行查看、更新、测试操作 授权控制 Authorization 多维授权 对用户、用户组、资产、资产节点、应用以及系统用户进行授权 资产授权 资产以树状结构进行展示 资产和节点均可灵活授权 节点内资产自动继承授权 子节点自动继承父节点授权 应用授权 实现更细粒度的应用级授权 MySQL 数据库应用、RemoteApp 远程应用（X-PACK） 动作授权 实现对授权资产的文件上传、下载以及连接动作的控制 时间授权 实现对授权资源使用时间段的限制 特权指令 实现对特权指令的使用（支持黑白名单） 命令过滤 实现对授权系统用户所执行的命令进行控制 文件传输 SFTP 文件上传&#x2F;下载 文件管理 实现 Web SFTP 文件管理 工单管理（X-PACK） 支持对用户登录请求行为进行控制 组织管理（X-PACK） 实现多租户管理与权限隔离 安全审计 Audit 操作审计 用户操作行为审计 会话审计 在线会话内容审计 历史会话内容审计 录像审计 支持对 Linux、Windows等资产操作的录像进行回放审计 支持对 RemoteApp（X-PACK）、MySQL 等应用操作的录像进行回放审计 指令审计 支持对资产和应用等操作的命令进行审计 文件传输 可对文件的上传、下载记录进行审计 1.5 JumpServer组成Lina前端 UI 项目，实现web页面展示，主要使用 Vue，Element UI 完成 Core现指 Jumpserver 管理后台，是核心组件（Core）, 使用 Django Class Based View 风格开发，支持 Restful API。 Coco&#x2F;Koko：实现了 SSH Server 和 Web Terminal Server 的组件，提供 SSH 和 WebSocket 接口, 使用Paramiko 和 Flask 开发。Koko 是 Go 版本的 coco，重构了 coco 的 SSH&#x2F;SFTP 服务和 Web Terminal 服务。 Luna现在是 Web Terminal 前端，计划前端页面都由该项目提供，Jumpserver 只提供 API，不再负责后台渲染html等。主要使用Angular CLI 完成 Lion：Lion 使用了 Apache 软件基金会的开源项目 Guacamole，JumpServer 使用 Golang 和 Vue 重构了 Guacamole 实现 RDP&#x2F;VNC 协议跳板机功能。 2 JumpServer安装官方说明：https://docs.jumpserver.org/zh/master/install/setup_by_fast/ 2.1 安装要求JumpServer 环境要求: 硬件配置: 2个CPU核心, 4G 内存, 50G 硬盘（最低） 操作系统: Linux 发行版 x86_64 Python &#x3D; 3.6.x MySQL Server ≥ 5.6 或者 Mariadb Server ≥ 5.5.56 数据库编码要求 uft8,新版要求5.7以上 Redis: 新版要求6.0以上 2.2 安装方法介绍官方提供了多种安装方法 手动部署: 按组件逐个实现 极速部署: 资产数量不多，或者测试体验的用户请使用本脚本快速部署 容器部署: 基于 docker 实现 分布式部署: 适用大型环境 2.3 基于容器部署官方文档: 12https://github.com/jumpserver/Dockerfile/tree/master/allinonehttps://docs.jumpserver.org/zh/master/install/docker_install/ 2.3.1 环境说明1https://github.com/jumpserver/Dockerfile/tree/master/allinone 使用外置 MySQL 数据库和 Redis: 123456789101112131415- 外置数据库要求 MySQL 版本大于等于 5.7- 外置 Redis 要求 Redis 版本大于等于 6.0# 自行部署 MySQL 可以参考(https://docs.jumpserver.org/zh/master/install/setup_by_lb/#mysql) # mysql 创建用户并赋予权限, 请自行替换 nu4x599Wq7u0Bn8EABh3J91G 为自己的密码mysql -u root -pcreate database jumpserver default charset &#x27;utf8&#x27;;create user &#x27;jumpserver&#x27;@&#x27;%&#x27; identified by &#x27;nu4x599Wq7u0Bn8EABh3J91G&#x27;;grant all on jumpserver.* to &#x27;jumpserver&#x27;@&#x27;%&#x27;;flush privileges; # 自行部署 Redis 可以参考(https://docs.jumpserver.org/zh/master/install/setup_by_lb/#redis) 基于容器,安装完毕后可以通过以下方式访问 浏览器访问: http:&#x2F;&#x2F;&lt;容器所在服务器IP&gt; 默认管理员账户 admin 密码 admin SSH 访问: ssh -p 2222 &lt;容器所在服务器IP&gt; XShell 等工具请添加 connection 连接, 默认 ssh 端口 2222 2.3.2 安装 docker 环境123456789[root@ubuntu2004 ~]#apt update &amp;&amp; apt list docker.io[root@ubuntu2004 ~]#apt-cache madison docker.io[root@ubuntu2004 ~]#apt -y install docker.io[root@ubuntu2004 ~]#docker version#镜像加速(可选)[root@ubuntu2004 ~]#echo &#x27;&#123;&quot;registry-mirrors&quot;:[&quot;http://si7y70hh.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;:[&quot;harbor.wang.org:80&quot;]&#x27; &gt; /etc/docker/daemon.json[root@ubuntu2004 ~]#systemctl restart docker 2.3.3 安装 MySQL 服务官方说明: 1https://docs.jumpserver.org/zh/master/install/prod/distributed_03/ MySQL要求 1234create database jumpserver default charset &#x27;utf8&#x27;;create user &#x27;jumpserver&#x27;@&#x27;%&#x27; identified by &#x27;nu4x599Wq7u0Bn8EABh3J91G&#x27;;grant all on jumpserver.* to &#x27;jumpserver&#x27;@&#x27;%&#x27;;flush privileges; 注意：JumpServer-v2.28.7之前版本默认不支持MySQL8.0，选择MySQL5.7 2.3.3.1 下载 MySQL 镜像查看默认配置下载MySQL镜像并运行, 查看当前默认配置不符合 JumpServer 安装要求，跳转到 2.3.3.2 执行操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#下载MySQL镜像并启动[root@ubuntu2004 ~]#docker run --rm --name mysql -e MYSQL_ROOT_PASSWORD=123456 -e MYSQL_DATABASE=jumpserver -e MYSQL_USER=jumpserver -e MYSQL_PASSWORD=123456 -d -p 3306:3306 mysql:5.7.30#查看默认的MySQL容器配置不符合jumpserver要求[root@ubuntu2004 ~]#docker exec -it mysql bashroot@f44e1c85f088:/# mysql -uroot -p123456mysql&gt; show create database jumpserver;+------------+-----------------------------------------------------------------------+| Database | Create Database |+------------+-----------------------------------------------------------------------+| jumpserver | CREATE DATABASE `jumpserver` /*!40100 DEFAULT CHARACTER SET latin1 */ |+------------+-----------------------------------------------------------------------+1 row in set (0.01 sec)mysql&gt; select user,host from mysql.user;+---------------+-----------+| user | host |+---------------+-----------+| jumpserver | % || root | % || mysql.session | localhost || mysql.sys | localhost || root | localhost |+---------------+-----------+5 rows in set (0.00 sec)mysql&gt; exit#查看配置文件路径root@f44e1c85f088:/# cat /etc/mysql/mysql.cnf......!includedir /etc/mysql/conf.d/!includedir /etc/mysql/mysql.conf.d/#默认配置文件root@f44e1c85f088:/# tree /etc/mysql//etc/mysql/|-- conf.d| |-- docker.cnf| |-- mysql.cnf| `-- mysqldump.cnf|-- my.cnf -&gt; /etc/alternatives/my.cnf|-- my.cnf.fallback|-- mysql.cnf`-- mysql.conf.d `-- mysqld.cnf2 directories, 7 filesroot@f44e1c85f088:/# ls -R /etc/mysql/etc/mysql:conf.d my.cnf my.cnf.fallback mysql.cnf mysql.conf.d/etc/mysql/conf.d:docker.cnf mysql.cnf mysqldump.cnf/etc/mysql/mysql.conf.d:mysqld.cnf#默认配置文件root@f44e1c85f088:/# grep &#x27;^[^#]&#x27; /etc/mysql/mysql.conf.d/mysqld.cnf[mysqld]pid-file = /var/run/mysqld/mysqld.pidsocket = /var/run/mysqld/mysqld.sockdatadir = /var/lib/mysqlsymbolic-links=0#默认配置文件root@f44e1c85f088:/# cat /etc/mysql/conf.d/mysql.cnf[mysql]root@f44e1c85f088:/# exitexit[root@centos8 ~]#docker stop mysql 2.3.3.2 在宿主机准备MySQL配置文件12345678910111213141516171819202122232425262728#准备相关目录[root@ubuntu2004 ~]#mkdir -p /etc/mysql/mysql.conf.d/[root@ubuntu2004 ~]#mkdir -p /etc/mysql/conf.d/#生成服务器配置文件,指定字符集[root@ubuntu2004 ~]#tee /etc/mysql/mysql.conf.d/mysqld.cnf &lt;&lt;EOF[mysqld]pid-file= /var/run/mysqld/mysqld.pidsocket= /var/run/mysqld/mysqld.sockdatadir= /var/lib/mysqlsymbolic-links=0character-set-server=utf8 #添加此行,指定字符集EOF#生成客户端配置文件,指定字符集[root@ubuntu2004 ~]#tee /etc/mysql/conf.d/mysql.cnf &lt;&lt;EOF[mysql]default-character-set=utf8 #添加此行,指定字符集EOF#查看配置文件列表[root@ubuntu2004 ~]#tree /etc/mysql//etc/mysql/├── conf.d│ └── mysql.cnf└── mysql.conf.d └── mysqld.cnf2 directories, 2 files 2.3.3.3 启动 MySQL 容器将上面宿主机的设置好的配置文件挂载至MySQL容器 12345678[root@ubuntu2004 ~]#docker run -d -p 3306:3306 --name mysql --restart always \\-e MYSQL_ROOT_PASSWORD=123456 \\-e MYSQL_DATABASE=jumpserver \\-e MYSQL_USER=jumpserver \\-e MYSQL_PASSWORD=123456 \\-v /data/mysql:/var/lib/mysql \\-v /etc/mysql/mysql.conf.d/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf \\-v /etc/mysql/conf.d/mysql.cnf:/etc/mysql/conf.d/mysql.cnf mysql:5.7.30 2.3.3.4 验证 MySQL123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117[root@ubuntu2004 ~]#docker exec -it mysql sh# mysql -p123456 -e &#x27;show variables like &quot;character%&quot;&#x27;mysql: [Warning] Using a password on the command line interface can be insecure.+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+# mysql -p123456 -e &#x27;show variables like &quot;collation%&quot;&#x27; mysql: [Warning] Using a password on the command line interface can be insecure.+----------------------+-----------------+| Variable_name | Value |+----------------------+-----------------+| collation_connection | utf8_general_ci || collation_database | utf8_general_ci || collation_server | utf8_general_ci |+----------------------+-----------------+# cat /var/lib/mysql/jumpserver/db.optdefault-character-set=utf8default-collation=utf8_general_ci# cat /etc/mysql/mysql.conf.d/mysqld.cnf[mysqld]pid-file= /var/run/mysqld/mysqld.pidsocket= /var/run/mysqld/mysqld.sockdatadir= /var/lib/mysqlsymbolic-links=0character-set-server=utf8# cat /etc/mysql/conf.d/mysql.cnf[mysql]default-character-set=utf8# mysql -p123456 -e &#x27;select user,host from mysql.user&#x27;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| user | host |+---------------+-----------+| jumpserver | % || root | % || mysql.session | localhost || mysql.sys | localhost || root | localhost |+---------------+-----------+# mysql -p123456 -e &#x27;select user,host from mysql.user&#x27;mysql: [Warning] Using a password on the command line interface can be insecure.+---------------+-----------+| user | host |+---------------+-----------+| jumpserver | % || root | % || mysql.session | localhost || mysql.sys | localhost || root | localhost |+---------------+-----------+# ls /var/lib/mysql/ -ltotal 188484-rw-r----- 1 mysql mysql 56 Aug 16 08:43 auto.cnf-rw------- 1 mysql mysql 1680 Aug 16 08:43 ca-key.pem-rw-r--r-- 1 mysql mysql 1112 Aug 16 08:43 ca.pem-rw-r--r-- 1 mysql mysql 1112 Aug 16 08:43 client-cert.pem-rw------- 1 mysql mysql 1676 Aug 16 08:43 client-key.pem-rw-r----- 1 mysql mysql 1346 Aug 16 08:43 ib_buffer_pool-rw-r----- 1 mysql mysql 50331648 Aug 16 08:43 ib_logfile0-rw-r----- 1 mysql mysql 50331648 Aug 16 08:43 ib_logfile1-rw-r----- 1 mysql mysql 79691776 Aug 16 08:43 ibdata1-rw-r----- 1 mysql mysql 12582912 Aug 16 08:43 ibtmp1drwxr-x--- 2 mysql mysql 20 Aug 16 08:43 jumpserverdrwxr-x--- 2 mysql mysql 4096 Aug 16 08:43 mysqldrwxr-x--- 2 mysql mysql 8192 Aug 16 08:43 performance_schema-rw------- 1 mysql mysql 1676 Aug 16 08:43 private_key.pem-rw-r--r-- 1 mysql mysql 452 Aug 16 08:43 public_key.pem-rw-r--r-- 1 mysql mysql 1112 Aug 16 08:43 server-cert.pem-rw------- 1 mysql mysql 1676 Aug 16 08:43 server-key.pemdrwxr-x--- 2 mysql mysql 8192 Aug 16 08:43 sys#或者执行下面[root@ubuntu2004 ~]#docker exec mysql mysql -p123456 -e &#x27;show variables like &quot;character%&quot;&#x27;[root@ubuntu2004 ~]#docker exec mysql mysql -p123456 -e &#x27;show variables like &quot;collation%&quot;&#x27;[root@ubuntu2004 ~]#docker exec mysql cat /var/lib/mysql/jumpserver/db.opt[root@ubuntu2004 ~]#docker exec mysql cat /etc/mysql/conf.d/mysql.cnf[root@ubuntu2004 ~]#docker exec mysql mysql -p123456 -e &#x27;select user,host from mysql.user&#x27;[root@ubuntu2004 ~]#docker exec mysql ls /var/lib/mysql/ -l[root@ubuntu2004 ~]#ls /data/mysql/auto.cnf client-cert.pem ibdata1 ibtmp1 performance_schema server-cert.pemca-key.pem client-key.pem ib_logfile0 jumpserver private_key.pem server-key.pemca.pem ib_buffer_pool ib_logfile1 mysql public_key.pem sys#测试连接MySQL[root@ubuntu2004 ~]#yum -y install mysql[root@ubuntu2004 ~]#mysql -ujumpserver -p123456 -h10.0.0.8MySQL [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || jumpserver |+--------------------+2 rows in set (0.00 sec)MySQL [(none)]&gt; use jumpserverDatabase changedMySQL [jumpserver]&gt; show tables;Empty set (0.00 sec) 2.3.4 安装 Redis 服务官方说明 1https://docs.jumpserver.org/zh/master/install/prod/distributed_04/ 新版要求: 12外置 Redis 要求 Redis 版本大于等于 6.0注意:不支持redis7.0 2.3.4.1 启动 Redis12[root@ubuntu2004 ~]#docker run -d -p 6379:6379 --name redis --restart always redis:6.2.72b75e77aa35b79dcb0e4cffb41d124c10124cdfda959e3131f7927e8808f9417 2.3.4.2 验证 Redis连接1234567[root@ubuntu2004 ~]#yum -y install redis[root@ubuntu2004 ~]#redis-cli -h 10.0.0.810.0.0.8:6379&gt; info# Serverredis_version:5.0.9redis_git_sha1:00000000redis_git_dirty:0 2.3.5 部署 JumpServer2.3.5.1 生成 key 和 token参考官方说明: 12https://github.com/jumpserver/Dockerfile/tree/master/allinonehttps://docs.jumpserver.org/zh/master/install/docker_install/ 需要先生成 key 和 token 123456789101112131415161718192021[root@ubuntu2004 ~]#cat key.sh#!/bin/bashif [ ! &quot;$SECRET_KEY&quot; ]; then SECRET_KEY=`cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 50`; echo &quot;SECRET_KEY=$SECRET_KEY&quot; &gt;&gt; ~/.bashrc; echo SECRET_KEY=$SECRET_KEY;else echo SECRET_KEY=$SECRET_KEY;fi if [ ! &quot;$BOOTSTRAP_TOKEN&quot; ]; then BOOTSTRAP_TOKEN=`cat /dev/urandom | tr -dc A-Za-z0-9 | head -c 16`; echo &quot;BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN&quot; &gt;&gt; ~/.bashrc; echo BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN;else echo BOOTSTRAP_TOKEN=$BOOTSTRAP_TOKEN;fi[root@ubuntu2004 ~]#bash key.sh[root@ubuntu2004 ~]#tail -n2 .bashrcSECRET_KEY=9RTRBg3AjHjvUUNCUpHUH5LirSFazRozk1UyOQcoKkwMExeUEmBOOTSTRAP_TOKEN=1OlaSdCoUpSQPjH6 2.3.5.2 运行容器范例：JumpServer v2.28.7 版 12345678910111213141516171819docker run --name jms_all -d \\ -v /opt/jumpserver/core/data:/opt/jumpserver/data \\ -v /opt/jumpserver/koko/data:/opt/koko/data \\ -v /opt/jumpserver/lion/data:/opt/lion/data \\ -p 80:80 \\ -p 2222:2222 \\ -e SECRET_KEY=vGeWBRADIPl7YYus0SsON7aI15TUW1dJ1rSskvizc3YgRr7hOL \\ -e BOOTSTRAP_TOKEN=OdY0kincspHW3bXt \\ -e LOG_LEVEL=ERROR \\ -e DB_HOST=10.0.0.200 \\ #不支持127.0.0.1 -e DB_PORT=3306 \\ -e DB_USER=jumpserver \\ -e DB_PASSWORD=123456 \\ -e DB_NAME=jumpserver \\ -e REDIS_HOST=10.0.0.200 \\ #不支持127.0.0.1 -e REDIS_PORT=6379 \\ -e REDIS_PASSWORD=&#x27;&#x27; \\ --privileged=true \\ jumpserver/jms_all:v2.28.7 2.3.5.3 验证是否成功2.3.5.3.1 查看日志范例：查看jumpserver 2.28.7日志确认成功 12345678910111213#以下错误提示不影响使用[root@ubuntu2204 ~]#docker logs -f jms_all.....Applying users.0039_auto_20211229_1852...2023-03-10 16:00:14 [ERRO] Post &quot;http://127.0.0.1:8080/api/v1/terminal/terminal-registrations/&quot;: dial tcp 127.0.0.1:8080: connect: connection refused 2023-03-10 16:00:14 main main.go [ERROR] Post&quot;http://127.0.0.1:8080/api/v1/terminal/terminal-registrations/&quot;: dial tcp 127.0.0.1:8080: connect: connection refused2023-03-10 16:00:14 [ERRO] Post &quot;http://127.0.0.1:8080/api/v1/terminal/terminal-registrations/&quot;: dial tcp 127.0.0.1:8080: connect: connection refused OKApplying users.0040_alter_user_source... OKAfter migration, update builtin role permissions2023-03-10 16:00:15 Fri Mar 10 16:00:15 20232023-03-10 16:00:15 JumpServer version v2.28.7, more see https://www.jumpserver.orgLion Version v2.28.7, more see https://www.jumpserver.org2023-03-10 16:00:30 KoKo Version v2.28.7, more see https://www.jumpserver.org Quit the server with CONTROL-C. 2.3.5.3.2 查看 MySQL 中生成相关表123456789[root@centos8 ~]#docker exec -it mysql sh# mysql -ujumpserver -p123456 jumperservermysql&gt; show tables;+----------------------------------------------+| Tables_in_jumpserver |+----------------------------------------------+| applications_databaseapp || applications_remoteapp |.... 2.3.5.3.3 查看端口12345678[root@centos8 ~]#ss -ntlState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 0.0.0.0:22 0.0.0.0:* LISTEN 0 100 127.0.0.1:25 0.0.0.0:* LISTEN 0 128 *:2222 *:* LISTEN 0 128 *:80 *:* LISTEN 0 128 [::]:22 [::]:* LISTEN 0 100 [::1]:25 [::]:* 2.3.5.3.4 验证登录 2.4 一键脚本安装1https://docs.jumpserver.org/zh/v4/installation/setup_linux_standalone/online_install/ 范例： 12345678910111213141516171819202122232425262728[root@ubuntu2204 ~]#curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash......&gt;&gt;&gt; 安装完成了1. 可以使用如下命令启动, 然后访问cd /opt/jumpserver-installer-v3.0.4./jmsctl.sh start2. 其它一些管理命令./jmsctl.sh stop./jmsctl.sh restart./jmsctl.sh backup./jmsctl.sh upgrade更多还有一些命令, 你可以 ./jmsctl.sh --help 来了解3. Web 访问http://192.168.250.1:80默认用户: admin 默认密码: admin4. SSH/SFTP 访问ssh -p2222 admin@192.168.250.1sftp -P2222 admin@192.168.250.15. 更多信息我们的官网: https://www.jumpserver.org/我们的文档: https://docs.jumpserver.org/#安装完成后，直接访问即可，默认用户名和密码是admin/adminhttp://jumpserver_ip/ 3 JumpServer 常见功能官方文档：https://docs.jumpserver.org/zh/master/ 3.1 登录并初始化配置3.1.1 首次登录3.1.1.1 浏览器访问1http:&lt;JumpServerIP&gt; 登录 JumpServer 默认用户: admin 密码: admin 第一次登录要求重置密码 用新密码重新登录 3.1.1.2 ssh 登录可以用admin用户和修改过的新密码, 连接jumpserver的ssh端口2222&#x2F;tcp进行连接访问 注意:新版需要实现资产授权后才能连接登录 3.1.2 修改 admin 用户的密码 3.1.3 配置邮件 测试连接,可以收到邮件 3.2 创建JumpServer用户和组JumpServer 支持三种登录用户 系统管理员 普通用户 系统审计员 3.2.1 创建用户 填写用户信息，注意: 邮箱地址不能重复 相同方法再创建xiaoming用户 3.2.2 创建组并将用户加入组中 创建development组,并将前面创建的用户加入组中 创建test组,将入用户 查看创建的组 3.2.3 使用新用户登录 首次登录完善个人信息 3.2.4 禁用和启用用户禁用用户 启用用户 3.2.5 启用用户多因子认证功能多因子认证Multi-Factor Authentication (MFA) 是一种简单有效的最佳安全实践方法，用户要通过两种以上的认证机制之后，才能得到授权，使用计算机资源 JumpServer 启用 MFA 后，用户登录网站时，系统将要求输入用户名和密码（第一安全要素），然后要求输入来自其 MFA 设备的动态验证码（第二安全要素），双因素的安全认证将为您的账户提供更高的安全保护 3.2.5.1 开启MFA在用户列表中,对指定用户更新配置 3.2.5.2 绑定手机APP 按下图下载APP并安装 3.2.5.3 重新登录验证 3.2.6 创建系统审计员系统审计员即查看录像带的人 使用审计员用户登录 3.3 管理资产JumpServer 可以管理各种类型的资产 JumpServer中的三种用户 登录用户: 分配给用户用于登录JumpServer时使用 系统用户中的特权用户(管理用户): 对后端服务器具有管理权限的系统帐号 root或administrator 及sudo ALL权限的用户,用于管理后端服务器 新版中已取消此名称,改名为系统用户中特权用户 系统用户中的普通用户(系统用户): 给登录用户使用ssh连接后端服务器时对应的系统用户 ,一般是后端服务器的普通的系统用户帐号 新版中改名为系统用户中普通用户 根据 1.3 架构图，比如有个开发人员小明，小明登录jumpserve的账号就是登录用户，通过jumpserver去连接后端服务器就是系统用户，后端服务器的系统用户账号可以先创建管理用户，后续创建完资产，授权等操作后，会通过管理用户自动使用ansible批量创建 3.3.1 创建管理用户(系统用户中特权用户)管理用户是jumpServer用来管理后端服务器或其它资产的管理员用户,此用户必须对后端服务器有管理权限 管理用户特点: 通常是后端服务器的root或者是具备root权限的超级用户 用于推送或者是创建系统用户 用于获取被管理的硬件资产信息 新版 旧版 3.3.2 创建资产创建可以被jumpServer用户访问的后端服务器和其它资产,比如:路由器,交换机等 3.3.2.1 创建资产 创建多个资产 3.3.2.2 创建节点实现资产分类创建节点将资产分类 3.4 授权管理 3.4.1 创建系统用户(系统用户中普通用户)系统用户是分配给JumpServer用户,用来让JumpServer用户在连接后端服务器和其它资产,一般不会给管理权限 生产环境中,一般都会利用自动化运维工具提前在后端服务器创建好系统用户,在所有后端服务器统一用户ID信息,而非在jumpserver中创建 新版界面 旧版 旧版界面 3.4.2 关联使用系统用户的资产将前面创建的系统用户推送到后端服务器并自动创建 在系统用户中添加使用此用户的资产 选择使用此系统用户的主机 测试可连接性 注意:连接性是利用系统用户进行连接的,所以之前必须先推送系统用户后才能测试连接 如果前面系统用户是自动推送,则无需推送系统用户,就可以在后端服务器自动创建用户 在后端服务器验证用户创建成功 1234[root@ubuntu1804 ~]#id mageduuid=1001(magedu) gid=1001(magedu) groups=1001(magedu)[root@centos7 ~]#id mageduuid=1002(magedu) gid=1002(magedu) groups=1002(magedu) 3.4.3 创建授权规则通过授权规则, 为JumpServer用户分配可以访问的资产及使用的系统用户 新版 旧版 同样再创建授权规则 3.4.4 测试登录访问后端服务器 选中主机后,输入命令,再执行 点右上角的web终端,可以打开终端界面 3.4.5 测试上传和下载文件可以通过web方式或者通过filezilla等工具直接连接jumpserver的2222端口都可以实现文件的上传下载，存放在了&#x2F;tmp 3.4.6 ssh连接JumpServerJumpServer 还支持用户通过ssh连接其2222&#x2F;tcp端口进行访问 连接方式 1ssh -p 2222 user@jumpserver 3.5 数据库授权注意: JumpServer v2.5 此功能有Bug ,可以选择 V2.4版 3.5.1 在后端服务器安装MySQL并创建数据库和用户范例：Ubuntu 1234567891011121314[root@ubuntu2204 ~]#apt -y install mysql-server[root@ubuntu2204 ~]#vim /etc/mysql/mysql.conf.d/mysqld.cnf#bind-address = 127.0.0.1#mysqlx-bind-address = 127.0.0.1[root@ubuntu2204 ~]#sed -i &#x27;/127.0.0.1/s/^/#/&#x27; /etc/mysql/mysql.conf.d/mysqld.cnf[root@ubuntu2204 ~]#systemctl restart mysql[root@ubuntu2204 ~]#mysqlmysql&gt; create database wordpress;mysql&gt; create user wordpress@&#x27;10.0.0.%&#x27; identified by &#x27;123456&#x27;;mysql&gt; grant all on wordpress.* to wordpress@&#x27;10.0.0.%&#x27;&#x27; ; 范例：红帽系统 123456789101112[root@centos8 ~]#hostname -I10.0.0.200[root@centos8 ~]#dnf -y install mariadb-server[root@centos8 ~]#systemctl enable --now mariadb[root@centos8 ~]#mysqlMariaDB [(none)]&gt; create database wordpress;MariaDB [(none)]&gt; create user wordpress@&#x27;10.0.0.%&#x27; identified by &#x27;123456&#x27;;MariaDB [(none)]&gt; grant all on wordpress.* to wordpress@&#x27;10.0.0.%&#x27; ;MariaDB [(none)]&gt; exitBye 3.5.2 创建数据库应用新版：资产管理–资产列表–数据库–创建 旧版：应用管理–数据库–创建 3.5.3 创建数据库的系统用户创建针对数据库的专用系统用户 注意: 协议选择mysql 3.5.4 创建数据库授权规则权限管理–应用授权 3.5.5 测试数据库连接3.5.5.1 web 方式连接MySQL资产 注意: v2.5.3有 bug,无法连接 3.5.5.2 用ssh连接MySQL资产 3.6 会话管理会话管理可以实现查看当前在线的会话命令记录、历史会话过去的执行过的命令,也可以强制将用户踢出 3.6.1 查看在线会话 3.6.2 查看历史会话3.6.2.1 在线回放 3.6.2.2 离线查看录像可以将历史会话过程下载再播放,但需要下载相关的播放器 注意：JumpServer-v2.19.1版的录像无法打开 1https://github.com/jumpserver/VideoPlayer/releases 3.6.3 查看命令历史 3.6.4 终端管理可以在线监控用户正在进行的操作过程,甚至可以强制中断用户的连接,将其踢出会话 3.7 命令过滤器使用命令过滤器可以禁止用户执行特定的危险命令,防止误操作或恶意行为 系统用户可以绑定一些命令过滤器，一个过滤器可以定义一些规则 当用户使用这个系统用户登录资产，然后执行一个命令.这个命令需要被绑定过滤器的所有规则匹配，高优先级先被匹配， 当一个规则匹配到了，如果规则的动作是允许，这个命令会被放行， 如果规则的动作是禁止，命令将会被禁止执行， 否则就匹配下一个规则，如果最后没有匹配到规则，则允许执行 3.7.1 创建命令过滤器新版3.4.0 创建命令组 旧版 3.7.2 将过滤器绑定系统用户 3.7.3 测试 3.8 资产的批量导出和导入当需要管理的后端服务器很多时,每台主机手动导入效率很低,可以利用资产的导出导入功能实现批量导入,提高效率 3.8.1 资产的批量导出 3.8.2 批量导入资产参考上面导出的文件格式,生成导入的文件,再批量导入资产 注意: 删除id列,修改主机名和IP字段,再导入 新版还需要将系统帐号推送到新添加的服务器 旧版","categories":[],"tags":[{"name":"JumpServer","slug":"JumpServer","permalink":"https://aquapluto.github.io/tags/JumpServer/"}]},{"title":"OpenVPN","slug":"Middleware/OpenVPN","date":"2023-08-21T02:59:41.000Z","updated":"2025-09-13T13:56:37.715Z","comments":true,"path":"Middleware/OpenVPN/","permalink":"https://aquapluto.github.io/Middleware/OpenVPN/","excerpt":"","text":"1 OpenVPN简介1.1 VPN 介绍一些组织需要实现跨地域的互相通信,比如:北京总公司和上海分公司之间进行网络互通,一般通过下面两种方式实现 专用网： 所谓专用网就是在两个网络（例如，北京总公司和广州分公司）的网络之间架设一条专用线路，但是它并不需要真正地去铺设光缆之类的物理线路。虽然没有实际去铺设网线，但是仍需要向电信运营商申请租用专线，在这条专用的线路上只传输自己组织之间的数据信息,所以此方式安全稳定,同时也费用高昂 VPN： Virtual Private Network，虚拟私有网络，或称为虚拟专用网络，是专用网络的一种延伸，属于远程访问技术的一种，可以在公用网络的基础上建立专用网络，但其并不是物理意义上的专线，而是在公共的互联网的基础上虚拟出一个专用网络，常用于在公用网络上实现专用网络功能，为了安全通常需要进行加密通讯。此方式在企业网络中有广泛应用。VPN网关通过对数据包的加密和数据包目标地址的转换实现远程访问。VPN功能可通过服务器、网络硬件、软件等多种方式实现。例如在企业中，员工出差到外地，所使用的终端不处于企业内网中，又需要访问企业内网，就可以使用VPN 技术来实现。VPN 的主要目的是确保数据传输的安全性和隐私性，通常用于远程访问内部网络资源、保护用户在互联网上的在线活动不被监控或跟踪，以及绕过地理限制和网络审查。 VPN 的工作原理通常包括以下几个步骤： 客户端请求：用户通过 VPN 客户端软件发起连接到目标网络的请求。 建立隧道：VPN 客户端与 VPN 服务器之间建立一个加密的隧道。这个隧道通过互联网或其他公共网络传输数据，但是数据是加密的，所以即使被拦截也无法读取。 身份验证：在建立隧道之前或之后，用户需要通过用户名、密码或其他认证方式进行身份验证，以确保只有授权用户可以访问 VPN。 数据传输：一旦隧道建立并且用户通过身份验证，数据就可以在客户端和服务器之间安全地传输。这些数据包括网站访问、文件下载、上传等。 解密和转发：VPN 服务器接收到加密的数据后，会对其进行解密，然后将数据转发到目标网络或互联网上的目的地。 响应返回：当目标服务器响应请求时，响应数据会按照相反的路径返回给用户。 1.2 VPN 常见应用模式1.2.1 点对站点 peer to site 1.2.2 站点对站点 site to site连接两个或多个地理位置分散的网络，通常用于公司分支机构之间的连接。 1.3 OpenVPNOpenVPN 是Linux下开源VPN的应用，提供了良好的性能和友好的用户GUI。 OpenVPN 是一个基于 OpenSSL 库的应用层 VPN 实现。和传统 VPN 相比，它的优点是简单易用。 OpenVPN允许参与建立VPN的单点使用共享密钥，电子证书，或者用户名&#x2F;密码来进行身份验证。 OpenVPN支持在各种系统，如:Linux、Windows、Mac OS X、Solaris、OpenBSD、FreeBSD、NetBSD上运行，并包含了许多安全性的功能。它并不是一个基于Web的VPN软件，也不与IPsec及其他VPN软件包兼容。 OpenVPN 提供了多种身份验证方式，用以确认参与连接双方的身份，包括：共享私钥，第三方证书以及用户名&#x2F;密码组合。共享密钥最为简单，但同时它只能用于建立点对点的 VPN；基于 PKI 的第三方证书提供了最完善的功能，但是需要额外的精力去维护一个 PKI 证书体系。 OpenVPN2.0 后引入了用户名&#x2F;口令组合的身份验证方式，它可以省略客户端证书，但是仍有一份服务器证书需要被用作加密 官方网站： https://openvpn.net GitHub地址：https://github.com/OpenVPN/openvpn OpenVPN 常见适用场景 实现远程主机到内网的连接 实现多个远程主机之间的连接 2 OpenVPN 部署2.1 准备 OpenVPN 部署环境官文文档: https://openvpn.net/community-resources/how-to/ 其他文档:https://www.softool.cn/blog-91.html 可选择以下两套环境之一实现OpenVPN 配置说明：在阿里云上购买三台主机，设置为同一个内网网段，模拟公司内网，其中一台添加一个公网IP，在该机器上部署 OpenVPN 服务，以便让不在公司内网的远程主机可以连进公司内网的目的 部署流程 准备阿里云主机，其中一台配置公网IP 在阿里云安全组中放行 1194 端口 在有公网IP的阿里云主机上部署 OpenVPN Server 在 Magedu 内网主机上部署 OpenVPN Client 测试OpenVPN 网络 2.1.1 环境1: 阿里云 OpenVPN 实战环境准备阿里云网络实验环境 12345678910111213141516171819202122232425262728291 阿里云创建专有网络指定城市和可用区:华北3张家口可用区A区网段名magedu-net1和地址段172.16.0.0/12,默认资源组交换机名magedu-net1-sw1 可用区A IPv4的地址段 172.30.0.0/24安全组开放22端口2 创建OpenVPN服务器有公网IP的实例1个指定城市和可用区:华北3张家口可用区A区计算型c6 2vCPU 4G网络:magedu-net1 交换机:magedu-net1-sw1公网IP 按量收费 10M默认安全组 默认配置 22,3389,icmpcentos8.2系统盘 存储默认高效云盘40G3 创建局域网的服务器无公网IP的实例2个按量付费指定城市和可用区:华北3张家口可用区A区共享型 2vCPU2Gcentos8.2系统盘 存储默认高效云盘40G网络:magedu-net1 magedu-net1-sw1无公网IP默认安全组主网卡sw14 重设所有实例密码5 修改安全组打开 1194/TCP/UDP 2.1.1.1 购买第一台有公网IP的ECS阿里云购买链接 1https://ecs-buy.aliyun.com/wizard?spm=5176.8789780.1092585.1.1db055ca1sGhza#/prepay/cn-hangzhou?periodType=Yearly&amp;period=1&amp;instanceType=ecs.g5.large 登录后点击右上角控制台，进入购买界面 选择付费类型；地域（注意：配一个局域网必须选同一个地域）；可用区（可用区表示在这个地域有多少个机房，互相互联互通）；内核个数，内存，架构类型，规格；镜像（装什么系统）；存储硬盘；快照服务 2.1.1.2 先创建网络和交换机创建专有网络 创建交换机 openvpn服务器需要分配一个公网地址 2.1.1.3 再购买两台内网无公网的ECS注意和第一台主机在同一个地域和可用区 网络和第一台主机一样,无公网IP 2.1.1.4 验证主机配置创建完成,三台主机 1234567891011121314151617181920212223242526272829[root@centos8 ~]#ssh 39.100.145.14root@39.100.145.14&#x27;s password:Welcome to Alibaba Cloud Elastic Compute Service !Activate the web console with: systemctl enable --now cockpit.socketLast failed login: Tue Sep 15 10:15:15 CST 2020 from 222.137.18.20 on ssh:nottyThere was 1 failed login attempt since the last successful login.Last login: Tue Sep 15 10:13:10 2020 from 222.137.18.20[root@vpn-server ~]# hostname -I172.30.0.1[root@vpn-server ~]# hostnamevpn-server.magedu.org[root@vpn-server ~]# cat /etc/centos-releaseCentOS Linux release 8.2.2004 (Core)#利用39.100.145.14当跳板连接web1和web2，即先远程连39.100.145.14，再连web1和web2[root@web1 ~]# hostname -I172.30.0.100[root@web1 ~]# hostnameweb1.magedu.org[root@web1 ~]# cat /etc/centos-releaseCentOS Linux release 8.2.2004 (Core)[root@web2 ~]# hostnameweb2.magedu.org[root@web2 ~]# hostname -I172.30.0.200[root@web2 ~]# cat /etc/centos-releaseCentOS Linux release 8.2.2004 (Core) 2.1.1.5 修改网络防火墙规则默认VPN的端口无法访问,修改网络防火墙规则,添加规则实现1194（vpn自身端口）&#x2F;TCP&#x2F;UDP端口允许通过，还有http80端口 2.1.2 环境2: 局域网 OpenVPN 实战环境 123456789101112131415共四台主机1 openvpn server：CentOS 8.2eth0:10.0.0.8/24 NAT模式,模拟公网IPeth1:172.30.0.1/24 仅主机模式,私网IP2 内网主机两台第一台主机eth0:172.30.0.100/24 仅主机模式,私网IP，无需网关第二台主机eth0:172.30.0.200/24 仅主机模式,私网IP，无需网关3 Windows 客户端Windows 10 2.2 安装 OpenVPN 相关软件包2.2.1 查看版本2.2.1.1 查看官网的OpenVPN的版本访问官网： https://openvpn.net 2.2.1.2 在不同OS上查看 OpenVPN 版本CentOS系统上的EPEL源OpenVPN版本比Ubuntu的仓库中版本更新,以下选择在CentOS8上部署OpenVPN 范例: CentOS 查看OpenVPN版本 1234567891011121314151617181920212223[root@centos8 ~]#yum list openvpnLast metadata expiration check: 0:04:22 ago on Sun 02 Aug 2020 01:11:49 PM CST.Installed Packagesopenvpn.x86_64 2.4.9-1.el8 epel [root@centos7 ~]#yum list openvpnLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile* base:Available Packagesopenvpn.x86_64 2.4.9-1.el7 epel [root@centos8 ~]#yum list easy-rsaLast metadata expiration check: 0:05:35 ago on Sun 02 Aug 2020 01:21:19 PM CST.Available Packageseasy-rsa.noarch 3.0.7-1.el8 epel [root@centos7 ~]#yum list easy-rsaLoaded plugins: fastestmirrorLoading mirror speeds from cached hostfile* base:Available Packageseasy-rsa.noarch 3.0.7-1.el7 epel 范例: Ubuntu 查看OpenVPN版本 12345678910111213141516171819202122root@ubuntu2004:~# apt show openvpnPackage: openvpnVersion: 2.4.7-1ubuntu2.20.04.1.....[root@ubuntu1804 ~]#apt list openvpnListing... Doneopenvpn/bionic-proposed 2.4.4-2ubuntu1.4 amd64N: There are 2 additional versions. Please use the &#x27;-a&#x27; switch to see the[root@ubuntu1804 ~]#apt-cache madison openvpnopenvpn | 2.4.4-2ubuntu1.4 | http://mirrors.aliyun.com/ubuntu bionic-proposed/main amd64 Packagesopenvpn | 2.4.4-2ubuntu1.3 | http://mirrors.aliyun.com/ubuntu bionic-updates/main amd64 Packagesopenvpn | 2.4.4-2ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/main amd64 Packagesopenvpn | 2.4.4-2ubuntu1 | http://mirrors.aliyun.com/ubuntu bionic/main Sourcesopenvpn | 2.4.4-2ubuntu1.3 | http://mirrors.aliyun.com/ubuntu bionic-updates/main Sourcesopenvpn | 2.4.4-2ubuntu1.4 | http://mirrors.aliyun.com/ubuntu bionic-proposed/main Sources[root@ubuntu1804 ~]#apt-cache madison easy-rsaeasy-rsa | 2.2.2-2 | http://mirrors.aliyun.com/ubuntu bionic/universe amd64 Packageseasy-rsa | 2.2.2-2 | http://mirrors.aliyun.com/ubuntu bionic/universe i386 Packageseasy-rsa | 2.2.2-2 | http://mirrors.aliyun.com/ubuntu bionic/universe Sources 2.2.2 安装 OpenVPN后面环境以CentOS8上基于EPEL源安装OpenVPN为例 2.2.2.1 安装 OpenVPN 和证书工具注意: 需要提前配置epel源 123456789101112[root@centos8 ~]#vim /etc/yum.repos.d/epel.repo[epel]name=EPELbaseurl=https://mirrors.aliyun.com/epel/$releasever/Everything/$basearchgpgcheck=0enabled=1#OpenVPN服务器端[root@centos8 ~]#yum -y install openvpn#证书管理工具[root@centos8 ~]#yum -y install easy-rsa 2.2.2.2 查看包中相关文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@centos8 ~]#rpm -qi openvpn easy-rsaName : openvpnVersion : 2.4.9Release : 1.el8Architecture: x86_64Install Date: Sun 02 Aug 2020 01:12:21 PM CSTGroup : UnspecifiedSize : 1526179License : GPLv2Signature : RSA/SHA256, Sat 25 Apr 2020 05:46:35 AM CST, Key ID21ea45ab2f86d6a1Source RPM : openvpn-2.4.9-1.el8.src.rpmBuild Date : Sat 25 Apr 2020 05:29:33 AM CSTBuild Host : buildvm-06.phx2.fedoraproject.orgRelocations : (not relocatable)Packager : Fedora ProjectVendor : Fedora ProjectURL : https://community.openvpn.net/Bug URL : https://bugz.fedoraproject.org/openvpn.........Name : easy-rsaVersion : 3.0.7Release : 1.el8Architecture: noarchInstall Date: Sun 02 Aug 2020 01:46:51 PM CSTGroup : UnspecifiedSize : 120221License : GPLv2Signature : RSA/SHA256, Tue 31 Mar 2020 09:11:54 PM CST, Key ID21ea45ab2f86d6a1Source RPM : easy-rsa-3.0.7-1.el8.src.rpm\\..........[root@centos8 ~]#rpm -ql openvpn/etc/openvpn/etc/openvpn/client/etc/openvpn/server/run/openvpn-client/run/openvpn-server.........../usr/lib64/openvpn/plugins/usr/lib64/openvpn/plugins/openvpn-plugin-auth-pam.so/usr/lib64/openvpn/plugins/openvpn-plugin-down-root.so[root@centos8 ~]#rpm -ql easy-rsa/usr/share/doc/easy-rsa........... 2.2.2.3 准备相关配置文件123456789101112131415161718192021222324252627282930313233343536#准备证书颁发相关文件，让证书和 openvpn 配置处于一个目录，方便迁移[root@centos8 ~]#cp -r /usr/share/easy-rsa/3/ /etc/openvpn/easy-rsa#准备颁发证书相关变量的配置文件[root@centos8 ~]#cp /usr/share/doc/easy-rsa/vars.example /etc/openvpn/easy-rsa/vars#建议修改给CA和OpenVPN服务器颁发的证书的有效期,可适当加长[root@centos8 ~]#vim /etc/openvpn/easy-rsa/vars#CA机构的证书默认有效期为10年,可以适当延长,比如:36500天#set_var EASYRSA_CA_EXPIRE 3650set_var EASYRSA_CA_EXPIRE 36500#openvpn服务器证书默为为825天,可适当加长,比如:3650天#set_var EASYRSA_CERT_EXPIRE 825#将上面行修改为下面set_var EASYRSA_CERT_EXPIRE 3650[root@centos8 ~]#tree /etc/openvpn//etc/openvpn/├── client├── easy-rsa│ ├── easyrsa│ ├── openssl-easyrsa.cnf│ ├── vars│ └── x509-types│ ├── ca│ ├── client│ ├── code-signing│ ├── COMMON│ ├── email│ ├── kdc│ ├── server│ └── serverClient├── server└── server.conf 2.3 准备证书相关文件2.3.1 初始化PKI和CA颁发机构环境2.3.1.1 脚本easyrsa帮助用法123456789101112131415161718192021222324252627282930313233343536#准备证书的总目录[root@centos8 ~]#cd /etc/openvpn/easy-rsa[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/#可以帮助颁发服务器和客户端证书[root@centos8 easy-rsa]#file ./easyrsa./easyrsa: POSIX shell script, ASCII text executable#查看用法[root@centos8 easy-rsa]#./easyrsa...init-pki #初始化环境，创建相关目录build-ca [ cmd-opts ] #创建CA机构证书gen-dh #创建 Diffie-Hellman参数文件gen-req &lt;filename_base&gt; [ cmd-opts ] #证书申请sign-req &lt;type&gt; &lt;filename_base&gt;build-client-full &lt;filename_base&gt; [ cmd-opts ]build-server-full &lt;filename_base&gt; [ cmd-opts ]revoke &lt;filename_base&gt; [cmd-opts]renew &lt;filename_base&gt; [cmd-opts]build-serverClient-full &lt;filename_base&gt; [ cmd-opts ]gen-crlupdate-dbshow-req &lt;filename_base&gt; [ cmd-opts ]show-cert &lt;filename_base&gt; [ cmd-opts ]show-ca [ cmd-opts ] #查看CA机构证书相关信息import-req &lt;request_file_path&gt; &lt;short_basename&gt;export-p7 &lt;filename_base&gt; [ cmd-opts ]export-p12 &lt;filename_base&gt; [ cmd-opts ]set-rsa-pass &lt;filename_base&gt; [ cmd-opts ]set-ec-pass &lt;filename_base&gt; [ cmd-opts ]upgrade &lt;type&gt;DIRECTORY STATUS (commands would take effect on these locations)EASYRSA: /etc/openvpn/easy-rsa.0.7PKI: /etc/openvpn/easy-rsa/pki 2.3.1.2 初始化PKI生成PKI相关目录和文件1234567891011121314151617181920212223242526272829303132[root@centos8 ~]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#lseasyrsa openssl-easyrsa.cnf vars x509-types#初始化数据,在当前目录下生成pki目录及相关文件[root@centos8 easy-rsa]#./easyrsa init-pkiNote: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/varsinit-pki complete; you may now create a CA or requests.Your newly created PKI dir is: /etc/openvpn/easy-rsa/pki#在当前目录下创建了一个 pki 目录，用来存放证书文件[root@centos8 easy-rsa]#tree├── easyrsa├── openssl-easyrsa.cnf├── pki #生成一个新目录及相关文件│ ├── openssl-easyrsa.cnf #颁发证书的若干信息│ ├── private│ ├── reqs│ └── safessl-easyrsa.cnf├── vars└── x509-types ├── ca ├── client ├── code-signing ├── COMMON ├── email ├── kdc ├── server └── serverClient 2.3.2 创建 CA 机构证书环境123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@centos8 ~]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#tree pkipki├── openssl-easyrsa.cnf├── private├── reqs└── safessl-easyrsa.cnf2 directories, 2 files#生成CA机构自签名证书，nopass表示不使用密码[root@centos8 easy-rsa]#./easyrsa build-ca nopass....Common Name (eg: your user, host, or server name) [Easy-RSA CA]: #回车接受默认值CA creation complete and you may now import and sign cert requests.Your new CA certificate file for publishing is at:/etc/openvpn/easy-rsa/pki/ca.crt #新生成自签名的证书文件[root@centos8 easy-rsa]#tree pkipki├── ca.crt #生成的自签名的证书文件├── certs_by_serial├── index.txt #证书列表的索引文件├── index.txt.attr├── issued├── openssl-easyrsa.cnf├── private│ └── ca.key #生成的私钥文件├── renewed│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── reqs├── revoked│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── safessl-easyrsa.cnf└── serial #颁发证书的证书编号12 directories, 7 files[root@vpn-server easy-rsa]# file pki/ca.crtpki/ca.crt: PEM certificate#查看生成CA相关的文件[root@centos8 easy-rsa]#cat pki/serial01[root@centos8 easy-rsa]#ll pki/index.txt-rw------- 1 root root 0 Aug 2 16:42 pki/index.txt[root@centos8 easy-rsa]#cat pki/serial01[root@centos8 easy-rsa]#ll pki/ca.crt pki/private/ca.key-rw------- 1 root root 1204 Aug 2 16:42 pki/ca.crt-rw------- 1 root root 1675 Aug 2 16:42 pki/private/ca.key#查看生成的自签名证书[root@centos8 easy-rsa]#cat pki/ca.crt......#使用 openssl 命令查看[root@centos8 easy-rsa]#openssl x509 -in pki/ca.crt -noout -text....#在windows 中查看[root@vpn-server easy-rsa]# sz pki/ca.crt 2.3.3 准备服务端证书环境2.3.3.1 创建服务端证书申请123456789101112131415161718192021222324252627282930313233343536[root@centos8 ~]#cd /etc/openvpn/easy-rsa[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa#创建服务器证书申请文件，其中server是文件前缀[root@centos8 easy-rsa]#./easyrsa gen-req server nopass......Common Name (eg: your user, host, or server name) [server]: #输入申请者信息，如果接受Common Name的默认值,直接回车Keypair and certificate request completed. Your files are:req: /etc/openvpn/easy-rsa/pki/reqs/server.req #生成证书申请文件key: /etc/openvpn/easy-rsa/pki/private/server.key #生成私钥文件[root@centos8 easy-rsa]#tree pkipki├── ca.crt├── certs_by_serial├── index.txt├── index.txt.attr├── issued├── openssl-easyrsa.cnf├── private│ ├── ca.key│ └── server.key #生成私钥文件├── renewed│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── reqs│ └── server.req #证书申请文件├── revoked│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── safessl-easyrsa.cnf└── serial12 directories, 9 files 2.3.3.2 颁发服务端证书2.3.3.2.1 查看颁发证书命令用法123456789101112131415[root@centos8 ~]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#./easyrsa help signNote: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/varssign-req &lt;type&gt; &lt;filename_base&gt; Sign a certificate request of the defined type. &lt;type&gt; must be a known type such as &#x27;client&#x27;, &#x27;server&#x27;, &#x27;serverClient&#x27;, or &#x27;ca&#x27; (or a user-addedtype.) #证书申请文件必须存在于reqs目录,以 .req 结尾的文件 This request file must exist in the reqs/ dir and have a .req file extension. See import-req below for importing reqs from other sources. #根据提交的申请颁发证书./easyrsa sign-req &lt;type&gt; &lt;filename_base&gt; #type 证书类型 client|server|serverClient|ca#filename_base 申请文件路径，必须是在reqs目录以 .req结尾 2.3.3.2.2 颁发服务端证书1234567891011#将上面server.req的申请,颁发server类型的证书[root@centos8 ~]#cd /etc/openvpn/easy-rsa#第一个server表示证书的类型,第二个server表示请求文件名的前缀[root@centos8 easy-rsa]#./easyrsa sign server server........subject=commonName = server #申请机构相关信息Request subject, to be signed as a server certificate for 3650 days: #可以看到vars文件指定的有效期Confirm request details: yes #输入yes回车Certificate created at: /etc/openvpn/easy-rsa/pki/issued/server.crt #生成服务器证书文件 2.3.3.2.3 验证结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@centos8 ~]#cd /etc/openvpn/easy-rsa[root@centos8 easy-rsa]#tree pkipki├── ca.crt├── certs_by_serial│ └── EDAEBAB8D65066D307AE58ADC1A56682.pem #生成的服务器证书文件├── index.txt├── index.txt.attr├── index.txt.attr.old├── index.txt.old├── issued│ └── server.crt #生成的服务器证书文件├── openssl-easyrsa.cnf├── private│ ├── ca.key│ └── server.key├── renewed│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── reqs│ └── server.req├── revoked│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── safessl-easyrsa.cnf├── serial└── serial.old12 directories, 14 files[root@centos8 easy-rsa]#diff pki/certs_by_serial/EDAEBAB8D65066D307AE58ADC1A56682.pem pki/issued/server.crt[root@centos8 easy-rsa]#ll !*ll pki/certs_by_serial/EDAEBAB8D65066D307AE58ADC1A56682.pem pki/issued/server.crt-rw------- 1 root root 4608 Aug 2 17:19pki/certs_by_serial/EDAEBAB8D65066D307AE58ADC1A56682.pem-rw------- 1 root root 4608 Aug 2 17:19 pki/issued/server.crt[root@centos8 easy-rsa]#cat pki/issued/server.crt....#查看证书相关文件[root@centos8 easy-rsa]#cat pki/serialEDAEBAB8D65066D307AE58ADC1A56683[root@centos8 easy-rsa]#cat pki/index.txtV 201031091943Z EDAEBAB8D65066D307AE58ADC1A56682 unknown /CN=server[root@centos8 easy-rsa]#cat pki/serial.oldedaebab8d65066d307ae58adc1a56682#用 openssl 命令查看[root@vpn-server easy-rsa]# openssl x509 -in pki/issued/server.crt -noout -text#在windows 中查看[root@vpn-server easy-rsa]# sz pki/issued/server.crt 2.3.4 创建 Diffie-Hellman 密钥2.3.4.1 Diffie-Hellman 算法说明123wiki参考链接:https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchangeDiffie-Hellman 密钥交换方法是迪菲（Whitefield Diffie）和赫尔曼（Martin Hellman）在1976年公布的一种秘钥交换算法，它是一种建立秘钥的方法，而不是加密方法，所以秘钥必须和其他一种加密算法结合使用。这种密钥交换技术的目的在于使两个用户安全地交换一个密钥，用此密钥做为对称密钥来加密后续的报文传输，在隧道两端设置的Diffie-Helman组必须相同 2.3.4.2 创建 Diffie-Hellman 密钥123456789101112131415161718192021[root@centos8 ~]#cd /etc/openvpn/easy-rsa[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa#方法1[root@centos8 easy-rsa]#./easyrsa gen-dh................+..................................+......................................................++*++*++*++*.......#需要等待一会儿DH parameters of size 2048 created at /etc/openvpn/easy-rsa-sever/3/pki/dh.pem#查看生成的文件，加密的时候用[root@centos8 easy-rsa]#ll pki/dh.pem-rw------- 1 root root 424 Aug 2 17:41 pki/dh.pem[root@centos8 easy-rsa]#cat pki/dh.pem ....#方法2[root@centos8 ~]#openssl dhparam -out /etc/openvpn/dh2048.pem 2048[root@centos8 ~]#ll /etc/openvpn/dh2048.pem-rw-r--r-- 1 root root 424 Aug 3 20:50 /etc/openvpn/dh2048.pem 2.3.5 准备客户端证书环境2.3.5.1 修改客户端证书有效期上面服务端证书配置完成，下面是配置客户端证书，客户端证书交由具体使用者作为登录 VPN 服务器的凭证，有效期不能太长 123456789[root@centos8 ~]#cd /etc/openvpn//easy-rsa/[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/#建议修改给客户端颁发证书的有效期,可适当减少,比如:90天[root@centos8 ~]#vim /etc/openvpn/easy-rsa/vars#set_var EASYRSA_CERT_EXPIRE 825#将上面行修改为下面set_var EASYRSA_CERT_EXPIRE 90 2.3.5.2 创建客户端证书申请12345678910111213141516171819202122232425262728293031323334[root@centos8 ~]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/#生成客户端用户的证书申请,wangxiaochun表示客户端[root@centos8 easy-rsa]#./easyrsa gen-req wangxiaochun nopass......Common Name (eg: your user, host, or server name) [wangxiaochun]: #接受默认值,直接回车Keypair and certificate request completed. Your files are:req: /etc/openvpn/easy-rsa/pki/reqs/wangxiaochun.req #证书申请文件key: /etc/openvpn/easy-rsa/pki/private/wangxiaochun.key #私钥#生成两个新文件[root@centos8 easy-rsa]#tree.├── easyrsa├── openssl-easyrsa.cnf├── pki│ ├── openssl-easyrsa.cnf│ ├── private│ │ └── wangxiaochun.key #私钥文件│ ├── reqs│ │ └── wangxiaochun.req #证书申请文件│ └── safessl-easyrsa.cnf├── vars└── x509-types ├── ca ├── client ├── code-signing ├── COMMON ├── email ├── kdc ├── server └── serverClient 2.3.5.3 颁发客户端证书12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[root@centos8 ~]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/#颁发客户端证书[root@centos8 easy-rsa]#./easyrsa sign client wangxiaochun.....Confirm request details: yes #输入yes后回车Certificate created at: /etc/openvpn/easy-rsa/pki/issued/wangxiaochun.crt #证书文件[root@centos8 easy-rsa]#tree pkipki├── ca.crt├── certs_by_serial│ ├── 5FE114ACC4FE6AB89D17E1B0EECF2B78.pem│ └── EDAEBAB8D65066D307AE58ADC1A56682.pem├── dh.pem├── index.txt├── index.txt.attr├── index.txt.attr.old├── index.txt.old├── issued│ ├── server.crt│ └── wangxiaochun.crt #生成客户端证书├── openssl-easyrsa.cnf├── private│ ├── ca.key│ └── server.key│ └── wangxiaochun.key├── renewed│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── reqs│ ├── server.req│ └── wangxiaochun.req├── revoked│ ├── certs_by_serial│ ├── private_by_serial│ └── reqs_by_serial├── safessl-easyrsa.cnf├── serial└── serial.old[root@centos8 easy-rsa]#cat pki/index.txtV 201031091943Z EDAEBAB8D65066D307AE58ADC1A56682 unknown /CN=serverV 201031153815Z 5FE114ACC4FE6AB89D17E1B0EECF2B78 unknown /CN=wangxiaochun[root@centos8 easy-rsa]#ll pki/issued/total 16-rw------- 1 root root 4608 Aug 2 17:19 server.crt-rw------- 1 root root 4506 Aug 2 23:38 wangxiaochun.crt[root@centos8 easy-rsa]#ll pki/certs_by_serial/total 16-rw------- 1 root root 4506 Aug 2 23:38 5FE114ACC4FE6AB89D17E1B0EECF2B78.pem-rw------- 1 root root 4608 Aug 2 17:19 EDAEBAB8D65066D307AE58ADC1A56682.pem[root@centos8 easy-rsa]#cat pki/issued/wangxiaochun.crt#openssl 命令查看[root@vpn-server easy-rsa]#openssl x509 -in pki/issued/wangxiaochun.crt -noout -text [root@vpn-server easy-rsa]#SZ pki/issued/wangxiaochun.crt 如果需要颁发的客户端证书较多,可以使用下面脚本实现客户端证书的批量颁发 客户端证书自动颁发脚本 1234567891011[root@centos8 ~]#cat openvpn-user-crt.sh#!/bin/bashread -p &quot;请输入用户的姓名拼音(如:$&#123;NAME&#125;): &quot; NAMEcd /etc/openvpn/easy-rsa/./easyrsa gen-req $&#123;NAME&#125; nopass &lt;&lt;EOFEOF./easyrsa sign client $&#123;NAME&#125; &lt;&lt;EOFyesEOF 2.3.6 将CA和服务器证书相关文件复制到服务器相应的目录1234567891011[root@centos8 ~]#cp /etc/openvpn/easy-rsa/pki/ca.crt /etc/openvpn/server/ [root@centos8 ~]#cp /etc/openvpn/easy-rsa/pki/issued/server.crt /etc/openvpn/server[root@centos8 ~]#cp /etc/openvpn/easy-rsa/pki/private/server.key /etc/openvpn/server [root@centos8 ~]#cp /etc/openvpn/easy-rsa/pki/dh.pem /etc/openvpn/server[root@centos8 ~]#ll /etc/openvpn/servertotal 20-rw------- 1 root root 1204 Aug 3 20:34 ca.crt-rw------- 1 root root 424 Aug 3 20:35 dh.pem-rw------- 1 root root 4608 Aug 3 20:34 server.crt-rw------- 1 root root 1704 Aug 3 20:35 server.key 2.3.7 将客户端私钥与证书相关文件复制到服务器相关的目录每个客户端要有单独的文件夹 123456789101112131415[root@centos8 ~]#mkdir /etc/openvpn/client/wangxiaochun/[root@centos8 ~]#find /etc/openvpn/easy-rsa -name &quot;wangxiaochun.key&quot; -o -name &quot;wangxiaochun.crt&quot; -o -name ca.crt/etc/openvpn/easy-rsa/pki/private/wangxiaochun.key/etc/openvpn/easy-rsa/pki/issued/wangxiaochun.crt/etc/openvpn/easy-rsa/pki/ca.crt[root@centos8 ~]#find /etc/openvpn/easy-rsa \\( -name &quot;wangxiaochun.key&quot; -o -name &quot;wangxiaochun.crt&quot; -o -name ca.crt \\) -exec cp &#123;&#125; /etc/openvpn/client/wangxiaochun/ \\;#以后这个目录要打包给客户本人[root@centos8 ~]#ll /etc/openvpn/client/wangxiaochun/total 16-rw------- 1 root root 1204 Aug 3 21:05 ca.crt-rw------- 1 root root 4506 Aug 3 21:05 wangxiaochun.crt-rw------- 1 root root 1704 Aug 3 21:05 wangxiaochun.key 2.4 配置 OpenVPN 服务器并启动服务2.4.1 服务器端配置文件说明配置文件默认不存在，需要手动创建 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#生成服务器配置文件[root@centos8 ~]#cp /usr/share/doc/openvpn/sample/sample-config-files/server.conf /etc/openvpn/#服务器配置文件server.conf文件中以#或;开头的行都为注释[root@centos8 ~]#grep -Ev &quot;^#|^$&quot; /etc/openvpn/server.conf;local a.b.c.d #本机监听IP,默认为本机所有IPport 1194 #端口;proto tcp #协议,生产推荐使用TCPproto udp #默认协议udp;dev tap #创建以太网隧道设备，tap设备实现以太网帧通过Openvpn隧道，可提供非IP协议如IPX和AppleTalk等的支持，tap等当于一个以太网设备，它操作第二层数据包如以太网数据帧。dev tun #创建IP路由隧道，生产推存使用tun.互联网使用tun,一个tun设备大多时候被用于基于IP协议的通讯。tun模拟了网络层设备，操作第三层数据包比如IP数据封包。;dev-node MyTap #TAP-Win32的设备驱动。非windows系统不需要ca ca.crt #ca证书文件cert server.crt #服务器证书文件key server.key #服务器私钥文件dh dh2048.pem #dh参数文件;topology subnetserver 10.8.0.0 255.255.255.0 #客户端连接后自动分配的虚拟IP网段，默认会给服务器分配此网段的第一个IP将做为客户端的网关,注意不要和内网网段相同ifconfig-pool-persist ipp.txt #记录客户端和虚拟ip地址分配的文件;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100 #配置网桥模式，无需配置,建议注释;server-bridge;push &quot;route 192.168.10.0 255.255.255.0&quot; #推送给客户端的到达服务器后面网段的静态路由，网关是服务器地址10.8.0.1;push &quot;route 192.168.10.100 255.255.255.255&quot; #用255.255.255.255可实现只能访问内网单个主机的功能,比如:jumpserver;push &quot;route 192.168.20.0 255.255.255.0&quot; #推送路由信息到客户端，以允许客户端能够连接到服务器背后的其它私有网络;client-config-dir ccd #为特定客户端添加路由信息，此路由是客户端后面的网段而非服务端的网段，无需设置;route 192.168.40.128 255.255.255.248;client-config-dir ccd ;route 10.9.0.0 255.255.255.252;learn-address ./script #指定外部脚本文件，实现创建不同组的iptables规则，无需配置;push &quot;redirect-gateway def1 bypass-dhcp&quot; #启用此配置后客户端所有流量都将通过VPN服务器进行转发，因此生产一般无需配置此项;push &quot;dhcp-option DNS 208.67.222.222&quot; #推送DNS服务器地址，无需配置;push &quot;dhcp-option DNS 208.67.220.220&quot;;client-to-client #允许不同的客户端直接通信,不安全,生产环境一般无需配置;duplicate-cn #多个用户共用一个证书，一般用于测试环境，生产环境建议一个用户一个证书,无需开启keepalive 10 120 #设置服务端活动的检测的间隔和超时时间，每隔10秒ping一次，120秒没有回应则认为已经断线tls-auth ta.key 0 #访止DoS等攻击的安全增强配置,服务器和每个客户端都需要拥有此密钥文件。第二个参数在服务器端为0，客户端为1cipher AES-256-CBC #加密算法;compress lz4-v2 #启用Openvpn2.4.X新版压缩算法;push &quot;compress lz4-v2&quot; #推送客户端使用新版压缩算法,和下面的comp-lzo不要同时使用;comp-lzo #旧户端兼容的压缩配置，需要客户端配置开启压缩,openvpn2.4.X等新版可以不用开启;max-clients 100 #最多支持的客户端数量;user nobody #指定openvpn服务的用户;group nobody #指定openvpn服务的组persist-key #重启服务时默认会重新读取key文件，开启此配置后保持使用第一次的key文件,生产环境无需开启persist-tun #Don’t close and reopen TUN/TAP device or run up/down scripts across SIGUSR1 or --ping-restart restarts,生产环境建议无需开启status openvpn-status.log #服务器状态记录文件，每分钟记录一次相关信息;log openvpn.log #第一种日志记录方式,并指定日志路径，log会在openvpn启动的时候清空日志文件,不建议使用;log-append openvpn.log #第二种日志记录方式,并指定日志路径，重启openvpn后在之前的日志后面追加新的日志,生产环境建议使用verb 3 #设置日志级别，0-9，级别越高记录的内容越详细,0 表示静默运行，只记录致命错误,4 表示合理的常规用法,5 和 6 可以帮助调试连接错误。9 表示极度冗余，输出非常详细的日志信息;mute 20 #对相同类别的信息只记录前20条到日志文件中explicit-exit-notify 1 #当服务端重启后通知客户端自动重新连接服务器，此项配置仅能用于udp模式，tcp模式无需配置即能实现重新连接功能,且开启此项后tcp配置后将导致openvpn服务无法启动,所以tcp时必须不能开启此项script-security 3 #允许使用自定义脚本auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env #指定自定义脚本路径username-as-common-name #开启用户密码验证client-cert-not-required #只支持用户和密码方式验证,不支持证书,无此配置表示需要证书和用户密码多种验证 2.4.2 修改服务器端配置文件12345678910111213141516171819202122[root@centos8 ~]#vim /etc/openvpn/server.conf[root@centos8 ~]#grep &#x27;^[a-Z].*&#x27; /etc/openvpn/server.confport 1194proto tcpdev tunca /etc/openvpn/server/ca.crtcert /etc/openvpn/server/server.crtkey /etc/openvpn/server/server.key # This file should be kept secretdh /etc/openvpn/server/dh.pemserver 10.8.0.0 255.255.255.0push &quot;route 172.30.0.0 255.255.255.0&quot;keepalive 10 120cipher AES-256-CBCcompress lz4-v2push &quot;compress lz4-v2&quot;max-clients 2048user openvpngroup openvpnstatus /var/log/openvpn/openvpn-status.log log-append /var/log/openvpn/openvpn.logverb 3mute 20 2.4.3 准备服务器日志相关目录123456789#安装包时,自动创建相关用户和组openvpn[root@centos8 ~]#getent passwd openvpnopenvpn:x:993:990:OpenVPN:/etc/openvpn:/sbin/nologin[root@centos8 ~]#mkdir /var/log/openvpn[root@centos8 ~]#chown openvpn.openvpn /var/log/openvpn[root@centos8 ~]#ll -d /var/log/openvpndrwxr-xr-x 2 openvpn openvpn 6 Aug 3 23:07 /var/log/openvpn 2.4.4 启动 OpenVPN 服务2.4.4.1 准备 OpenVPN 服务的service文件此处仅限于 centos8 系列，因为 CentOS8 中 openvpn 服务脚本不可用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#软件包中自带service 文件[root@centos7 ~]#rpm -ql openvpn|grep systemd/usr/lib/systemd/system/openvpn-client@.service/usr/lib/systemd/system/openvpn-server@.service/usr/lib/systemd/system/openvpn@.service/usr/share/doc/openvpn-2.4.9/README.systemd#但上述文件不能作为服务启动文件，无法使用[root@openvpn-server openvpn]# systemctl start openvpn-server@serviceJob for openvpn-server@service.service failed because the control process exited with error code.See &quot;systemctl status openvpn-server@service.service&quot; and &quot;journalctl -xe&quot; for details.#centos7中的相关文件[root@centos7 ~]# rpm -ql openvpn | grep service/usr/lib/systemd/system/openvpn-client@.service/usr/lib/systemd/system/openvpn-server@.service/usr/lib/systemd/system/openvpn@.service#CentOS8 缺失unit文件,从CentOS7复制文件[root@centos8 ~]#rpm -ql openvpn|grep systemd/usr/lib/systemd/system/openvpn-client@.service/usr/lib/systemd/system/openvpn-server@.service/usr/share/doc/openvpn/README.systemd[root@centos7 ~]#cat /usr/lib/systemd/system/openvpn@.service[Unit]Description=OpenVPN Robust And Highly Flexible Tunneling Application On %IAfter=network.target[Service]Type=notifyPrivateTmp=trueExecStart=/usr/sbin/openvpn --cd /etc/openvpn/ --config %i.conf[Install]WantedBy=multi-user.target[root@centos7 ~]#scp /lib/systemd/system/openvpn@.service 10.0.0.8:/lib/systemd/system/#启动OpenVPN服务,注意service名称和文件名不一致[root@centos8 openvpn]#systemctl daemon-reload#如果初始状态起不来，enable 一下[root@centos8 openvpn]#systemctl enable --now openvpn@server[root@centos8 openvpn]#systemctl is-active openvpn@serveractive#重启一下[root@centos8 openvpn]#systemctl restart openvpn@server 2.4.4.2 查看服务状态1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@centos8 openvpn]#systemctl status openvpn@server#查看端口，监听了TCP的1194#在公有云上要在安全组中放行1194端口，如果有防火墙，也要放行1194[root@centos8 ~]#ss -ntlpState Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 32 0.0.0.0:1194 0.0.0.0:* users:((&quot;openvpn&quot;,pid=7340,fd=9))LISTEN 0 128 0.0.0.0:22 0.0.0.0:* users:((&quot;sshd&quot;,pid=761,fd=4)) LISTEN 0 100 127.0.0.1:25 0.0.0.0:* users:((&quot;master&quot;,pid=1093,fd=16))LISTEN 0 128 [::]:22 [::]:* users:((&quot;sshd&quot;,pid=761,fd=6)) LISTEN 0 100 [::1]:25 [::]:* users:((&quot;master&quot;,pid=1093,fd=17))#查看日志[root@centos8 ~]#cat /var/log/openvpn/openvpn.log#查看网卡，多了一个tun0 设备，这是一个点对点的设备[root@centos8 ~]#ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN groupdefault qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UPgroup default qlen 1000 link/ether 00:0c:29:8a:51:21 brd ff:ff:ff:ff:ff:ff inet 10.0.0.8/24 brd 10.0.0.255 scope global noprefixroute eth0 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe8a:5121/64 scope link valid_lft forever preferred_lft forever3: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel stateUNKNOWN group default qlen 100 link/none inet 10.8.0.1 peer 10.8.0.2/32 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::c8db:a8ca:b492:a3e0/64 scope link stable-privacy valid_lft forever preferred_lft forever [root@centos8 ~]#route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.30.0.253 0.0.0.0 UG 100 0 0 eth010.8.0.0 10.8.0.2 255.255.255.0 UG 0 0 0 tun010.8.0.2 0.0.0.0 255.255.255.255 UH 0 0 0 tun0172.30.0.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 验证tun网卡设备： 2.5 准备 OpenVPN 客户端配置文件2.5.1 客户端默认范例配置文件说明1234567891011121314151617181920212223[root@centos8 ~]#grep &#x27;^[[:alpha:]].*&#x27; /usr/share/doc/openvpn/sample/sample-config-files/client.confclient #指明客户端dev tun #指定和服务端一致的接口类型proto udp #指定和服务端一致的协议类型remote my-server-1 1194 #服务器端的ip或FQDN及端口;remote-random #从主机列表中随机选择服务端主机进行连接resolv-retry infinite #指定服务器端FQDN而非IP时，当客户端重新连接后会重新解FQDN对应的IPnobind #客户端不绑定监听端口，随机打开端口连接到服务端的端口;user nobody #属主;group nobody #属组persist-key #重新连接时使用原来加载的keypersist-tun #重新连接时使用原来加载的隧道;http-proxy-retry #代理连接失败是否重试;http-proxy [proxy server] [proxy port #] #是否启用客户端代理，如果启此，此处填写代理服务器IP和端口;mute-replay-warnings #取消重复数据包告警ca ca.crt #CA机构证书cert client.crt #客户端证书key client.key #客户端证书私钥remote-cert-tls server #使用服务器证书校验方式tls-auth ta.key 1 #安全加强cipher AES-256-CBC #加密算法verb 3 #日志级别;mute 20 #同类型日志只记录前20条 2.5.2 生成客户端用户的配置文件12345678910111213141516171819202122#生成客户端文件,文件后缀必须为.ovpn[root@centos8 ~]#grep &#x27;^[[:alpha:]].*&#x27; /usr/share/doc/openvpn/sample/sample-config-files/client.conf &gt; /etc/openvpn/client/wangxiaochun/client.ovpn#修改配置文件,内容如下[root@centos8 ~]#vim /etc/openvpn/client/wangxiaochun/client.ovpn[root@centos8 ~]#cat /etc/openvpn/client/wangxiaochun/client.ovpnclientdev tunproto tcpremote openvpn.wangxiaochun.com 1194 #生产中为OpenVPN服务器的FQDN或者公网IP，建议写FQDNresolv-retry infinitenobind#persist-key#persist-tunca ca.crtcert wangxiaochun.crt key wangxiaochun.keyremote-cert-tls server#tls-auth ta.key 1cipher AES-256-CBCverb 3 #此值不能随意指定,否则无法通信compress lz4-v2 #此项在OpenVPN2.4.X版本使用,需要和服务器端保持一致,如不指定,默认使用comp-lz压缩 remote 项对应的服务端IP可以写成域名，将域名解析到对应的IP即可，这样即使以后服务端IP发生了变化，也只需要修改域名解析，而不用修改此处的配置文件。 2.6 实现 OpenVPN 客户端2.6.1 配置部署Windows 的 OpenVPN 客户端2.6.1.1 Windows 安装 OpenVPN 客户端官方客户端下载地址：https://openvpn.net/community-downloads/ 2.6.1.2 Windows 客户端配置准备保存证书到openvpn 客户端安装目录 123456789101112131415161718192021222324#在服务器打包证书并下载发送给windows客户端[root@centos8 ~]#cd /etc/openvpn/client/wangxiaochun/[root@centos8 wangxiaochun]#pwd/etc/openvpn/client/wangxiaochun[root@centos8 wangxiaochun]#tar cf wangxiaochun.tar ./tar: ./wangxiaochun.tar: file is the archive; not dumped[root@centos8 wangxiaochun]#lltotal 40-rw------- 1 root root 1204 Aug 3 21:05 ca.crt-rw-r--r-- 1 root root 231 Aug 3 23:31 client.ovpn-rw------- 1 root root 4506 Aug 3 21:05 wangxiaochun.crt-rw------- 1 root root 1704 Aug 3 21:05 wangxiaochun.key-rw-r--r-- 1 root root 20480 Aug 4 10:48 wangxiaochun.tar[root@centos8 wangxiaochun]#tar tf wangxiaochun.tar././wangxiaochun.key./wangxiaochun.crt./ca.crt./client.ovpn[root@centos8 wangxiaochun]#sz wangxiaochun.tar 放置到windows客户端默认安装目录下 C:\\Program Files\\OpenVPN\\config 目录 2.6.1.3 Windows 客户端建立OpenVPN连接在windows 中打开 OpenVPN GUI 工具 稍等一会儿,在状态栏显示以下图标,右键点连接 2.6.1.4 Windows 客户端验证通信2.6.1.4.1 在Windows 客户端测试访问OpenVPN后端服务器后端服务器显示是来自于OpenVPN服务器的连接 2.6.1.4.2 观察OpenVPN服务器日志123[root@centos8 ~]#tail /var/log/openvpn/openvpn.log -f -n0[root@centos8 ~]#cat /var/log/openvpn/openvpn-status.log 2.6.1.4.3 验证OpenVPN服务器连接状态123456[root@centos8 ~]#ss -ntState Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 0 52 10.0.0.8:22 10.0.0.1:14009 ESTAB 0 0 10.0.0.8:1194 10.0.0.1:6913 2.6.1.4.4 验证 Windows 客户端的 IP地址可以看到，此时windows主机多出来了一个 10.8.0.6 的IP地址 2.6.1.4.5 验证Windows 客户端的路由表生成了一条指向指向172.30.0.0&#x2F;255.255.0.0的路由记录 该路由记录是从OpenVPN服务端推送过来的 12[root@vpn-server ~]# cat /etc/openvpn/server.conf | grep routepush &quot;route 172.30.0.0 255.255.0.0&quot; 也可以在客户端日志中查到该信息 客户端测试，此时无法ping 通 OpenVPN 服务器后端内网主机，但可以 ping 通 OpenVPN 服务器 1234567891011121314151617181920212223242526272829303132333435363738PS C:\\WINDOWS\\system32&gt; ping 172.30.0.100正在 Ping 172.30.0.100 具有 32 字节的数据:请求超时。请求超时。请求超时。请求超时。172.30.0.100 的 Ping 统计信息: 数据包: 已发送 = 4，已接收 = 0，丢失 = 4 (100% 丢失)， #OpenVPN服务器上的内网IPPS C:\\WINDOWS\\system32&gt; ping 172.30.0.1正在 Ping 172.30.0.66 具有 32 字节的数据:来自 172.30.0.66 的回复: 字节=32 时间=73ms TTL=64来自 172.30.0.66 的回复: 字节=32 时间=12ms TTL=64...... #在服务端抓包，数据都是走 tun0 网卡[root@vpn-server ~]# tcpdump -i tun0 -nn icmptcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on tun0, link-type RAW (Raw IP), snapshot length 262144 bytes12:12:21.703232 IP 10.8.0.6 &gt; 172.30.0.1: ICMP echo request, id 1, seq 296,length 4012:12:21.703255 IP 172.30.0.1 &gt; 10.8.0.6: ICMP echo reply, id 1, seq 296,length 40......#OpenVPN服务器上的虚拟IPPS C:\\WINDOWS\\system32&gt; ping 10.8.0.1正在 Ping 10.8.0.1 具有 32 字节的数据:来自 10.8.0.1 的回复: 字节=32 时间=76ms TTL=64来自 10.8.0.1 的回复: 字节=32 时间=16ms TTL=64......#在服务端抓包，数据都是走 tun0 网卡 [root@vpn-server ~]# tcpdump -i tun0 -nn icmpdropped privs to tcpdumptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on tun0, link-type RAW (Raw IP), capture size 262144 bytes16:55:12.905277 IP 10.8.0.6 &gt; 10.8.0.1: ICMP echo request, id 1, seq 492, length 4016:55:12.905293 IP 10.8.0.1 &gt; 10.8.0.6: ICMP echo reply, id 1, seq 492, length 40 2.6.2 配置部署 Linux 的 OpenVPN 客户端2.6.2.1 客户端安装 openvpn1[root@openvpn-client ~]# yum -y install openvpn 2.6.2.2 下载客户端公钥与私钥以及Ca证书至客户端12[root@openvpn-client ~]# ls /etc/openvpn/ca.crt wangxiaochun.crt wangxiaochun.key 2.6.2.3 客户端有了公钥和私钥后，还需要准备对应的客户端配置文件12345678910111213[root@openvpn-client ~]# cat /etc/openvpn/client.ovpnclient #指定当前VPN是客户端dev tun #使用tun隧道传输协议proto tcp #使用tcp协议传输数据remote OpenVPN服务器地址 1194 #openvpn服务器IP地址端口号resolv-retry infinite #断线自动重新连接，在网络不稳定的情况下非常有用nobind #不绑定本地特定的端口号ca ca.crt #指定CA证书的文件路径cert client.crt #指定当前客户端的证书文件路径key client.key #指定当前客户端的私钥文件路径verb 3 #指定日志文件的记录详细级别，可选0-9，等级越高日志内容越详细persist-key #重启服务时默认会重新读取key文件，开启此配置后保持使用第一次的key文件persist-tun #重新启动VPN时，仍会一直保持tun是up状态 2.6.2.4 启动 openvpn 客户端123456[root@openvpn-client ~]# openvpn --daemon --cd /etc/openvpn --config client.ovpn--log-append /var/log/openvpn.log--daemon #openvpn以daemon方式启动--cd dir #配置文件的目录，openvpn初始化前，先切换到此目录--config file #客户端配置文件的路径--log-append file #日志文件路径，如果文件不存在会自动创建 2.6.3 配置部署 Mac OS 的 OpenVPN 客户端由于官方并没有提供基于Mac OS的OpenVPN的客户端软件,,可以使用第三方 OpenVPN 客户端 参考链接:https://tunnelblick.net/downloads.html 需要科学访问 2.7 实现访问VPN服务器的内网主机当前客户端只能连接OpenVPN服务器,无法连接OpenVPN服务器网络的中其它主机,还需要做如下配置才能实现 2.7.1 OpenVPN服务器打开 ip_forward功能1234#在服务器开启ip_forward转发功能[root@centos8 ~]#echo net.ipv4.ip_forward = 1 &gt;&gt; /etc/sysctl.conf[root@centos8 ~]#sysctl -pnet.ipv4.ip_forward = 1 再次测试 1234567891011121314151617181920212223242526PS C:\\WINDOWS\\system32&gt; ping 172.30.0.100 -t正在 Ping 172.30.0.100 具有 32 字节的数据:请求超时。请求超时。请求超时。请求超时。#server 主机只有来，没有回[root@vpn-server ~]# tcpdump -i tun0 -nn icmptcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on tun0, link-type RAW (Raw IP), snapshot length 262144 bytes14:26:52.358836 IP 10.8.0.6 &gt; 172.30.0.100: ICMP echo request, id 1, seq 315,length 4014:26:57.339303 IP 10.8.0.6 &gt; 172.30.0.100: ICMP echo request, id 1, seq 316,length 4014:27:02.347990 IP 10.8.0.6 &gt; 172.30.0.100: ICMP echo request, id 1, seq 317,length 40#后端主机有来有回[root@node-1 ~]# tcpdump -i eth0 -nn icmpdropped privs to tcpdumptcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes20:46:51.559932 IP 10.8.0.6 &gt; 172.30.0.100: ICMP echo request, id 1, seq 655,length 4020:46:51.559958 IP 172.30.0.100 &gt; 10.8.0.6: ICMP echo reply, id 1, seq 655,length 4020:46:56.550771 IP 10.8.0.6 &gt; 172.30.0.100: ICMP echo request, id 1, seq 656,length 4020:46:56.550796 IP 172.30.0.100 &gt; 10.8.0.6: ICMP echo reply, id 1, seq 656,length 40#因为后端主机没有可达 10.8.0.6 主机的路由 2.7.2 配置实现内网服务器回应外网的请求的路由可以使用下面三种方法实现OpenVPN服务器网段中的其它主机的访问 2.7.2.1 在内网每个主机上添加路由此方式比较麻烦,不太推荐 12#阿里云服务器不支持修改路由[root@rocky8 ~]#route add -net 10.8.0.0/24 gw 172.30.0.1 2.7.2.2 在内网主机指定的路由器上添加路由将10.8.0.0&#x2F;24 网段的网关指向 172.30.0.1，此方法理论可行，但在此处不行，因为我们设置的是点对点网络，后端主机并不是处于 OpenVPN 的客户端角色，所以在配置了网关的基础上，也无法使用 OpenVPN 的相关功能 123456789101112[root@router ~]#route add -net 10.8.0.0/24 gw 172.30.0.1[root@node-2 ~]# route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.30.0.1 0.0.0.0 UG 0 0 0 eth0#但并不能 Ping 通 10.8.0.1[root@node-2 ~]# ping 10.8.0.1PING 10.8.0.1 (10.8.0.1) 56(84) bytes of data.#自然也Ping不通 10.8.0.6 2.7.2.3 在OpenVPN服务器配置 iptables 规则在OpenVPN server 主机上设置 SNAT 转发，将从 10.8.0.0&#x2F;24 网段主机请求的源IP转换成本机IP（172.30.0.1），不然web1和web2就会将数据返回到互联网上 将 OpenVPN server 主机的 iptables 规则设置为开机加载，保证重启后有效 123456789101112131415161718192021222324#添加SNAT规则#方法1[root@centos8 ~]#echo &#x27;iptables -t nat -A POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j MASQUERADE&#x27; &gt;&gt; /etc/rc.d/rc.local#方法2[root@centos8 ~]#echo &#x27;iptables -t nat -A POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to-source 172.30.0.1&#x27; &gt;&gt; /etc/rc.d/rc.local[root@centos8 ~]#chmod +x /etc/rc.d/rc.local[root@centos8 ~]#/etc/rc.d/rc.local[root@centos8 ~]#iptables -vnL -t natChain PREROUTING (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destination Chain INPUT (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * * 10.8.0.0/24 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)pkts bytes target prot opt in out source destination 再次测试 123456789101112PS C:\\WINDOWS\\system32&gt; ping 172.30.0.100 -t正在 Ping 172.30.0.100 具有 32 字节的数据:来自 172.30.0.100 的回复: 字节=32 时间=15ms TTL=63来自 172.30.0.100 的回复: 字节=32 时间=27ms TTL=63来自 172.30.0.100 的回复: 字节=32 时间=17ms TTL=63#后端内网主机收到的请求己经变成了源地址是 172.30.0.1 了[root@node-1 ~]# tcpdump -i eth0 -nn icmptcpdump: verbose output suppressed, use -v[v]... for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), snapshot length 262144 bytes14:31:17.702984 IP 172.30.0.1 &gt; 172.30.0.100: ICMP echo request, id 1, seq 396,length 4014:31:17.703017 IP 172.30.0.100 &gt; 172.30.0.1: ICMP echo reply, id 1, seq 396,length 40 3 OpenVPN 管理OpenVPN的管理功能主要关于安全加强及客户端的证书管理,用户密码验证等 3.1 启用安全增强功能启用防止DoS攻击的安全增强配置 1234567891011121314151617181920212223[root@centos8 ~]#openvpn --genkey --secret /etc/openvpn/server/ta.key[root@centos8 ~]#cat /etc/openvpn/server/ta.key[root@centos8 ~]#ll /etc/openvpn/server/total 24-rw------- 1 root root 1204 Aug 3 20:34 ca.crt-rw------- 1 root root 424 Aug 3 20:35 dh.pem-rw------- 1 root root 4608 Aug 3 20:34 server.crt-rw------- 1 root root 1704 Aug 3 20:35 server.key-rw------- 1 root root 636 Aug 4 15:53 ta.key[root@centos8 ~]#vim /etc/openvpn/server.conf#tls-auth ta.key 0 # This file is secrettls-auth /etc/openvpn/server/ta.key 0 #客户端为1,服务器端为0#重启服务端，客户端会断开连接，小图标会变成黄色[root@centos8 ~]#systemctl restart openvpn@server.service#日志提示出错root@centos8 ~]#tail -n 20 /var/log/openvpn/openvpn.log -fTue Aug 4 15:56:30 2020 TLS Error: cannot locate HMAC in incoming packet from [AF_INET]10.0.0.1:59743Tue Aug 4 15:56:39 2020 TLS Error: cannot locate HMAC in incoming packet from [AF_INET]10.0.0.1:59743Tue Aug 4 15:56:45 2020 TLS Error: cannot locate HMAC in incoming packet from [AF_INET]10.0.0.1:59073 客户端无法直接连接 将ta.key 传到客户端相关目录下 12[root@vpn-server ~]# sz /etc/openvpn/server/ta.key将key文件下载到本地，到到 C:\\Program Files\\OpenVPN\\config 目录下 修改客户端配置文件client.ovpn添加一行 12345#修改客户端配置文件 C:\\Program Files\\OpenVPN\\config\\client.ovpn #新增此行tls-auth ta.key 1#客户端重新连接即可 客户端重新连接成功 3.2 为客户端的私钥和证书设置密码增强安全性新建一个账户magedu，并且设置证书密码，提高证书及登录VPN的安全性。 3.2.1 创建新用户对应的有密码的私钥和证书申请12345678910111213141516171819202122232425262728[root@centos8 easy-rsa]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/ #创建私钥，并生成证书申请文件[root@centos8 easy-rsa]#./easyrsa gen-req mageduNote: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/varsUsing SSL: openssl OpenSSL 1.1.1c FIPS 28 May 2019Generating a RSA private key...............................................................+++++................+++++writing new private key to &#x27;/etc/openvpn/easy-rsa/pki/easy-rsa-35371.iJfHbs/tmp.9lGUMy&#x27;Enter PEM pass phrase: #输入两遍密码Verifying - Enter PEM pass phrase:#输入两遍密码-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Common Name (eg: your user, host, or server name) [magedu]: #接受默认值,直接回车Keypair and certificate request completed. Your files are:req: /etc/openvpn/easy-rsa/pki/reqs/magedu.req #证书申请文件key: /etc/openvpn/easy-rsa/pki/private/magedu.key #私钥 3.2.2 给新用户颁发用户证书1234567891011121314151617181920[root@centos8 easy-rsa]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#pwd/etc/openvpn/easy-rsa/#确保证书有效期是合理值[root@centos8 easy-rsa]#grep EASYRSA_CERT_EXPIRE varsset_var EASYRSA_CERT_EXPIRE 90#颁发证书[root@centos8 easy-rsa]#./easyrsa sign client magedu....Confirm request details: yes #输入yesCertificate is to be certified until Nov 2 08:30:43 2020 GMT (90 days) #有效期Certificate created at: /etc/openvpn/easy-rsa/pki/issued/magedu.crt#查看证书索引[root@centos8 ~]#cat /etc/openvpn/easy-rsa/pki/index.txtV 201031091943Z EDAEBAB8D65066D307AE58ADC1A56682 unknown /CN=serverV 201031153815Z 5FE114ACC4FE6AB89D17E1B0EECF2B78 unknown /CN=wangxiaochunV 201102083043Z C971227DA77824C8ACB7D655D09D4081 unknown /CN=magedu 3.2.3 将用户的证书相关文件放在指定的目录中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#配置客户端[root@centos8 ~]#mkdir /etc/openvpn/client/magedu [root@centos8 ~]#cp /etc/openvpn/easy-rsa/pki/issued/magedu.crt /etc/openvpn/client/magedu #用户证书[root@centos8 ~]#cp /etc/openvpn/easy-rsa/pki/private/magedu.key /etc/openvpn/client/magedu #用户私钥[root@centos8 ~]#cp /etc/openvpn/server/&#123;ca.crt,ta.key&#125; /etc/openvpn/client/magedu/ #CA证书[root@centos8 ~]#cp /etc/openvpn/client/wangxiaochun/client.ovpn /etc/openvpn/client/magedu/[root@centos8 ~]#ll /etc/openvpn/client/magedu/total 28-rw------- 1 root root 1204 Aug 4 16:41 ca.crt-rw-r--r-- 1 root root 231 Aug 4 16:44 client.ovpn-rw------- 1 root root 4492 Aug 4 16:38 magedu.crt-rw------- 1 root root 1854 Aug 4 16:38 magedu.key-rw------- 1 root root 636 Aug 4 16:41 ta.key[root@centos8 ~]#cd /etc/openvpn/client/magedu/#根据服务器端修改下面配置,需要和服务器同步[root@centos8 magedu]#vim client.ovpn[root@centos8 magedu]#cat client.ovpnclientdev tunproto tcpremote OpenVPN服务器地址 1194resolv-retry infinitenobind#persist-key#persist-tunca ca.crtcert magedu.crtkey magedu.keyremote-cert-tls servertls-auth ta.key 1cipher AES-256-CBCverb 3compress lz4-v2#打包，并下载到 windows 主机[root@centos8 magedu]#tar cf magedu.tar *[root@centos8 magedu]#tar tvf magedu.tar-rw------- root/root 1204 2020-08-04 16:41 ca.crt-rw-r--r-- root/root 225 2020-08-04 16:47 client.ovpn-rw------- root/root 4492 2020-08-04 16:38 magedu.crt-rw------- root/root 1854 2020-08-04 16:38 magedu.key-rw------- root/root 636 2020-08-04 16:41 ta.key#打包加密[root@centos8 magedu]#zip -e wang.zip * 3.2.4 将相关文件传给客户端主机相应目录1[root@centos8 magedu]#sz magedu.tar 放置到windows客户端的 C:\\Program Files\\OpenVPN\\config 目录下，再次使用客户端连接，要求输入密码 3.2.5 Windows 客户端重新连接 查看服务端日志，这次是 magedu 连接的 12345[root@centos8 ~]#tail -n0 /var/log/openvpn/openvpn.log -fSun Feb 26 13:51:45 2023 115.171.60.47:28462 [magedu] Peer Connection Initiated with [AF_INET]115.171.60.47:28462Sun Feb 26 13:51:45 2023 magedu/115.171.60.47:28462 MULTI_sva: pool returned IPv4=10.8.0.10, IPv6=(Not enabled)Sun Feb 26 13:51:45 2023 magedu/115.171.60.47:28462 MULTI: Learn: 10.8.0.10 -&gt;magedu/115.171.60.47:28462..... 客户端测试 3.3 账户证书管理对新员工的入职和老员工的离职，以及时间的推移，会涉及到证书的创建和吊销 3.3.1 证书自动过期前面颁发的证书都有有一定的有效期,过期后就需要重新颁发新证书 123456789#过期时间由以下设置决定[root@centos8 ~]#grep EASYRSA_CERT_EXPIRE /etc/openvpn/easy-rsa/varsset_var EASYRSA_CERT_EXPIRE 90#在服务端证书索引文件中查看每个证书的过期时间，该时间是UTC时间[root@centos8 ~]#cat /etc/openvpn/easy-rsa/pki/index.txtV 201031091943Z EDAEBAB8D65066D307AE58ADC1A56682 unknown /CN=serverV 201031153815Z 5FE114ACC4FE6AB89D17E1B0EECF2B78 unknown /CN=wangxiaochunV 201102083043Z C971227DA77824C8ACB7D655D09D4081 unknown /CN=magedu 如果证书过期,在服务器端可以看到以下日志 12345#让服务器时间改为2年后时间[root@centos8 ~]#date -s &#x27;2 year&#x27;Thu Aug 4 17:41:04 CST 2022#客户端再次连接失败 123456789101112#服务器端日志中会显示用户证书过期[root@centos8 ~]#tail -n0 /var/log/openvpn/openvpn.log -fThu Aug 4 17:42:22 2022 TCP connection established with [AF_INET]10.0.0.1:11324Thu Aug 4 17:42:23 2022 10.0.0.1:11324 TLS: Initial packet from [AF_INET]10.0.0.1:11324, sid=a2957674 874cf1f7Thu Aug 4 17:42:24 2022 10.0.0.1:11324 VERIFY OK: depth=1, CN=Easy-RSA CAThu Aug 4 17:42:24 2022 10.0.0.1:11324 VERIFY ERROR: depth=0, error=certificate has expired: CN=mageduThu Aug 4 17:42:24 2022 10.0.0.1:11324 OpenSSL: error:1417C086:SSL routines:tls_process_client_certificate:certificate verify failedThu Aug 4 17:42:24 2022 10.0.0.1:11324 TLS_ERROR: BIO read tls_read_plaintext errorThu Aug 4 17:42:24 2022 10.0.0.1:11324 TLS Error: TLS object -&gt; incoming plaintext read errorThu Aug 4 17:42:24 2022 10.0.0.1:11324 TLS Error: TLS handshake failedThu Aug 4 17:42:24 2022 10.0.0.1:11324 Fatal TLS error (check_tls_errors_co), restartingThu Aug 4 17:42:24 2022 10.0.0.1:11324 SIGUSR1[soft,tls-error] received,client-instance restarting 3.3.2 证书手动注销3.3.2.1 查看当前证书的有效性12345#证书有效为V,无效为R[root@centos8 ~]#cat /etc/openvpn/easy-rsa/pki/index.txtV 201031091943Z EDAEBAB8D65066D307AE58ADC1A56682 unknown /CN=serverV 201031153815Z 5FE114ACC4FE6AB89D17E1B0EECF2B78 unknown /CN=wangxiaochunV 201102083043Z C971227DA77824C8ACB7D655D09D4081 unknown /CN=magedu 3.3.2.2 吊销指定的用户的证书123456789101112131415161718192021222324[root@centos8 ~]# cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#./easyrsa revoke mageduNote: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/varsUsing SSL: openssl OpenSSL 1.1.1c FIPS 28 May 2019Please confirm you wish to revoke the certificate with the following subject:subject= commonName = mageduType the word &#x27;yes&#x27; to continue, or any other input to abort.Continue with revocation: yesUsing configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-3787.Q2lnW6/tmp.18ZdMORevoking Certificate C971227DA77824C8ACB7D655D09D4081.Data Base UpdatedIMPORTANT!!!Revocation was successful. You must run gen-crl and upload a CRL to yourinfrastructure in order to prevent the revoked cert from being accepted.#查看当前证书的有效性,有效为V,无效为R[root@centos8 easy-rsa]#cat /etc/openvpn/easy-rsa/pki/index.txtV 201031091943Z EDAEBAB8D65066D307AE58ADC1A56682 unknown /CN=serverV 201031153815Z 5FE114ACC4FE6AB89D17E1B0EECF2B78 unknown /CN=wangxiaochunR 201102083043Z 200805123127Z C971227DA77824C8ACB7D655D09D4081 unknown /CN=magedu#当前断开客户端连接,magedu用户仍然能连接成功 3.3.2.3 生成证书吊销列表此时证书虽然被吊销，但客户端还是能使用，还需要在证书吊销列表文件中指定被吊销的证书，并且需要在 OpenVPN 服务端配置文件中指定证书吊销列表文件路径，然后再重启 OpenVPN服务 123456789101112131415#每次吊销证书后都需要更新证书吊销列表文件,并且需要重启OpenVPN服务#更新证书吊销列表文件[root@centos8 easy-rsa]#./easyrsa gen-crlNote: using Easy-RSA configuration from: /etc/openvpn/easy-rsa/varsUsing SSL: openssl OpenSSL 1.1.1c FIPS 28 May 2019Using configuration from /etc/openvpn/easy-rsa/pki/easy-rsa-4323.gUt4WS/tmp.ybAcAUAn updated CRL has been created.CRL file: /etc/openvpn/easy-rsa/pki/crl.pem[root@centos8 easy-rsa]#cat pki/crl.pem#传到windows上,修改文件后缀为crl,双击就可以打开此文件,看到下面显示信息[root@centos8 easy-rsa]#sz /etc/openvpn/easy-rsa/pki/crl.pem 3.3.2.4 将吊销列表文件发布12345678#第一次吊销证时需要编辑配置文件调用吊销证书的文件,后续吊销无需此步#修改服务端配置，指定证书吊销列表文件路径[root@openvpn-server ~]# vim /etc/openvpn/server.confcrl-verify /etc/openvpn/easy-rsa/pki/crl.pem#每次吊销证书后,都需要重新启动才能生效[root@centos8 ~]#systemctl restart openvpn@server.service 3.3.2.5 再次测试连接失败用户端再次连接失败 1234567#观察OpenVPN目志，提示证书被吊销[root@centos8 easy-rsa]#tail -f /var/log/openvpn/openvpn.log -n0Wed Aug 5 21:13:25 2020 TCP connection established with [AF_INET]10.0.0.1:7419Wed Aug 5 21:13:26 2020 10.0.0.1:7419 TLS: Initial packet from [AF_INET]10.0.0.1:7419, sid=169fe75b 63908b79Wed Aug 5 21:13:26 2020 10.0.0.1:7419 WARNING: Failed to stat CRL file, not(re)loading CRL.Wed Aug 5 21:13:26 2020 10.0.0.1:7419 VERIFY ERROR: depth=0, error=certificate revoked: CN=mageduWed Aug 5 21:13:26 2020 10.0.0.1:7419 OpenSSL: error:1417C086:SSL routines:tls_process_client_certificate:certificate verify failed 3.3.3 账户重名证书颁发对于员工重名的情况，可以为新员工指定新的名称，或者删除不再使用的重名用户证书，重新创建新证书 3.3.3.1 手动重新颁发证书123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#删除已被吊销的账户证书[root@centos8 ~]#cd /etc/openvpn/easy-rsa/[root@centos8 easy-rsa]#rm -f pki/private/magedu.key[root@centos8 easy-rsa]#rm -f pki/reqs/magedu.req[root@centos8 easy-rsa]#rm -f /etc/openvpn/client/magedu/*[root@centos8 easy-rsa]#rm -f /etc/openvpn/easy-rsa/pki/reqs/magedu.req[root@centos8 easy-rsa]#rm -f /etc/openvpn/easy-rsa/pki/issued/magedu.crt#删除之前的带R的吊销记录,此为可选项[root@centos8 easy-rsa]#vim /etc/openvpn/easy-rsa/pki/index.txt#重新生成新的账户证书申请和私钥[root@centos8 easy-rsa]#cd /etc/openvpn//easy-rsa/[root@centos8 easy-rsa]#./easyrsa gen-req magedu...Common Name (eg: your user, host, or server name) [magedu]:#直接回车Keypair and certificate request completed. Your files are:req: /etc/openvpn/easy-rsa/pki/reqs/magedu.reqkey: /etc/openvpn/easy-rsa/pki/private/magedu.key#CA颁发证书[root@centos8 easy-rsa]#cd /etc/openvpn/easy-rsa[root@centos8 easy-rsa]#./easyrsa import-req /etc/openvpn/easy-rsa/pki/reqs/magedu.req magedu[root@centos8 easy-rsa]#./easyrsa sign client magedu......Type the word &#x27;yes&#x27; to continue, or any other input to abort.Confirm request details: yes #输入yes......Write out database with 1 new entriesData Base UpdatedCertificate created at: /etc/openvpn/easy-rsa/pki/issued/magedu.crt#生成相关文件[root@centos8 easy-rsa]#cp /etc/openvpn/easy-rsa/pki/issued/magedu.crt/etc/openvpn/client/magedu/[root@centos8 easy-rsa]#cp /etc/openvpn/easy-rsa/pki/private/magedu.key/etc/openvpn/client/magedu/[root@centos8 easy-rsa]#cp /etc/openvpn/server/&#123;ca.crt,ta.key&#125;/etc/openvpn/client/magedu/[root@centos8 easy-rsa]#cp /etc/openvpn/client/wangxiaochun/client.ovpn/etc/openvpn/client/magedu/#修改客户端配置文件[root@centos8 easy-rsa]#vim /etc/openvpn/client/magedu/client.ovpnclientdev tunproto tcpremote 10.0.0.8 1194resolv-retry infinitenobind#persist-key#persist-tunca ca.crtcert magedu.crtkey magedu.keyremote-cert-tls servertls-auth ta.key 1cipher AES-256-CBCverb 3compress lz4-v2[root@centos8 ~]#tree /etc/openvpn/client/magedu/etc/openvpn/client/magedu├── ca.crt├── client.ovpn├── dh.pem├── magedu.crt├── magedu.key└── ta.key0 directories, 6 files#将/etc/openvpn/client/magedu所有文件打包传到客户端使用 3.3.3.2 自动化的证书颁发脚本通过脚本实现自动化的证书颁发 3.3.3.2.1 脚本内容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@centos8 ~]#cat openvpn-user-crt.sh#!/bin/bashOPENVPN_SERVER=openvpn.wangxiaochun.comPASS=123456 #给证书和私钥加密的密码remove_cert () &#123; rm -rf /etc/openvpn/client/$&#123;NAME&#125; find /etc/openvpn/ -name &quot;$NAME.*&quot; -delete&#125;create_cert () &#123; cd /etc/openvpn/easy-rsa ./easyrsa gen-req $&#123;NAME&#125; nopass &lt;&lt;EOFEOF cd /etc/openvpn/easy-rsa ./easyrsa import-req /etc/openvpn/easy-rsa/pki/reqs/$&#123;NAME&#125;.req $&#123;NAME&#125; ./easyrsa sign client $&#123;NAME&#125; &lt;&lt;EOF yes EOF mkdir /etc/openvpn/client/$&#123;NAME&#125; cp /etc/openvpn/easy-rsa/pki/issued/$&#123;NAME&#125;.crt /etc/openvpn/client/$&#123;NAME&#125; cp /etc/openvpn/easy-rsa/pki/private/$&#123;NAME&#125;.key /etc/openvpn/client/$&#123;NAME&#125; cp /etc/openvpn/server/&#123;ca.crt,ta.key&#125; /etc/openvpn/client/$&#123;NAME&#125; cat &gt; /etc/openvpn/client/$&#123;NAME&#125;/client.ovpn &lt;&lt;EOFclientdev tunproto tcpremote $OPENVPN_SERVER 1194resolv-retry infinitenobind#persist-key#persist-tunca ca.crtcert $NAME.crtkey $NAME.keyremote-cert-tls servertls-auth ta.key 1cipher AES-256-CBCverb 3compress lz4-v2EOF echo &quot;证书存放路径:/etc/openvpn/client/$&#123;NAME&#125;,证书文件如下:&quot; echo -e&quot;\\E[1;32m******************************************************************\\E[0m&quot; ls -l /etc/openvpn/client/$&#123;NAME&#125; echo -e&quot;\\E[1;32m******************************************************************\\E[0m&quot; cd /etc/openvpn/client/$&#123;NAME&#125; zip -qP &quot;$PASS&quot; /root/$&#123;NAME&#125;.zip * echo &quot;证书的打包文件已生成: /root/$&#123;NAME&#125;.zip&quot;&#125;read -p &quot;请输入用户的姓名拼音(如:wangxiaochun): &quot; NAMEremove_certcreate_cert 3.3.3.2.2 执行脚本1234567891011121314[root@centos8 ~]#bash openvpn-user-crt.sh请输入用户的姓名拼音(如:): magedu.....证书存放路径:/etc/openvpn/client/magedu,证书文件如下:******************************************************************total 28-rw------- 1 root root 1204 Aug 5 21:50 ca.crt-rw-r--r-- 1 root root 225 Aug 5 21:50 client.ovpn-rw------- 1 root root 424 Aug 5 21:50 dh.pem-rw------- 1 root root 4492 Aug 5 21:50 magedu.crt-rw------- 1 root root 1704 Aug 5 21:50 magedu.key-rw------- 1 root root 636 Aug 5 21:50 ta.key******************************************************************证书的打包文件已生成: /root/magedu.zip [ OK ] 3.3.3.3 在Windows客户端用新证书连接登录 3.4 实现用户密码认证基于证书验证的基础上再加上用户名密码验证可以实现更高的安全性 场景：要给公司10个人颁发证书，太麻烦，只需要颁发一个证书，设置各自的用户名和密码，用的时候输入各自的用户名和密码就行了，到时候有人离职了，也不用吊销证书 3.4.1 修改服务端配置12345[root@centos8 ~]# vim /etc/openvpn/server.conf# 添加三行，实现服务端支持密码认证方式script-security 3 #允许使用自定义脚本auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env #指定自定义脚本路径username-as-common-name #开启用户密码验证 3.4.2 创建自定义脚本官方脚本下载 1http://openvpn.se/files/other/checkpsw.sh 范例: 123456789101112131415161718192021222324252627282930[root@centos8 ~]# vim /etc/openvpn/checkpsw.sh#!/bin/shPASSFILE=&quot;/etc/openvpn/psw-file&quot;#LOG_FILE=&quot;/var/log/openvpn-password.log&quot; #修改此处LOG_FILE=&quot;/var/log/openvpn/openvpn-password.log&quot;TIME_STAMP=`date &quot;+%Y-%m-%d %T&quot;`if [ ! -r &quot;$&#123;PASSFILE&#125;&quot; ]; then echo &quot;$&#123;TIME_STAMP&#125;: Could not open password file \\&quot;$&#123;PASSFILE&#125;\\&quot; for reading.&quot; &gt;&gt; $&#123;LOG_FILE&#125; exit 1fiCORRECT_PASSWORD=`awk &#x27;!/^;/&amp;&amp;!/^#/&amp;&amp;$1==&quot;&#x27;$&#123;username&#125;&#x27;&quot;&#123;print $2;exit&#125;&#x27;$&#123;PASSFILE&#125;`if [ &quot;$&#123;CORRECT_PASSWORD&#125;&quot; = &quot;&quot; ]; then echo &quot;$&#123;TIME_STAMP&#125;: User does not exist: username=\\&quot;$&#123;username&#125;\\&quot;,password=\\&quot;$&#123;password&#125;\\&quot;.&quot; &gt;&gt; $&#123;LOG_FILE&#125; exit 1fiif [ &quot;$&#123;password&#125;&quot; = &quot;$&#123;CORRECT_PASSWORD&#125;&quot; ]; then echo &quot;$&#123;TIME_STAMP&#125;: Successful authentication: username=\\&quot;$&#123;username&#125;\\&quot;.&quot; &gt;&gt; $&#123;LOG_FILE&#125; exit 0fiecho &quot;$&#123;TIME_STAMP&#125;: Incorrect password: username=\\&quot;$&#123;username&#125;\\&quot;,password=\\&quot;$&#123;password&#125;\\&quot;.&quot; &gt;&gt; $&#123;LOG_FILE&#125;exit 1#增加执行权限[root@centos8 ~]# chmod +x /etc/openvpn/checkpsw.sh 3.4.3 创建用户密码文件1234567# 创建用户和密码认证文件,每行是一个用户和密码[root@centos8 ~]# cat &gt; /etc/openvpn/psw-file &lt;&lt;EOFwang 123456test 654321EOF[root@centos8 ~]# systemctl restart openvpn@server 3.4.4 修改客户端配置修改客户端配置文件client.ovpn文件，增加下面一行,使其支持用户名&#x2F;密码与服务器进行身份验证. 123[root@centos8 ~]#vim /etc/openvpn/client/wangxiaochun/client.ovpn#加下面一行,可以支持用户密码认证auth-user-pass 3.4.5 客户端连接验证客户端测试，连接时需要输入用户名密码，输入 user1&#x2F;123456 或 user2&#x2F;654321 都可以连接 查看服务端日志 123456[root@openvpn-server client]# cat /var/log/openvpn/openvpn.logSun Feb 26 23:16:38 2023 115.171.60.47:28289 TLS: Username/Password authentication succeeded for username &#x27;wang&#x27; [CN SET]....[root@openvpn-server client]# cat /var/log/openvpn/openvpn-password.log2023-02-26 23:16:38: Successful authentication: username=&quot;wang&quot;.","categories":[],"tags":[{"name":"OpenVPN","slug":"OpenVPN","permalink":"https://aquapluto.github.io/tags/OpenVPN/"}]}],"categories":[{"name":"CICD","slug":"CICD","permalink":"https://aquapluto.github.io/categories/CICD/"},{"name":"监控","slug":"monitor","permalink":"https://aquapluto.github.io/categories/monitor/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://aquapluto.github.io/categories/kubernetes/"},{"name":"容器","slug":"container","permalink":"https://aquapluto.github.io/categories/container/"},{"name":"Linux","slug":"Linux","permalink":"https://aquapluto.github.io/categories/Linux/"},{"name":"计算机基础","slug":"计算机基础","permalink":"https://aquapluto.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"存储","slug":"存储","permalink":"https://aquapluto.github.io/categories/%E5%AD%98%E5%82%A8/"}],"tags":[{"name":"Ansible","slug":"Ansible","permalink":"https://aquapluto.github.io/tags/Ansible/"},{"name":"Alertmanager","slug":"Alertmanager","permalink":"https://aquapluto.github.io/tags/Alertmanager/"},{"name":"Prometheus","slug":"Prometheus","permalink":"https://aquapluto.github.io/tags/Prometheus/"},{"name":"kubectl","slug":"kubectl","permalink":"https://aquapluto.github.io/tags/kubectl/"},{"name":"CRI","slug":"CRI","permalink":"https://aquapluto.github.io/tags/CRI/"},{"name":"Pod","slug":"Pod","permalink":"https://aquapluto.github.io/tags/Pod/"},{"name":"ConfigMap","slug":"ConfigMap","permalink":"https://aquapluto.github.io/tags/ConfigMap/"},{"name":"Secret","slug":"Secret","permalink":"https://aquapluto.github.io/tags/Secret/"},{"name":"StorageClass","slug":"StorageClass","permalink":"https://aquapluto.github.io/tags/StorageClass/"},{"name":"PV","slug":"PV","permalink":"https://aquapluto.github.io/tags/PV/"},{"name":"PVC","slug":"PVC","permalink":"https://aquapluto.github.io/tags/PVC/"},{"name":"CSI","slug":"CSI","permalink":"https://aquapluto.github.io/tags/CSI/"},{"name":"LocalDNS","slug":"LocalDNS","permalink":"https://aquapluto.github.io/tags/LocalDNS/"},{"name":"CoreDNS","slug":"CoreDNS","permalink":"https://aquapluto.github.io/tags/CoreDNS/"},{"name":"Ingress","slug":"Ingress","permalink":"https://aquapluto.github.io/tags/Ingress/"},{"name":"Service","slug":"Service","permalink":"https://aquapluto.github.io/tags/Service/"},{"name":"Operator","slug":"Operator","permalink":"https://aquapluto.github.io/tags/Operator/"},{"name":"HPA","slug":"HPA","permalink":"https://aquapluto.github.io/tags/HPA/"},{"name":"job-cronjob","slug":"job-cronjob","permalink":"https://aquapluto.github.io/tags/job-cronjob/"},{"name":"Daemonset","slug":"Daemonset","permalink":"https://aquapluto.github.io/tags/Daemonset/"},{"name":"Statefulset","slug":"Statefulset","permalink":"https://aquapluto.github.io/tags/Statefulset/"},{"name":"Deployment","slug":"Deployment","permalink":"https://aquapluto.github.io/tags/Deployment/"},{"name":"k8s部署","slug":"k8s部署","permalink":"https://aquapluto.github.io/tags/k8s%E9%83%A8%E7%BD%B2/"},{"name":"docker","slug":"docker","permalink":"https://aquapluto.github.io/tags/docker/"},{"name":"Grafana","slug":"Grafana","permalink":"https://aquapluto.github.io/tags/Grafana/"},{"name":"linux基础","slug":"linux基础","permalink":"https://aquapluto.github.io/tags/linux%E5%9F%BA%E7%A1%80/"},{"name":"加密和安全","slug":"加密和安全","permalink":"https://aquapluto.github.io/tags/%E5%8A%A0%E5%AF%86%E5%92%8C%E5%AE%89%E5%85%A8/"},{"name":"防火墙","slug":"防火墙","permalink":"https://aquapluto.github.io/tags/%E9%98%B2%E7%81%AB%E5%A2%99/"},{"name":"服务管理","slug":"服务管理","permalink":"https://aquapluto.github.io/tags/%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/"},{"name":"网络管理","slug":"网络管理","permalink":"https://aquapluto.github.io/tags/%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86/"},{"name":"磁盘管理","slug":"磁盘管理","permalink":"https://aquapluto.github.io/tags/%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86/"},{"name":"进程管理","slug":"进程管理","permalink":"https://aquapluto.github.io/tags/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"name":"软件包管理","slug":"软件包管理","permalink":"https://aquapluto.github.io/tags/%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"操作系统","slug":"操作系统","permalink":"https://aquapluto.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"计算机组成原理","slug":"计算机组成原理","permalink":"https://aquapluto.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86/"},{"name":"shell编程","slug":"shell编程","permalink":"https://aquapluto.github.io/tags/shell%E7%BC%96%E7%A8%8B/"},{"name":"系统优化","slug":"系统优化","permalink":"https://aquapluto.github.io/tags/%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96/"},{"name":"NFS","slug":"NFS","permalink":"https://aquapluto.github.io/tags/NFS/"},{"name":"introduce","slug":"introduce","permalink":"https://aquapluto.github.io/tags/introduce/"},{"name":"JumpServer","slug":"JumpServer","permalink":"https://aquapluto.github.io/tags/JumpServer/"},{"name":"OpenVPN","slug":"OpenVPN","permalink":"https://aquapluto.github.io/tags/OpenVPN/"}]}